,title,authors,abstract,document
0,EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association,"['Weiqi Wang', 'Limeng Cui', 'Xin Liu', 'Sreyashi Nag', 'Wenju Xu', 'Chen Luo', 'Sheikh Muhammad Sarwar', 'Yang Li', 'Hansu Gu', 'Hui Liu', 'Changlong Yu', 'Jiaxin Bai', 'Yifan Gao', 'Haiyang Zhang', 'Qi He', 'Shuiwang Ji', 'Yangqiu Song']","Goal-oriented script planning, or the ability to devise coherent sequences of actions toward specific goals, is commonly employed by humans to plan for typical activities. In e-commerce, customers increasingly seek LLM-based assistants to generate scripts and recommend products at each step, thereby facilitating convenient and efficient shopping experiences. However, this capability remains underexplored due to several challenges, including the inability of LLMs to simultaneously conduct script planning and product retrieval, difficulties in matching products caused by semantic discrepancies between planned actions and search queries, and a lack of methods and benchmark data for evaluation. In this paper, we step forward by formally defining the task of E-commerce Script Planning (EcomScript) as three sequential subtasks. We propose a novel framework that enables the scalable generation of product-enriched scripts by associating products with each step based on the semantic similarity between the actions and their purchase intentions. By applying our framework to real-world e-commerce data, we construct the very first large-scale EcomScript dataset, EcomScriptBench, which includes 605,229 scripts sourced from 2.4 million products. Human annotations are then conducted to provide gold labels for a sampled subset, forming an evaluation benchmark. Extensive experiments reveal that current (L)LMs face significant challenges with EcomScript tasks, even after fine-tuning, while injecting product purchase intentions improves their performance.",EcomScriptBench Multi task Benchmark E commerce Script Planning Step wise Intention Driven Product Association Goal oriented script planning ability devise coherent sequences actions specific goals commonly employed humans plan typical activities e commerce customers increasingly seek LLM based assistants generate scripts recommend products step facilitating convenient efficient shopping experiences capability remains underexplored challenges including inability LLMs simultaneously conduct script planning product retrieval difficulties matching products caused semantic discrepancies planned actions search queries lack methods benchmark data evaluation paper step forward formally defining task E commerce Script Planning EcomScript sequential subtasks propose novel framework enables scalable generation product enriched scripts associating products step based semantic similarity actions purchase intentions applying framework real world e commerce data construct large scale EcomScript dataset EcomScriptBench includes 605 229 scripts sourced 2 4 million products Human annotations conducted provide gold labels sampled subset forming evaluation benchmark Extensive experiments reveal current L LMs face significant challenges EcomScript tasks fine tuning injecting product purchase intentions improves performance
1,TAGExplainer: Narrating Graph Explanations for Text-Attributed Graph Learning Models,"['Bo Pan', 'Zhen Xiong', 'Guanchen Wu', 'Zheng Zhang', 'Yifei Zhang', 'Yuntong Hu', 'Liang Zhao']",,TAGExplainer Narrating Graph Explanations Text Attributed Graph Learning Models
2,M-RewardBench: Evaluating Reward Models in Multilingual Settings,"['Srishti Gureja', 'Lester James Validad Miranda', 'Shayekh Bin Islam', 'Rishabh Maheshwary', 'Drishti Sharma', 'Gusti Triandi Winata', 'Nathan Lambert', 'Sebastian Ruder', 'Sara Hooker', 'Marzieh Fadaee']",,M RewardBench Evaluating Reward Models Multilingual Settings
3,ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming,"['Xinwei Yang', 'Zhaofeng Liu', 'Chen Huang', 'Jiashuai Zhang', 'Tong Zhang', 'Yifan Zhang', 'Wenqiang Lei']","While recent research increasingly emphasizes the value of human-LLM collaboration in competitive programming and proposes numerous empirical methods, a comprehensive understanding remains elusive due to the fragmented nature of existing studies and their use of diverse, application-specific human feedback. Thus, our work serves a three-fold purpose: First, we present the first taxonomy of human feedback consolidating the entire programming process, which promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a novel programming dataset specifically designed for human-LLM collaboration, meticulously annotated to enable large-scale simulated human feedback and facilitate costeffective real human interaction studies. Third, we introduce ELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM competitive programming. With ELABORATION, we pinpoint strengthes and weaknesses of existing methods, thereby setting the foundation for future improvement. Our code and dataset are available at https://github.com/SCUNLP/ELABORATION",ELABORATION Comprehensive Benchmark Human LLM Competitive Programming recent research increasingly emphasizes value human LLM collaboration competitive programming proposes numerous empirical methods comprehensive understanding remains elusive fragmented nature existing studies use diverse application specific human feedback work serves fold purpose present taxonomy human feedback consolidating entire programming process promotes fine grained evaluation Second introduce ELABORATIONSET novel programming dataset specifically designed human LLM collaboration meticulously annotated enable large scale simulated human feedback facilitate costeffective real human interaction studies introduce ELABORATION novel benchmark facilitate thorough assessment human LLM competitive programming ELABORATION pinpoint strengthes weaknesses existing methods setting foundation future improvement code dataset available https github com SCUNLP ELABORATION
4,The Impossibility of Fair LLMs,"['Jacy Reese Anthis', 'Kristian Lum', 'Michael Ekstrand', 'Avi Feller', 'Chenhao Tan']","The rise of general-purpose artificial intelligence (AI) systems, particularly large language models (LLMs), has raised pressing moral questions about how to reduce bias and ensure fairness at scale. Researchers have documented a sort of ""bias"" in the significant correlations between demographics (e.g., race, gender) in LLM prompts and responses, but it remains unclear how LLM fairness could be evaluated with more rigorous definitions, such as group fairness or fair representations. We analyze a variety of technical fairness frameworks and find inherent challenges in each that make the development of a fair LLM intractable. We show that each framework either does not logically extend to the general-purpose AI context or is infeasible in practice, primarily due to the large amounts of unstructured training data and the many potential combinations of human populations, use cases, and sensitive attributes. These inherent challenges would persist for general-purpose AI, including LLMs, even if empirical challenges, such as limited participatory input and limited measurement methods, were overcome. Nonetheless, fairness will remain an important type of model evaluation, and there are still promising research directions, particularly the development of standards for the responsibility of LLM developers, context-specific evaluations, and methods of iterative, participatory, and AI-assisted evaluation that could scale fairness across the diverse contexts of modern human-AI interaction.",Impossibility Fair LLMs rise general purpose artificial intelligence AI systems particularly large language models LLMs raised pressing moral questions reduce bias ensure fairness scale Researchers documented sort bias significant correlations demographics e g race gender LLM prompts responses remains unclear LLM fairness evaluated rigorous definitions group fairness fair representations analyze variety technical fairness frameworks inherent challenges make development fair LLM intractable framework does logically extend general purpose AI context infeasible practice primarily large amounts unstructured training data potential combinations human populations use cases sensitive attributes inherent challenges persist general purpose AI including LLMs empirical challenges limited participatory input limited measurement methods overcome Nonetheless fairness remain important type model evaluation promising research directions particularly development standards responsibility LLM developers context specific evaluations methods iterative participatory AI assisted evaluation scale fairness diverse contexts modern human AI interaction
5,Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process,"['Ermo Hua', 'Biqing Qi', 'Kaiyan Zhang', 'Kai Tian', 'Xingtai Lv', 'Ning Ding', 'Bowen Zhou']","Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are two fundamental processes for enhancing the capabilities of Language Models (LMs) post pre-training, aligning them better with human preferences. Although SFT advances in training efficiency, PO delivers better alignment, thus they are often combined. However, common practices simply apply them sequentially without integrating their optimization objectives, ignoring the opportunities to bridge their paradigm gap and take the strengths from both. To obtain a unified understanding, we interpret SFT and PO with two sub-processes -- Preference Estimation and Transition Optimization -- defined at token level within the Markov Decision Process (MDP) framework. This modeling shows that SFT is only a specialized case of PO with inferior estimation and optimization. PO evaluates the quality of model's entire generated answer, whereas SFT only scores predicted tokens based on preceding tokens from target answers. Therefore, SFT overestimates the ability of model, leading to inferior optimization. Building on this view, we introduce Intuitive Fine-Tuning (IFT) to integrate SFT and Preference Optimization into a single process. IFT captures LMs' intuitive sense of the entire answers through a temporal residual connection, but it solely relies on a single policy and the same volume of non-preference-labeled data as SFT. Our experiments show that IFT performs comparably or even superiorly to sequential recipes of SFT and some typical Preference Optimization methods across several tasks, particularly those requires generation, reasoning, and fact-following abilities. An explainable Frozen Lake game further validates the effectiveness of IFT for getting competitive policy.",Intuitive Fine Tuning Simplifying Alignment Single Process Supervised Fine Tuning SFT Preference Optimization PO fundamental processes enhancing capabilities Language Models LMs post pre training aligning better human preferences SFT advances training efficiency PO delivers better alignment combined common practices simply apply sequentially integrating optimization objectives ignoring opportunities bridge paradigm gap strengths obtain unified understanding interpret SFT PO sub processes Preference Estimation Transition Optimization defined token level Markov Decision Process MDP framework modeling shows SFT specialized case PO inferior estimation optimization PO evaluates quality model s entire generated answer SFT scores predicted tokens based preceding tokens target answers SFT overestimates ability model leading inferior optimization Building view introduce Intuitive Fine Tuning IFT integrate SFT Preference Optimization single process IFT captures LMs intuitive sense entire answers temporal residual connection solely relies single policy volume non preference labeled data SFT experiments IFT performs comparably superiorly sequential recipes SFT typical Preference Optimization methods tasks particularly requires generation reasoning fact following abilities explainable Frozen Lake game validates effectiveness IFT getting competitive policy
6,Bias in Language Models: Beyond Trick Tests and Towards RUTEd Evaluation,"['Kristian Lum', 'Jacy Reese Anthis', 'Kevin Robinson', 'Chirag Nagpal', 'Alexander Nicholas D’Amour']",,Bias Language Models Trick Tests RUTEd Evaluation
7,Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models,"['Wenhan Liu', 'Xinyu Ma', 'Yutao Zhu', 'Ziliang Zhao', 'Shuaiqiang Wang', 'Dawei Yin', 'Zhicheng Dou']","Large Language Models (LLMs) have shown exciting performance in listwise passage ranking. Due to the limited input length, existing methods often adopt the sliding window strategy. Such a strategy, though effective, is inefficient as it involves repetitive and serialized processing, which usually re-evaluates relevant passages multiple times. As a result, it incurs redundant API costs, which are proportional to the number of inference tokens. The development of long-context LLMs enables the full ranking of all passages within a single inference, avoiding redundant API costs. In this paper, we conduct a comprehensive study of long-context LLMs for ranking tasks in terms of efficiency and effectiveness. Surprisingly, our experiments reveal that full ranking with long-context LLMs can deliver superior performance in the supervised fine-tuning setting with a huge efficiency improvement. Furthermore, we identify two limitations of fine-tuning the full ranking model based on existing methods: (1) sliding window strategy fails to produce a full ranking list as a training label, and (2) the language modeling loss cannot emphasize top-ranked passage IDs in the label. To alleviate these issues, we propose a new complete listwise label construction approach and a novel importance-aware learning objective for full ranking. Experiments show the superior performance of our method over baselines. Our codes are available at \url{https://github.com/8421BCD/fullrank}.",Sliding Windows End Exploring Ranking Long Context Large Language Models Large Language Models LLMs shown exciting performance listwise passage ranking limited input length existing methods adopt sliding window strategy strategy effective inefficient involves repetitive serialized processing usually evaluates relevant passages multiple times result incurs redundant API costs proportional number inference tokens development long context LLMs enables ranking passages single inference avoiding redundant API costs paper conduct comprehensive study long context LLMs ranking tasks terms efficiency effectiveness Surprisingly experiments reveal ranking long context LLMs deliver superior performance supervised fine tuning setting huge efficiency improvement Furthermore identify limitations fine tuning ranking model based existing methods 1 sliding window strategy fails produce ranking list training label 2 language modeling loss emphasize ranked passage IDs label alleviate issues propose new complete listwise label construction approach novel importance aware learning objective ranking Experiments superior performance method baselines codes available url https github com 8421BCD fullrank
8,The Impact of Auxiliary Patient Data on Automated Chest X-Ray Report Generation and How to Incorporate It,"['Aaron Nicolson', 'Shengyao Zhuang', 'Jason Dowling', 'Bevan Koopman']","This study investigates the integration of diverse patient data sources into multimodal language models for automated chest X-ray (CXR) report generation. Traditionally, CXR report generation relies solely on CXR images and limited radiology data, overlooking valuable information from patient health records, particularly from emergency departments. Utilising the MIMIC-CXR and MIMIC-IV-ED datasets, we incorporate detailed patient information such as vital signs, medicines, and clinical history to enhance diagnostic accuracy. We introduce a novel approach to transform these heterogeneous data sources into embeddings that prompt a multimodal language model; this significantly enhances the diagnostic accuracy of generated radiology reports. Our comprehensive evaluation demonstrates the benefits of using a broader set of patient data, underscoring the potential for enhanced diagnostic capabilities and better patient outcomes through the integration of multimodal data in CXR report generation.",Impact Auxiliary Patient Data Automated Chest X Ray Report Generation Incorporate study investigates integration diverse patient data sources multimodal language models automated chest X ray CXR report generation Traditionally CXR report generation relies solely CXR images limited radiology data overlooking valuable information patient health records particularly emergency departments Utilising MIMIC CXR MIMIC IV ED datasets incorporate detailed patient information vital signs medicines clinical history enhance diagnostic accuracy introduce novel approach transform heterogeneous data sources embeddings prompt multimodal language model significantly enhances diagnostic accuracy generated radiology reports comprehensive evaluation demonstrates benefits using broader set patient data underscoring potential enhanced diagnostic capabilities better patient outcomes integration multimodal data CXR report generation
9,CLEME2.0: Towards Interpretable Evaluation by Disentangling Edits for Grammatical Error Correction,"['Jingheng Ye', 'Zishan Xu', 'Yinghui Li', 'Linlin Song', 'Qingyu Zhou', 'Hai-Tao Zheng', 'Ying Shen', 'Wenhao Jiang', 'Hong-Gee Kim', 'Ruitong Liu', 'Xin Su', 'Zifei Shan']","The paper focuses on the interpretability of Grammatical Error Correction (GEC) evaluation metrics, which received little attention in previous studies. To bridge the gap, we introduce **CLEME2.0**, a reference-based metric describing four fundamental aspects of GEC systems: hit-correction, wrong-correction, under-correction, and over-correction. They collectively contribute to exposing critical qualities and locating drawbacks of GEC systems. Evaluating systems by combining these aspects also leads to superior human consistency over other reference-based and reference-less metrics. Extensive experiments on two human judgment datasets and six reference datasets demonstrate the effectiveness and robustness of our method, achieving a new state-of-the-art result. Our codes are released at https://github.com/THUKElab/CLEME.",CLEME2 0 Interpretable Evaluation Disentangling Edits Grammatical Error Correction paper focuses interpretability Grammatical Error Correction GEC evaluation metrics received little attention previous studies bridge gap introduce CLEME2 0 reference based metric describing fundamental aspects GEC systems hit correction wrong correction correction correction collectively contribute exposing critical qualities locating drawbacks GEC systems Evaluating systems combining aspects leads superior human consistency reference based reference metrics Extensive experiments human judgment datasets reference datasets demonstrate effectiveness robustness method achieving new state art result codes released https github com THUKElab CLEME
10,Towards LLM-powered Attentive Listener: A Pragmatic Approach through Quantity Self-Repair,"['Junlin Li', 'Bo Peng', 'Yu-Yin Hsu']",,LLM powered Attentive Listener Pragmatic Approach Quantity Self Repair
11,StrucText-Eval: Evaluating Large Language Model’s Reasoning Ability in Structure-Rich Text,"['Zhouhong Gu', 'Haoning Ye', 'Xingzhou Chen', 'Zeyang Zhou', 'Hongwei Feng', 'Yanghua Xiao']",,StrucText Eval Evaluating Large Language Model s Reasoning Ability Structure Rich Text
12,Literature Meets Data: A Synergistic Approach to Hypothesis Generation,"['Haokun Liu', 'Yangqiaoyu Zhou', 'Mingxuan Li', 'Chenfei Yuan', 'Chenhao Tan']","AI holds promise for transforming scientific processes, including hypothesis generation. Prior work on hypothesis generation can be broadly categorized into theory-driven and data-driven approaches. While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other. To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation. We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97\% over few-shot, 15.75\% over literature-based alone, and 3.37\% over data-driven alone). Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection. Our results show that human accuracy improves significantly by 7.44\% and 14.19\% on these tasks, respectively. These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry.",Literature Meets Data Synergistic Approach Hypothesis Generation AI holds promise transforming scientific processes including hypothesis generation Prior work hypothesis generation broadly categorized theory driven data driven approaches proven effective generating novel plausible hypotheses remains open question complement address develop method combines literature based insights data perform LLM powered hypothesis generation apply method different datasets demonstrate integrating literature data outperforms baselines 8 97 shot 15 75 literature based 3 37 data driven Additionally conduct human evaluation assess utility LLM generated hypotheses assisting human decision making challenging tasks deception detection AI generated content detection results human accuracy improves significantly 7 44 14 19 tasks respectively findings suggest integrating literature based data driven approaches provides comprehensive nuanced framework hypothesis generation open new avenues scientific inquiry
13,GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization,"['Zhouhong Gu', 'Xingzhou Chen', 'Xiaoran Shi', 'Tao Wang', 'Suhang Zheng', 'Tianyu Li', 'Hongwei Feng', 'Yanghua Xiao']","Recent advances in large language models have highlighted the critical need for precise control over model outputs through predefined constraints. While existing methods attempt to achieve this through either direct instruction-response synthesis or preferential response optimization, they often struggle with constraint understanding and adaptation. This limitation becomes particularly evident when handling fine-grained constraints, leading to either hallucination or brittle performance. We introduce Generative Adversarial Policy Optimization (GAPO), a novel framework that combines GAN-based training dynamics with an encoder-only reward model to progressively learn and adapt to increasingly complex constraints. GAPO leverages adversarial training to automatically generate training samples of varying difficulty while utilizing the encoder-only architecture to better capture prompt-response relationships. Extensive experiments demonstrate GAPO's superior performance across multiple benchmarks, particularly in scenarios requiring fine-grained constraint handling, where it significantly outperforms existing methods like PPO, DPO, and KTO. Our results suggest that GAPO's unique approach to preferential prompt learning offers a more robust and effective solution for controlling LLM outputs. Code is avaliable in https://github.com/MikeGu721/GAPO.",GAPO Learning Preferential Prompt Generative Adversarial Policy Optimization Recent advances large language models highlighted critical need precise control model outputs predefined constraints existing methods attempt achieve direct instruction response synthesis preferential response optimization struggle constraint understanding adaptation limitation particularly evident handling fine grained constraints leading hallucination brittle performance introduce Generative Adversarial Policy Optimization GAPO novel framework combines GAN based training dynamics encoder reward model progressively learn adapt increasingly complex constraints GAPO leverages adversarial training automatically generate training samples varying difficulty utilizing encoder architecture better capture prompt response relationships Extensive experiments demonstrate GAPO s superior performance multiple benchmarks particularly scenarios requiring fine grained constraint handling significantly outperforms existing methods like PPO DPO KTO results suggest GAPO s unique approach preferential prompt learning offers robust effective solution controlling LLM outputs Code avaliable https github com MikeGu721 GAPO
14,Tree-of-Evolution: Tree-Structured Instruction Evolution for Code Generation in Large Language Models,"['Ziyang Luo', 'Kaixin Li', 'Hongzhan Lin', 'Yuchen Tian', 'Mohan Kankanhalli', 'Jing Ma']",,Tree Evolution Tree Structured Instruction Evolution Code Generation Large Language Models
15,Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models,"['Seunguk Yu', 'Juhwan Choi', 'YoungBin Kim']","Despite the recent strides in large language models, studies have underscored the existence of social biases within these systems. In this paper, we delve into the validation and comparison of the ethical biases of LLMs concerning globally discussed and potentially sensitive topics, hypothesizing that these biases may arise from language-specific distinctions. Introducing the Multilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news articles from Human Rights Watch covering 17 topics, and generated socially sensitive questions along with corresponding responses in multiple languages. We scrutinized the biases of these responses across languages and topics, employing two statistical hypothesis tests. The results showed that the null hypotheses were rejected in most cases, indicating biases arising from cross-language differences. It demonstrates that ethical biases in responses are widespread across various languages, and notably, these biases were prevalent even among different LLMs. By making the proposed MSQAD openly available, we aim to facilitate future research endeavors focused on examining cross-language biases in LLMs and their variant models.",Delving Multilingual Ethical Bias MSQAD Statistical Hypothesis Tests Large Language Models Despite recent strides large language models studies underscored existence social biases systems paper delve validation comparison ethical biases LLMs concerning globally discussed potentially sensitive topics hypothesizing biases arise language specific distinctions Introducing Multilingual Sensitive Questions Answers Dataset MSQAD collected news articles Human Rights Watch covering 17 topics generated socially sensitive questions corresponding responses multiple languages scrutinized biases responses languages topics employing statistical hypothesis tests results showed null hypotheses rejected cases indicating biases arising cross language differences demonstrates ethical biases responses widespread various languages notably biases prevalent different LLMs making proposed MSQAD openly available aim facilitate future research endeavors focused examining cross language biases LLMs variant models
16,ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision,"['Dosung Lee', 'Wonjun Oh', 'Boyoung Kim', 'Minyoung Kim', 'Joonsuk Park', 'Paul Hongsuck Seo']","Multi-hop question answering (MHQA) involves reasoning across multiple documents to answer complex questions. Dense retrievers typically outperform sparse methods like BM25 by leveraging semantic embeddings; however, they require labeled query-document pairs for fine-tuning. This poses a significant challenge in MHQA due to the high variability of queries (reformulated) questions throughout the reasoning steps. To overcome this limitation, we introduce Retriever Supervision with Consistency and Relevance (ReSCORE), a novel method for training dense retrievers for MHQA without labeled documents. ReSCORE leverages large language models to capture each documents relevance to the question and consistency with the correct answer and use them to train a retriever within an iterative question-answering framework. Experiments on three MHQA benchmarks demonstrate the effectiveness of ReSCORE, with significant improvements in retrieval, and in turn, the state-of-the-art MHQA performance. Our implementation is available at: https://leeds1219.github.io/ReSCORE.",ReSCORE Label free Iterative Retriever Training Multi hop Question Answering Relevance Consistency Supervision Multi hop question answering MHQA involves reasoning multiple documents answer complex questions Dense retrievers typically outperform sparse methods like BM25 leveraging semantic embeddings require labeled query document pairs fine tuning poses significant challenge MHQA high variability queries reformulated questions reasoning steps overcome limitation introduce Retriever Supervision Consistency Relevance ReSCORE novel method training dense retrievers MHQA labeled documents ReSCORE leverages large language models capture documents relevance question consistency correct answer use train retriever iterative question answering framework Experiments MHQA benchmarks demonstrate effectiveness ReSCORE significant improvements retrieval turn state art MHQA performance implementation available https leeds1219 github io ReSCORE
17,MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments,"['Yin Cai', 'Zhouhong Gu', 'Zhaohan Du', 'Zheyu Ye', 'Shaosheng Cao', 'Yiqian xu', 'Hongwei Feng', 'Ping Chen']","Large Language Models (LLMs) have shown remarkable capabilities in environmental perception, reasoning-based decision-making, and simulating complex human behaviors, particularly in interactive role-playing contexts. This paper introduces the Multiverse Interactive Role-play Ability General Evaluation (MIRAGE), a comprehensive framework designed to assess LLMs' proficiency in portraying advanced human behaviors through murder mystery games. MIRAGE features eight intricately crafted scripts encompassing diverse themes and styles, providing a rich simulation. To evaluate LLMs' performance, MIRAGE employs four distinct methods: the Trust Inclination Index (TII) to measure dynamics of trust and suspicion, the Clue Investigation Capability (CIC) to measure LLMs' capability of conducting information, the Interactivity Capability Index (ICI) to assess role-playing capabilities and the Script Compliance Index (SCI) to assess LLMs' capability of understanding and following instructions. Our experiments indicate that even popular models like GPT-4 face significant challenges in navigating the complexities presented by the MIRAGE. The datasets and simulation codes are available in \href{https://github.com/lime728/MIRAGE}{github}.",MIRAGE Exploring Large Language Models Perform Complex Social Interactive Environments Large Language Models LLMs shown remarkable capabilities environmental perception reasoning based decision making simulating complex human behaviors particularly interactive role playing contexts paper introduces Multiverse Interactive Role play Ability General Evaluation MIRAGE comprehensive framework designed assess LLMs proficiency portraying advanced human behaviors murder mystery games MIRAGE features intricately crafted scripts encompassing diverse themes styles providing rich simulation evaluate LLMs performance MIRAGE employs distinct methods Trust Inclination Index TII measure dynamics trust suspicion Clue Investigation Capability CIC measure LLMs capability conducting information Interactivity Capability Index ICI assess role playing capabilities Script Compliance Index SCI assess LLMs capability understanding following instructions experiments indicate popular models like GPT 4 face significant challenges navigating complexities presented MIRAGE datasets simulation codes available href https github com lime728 MIRAGE github
18,FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models,"['Hongzhan Lin', 'Yang Deng', 'Yuxuan Gu', 'Wenxuan Zhang', 'Jing Ma', 'See-Kiong Ng', 'Tat-Seng Chua']","Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the justification production and uncover the nuanced limitations of LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven framework that adaptively and dynamically assesses LLMs' fact-checking capabilities. Leveraging importance sampling principles and multi-agent collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs iterative model-centric evaluations, and updates assessments based on model-specific responses. By incorporating justification production alongside verdict prediction, this framework provides a comprehensive and evolving audit of LLMs' factual reasoning capabilities, to investigate their trustworthiness. Extensive experiments demonstrate that FACT-AUDIT effectively differentiates among state-of-the-art LLMs, providing valuable insights into model strengths and limitations in model-centric fact-checking analysis.",FACT AUDIT Adaptive Multi Agent Framework Dynamic Fact Checking Evaluation Large Language Models Large Language Models LLMs significantly advanced fact checking studies existing automated fact checking evaluation methods rely static datasets classification metrics fail automatically evaluate justification production uncover nuanced limitations LLMs fact checking work introduce FACT AUDIT agent driven framework adaptively dynamically assesses LLMs fact checking capabilities Leveraging importance sampling principles multi agent collaboration FACT AUDIT generates adaptive scalable datasets performs iterative model centric evaluations updates assessments based model specific responses incorporating justification production alongside verdict prediction framework provides comprehensive evolving audit LLMs factual reasoning capabilities investigate trustworthiness Extensive experiments demonstrate FACT AUDIT effectively differentiates state art LLMs providing valuable insights model strengths limitations model centric fact checking analysis
19,Statistical Deficiency for Task Inclusion Estimation,"['Loïc Fosse', 'Frederic Bechet', 'Benoit Favre', 'Géraldine Damnati', 'Gwénolé Lecorvé', 'Maxime DARRIN', 'Philippe Formont', 'Pablo Piantanida']","Tasks are central in machine learning, as they are the most natural objects to assess the capabilities of current models. The trend is to build general models able to address any task. Even though transfer learning and multitask learning try to leverage the underlying task space, no well-founded tools are available to study its structure. This study proposes a theoretically grounded setup to define the notion of task and to compute the {\bf inclusion} between two tasks from a statistical deficiency point of view. We propose a tractable proxy as information sufficiency to estimate the degree of inclusion between tasks, show its soundness on synthetic data, and use it to reconstruct empirically the classic NLP pipeline.",Statistical Deficiency Task Inclusion Estimation Tasks central machine learning natural objects assess capabilities current models trend build general models able address task transfer learning multitask learning try leverage underlying task space founded tools available study structure study proposes theoretically grounded setup define notion task compute bf inclusion tasks statistical deficiency point view propose tractable proxy information sufficiency estimate degree inclusion tasks soundness synthetic data use reconstruct empirically classic NLP pipeline
20,Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients,"['Jabin Koo', 'Minwoo Jang', 'Jungseul Ok']","Federated fine-tuning for Large Language Models (LLMs) faces significant challenges due to the heavy communication overhead of transmitting large model updates. Although Low Rank Adaptation (LoRA) has been proposed as a solution, yet its application in federated learning is complicated by discordance in aggregation. Existing methods addressing this discordance often suffer from performance degradation at low ranks in heterogeneous data settings. In response, we introduce LoRA-A$^2$ (Low Rank Adaptation with Alternating freeze and Adaptive rank selection), which demonstrates robustness in challenging settings with low ranks and high data heterogeneity. Our experimental findings reveal that LoRA-A$^2$ maintains performance even under extreme heterogeneity and low rank conditions, achieving up to a significant reduction in uploaded parameters compared to full fine-tuning without compromising performance. This adaptive mechanism increases robustness and communication efficiency in federated fine-tuning, enabling the practical deployment of LLMs in resource-constrained environments.",Robust Efficient Federated Low Rank Adaptation Heterogeneous Clients Federated fine tuning Large Language Models LLMs faces significant challenges heavy communication overhead transmitting large model updates Low Rank Adaptation LoRA proposed solution application federated learning complicated discordance aggregation Existing methods addressing discordance suffer performance degradation low ranks heterogeneous data settings response introduce LoRA 2 Low Rank Adaptation Alternating freeze Adaptive rank selection demonstrates robustness challenging settings low ranks high data heterogeneity experimental findings reveal LoRA 2 maintains performance extreme heterogeneity low rank conditions achieving significant reduction uploaded parameters compared fine tuning compromising performance adaptive mechanism increases robustness communication efficiency federated fine tuning enabling practical deployment LLMs resource constrained environments
21,Dynamic Label Name Refinement for Few-Shot Dialogue Intent Classification,"['Gyutae Park', 'Ingeol Baek', 'Byeongjeong Kim', 'Joongbo Shin', 'Hwanhee Lee']","Dialogue intent classification aims to identify the underlying purpose or intent of a user's input in a conversation. Current intent classification systems encounter considerable challenges, primarily due to the vast number of possible intents and the significant semantic overlap among similar intent classes. In this paper, we propose a novel approach to few-shot dialogue intent classification through in-context learning, incorporating dynamic label refinement to address these challenges. Our method retrieves relevant examples for a test input from the training set and leverages a large language model to dynamically refine intent labels based on semantic understanding, ensuring that intents are clearly distinguishable from one another. Experimental results demonstrate that our approach effectively resolves confusion between semantically similar intents, resulting in significantly enhanced performance across multiple datasets compared to baselines. We also show that our method generates more interpretable intent labels, and has a better semantic coherence in capturing underlying user intents compared to baselines.",Dynamic Label Refinement Shot Dialogue Intent Classification Dialogue intent classification aims identify underlying purpose intent user s input conversation Current intent classification systems encounter considerable challenges primarily vast number possible intents significant semantic overlap similar intent classes paper propose novel approach shot dialogue intent classification context learning incorporating dynamic label refinement address challenges method retrieves relevant examples test input training set leverages large language model dynamically refine intent labels based semantic understanding ensuring intents clearly distinguishable Experimental results demonstrate approach effectively resolves confusion semantically similar intents resulting significantly enhanced performance multiple datasets compared baselines method generates interpretable intent labels better semantic coherence capturing underlying user intents compared baselines
22,LLM-Powered Test Case Generation for Detecting Bugs in Plausible Programs,"['Kaibo Liu', 'Zhenpeng Chen', 'Yiyang Liu', 'Jie Zhang', 'Mark Harman', 'Yudong Han', 'Yun Ma', 'Yihong Dong', 'Ge Li', 'Gang Huang']","Detecting tricky bugs in plausible programs, those that pass existing test suites yet still contain bugs, remains a significant challenge in software testing. To address this problem, we propose TrickCatcher, an LLM-powered approach to generating test cases for uncovering bugs in plausible programs. TrickCatcher operates in three stages: First, it uses an LLM to generate program variants based on the program under test (PUT) and its specification. Second, it employs an LLM to construct an input generator from the specification for producing test inputs. Finally, these inputs are executed on both the PUT and its program variants to detect inconsistencies in their outputs. We evaluate TrickCatcher on two datasets, TrickyBugs and EvalPlus, which include 366 human-written and 151 AI-generated plausible programs with tricky bugs. TrickCatcher achieves recall, precision, and F1 scores that are 1.80x, 2.65x, and 1.66x those of the state-of-the-art baselines, respectively. Code and data used are available at https://github.com/RinCloud/TrickCatcher.",LLM Powered Test Case Generation Detecting Bugs Plausible Programs Detecting tricky bugs plausible programs pass existing test suites contain bugs remains significant challenge software testing address problem propose TrickCatcher LLM powered approach generating test cases uncovering bugs plausible programs TrickCatcher operates stages uses LLM generate program variants based program test specification Second employs LLM construct input generator specification producing test inputs Finally inputs executed program variants detect inconsistencies outputs evaluate TrickCatcher datasets TrickyBugs EvalPlus include 366 human written 151 AI generated plausible programs tricky bugs TrickCatcher achieves recall precision F1 scores 1 80x 2 65x 1 66x state art baselines respectively Code data used available https github com RinCloud TrickCatcher
23,Capture the Key in Reasoning to Enhance CoT Distillation Generalization,"['Chengwei Dai', 'Kun Li', 'Wei Zhou', 'Songlin Hu']",,Capture Key Reasoning Enhance CoT Distillation Generalization
24,"How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond","['Chen Huang', 'Yang Deng', 'Wenqiang Lei', 'Jiancheng Lv', 'Tat-Seng Chua', 'Jimmy Huang']","With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.",Enable Effective Cooperation Humans NLP Models Survey Principles Formalizations advancement large language models LLMs intelligent models evolved mere tools autonomous agents goals strategies cooperating humans evolution birthed novel paradigm NLP e human model cooperation yielded remarkable progress numerous NLP tasks recent years paper step present thorough review human model cooperation exploring principles formalizations open challenges particular introduce new taxonomy provides unified perspective summarize existing approaches discuss potential frontier areas corresponding challenges regard work entry point paving way breakthrough research regard
25,Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge,"['Li Zheng', 'Sihang Wang', 'Hao Fei', 'Zuquan Peng', 'Fei Li', 'Jianming Fu', 'Chong Teng', 'Donghong Ji']","Text-based hyperbole and metaphor detection are of great significance for natural language processing (NLP) tasks. However, due to their semantic obscurity and expressive diversity, it is rather challenging to identify them. Existing methods mostly focus on superficial text features, ignoring the associations of hyperbole and metaphor as well as the effect of implicit emotion on perceiving these rhetorical devices. To implement these hypotheses, we propose an emotion-guided hyperbole and metaphor detection framework based on bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis module deeply mines the emotion connotations behind hyperbole and metaphor. Next, the emotion-based domain mapping module identifies the target and source domains to gain a deeper understanding of the implicit meanings of hyperbole and metaphor. Finally, the bidirectional dynamic interaction module enables the mutual promotion between hyperbole and metaphor. Meanwhile, a verification mechanism is designed to ensure detection accuracy and reliability. Experiments show that EmoBi outperforms all baseline methods on four datasets. Specifically, compared to the current SoTA, the F1 score increased by 28.1% for hyperbole detection on the TroFi dataset and 23.1% for metaphor detection on the HYPO-L dataset. These results, underpinned by in-depth analyses, underscore the effectiveness and potential of our approach for advancing hyperbole and metaphor detection.",Enhancing Hyperbole Metaphor Detection Bidirectional Dynamic Interaction Emotion Knowledge Text based hyperbole metaphor detection great significance natural language processing NLP tasks semantic obscurity expressive diversity challenging identify Existing methods focus superficial text features ignoring associations hyperbole metaphor effect implicit emotion perceiving rhetorical devices implement hypotheses propose emotion guided hyperbole metaphor detection framework based bidirectional dynamic interaction EmoBi Firstly emotion analysis module deeply mines emotion connotations hyperbole metaphor emotion based domain mapping module identifies target source domains gain deeper understanding implicit meanings hyperbole metaphor Finally bidirectional dynamic interaction module enables mutual promotion hyperbole metaphor verification mechanism designed ensure detection accuracy reliability Experiments EmoBi outperforms baseline methods datasets Specifically compared current SoTA F1 score increased 28 1 hyperbole detection TroFi dataset 23 1 metaphor detection HYPO L dataset results underpinned depth analyses underscore effectiveness potential approach advancing hyperbole metaphor detection
26,"UniICL: An Efficient ICL Framework Unifying Compression, Selection, and Generation","['Jun Gao', 'Qi Lv', 'Zili Wang', 'Tianxiang Wu', 'Ziqiang Cao', 'Wenjie Li']",,UniICL Efficient ICL Framework Unifying Compression Selection Generation
27,BelarusianGLUE: Towards a Natural Language Understanding Benchmark for Belarusian,"['Maksim Aparovich', 'Volha Harytskaya', 'Vladislav Poritski', 'Oksana Volchek', 'Pavel Smrz']",,BelarusianGLUE Natural Language Understanding Benchmark Belarusian
28,A Survey on Foundation Language Models for Single-cell Biology,"['Fan Zhang', 'Hao Chen', 'Zhihong Zhu', 'Ziheng Zhang', 'Zhenxi Lin', 'Ziyue Qiao', 'Yefeng Zheng', 'Xian Wu']",,Survey Foundation Language Models Single cell Biology
29,RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios,"['Ruiwen Zhou', 'Wenyue Hua', 'Liangming Pan', 'Sitao Cheng', 'Xiaobao Wu', 'En Yu', 'William Yang Wang']","This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs' proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. We also observe a significant performance boost when LLMs are provided with external tools for oracle math and logic operations. These results highlight significant challenges and promising research directions in advancing LLMs' rule-guided reasoning capabilities in real-life applications. Our codes and data are publicly available on https://github.com/skyriver-2000/RuleArena.",RuleArena Benchmark Rule Guided Reasoning LLMs Real World Scenarios paper introduces RuleArena novel challenging benchmark designed evaluate ability large language models LLMs follow complex real world rules reasoning Covering practical domains airline baggage fees NBA transactions tax regulations RuleArena assesses LLMs proficiency handling intricate natural language instructions demand long context understanding logical reasoning accurate mathematical computation key attributes distinguish RuleArena traditional rule based reasoning benchmarks 1 extends standard order logic representations 2 grounded authentic practical scenarios providing insights suitability reliability LLMs real world applications findings reveal notable limitations LLMs 1 struggle identify apply appropriate rules frequently confused similar distinct regulations 2 consistently perform accurate mathematical computations correctly identify relevant rules 3 general perform poorly benchmark observe significant performance boost LLMs provided external tools oracle math logic operations results highlight significant challenges promising research directions advancing LLMs rule guided reasoning capabilities real life applications codes data publicly available https github com skyriver 2000 RuleArena
30,Extending LLM Context Window with Adaptive Grouped Positional Encoding: A Training-Free Method,"['Xinhao Xu', 'Jiaxin Li', 'Hui Chen', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding']",,Extending LLM Context Window Adaptive Grouped Positional Encoding Training Free Method
31,Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models,"['Sungjae Lee', 'Hyejin Park', 'Jaechang Kim', 'Jungseul Ok']","Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer from computational inefficiency and redundancy. First, they overlook the diversity of task difficulties, leading to unnecessarily extensive searches even for easy tasks. Second, they neglect the semantics of reasoning paths, resulting in redundant exploration of semantically identical paths. To address these limitations, we propose Semantic Exploration with Adaptive Gating (SEAG), a computationally efficient method. SEAG employs an adaptive gating mechanism that dynamically decides whether to conduct a tree search, based on the confidence level of answers from a preceding simple reasoning method. Furthermore, its tree-based exploration consolidates semantically identical reasoning steps, reducing redundant explorations while maintaining or even improving accuracy. Our extensive experiments demonstrate that SEAG significantly improves accuracy by 4.3% on average while requiring only 31% of computational costs compared to existing tree search-based methods on complex reasoning benchmarks including GSM8K and ARC with diverse language models such as Llama2, Llama3, and Mistral. Our code is available at https://github.com/ml-postech/SEAG-semantic-exploration-with-adaptive-gating .",Semantic Exploration Adaptive Gating Efficient Problem Solving Language Models Recent advancements large language models LLMs shown remarkable potential various complex tasks requiring multi step reasoning methods like tree search explore diverse reasoning paths existing methods suffer computational inefficiency redundancy overlook diversity task difficulties leading unnecessarily extensive searches easy tasks Second neglect semantics reasoning paths resulting redundant exploration semantically identical paths address limitations propose Semantic Exploration Adaptive Gating SEAG computationally efficient method SEAG employs adaptive gating mechanism dynamically decides conduct tree search based confidence level answers preceding simple reasoning method Furthermore tree based exploration consolidates semantically identical reasoning steps reducing redundant explorations maintaining improving accuracy extensive experiments demonstrate SEAG significantly improves accuracy 4 3 average requiring 31 computational costs compared existing tree search based methods complex reasoning benchmarks including GSM8K ARC diverse language models Llama2 Llama3 Mistral code available https github com ml postech SEAG semantic exploration adaptive gating
32,HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval,"['Arian Askari', 'Emmanouil Stergiadis', 'Ilya Gusev', 'Moran Beladev']","We present HotelMatch-LLM, a multimodal dense retrieval model for the travel domain that enables natural language property search, addressing the limitations of traditional travel search engines which require users to start with a destination and editing search parameters. HotelMatch-LLM features three key innovations: (1) Domain-specific multi-task optimization with three novel retrieval, visual, and language modeling objectives; (2) Asymmetrical dense retrieval architecture combining a small language model (SLM) for efficient online query processing and a large language model (LLM) for embedding hotel data; and (3) Extensive image processing to handle all property image galleries. Experiments on four diverse test sets show HotelMatch-LLM significantly outperforms state-of-the-art models, including VISTA and MARVEL. Specifically, on the test set -- main query type -- we achieve 0.681 for HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our analysis highlights the impact of our multi-task optimization, the generalizability of HotelMatch-LLM across LLM architectures, and its scalability for processing large image galleries.",HotelMatch LLM Joint Multi Task Training Small Large Language Models Efficient Multimodal Hotel Retrieval present HotelMatch LLM multimodal dense retrieval model travel domain enables natural language property search addressing limitations traditional travel search engines require users start destination editing search parameters HotelMatch LLM features key innovations 1 Domain specific multi task optimization novel retrieval visual language modeling objectives 2 Asymmetrical dense retrieval architecture combining small language model SLM efficient online query processing large language model LLM embedding hotel data 3 Extensive image processing handle property image galleries Experiments diverse test sets HotelMatch LLM significantly outperforms state art models including VISTA MARVEL Specifically test set main query type achieve 0 681 HotelMatch LLM compared 0 603 effective baseline MARVEL analysis highlights impact multi task optimization generalizability HotelMatch LLM LLM architectures scalability processing large image galleries
33,Can Multimodal Large Language Models Understand Spatial Relations?,"['Jingping Liu', 'Ziyan Liu', 'Zhedong Cen', 'Yan Zhou', 'Yinan Zou', 'Weiyan Zhang', 'Haiyun Jiang', 'Tong Ruan']",,Multimodal Large Language Models Understand Spatial Relations
34,$S^3$ - Semantic Signal Separation,"['Márton Kardos', 'Jan Kostkan', 'Kenneth Enevoldsen', 'Arnault-Quentin Vermillet', 'Kristoffer Nielbo', 'Roberta Rocca']",,S 3 Semantic Signal Separation
35,TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs,"['Lanxiang Hu', 'Tajana Rosing', 'Hao Zhang']","Specializing large language models (LLMs) for local deployment in domain-specific use cases is necessary for strong performance while meeting latency and privacy constraints. However, conventional task-specific adaptation approaches do not show simultaneous memory saving and inference speedup at deployment time. Practical compression techniques like quantization and pruning require dedicated hardware or kernel support to achieve measured inference speedup. We develop TrimLLM based on the layer-wise specialization phenomenon we empirically observed and verified on contemporary LLMs. TrimLLM reduces the depth of LLMs via progressive layer dropping. We show it retains LLMs' capacity in specific domains and achieves inference speedup irrespective of hardware and deep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for inference; models adapted on medical, legal, and financial datasets all demonstrate $2.1-5.7\times$ inference speedup on consumer GPUs and up to $3.1\times$ speedup on A100 when compared to state-of-the-art model compression algorithms, with no loss in accuracy at 50$\sim$60\% model compression ratio.",TrimLLM Progressive Layer Dropping Domain Specific LLMs Specializing large language models LLMs local deployment domain specific use cases necessary strong performance meeting latency privacy constraints conventional task specific adaptation approaches simultaneous memory saving inference speedup deployment time Practical compression techniques like quantization pruning require dedicated hardware kernel support achieve measured inference speedup develop TrimLLM based layer wise specialization phenomenon empirically observed verified contemporary LLMs TrimLLM reduces depth LLMs progressive layer dropping retains LLMs capacity specific domains achieves inference speedup irrespective hardware deep learning frameworks evaluated TrimLLM LLMs various sizes inference models adapted medical legal financial datasets demonstrate 2 1 5 7 times inference speedup consumer GPUs 3 1 times speedup A100 compared state art model compression algorithms loss accuracy 50 sim 60 model compression ratio
36,JuStRank: Benchmarking LLM Judges for System Ranking,"['Ariel Gera', 'Odellia Boni', 'Yotam Perlitz', 'Roy Bar-Haim', 'Lilach Eden', 'Asaf Yehudai']","Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias.",JuStRank Benchmarking LLM Judges Ranking Given rapid progress generative AI pressing need systematically compare choose numerous models configurations available scale versatility evaluations make use LLM based judges compelling solution challenge Crucially approach requires validate quality LLM judge Previous work focused instance based assessment LLM judges judge evaluated set responses response pairs agnostic source systems argue setting overlooks critical factors affecting level ranking judge s positive negative bias certain systems address gap conduct large scale study LLM judges rankers scores generated aggregating judgment scores multiple outputs judge s quality assessed comparing resulting ranking human based ranking overall judge assessment analysis provides fine grained characterization judge behavior including decisiveness bias
37,Generating Diverse Training Samples for Relation Extraction with Large Language Models,"['Zexuan Li', 'Hongliang Dai', 'Piji Li']","Using Large Language Models (LLMs) to generate training data can potentially be a preferable way to improve zero or few-shot NLP tasks. However, many problems remain to be investigated for this direction. For the task of Relation Extraction (RE), we find that samples generated by directly prompting LLMs may easily have high structural similarities with each other. They tend to use a limited variety of phrasing while expressing the relation between a pair of entities. Therefore, in this paper, we study how to effectively improve the diversity of the training samples generated with LLMs for RE, while also maintaining their correctness. We first try to make the LLMs produce dissimilar samples by directly giving instructions in In-Context Learning (ICL) prompts. Then, we propose an approach to fine-tune LLMs for diversity training sample generation through Direct Preference Optimization (DPO). Our experiments on commonly used RE datasets show that both attempts can improve the quality of the generated training data. We also find that comparing with directly performing RE with an LLM, training a non-LLM RE model with its generated samples may lead to better performance.",Generating Diverse Training Samples Relation Extraction Large Language Models Using Large Language Models LLMs generate training data potentially preferable way improve zero shot NLP tasks problems remain investigated direction task Relation Extraction samples generated directly prompting LLMs easily high structural similarities tend use limited variety phrasing expressing relation pair entities paper study effectively improve diversity training samples generated LLMs maintaining correctness try make LLMs produce dissimilar samples directly giving instructions Context Learning ICL prompts propose approach fine tune LLMs diversity training sample generation Direct Preference Optimization DPO experiments commonly used datasets attempts improve quality generated training data comparing directly performing LLM training non LLM model generated samples lead better performance
38,MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts,"['Dominik Macko', 'Jakub Kopál', 'Robert Moro', 'Ivan Srba']","Recent LLMs are able to generate high-quality multilingual texts, indistinguishable for humans from authentic human-written ones. Research in machine-generated text detection is however mostly focused on the English language and longer texts, such as news articles, scientific papers or student essays. Social-media texts are usually much shorter and often feature informal language, grammatical errors, or distinct linguistic items (e.g., emoticons, hashtags). There is a gap in studying the ability of existing methods in detection of such texts, reflected also in the lack of existing multilingual benchmark datasets. To fill this gap we propose the first multilingual (22 languages) and multi-platform (5 social media platforms) dataset for benchmarking machine-generated text detection in the social-media domain, called MultiSocial. It contains 472,097 texts, of which about 58k are human-written and approximately the same amount is generated by each of 7 multilingual LLMs. We use this benchmark to compare existing detection methods in zero-shot as well as fine-tuned form. Our results indicate that the fine-tuned detectors have no problem to be trained on social-media texts and that the platform selection for training matters.",MultiSocial Multilingual Benchmark Machine Generated Text Detection Social Media Texts Recent LLMs able generate high quality multilingual texts indistinguishable humans authentic human written ones Research machine generated text detection focused English language longer texts news articles scientific papers student essays Social media texts usually shorter feature informal language grammatical errors distinct linguistic items e g emoticons hashtags gap studying ability existing methods detection texts reflected lack existing multilingual benchmark datasets gap propose multilingual 22 languages multi platform 5 social media platforms dataset benchmarking machine generated text detection social media domain called MultiSocial contains 472 097 texts 58k human written approximately generated 7 multilingual LLMs use benchmark compare existing detection methods zero shot fine tuned form results indicate fine tuned detectors problem trained social media texts platform selection training matters
39,Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection,"['Cilin Yan', 'Jingyun Wang', 'Lin Zhang', 'Ruihui Zhao', 'Xiaopu Wu', 'Kai Xiong', 'Qingsong Liu', 'Guoliang Kang', 'Yangyang Kang']","Automatic prompt engineering aims to enhance the generation quality of large language models (LLMs). Recent works utilize feedbacks generated from erroneous cases to guide the prompt optimization. During inference, they may further retrieve several semantically-related exemplars and concatenate them to the optimized prompts to improve the performance. However, those works only utilize the feedback at the current step, ignoring historical and unseleccted feedbacks which are potentially beneficial. Moreover, the selection of exemplars only considers the general semantic relationship and may not be optimal in terms of task performance and matching with the optimized prompt. In this work, we propose an Exemplar-Guided Reflection with Memory mechanism (ERM) to realize more efficient and accurate prompt optimization. Specifically, we design an exemplar-guided reflection mechanism where the feedback generation is additionally guided by the generated exemplars. We further build two kinds of memory to fully utilize the historical feedback information and support more effective exemplar retrieval. Empirical evaluations show our method surpasses previous state-of-the-arts with less optimization steps, i.e., improving F1 score by 10.1 on LIAR dataset, and reducing half of the optimization steps on ProTeGi.",Efficient Accurate Prompt Optimization Benefit Memory Exemplar Guided Reflection Automatic prompt engineering aims enhance generation quality large language models LLMs Recent works utilize feedbacks generated erroneous cases guide prompt optimization inference retrieve semantically related exemplars concatenate optimized prompts improve performance works utilize feedback current step ignoring historical unseleccted feedbacks potentially beneficial selection exemplars considers general semantic relationship optimal terms task performance matching optimized prompt work propose Exemplar Guided Reflection Memory mechanism ERM realize efficient accurate prompt optimization Specifically design exemplar guided reflection mechanism feedback generation additionally guided generated exemplars build kinds memory fully utilize historical feedback information support effective exemplar retrieval Empirical evaluations method surpasses previous state arts optimization steps e improving F1 score 10 1 LIAR dataset reducing half optimization steps ProTeGi
40,Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation,"['Aneta Zugecova', 'Dominik Macko', 'Ivan Srba', 'Robert Moro', 'Jakub Kopál', 'Katarína Marcinčinová', 'Matúš Mesarčík']","The capabilities of recent large language models (LLMs) to generate high-quality content indistinguishable by humans from human-written texts rises many concerns regarding their misuse. Previous research has shown that LLMs can be effectively misused for generating disinformation news articles following predefined narratives. Their capabilities to generate personalized (in various aspects) content have also been evaluated and mostly found usable. However, a combination of personalization and disinformation abilities of LLMs has not been comprehensively studied yet. Such a dangerous combination should trigger integrated safety filters of the LLMs, if there are some. This study fills this gap by evaluation of vulnerabilities of recent open and closed LLMs, and their willingness to generate personalized disinformation news articles in English. We further explore whether the LLMs can reliably meta-evaluate the personalization quality and whether the personalization affects the generated-texts detectability. Our results demonstrate the need for stronger safety-filters and disclaimers, as those are not properly functioning in most of the evaluated LLMs. Additionally, our study revealed that the personalization actually reduces the safety-filter activations; thus effectively functioning as a jailbreak. Such behavior must be urgently addressed by LLM developers and service providers.",Evaluation LLM Vulnerabilities Misused Personalized Disinformation Generation capabilities recent large language models LLMs generate high quality content indistinguishable humans human written texts rises concerns regarding misuse Previous research shown LLMs effectively misused generating disinformation news articles following predefined narratives capabilities generate personalized various aspects content evaluated usable combination personalization disinformation abilities LLMs comprehensively studied dangerous combination trigger integrated safety filters LLMs study fills gap evaluation vulnerabilities recent open closed LLMs willingness generate personalized disinformation news articles English explore LLMs reliably meta evaluate personalization quality personalization affects generated texts detectability results demonstrate need stronger safety filters disclaimers properly functioning evaluated LLMs Additionally study revealed personalization actually reduces safety filter activations effectively functioning jailbreak behavior urgently addressed LLM developers service providers
41,EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents,"['Cheng Qian', 'Peixuan Han', 'Qinyu Luo', 'Bingxiang He', 'Xiusi Chen', 'Yuji Zhang', 'Hongyi Du', 'Jiarui Yao', 'Xiaocheng Yang', 'Denghui Zhang', 'Yunzhu Li', 'Heng Ji']","Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench, a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15% average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40% fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies.",EscapeBench Advancing Creative Intelligence Language Model Agents Language model agents excel long session planning reasoning existing benchmarks primarily focus goal oriented tasks explicit objectives neglecting creative adaptation unfamiliar environments address introduce EscapeBench benchmark suite room escape game environments designed challenge agents creative reasoning unconventional tool use iterative problem solving uncover implicit goals results current LM models despite employing working memory Chain Thought reasoning achieve 15 average progress hints highlighting limitations creativity bridge gap propose EscapeAgent framework designed enhance creative reasoning Foresight innovative tool use Reflection identifying unsolved tasks Experiments EscapeAgent execute action chains 1 000 steps maintaining logical coherence navigates completes games 40 fewer steps hints performs robustly difficulty levels achieves higher action success rates efficient innovative puzzle solving strategies
42,BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving,"['Teng Wang', 'Wing Yin YU', 'Zhenqi He', 'Zehua Liu', 'HaileiGong', 'Han Wu', 'Xiongwei Han', 'Wei Shi', 'Ruifeng She', 'Fangzhou Zhu', 'Tao Zhong']",,BPP Search Enhancing Tree Thought Reasoning Mathematical Modeling Problem Solving
43,LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation,"['Jakub Šmíd', 'Pavel Priban', 'Pavel Kral']",,LACA Improving Cross lingual Aspect Based Sentiment Analysis LLM Data Augmentation
44,Fusing Highly Specialized Language Models for Comprehensive Expertise,"['Ning Ding', 'Yulin Chen', 'Ganqu Cui', 'Xingtai Lv', 'Weilin Zhao', 'Kaiyan Zhang', 'Ruobing Xie', 'Bowen Zhou', 'Zhiyuan Liu', 'Maosong Sun']",,Fusing Highly Specialized Language Models Comprehensive Expertise
45,HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases,"['Meng-Chieh Lee', 'Qi Zhu', 'Costas Mavromatis', 'Zhen Han', 'Soji Adeshina', 'Vassilis N. Ioannidis', 'Huzefa Rangwala', 'Christos Faloutsos']","Given a semi-structured knowledge base (SKB), where text documents are interconnected by relations, how can we effectively retrieve relevant information to answer user questions? Retrieval-Augmented Generation (RAG) retrieves documents to assist large language models (LLMs) in question answering; while Graph RAG (GRAG) uses structured knowledge bases as its knowledge source. However, many questions require both textual and relational information from SKB - referred to as ""hybrid"" questions - which complicates the retrieval process and underscores the need for a hybrid retrieval method that leverages both information. In this paper, through our empirical analysis, we identify key insights that show why existing methods may struggle with hybrid question answering (HQA) over SKB. Based on these insights, we propose HybGRAG for HQA consisting of a retriever bank and a critic module, with the following advantages: (1) Agentic, it automatically refines the output by incorporating feedback from the critic module, (2) Adaptive, it solves hybrid questions requiring both textual and relational information with the retriever bank, (3) Interpretable, it justifies decision making with intuitive refinement path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In experiments on the STaRK benchmark, HybGRAG achieves significant performance gains, with an average relative improvement in Hit@1 of 51%.",HybGRAG Hybrid Retrieval Augmented Generation Textual Relational Knowledge Bases Given semi structured knowledge base SKB text documents interconnected relations effectively retrieve relevant information answer user questions Retrieval Augmented Generation RAG retrieves documents assist large language models LLMs question answering Graph RAG GRAG uses structured knowledge bases knowledge source questions require textual relational information SKB referred hybrid questions complicates retrieval process underscores need hybrid retrieval method leverages information paper empirical analysis identify key insights existing methods struggle hybrid question answering HQA SKB Based insights propose HybGRAG HQA consisting retriever bank critic module following advantages 1 Agentic automatically refines output incorporating feedback critic module 2 Adaptive solves hybrid questions requiring textual relational information retriever bank 3 Interpretable justifies decision making intuitive refinement path 4 Effective surpasses baselines HQA benchmarks experiments STaRK benchmark HybGRAG achieves significant performance gains average relative improvement Hit 1 51
46,Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms,"['Rajvardhan Oak', 'Muhammad Haroon', 'Claire Wonjeong jo', 'Magdalena Wojcieszak', 'Anshuman Chhabra']","Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation.",ranking Using Large Language Models Mitigating Exposure Harmful Content Social Media Platforms Social media platforms utilize Machine Learning ML Artificial Intelligence AI powered recommendation algorithms maximize user engagement result inadvertent exposure harmful content Current moderation efforts reliant classifiers trained extensive human annotated data struggle scalability adapting new forms harm address challenges propose novel ranking approach using Large Language Models LLMs zero shot shot settings method dynamically assesses ranks content sequences effectively mitigating harmful content exposure requiring extensive labeled data Alongside traditional ranking metrics introduce new metrics evaluate effectiveness ranking reducing exposure harmful content experiments datasets models configurations demonstrate LLM based approach significantly outperforms existing proprietary moderation approaches offering scalable adaptable solution harm mitigation
47,Aligning AI Research with the Needs of Clinical Coding Workflows: Eight Recommendations Based on US Data Analysis and Critical Review,"['Yidong Gan', 'Maciej Rybinski', 'Ben Hachey', 'Jonathan K. Kummerfeld']","Clinical coding is crucial for healthcare billing and data analysis. Manual clinical coding is labour-intensive and error-prone, which has motivated research towards full automation of the process. However, our analysis, based on US English electronic health records and automated coding research using these records, shows that widely used evaluation methods are not aligned with real clinical contexts. For example, evaluations that focus on the top 50 most common codes are an oversimplification, as there are thousands of codes used in practice. This position paper aims to align AI coding research more closely with practical challenges of clinical coding. Based on our analysis, we offer eight specific recommendations, suggesting ways to improve current evaluation methods. Additionally, we propose new AI-based methods beyond automated coding, suggesting alternative approaches to assist clinical coders in their workflows.",Aligning AI Research Needs Clinical Coding Workflows Recommendations Based Data Analysis Critical Review Clinical coding crucial healthcare billing data analysis Manual clinical coding labour intensive error prone motivated research automation process analysis based English electronic health records automated coding research using records shows widely used evaluation methods aligned real clinical contexts example evaluations focus 50 common codes oversimplification thousands codes used practice position paper aims align AI coding research closely practical challenges clinical coding Based analysis offer specific recommendations suggesting ways improve current evaluation methods Additionally propose new AI based methods automated coding suggesting alternative approaches assist clinical coders workflows
48,MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection,"['Ziyan Liu', 'Chunxiao Fan', 'Haoran Lou', 'Yuexin Wu', 'Kaiwei Deng']","The rapid expansion of memes on social media has highlighted the urgent need for effective approaches to detect harmful content. However, traditional data-driven approaches struggle to detect new memes due to their evolving nature and the lack of up-to-date annotated data. To address this issue, we propose MIND, a multi-agent framework for zero-shot harmful meme detection that does not rely on annotated data. MIND implements three key strategies: 1) We retrieve similar memes from an unannotated reference set to provide contextual information. 2) We propose a bi-directional insight derivation mechanism to extract a comprehensive understanding of similar memes. 3) We then employ a multi-agent debate mechanism to ensure robust decision-making through reasoned arbitration. Extensive experiments on three meme datasets demonstrate that our proposed framework not only outperforms existing zero-shot approaches but also shows strong generalization across different model architectures and parameter scales, providing a scalable solution for harmful meme detection. The code is available at https://github.com/destroy-lonely/MIND.",MIND Multi agent Framework Zero shot Harmful Meme Detection rapid expansion memes social media highlighted urgent need effective approaches detect harmful content traditional data driven approaches struggle detect new memes evolving nature lack date annotated data address issue propose MIND multi agent framework zero shot harmful meme detection does rely annotated data MIND implements key strategies 1 retrieve similar memes unannotated reference set provide contextual information 2 propose bi directional insight derivation mechanism extract comprehensive understanding similar memes 3 employ multi agent debate mechanism ensure robust decision making reasoned arbitration Extensive experiments meme datasets demonstrate proposed framework outperforms existing zero shot approaches shows strong generalization different model architectures parameter scales providing scalable solution harmful meme detection code available https github com destroy lonely MIND
49,EvoWiki: Evaluating LLMs on Evolving Knowledge,"['Wei Tang', 'Yixin Cao', 'Yang Deng', 'Jiahao Ying', 'Bo Wang', 'Yizhe Yang', 'Yuyue Zhao', 'Qi Zhang', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Yong Liao']","Knowledge utilization is a critical aspect of LLMs, and understanding how they adapt to evolving knowledge is essential for their effective deployment. However, existing benchmarks are predominantly static, failing to capture the evolving nature of LLMs and knowledge, leading to inaccuracies and vulnerabilities such as contamination. In this paper, we introduce EvoWiki, an evolving dataset designed to reflect knowledge evolution by categorizing information into stable, evolved, and uncharted states. EvoWiki is fully auto-updatable, enabling precise evaluation of continuously changing knowledge and newly released LLMs. Through experiments with Retrieval-Augmented Generation (RAG) and Contunual Learning (CL), we evaluate how effectively LLMs adapt to evolving knowledge. Our results indicate that current models often struggle with evolved knowledge, frequently providing outdated or incorrect responses. Moreover, the dataset highlights a synergistic effect between RAG and CL, demonstrating their potential to better adapt to evolving knowledge. EvoWiki provides a robust benchmark for advancing future research on the knowledge evolution capabilities of large language models.",EvoWiki Evaluating LLMs Evolving Knowledge Knowledge utilization critical aspect LLMs understanding adapt evolving knowledge essential effective deployment existing benchmarks predominantly static failing capture evolving nature LLMs knowledge leading inaccuracies vulnerabilities contamination paper introduce EvoWiki evolving dataset designed reflect knowledge evolution categorizing information stable evolved uncharted states EvoWiki fully auto updatable enabling precise evaluation continuously changing knowledge newly released LLMs experiments Retrieval Augmented Generation RAG Contunual Learning CL evaluate effectively LLMs adapt evolving knowledge results indicate current models struggle evolved knowledge frequently providing outdated incorrect responses dataset highlights synergistic effect RAG CL demonstrating potential better adapt evolving knowledge EvoWiki provides robust benchmark advancing future research knowledge evolution capabilities large language models
50,Rethinking Repetition Problems of LLMs in Code Generation,"['Yihong Dong', 'Yuchen Liu', 'Xue Jiang', 'Zhi Jin', 'Ge Li']","With the advent of neural language models, the performance of code generation has been significantly boosted. However, the problem of repetitions during the generation process continues to linger. Previous work has primarily focused on content repetition, which is merely a fraction of the broader repetition problem in code generation. A more prevalent and challenging problem is structural repetition. In structural repetition, the repeated code appears in various patterns but possesses a fixed structure, which can be inherently reflected in grammar. In this paper, we formally define structural repetition and propose an efficient decoding approach called RPG, which stands for Repetition Penalization based on Grammar, to alleviate the repetition problems in code generation for LLMs. Specifically, RPG first leverages grammar rules to identify repetition problems during code generation, and then strategically decays the likelihood of critical tokens that contribute to repetitions, thereby mitigating them in code generation. To facilitate this study, we construct a new dataset CodeRepetEval to comprehensively evaluate approaches for mitigating the repetition problems in code generation. Extensive experimental results demonstrate that RPG substantially outperforms the best-performing baselines on CodeRepetEval dataset as well as HumanEval and MBPP benchmarks, effectively reducing repetitions and enhancing the quality of generated code.",Rethinking Repetition Problems LLMs Code Generation advent neural language models performance code generation significantly boosted problem repetitions generation process continues linger Previous work primarily focused content repetition merely fraction broader repetition problem code generation prevalent challenging problem structural repetition structural repetition repeated code appears various patterns possesses fixed structure inherently reflected grammar paper formally define structural repetition propose efficient decoding approach called RPG stands Repetition Penalization based Grammar alleviate repetition problems code generation LLMs Specifically RPG leverages grammar rules identify repetition problems code generation strategically decays likelihood critical tokens contribute repetitions mitigating code generation facilitate study construct new dataset CodeRepetEval comprehensively evaluate approaches mitigating repetition problems code generation Extensive experimental results demonstrate RPG substantially outperforms best performing baselines CodeRepetEval dataset HumanEval MBPP benchmarks effectively reducing repetitions enhancing quality generated code
51,PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension,"['Kun Ouyang', 'Yuanxin Liu', 'Shicheng Li', 'Yi Liu', 'Hao Zhou', 'Fandong Meng', 'Jie Zhou', 'Xu Sun']","Multimodal punchlines, which involve humor or sarcasm conveyed in image-caption pairs, are a popular way of communication on online multimedia platforms. With the rapid development of multimodal large language models (MLLMs), it is essential to assess their ability to effectively comprehend these punchlines. However, existing benchmarks on punchline comprehension suffer from three major limitations: 1) language shortcuts that allow models to solely rely on text, 2) lack of question diversity, and 3) narrow focus on a specific domain of multimodal content (e.g., cartoon). To address these limitations, we introduce a multimodal \textbf{Punch}line comprehension \textbf{Bench}mark, named \textbf{PunchBench}, which is tailored for accurate and comprehensive evaluation of punchline comprehension. To enhance the evaluation accuracy, we generate synonymous and antonymous captions by modifying original captions, which mitigates the impact of shortcuts in the captions. To provide a comprehensive evaluation, PunchBench incorporates diverse question formats and image-captions from various domains. On this basis, we conduct extensive evaluations and reveal a significant gap between state-of-the-art MLLMs and humans in punchline comprehension. To improve punchline comprehension, we propose Simple-to-Complex Chain-of-Question (SC-CoQ) strategy, enabling the models to incrementally address complicated questions by first mastering simple ones. SC-CoQ effectively enhances the performance of various MLLMs on PunchBench, surpassing in-context learning and chain-of-thought.",PunchBench Benchmarking MLLMs Multimodal Punchline Comprehension Multimodal punchlines involve humor sarcasm conveyed image caption pairs popular way communication online multimedia platforms rapid development multimodal large language models MLLMs essential assess ability effectively comprehend punchlines existing benchmarks punchline comprehension suffer major limitations 1 language shortcuts allow models solely rely text 2 lack question diversity 3 narrow focus specific domain multimodal content e g cartoon address limitations introduce multimodal textbf Punch line comprehension textbf Bench mark named textbf PunchBench tailored accurate comprehensive evaluation punchline comprehension enhance evaluation accuracy generate synonymous antonymous captions modifying original captions mitigates impact shortcuts captions provide comprehensive evaluation PunchBench incorporates diverse question formats image captions various domains basis conduct extensive evaluations reveal significant gap state art MLLMs humans punchline comprehension improve punchline comprehension propose Simple Complex Chain Question SC CoQ strategy enabling models incrementally address complicated questions mastering simple ones SC CoQ effectively enhances performance various MLLMs PunchBench surpassing context learning chain thought
52,ProcessBench: Identifying Process Errors in Mathematical Reasoning,"['Chujie Zheng', 'Zhenru Zhang', 'Beichen Zhang', 'Runji Lin', 'Keming Lu', 'Bowen Yu', 'Dayiheng Liu', 'Jingren Zhou', 'Junyang Lin']","As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.",ProcessBench Identifying Process Errors Mathematical Reasoning language models regularly make mistakes solving math problems automated identification errors reasoning process increasingly significant scalable oversight paper introduce ProcessBench measuring ability identify erroneous steps mathematical reasoning consists 3 400 test cases primarily focused competition Olympiad level math problems test case contains step step solution error location annotated human experts Models required identify earliest step contains error conclude steps correct conduct extensive evaluation ProcessBench involving types models process reward models PRMs critic models prompt general language models critique solution step step draw main observations 1 Existing PRMs typically fail generalize challenging math problems GSM8K MATH underperform critic models e prompted general language models trained PRM straightforwardly fine tuned PRM800K dataset 2 best open source model QwQ 32B Preview demonstrated critique capability competitive proprietary model GPT 4o despite lags reasoning specialized o1 mini hope ProcessBench foster future research reasoning process assessment paving way scalable oversight language models
53,Model Extrapolation Expedites Alignment,"['Chujie Zheng', 'Ziqi Wang', 'Heng Ji', 'Minlie Huang', 'Nanyun Peng']","Given the high computational cost of preference alignment training of large language models (LLMs), exploring efficient methods to reduce the training overhead remains an important and compelling research problem. Motivated by the observation that alignment training typically involves only small parameter changes without injecting new knowledge into models, we propose a straightforward method called ExPO (model extrapolation) to expedite LLMs' alignment with human preferences. Given a partially-trained model and its initial SFT checkpoint, ExPO improves the implicit optimization objective of alignment training by simply amplifying the parameter change based on a first-order approximation, without any additional training overhead. Through controlled experiments, we demonstrate that ExPO boosts a DPO model trained with only 20% steps to outperform the fully-trained one. Moreover, we show that ExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B parameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which highlights ExPO's broader utility in efficiently enhancing LLM alignment.",Model Extrapolation Expedites Alignment Given high computational cost preference alignment training large language models LLMs exploring efficient methods reduce training overhead remains important compelling research problem Motivated observation alignment training typically involves small parameter changes injecting new knowledge models propose straightforward method called ExPO model extrapolation expedite LLMs alignment human preferences Given partially trained model initial SFT checkpoint ExPO improves implicit optimization objective alignment training simply amplifying parameter change based order approximation additional training overhead controlled experiments demonstrate ExPO boosts DPO model trained 20 steps outperform fully trained ExPO notably improves existing open source LLMs ranging 1 8B 70B parameters leading AlpacaEval 2 0 MT Bench benchmarks highlights ExPO s broader utility efficiently enhancing LLM alignment
54,ATLANTIS: Weak-to-Strong Learning via Importance Sampling,"['Yi Liu', 'Guoyin Wang', 'Shicheng Li', 'Feifan Song', 'Xu Sun']",,ATLANTIS Weak Strong Learning Importance Sampling
55,MPVStance: Mitigating Hallucinations in Stance Detection with Multi-Perspective Verification,"['ZhaoDan Zhang', 'Zhao Zhang', 'Jin Zhang', 'Hui Xu', 'Xueqi Cheng']",,MPVStance Mitigating Hallucinations Stance Detection Multi Perspective Verification
56,Personality-Guided Code Generation Using Large Language Models,"['Yaoqi Guo', 'Zhenpeng Chen', 'Jie Zhang', 'Yang Liu', 'Yun Ma']",,Personality Guided Code Generation Using Large Language Models
57,PsyDT: Using LLMs to Construct the Digital Twin of Psychological Counselor with Personalized Counseling Style for Psychological Counseling,"['Haojie Xie', 'Yirong Chen', 'Xiaofen Xing', 'Jingkai Lin', 'Xiangmin Xu']","Currently, large language models (LLMs) have made significant progress in the field of psychological counseling. However, existing mental health LLMs overlook a critical issue where they do not consider the fact that different psychological counselors exhibit different personal styles, including linguistic style and therapy techniques, etc. As a result, these LLMs fail to satisfy the individual needs of clients who seek different counseling styles. To help bridge this gap, we propose PsyDT, a novel framework using LLMs to construct the Digital Twin of Psychological counselor with personalized counseling style. Compared to the time-consuming and costly approach of collecting a large number of real-world counseling cases to create a specific counselor's digital twin, our framework offers a faster and more cost-effective solution. To construct PsyDT, we utilize dynamic one-shot learning by using GPT-4 to capture counselor's unique counseling style, mainly focusing on linguistic style and therapy techniques. Subsequently, using existing single-turn long-text dialogues with client's questions, GPT-4 is guided to synthesize multi-turn dialogues of specific counselor. Finally, we fine-tune the LLMs on the synthetic dataset, PsyDTCorpus, to achieve the digital twin of psychological counselor with personalized counseling style. Experimental results indicate that our proposed PsyDT framework can synthesize multi-turn dialogues that closely resemble real-world counseling cases and demonstrate better performance compared to other baselines, thereby show that our framework can effectively construct the digital twin of psychological counselor with a specific counseling style.",PsyDT Using LLMs Construct Digital Twin Psychological Counselor Personalized Counseling Style Psychological Counseling Currently large language models LLMs significant progress field psychological counseling existing mental health LLMs overlook critical issue consider fact different psychological counselors exhibit different personal styles including linguistic style therapy techniques result LLMs fail satisfy individual needs clients seek different counseling styles help bridge gap propose PsyDT novel framework using LLMs construct Digital Twin Psychological counselor personalized counseling style Compared time consuming costly approach collecting large number real world counseling cases create specific counselor s digital twin framework offers faster cost effective solution construct PsyDT utilize dynamic shot learning using GPT 4 capture counselor s unique counseling style mainly focusing linguistic style therapy techniques Subsequently using existing single turn long text dialogues client s questions GPT 4 guided synthesize multi turn dialogues specific counselor Finally fine tune LLMs synthetic dataset PsyDTCorpus achieve digital twin psychological counselor personalized counseling style Experimental results indicate proposed PsyDT framework synthesize multi turn dialogues closely resemble real world counseling cases demonstrate better performance compared baselines framework effectively construct digital twin psychological counselor specific counseling style
58,BIPro: Zero-shot Chinese Poem Generation via Block Inverse Prompting Constrained Generation Framework,['Xu Zou'],"Recently, generative pre-trained models have made significant strides, particularly highlighted by the release of ChatGPT and GPT-4, which exhibit superior cross-domain capabilities. However, these models still face challenges on constrained writing tasks like poem generation under open-domain titles. In response to this challenge, we introduce Block Inverse Prompting (BIPro) constrained generation framework. BIPro leverages two block inverse prompting methods, revise and rewrite, that mimic the process of human text writing using block generative models. It significantly improves the zero-shot generation quality on the formidable constrained generation task of open-domain traditional-form Chinese poem generation. Based on a less powerful block generative model GLM-10B-Chinese, poems composed via BIPro without priming or additional training outperform both most advanced direct generative systems like GPT-4 or GLM-4 and best domain-specific systems such as Yusheng, Shisanbai, or Baidu Poetry Helper in human evaluation by proficient poets. Finally, BIPro considerably narrows the gap between AI-generated works and short-listed human literary arts in another human evaluation, unveiling the promising potential of block generative models in improving the quality of constrained generation.",BIPro Zero shot Chinese Poem Generation Block Inverse Prompting Constrained Generation Framework Recently generative pre trained models significant strides particularly highlighted release ChatGPT GPT 4 exhibit superior cross domain capabilities models face challenges constrained writing tasks like poem generation open domain titles response challenge introduce Block Inverse Prompting BIPro constrained generation framework BIPro leverages block inverse prompting methods revise rewrite mimic process human text writing using block generative models significantly improves zero shot generation quality formidable constrained generation task open domain traditional form Chinese poem generation Based powerful block generative model GLM 10B Chinese poems composed BIPro priming additional training outperform advanced direct generative systems like GPT 4 GLM 4 best domain specific systems Yusheng Shisanbai Baidu Poetry Helper human evaluation proficient poets Finally BIPro considerably narrows gap AI generated works short listed human literary arts human evaluation unveiling promising potential block generative models improving quality constrained generation
59,Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text Quality Filtering in Large Web Corpora,"['Yungi Kim', 'Hyunsoo Ha', 'Sukyung Lee', 'Jihoo Kim', 'Seonghoon Yang', 'Chanjun Park']","With the increasing demand for substantial amounts of high-quality data to train large language models (LLMs), efficiently filtering large web corpora has become a critical challenge. For this purpose, KenLM, a lightweight n-gram-based language model that operates on CPUs, is widely used. However, the traditional method of training KenLM utilizes only high-quality data and, consequently, does not explicitly learn the linguistic patterns of low-quality data. To address this issue, we propose an ensemble approach that leverages two contrasting KenLMs: (i) Good KenLM, trained on high-quality data; and (ii) Bad KenLM, trained on low-quality data. Experimental results demonstrate that our approach significantly reduces noisy content while preserving high-quality content compared to the traditional KenLM training method. This indicates that our method can be a practical solution with minimal computational overhead for resource-constrained environments.",Rethinking KenLM Good Bad Model Ensembles Efficient Text Quality Filtering Large Web Corpora increasing demand substantial amounts high quality data train large language models LLMs efficiently filtering large web corpora critical challenge purpose KenLM lightweight n gram based language model operates CPUs widely used traditional method training KenLM utilizes high quality data consequently does explicitly learn linguistic patterns low quality data address issue propose ensemble approach leverages contrasting KenLMs Good KenLM trained high quality data ii Bad KenLM trained low quality data Experimental results demonstrate approach significantly reduces noisy content preserving high quality content compared traditional KenLM training method indicates method practical solution minimal computational overhead resource constrained environments
60,Automatic detection of dyslexia based on eye movements during reading in Russian,"['Anna Laurinavichyute', 'Anastasiya Lopukhina', 'David Robert Reich']",,Automatic detection dyslexia based eye movements reading Russian
61,"LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating","['CHAO DENG', 'Jiale Yuan', 'Pi Bu', 'Peijie Wang', 'Zhong-Zhi Li', 'Jian Xu', 'Xiao-Hui Li', 'Yuan Gao', 'Jun Song', 'Bo Zheng', 'Cheng-Lin Liu']","Large vision language models (LVLMs) have improved the document understanding capabilities remarkably, enabling the handling of complex document elements, longer contexts, and a wider range of tasks. However, existing document understanding benchmarks have been limited to handling only a small number of pages and fail to provide a comprehensive analysis of layout elements locating. In this paper, we first define three primary task categories: Long Document Understanding, numerical Reasoning, and cross-element Locating, and then propose a comprehensive benchmark, LongDocURL, integrating above three primary tasks and comprising 20 sub-tasks categorized based on different primary tasks and answer evidences. Furthermore, we develop a semi-automated construction pipeline and collect 2,325 high-quality question-answering pairs, covering more than 33,000 pages of documents, significantly outperforming existing benchmarks. Subsequently, we conduct comprehensive evaluation experiments on both open-source and closed-source models across 26 different configurations, revealing critical performance gaps in this field.",LongDocURL Comprehensive Multimodal Long Document Benchmark Integrating Understanding Reasoning Locating Large vision language models LVLMs improved document understanding capabilities remarkably enabling handling complex document elements longer contexts wider range tasks existing document understanding benchmarks limited handling small number pages fail provide comprehensive analysis layout elements locating paper define primary task categories Long Document Understanding numerical Reasoning cross element Locating propose comprehensive benchmark LongDocURL integrating primary tasks comprising 20 sub tasks categorized based different primary tasks answer evidences Furthermore develop semi automated construction pipeline collect 2 325 high quality question answering pairs covering 33 000 pages documents significantly outperforming existing benchmarks Subsequently conduct comprehensive evaluation experiments open source closed source models 26 different configurations revealing critical performance gaps field
62,ObfusLM: Privacy-preserving Language Model Service against Embedding Inversion Attacks,"['Yu Lin', 'Ruining Yang', 'Yunlong Mao', 'Qizhi Zhang', 'Jue Hong', 'Quanwei Cai', 'Ye Wu', 'Huiqi Liu', 'zhiyu chen', 'Bing Duan', 'Sheng Zhong']",,ObfusLM Privacy preserving Language Model Service Embedding Inversion Attacks
63,Interlocking-free Selective Rationalization Through Genetic-based Learning,"['Federico Ruggeri', 'Gaetano Signorelli']","A popular end-to-end architecture for selective rationalization is the select-then-predict pipeline, comprising a generator to extract highlights fed to a predictor. Such a cooperative system suffers from suboptimal equilibrium minima due to the dominance of one of the two modules, a phenomenon known as interlocking. While several contributions aimed at addressing interlocking, they only mitigate its effect, often by introducing feature-based heuristics, sampling, and ad-hoc regularizations. We present GenSPP, the first interlocking-free architecture for selective rationalization that does not require any learning overhead, as the above-mentioned. GenSPP avoids interlocking by performing disjoint training of the generator and predictor via genetic global search. Experiments on a synthetic and a real-world benchmark show that our model outperforms several state-of-the-art competitors.",Interlocking free Selective Rationalization Genetic based Learning popular end end architecture selective rationalization select predict pipeline comprising generator extract highlights fed predictor cooperative suffers suboptimal equilibrium minima dominance modules phenomenon known interlocking contributions aimed addressing interlocking mitigate effect introducing feature based heuristics sampling ad hoc regularizations present GenSPP interlocking free architecture selective rationalization does require learning overhead mentioned GenSPP avoids interlocking performing disjoint training generator predictor genetic global search Experiments synthetic real world benchmark model outperforms state art competitors
64,Re-identification of De-identified Documents with Autoregressive Infilling,"['Lucas Georges Gabriel Charpentier', 'Pierre Lison']","Documents revealing sensitive information about individuals must typically be de-identified. This de-identification is often done by masking all mentions of personally identifiable information (PII), thereby making it more difficult to uncover the identity of the person(s) in question. To investigate the robustness of de-identification methods, we present a novel, RAG-inspired approach that attempts the reverse process of re-identification based on a database of documents representing background knowledge. Given a text in which personal identifiers have been masked, the re-identification proceeds in two steps. A retriever first selects from the background knowledge passages deemed relevant for the re-identification. Those passages are then provided to an infilling model which seeks to infer the original content of each text span. This process is repeated until all masked spans are replaced. We evaluate the re-identification on three datasets (Wikipedia biographies, court rulings and clinical notes). Results show that (1) as many as 80% of de-identified text spans can be successfully recovered and (2) the re-identification accuracy increases along with the level of background knowledge.",identification identified Documents Autoregressive Infilling Documents revealing sensitive information individuals typically identified identification masking mentions personally identifiable information PII making difficult uncover identity person s question investigate robustness identification methods present novel RAG inspired approach attempts reverse process identification based database documents representing background knowledge Given text personal identifiers masked identification proceeds steps retriever selects background knowledge passages deemed relevant identification passages provided infilling model seeks infer original content text span process repeated masked spans replaced evaluate identification datasets Wikipedia biographies court rulings clinical notes Results 1 80 identified text spans successfully recovered 2 identification accuracy increases level background knowledge
65,Modeling Uncertainty in Composed Image Retrieval via Probabilistic Embedding,"['Haomiao Tang', 'Jinpeng Wang', 'Yuang Peng', 'GuangHao Meng', 'Ruisheng Luo', 'Bin Chen', 'Long Chen', 'Yaowei Wang', 'Shu-Tao Xia']",,Modeling Uncertainty Composed Image Retrieval Probabilistic Embedding
66,Untie the Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models,"['Junfeng Tian', 'Da Zheng', 'Yang Chen', 'Rui Wang', 'colin zhang', 'Debing Zhang']","Large language models (LLM) have prioritized expanding the context window from which models can incorporate more information. However, training models to handle long contexts presents significant challenges. These include the scarcity of high-quality natural long-context data, the potential for performance degradation on short-context tasks, and the reduced training efficiency associated with attention mechanisms. In this paper, we introduce Untie the Knots (\textbf{UtK}), a novel data augmentation strategy employed during the continue pre-training phase, designed to efficiently enable LLMs to gain long-context capabilities without the need to modify the existing data mixture. In particular, we chunk the documents, shuffle the chunks, and create a complex and knotted structure of long texts; LLMs are then trained to untie these knots and identify relevant segments within seemingly chaotic token sequences. This approach greatly improves the model's performance by accurately attending to relevant information in long context and the training efficiency is also largely increased. We conduct extensive experiments on models with 7B and 72B parameters, trained on 20 billion tokens, demonstrating that UtK achieves 75\% and 84.5\% accurracy on RULER at 128K context length, significantly outperforming other long context strategies. The trained models will open-source for further research.",Untie Knots Efficient Data Augmentation Strategy Long Context Pre Training Language Models Large language models LLM prioritized expanding context window models incorporate information training models handle long contexts presents significant challenges include scarcity high quality natural long context data potential performance degradation short context tasks reduced training efficiency associated attention mechanisms paper introduce Untie Knots textbf UtK novel data augmentation strategy employed continue pre training phase designed efficiently enable LLMs gain long context capabilities need modify existing data mixture particular chunk documents shuffle chunks create complex knotted structure long texts LLMs trained untie knots identify relevant segments seemingly chaotic token sequences approach greatly improves model s performance accurately attending relevant information long context training efficiency largely increased conduct extensive experiments models 7B 72B parameters trained 20 billion tokens demonstrating UtK achieves 75 84 5 accurracy RULER 128K context length significantly outperforming long context strategies trained models open source research
67,Doc-React: Multi-page Heterogeneous Document Question-answering,"['Junda Wu', 'Yu Xia', 'Tong Yu', 'Xiang Chen', 'Sai Sree Harsha', 'Akash V Maharaj', 'Ruiyi Zhang', 'Victor Bursztyn', 'Sungchul Kim', 'Ryan A. Rossi', 'Julian McAuley', 'Yunyao Li', 'Ritwik Sinha']",,Doc React Multi page Heterogeneous Document Question answering
68,CECT dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT,"['Mikołaj Pokrywka', 'Wojciech Kusa', 'Mieszko Rutkowski', 'Mikołaj Koszowski']",,CECT dataset Overcoming Data Scarcity Context Aware E Commerce MT
69,APPL: A Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts,"['Honghua Dong', 'Qidong Su', 'Yubo Gao', 'Zhaoyu Li', 'Yangjun Ruan', 'Gennady Pekhimenko', 'Chris J. Maddison', 'Xujie Si']","Large Language Models (LLMs) have become increasingly capable of handling diverse tasks with the aid of well-crafted prompts and integration of external tools, but as task complexity rises, the workflow involving LLMs can be complicated and thus challenging to implement and maintain. To address this challenge, we propose APPL, A Prompt Programming Language that acts as a bridge between computer programs and LLMs, allowing seamless embedding of prompts into Python functions, and vice versa. APPL provides an intuitive and Python-native syntax, an efficient parallelized runtime with asynchronous semantics, and a tracing module supporting effective failure diagnosis and replaying without extra costs. We demonstrate that APPL programs are intuitive, concise, and efficient through three representative scenarios: Chain-of-Thought with self-consistency (CoT-SC), ReAct tool use agent, and multi-agent chat. Experiments on three parallelizable workflows further show that APPL can effectively parallelize independent LLM calls, with a significant speedup ratio that almost matches the estimation.",APPL Prompt Programming Language Harmonious Integration Programs Large Language Model Prompts Large Language Models LLMs increasingly capable handling diverse tasks aid crafted prompts integration external tools task complexity rises workflow involving LLMs complicated challenging implement maintain address challenge propose APPL Prompt Programming Language acts bridge computer programs LLMs allowing seamless embedding prompts Python functions vice versa APPL provides intuitive Python native syntax efficient parallelized runtime asynchronous semantics tracing module supporting effective failure diagnosis replaying extra costs demonstrate APPL programs intuitive concise efficient representative scenarios Chain Thought self consistency CoT SC ReAct tool use agent multi agent chat Experiments parallelizable workflows APPL effectively parallelize independent LLM calls significant speedup ratio matches estimation
70,A Measure of the System Dependence of Automated Metrics,"['Pius von Däniken', 'Jan Milan Deriu', 'Mark Cieliebak']","Automated metrics for Machine Translation have made significant progress, with the goal of replacing expensive and time-consuming human evaluations. These metrics are typically assessed by their correlation with human judgments, which captures the monotonic relationship between human and metric scores. However, we argue that it is equally important to ensure that metrics treat all systems fairly and consistently. In this paper, we introduce a method to evaluate this aspect.",Measure Dependence Automated Metrics Automated metrics Machine Translation significant progress goal replacing expensive time consuming human evaluations metrics typically assessed correlation human judgments captures monotonic relationship human metric scores argue equally important ensure metrics treat systems fairly consistently paper introduce method evaluate aspect
71,Evaluating Lexical Proficiency in Neural Language Models,"['Cristiano Ciaccio', 'Alessio Miaschi', 'Felice Dell’Orletta']",,Evaluating Lexical Proficiency Neural Language Models
72,Autoregressive Speech Synthesis without Vector Quantization,"['Lingwei Meng', 'Long Zhou', 'Shujie LIU', 'Sanyuan Chen', 'Bing Han', 'Shujie HU', 'Yanqing Liu', 'Jinyu Li', 'sheng zhao', 'Xixin Wu', 'Helen M. Meng', 'Furu Wei']","We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.",Autoregressive Speech Synthesis Vector Quantization present MELLE novel continuous valued token based language modeling approach text speech synthesis TTS MELLE autoregressively generates continuous mel spectrogram frames directly text condition bypassing need vector quantization typically designed audio compression sacrifices fidelity compared continuous representations Specifically instead cross entropy loss apply regression loss proposed spectrogram flux loss function model probability distribution continuous valued tokens ii incorporated variational inference MELLE facilitate sampling mechanisms enhancing output diversity model robustness Experiments demonstrate compared stage codec language model VALL E variants single stage MELLE mitigates robustness issues avoiding inherent flaws sampling vector quantized codes achieves superior performance multiple metrics importantly offers streamlined paradigm demos work provided https aka ms melle
73,Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM’s Nest,"['Letian Peng', 'Zilong Wang', 'Feng Yao', 'Jingbo Shang']",,Cuckoo Free Rider Hatched Massive Nutrition LLM s Nest
74,FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Large Language Models,"['Raghav Singhal', 'Kaustubh Ponkshe', 'Praneeth Vepakomma']",,FedEx LoRA Exact Aggregation Federated Efficient Fine Tuning Large Language Models
75,Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality,"['Rahul Zalkikar', 'Kanchan Chandra']","Transformer language models have achieved state-of-the-art performance for a variety of natural language tasks but have been shown to encode unwanted biases. We evaluate the social biases encoded by transformers trained with the masked language modeling objective using proposed proxy functions within an iterative masking experiment to measure the quality of transformer models' predictions and assess the preference of MLMs towards disadvantaged and advantaged groups. We find all models encode concerning social biases. We compare bias estimations with those produced by other evaluation methods using benchmark datasets and assess their alignment with human annotated biases. We extend previous work by evaluating social biases introduced after retraining an MLM under the masked language modeling objective and find proposed measures produce more accurate and sensitive estimations of biases introduced by retraining MLMs based on relative preference for biased sentences between models, while other methods tend to underestimate biases after retraining on sentences biased towards disadvantaged groups.",Measuring Social Biases Masked Language Models Proxy Prediction Quality Transformer language models achieved state art performance variety natural language tasks shown encode unwanted biases evaluate social biases encoded transformers trained masked language modeling objective using proposed proxy functions iterative masking experiment measure quality transformer models predictions assess preference MLMs disadvantaged advantaged groups models encode concerning social biases compare bias estimations produced evaluation methods using benchmark datasets assess alignment human annotated biases extend previous work evaluating social biases introduced retraining MLM masked language modeling objective proposed measures produce accurate sensitive estimations biases introduced retraining MLMs based relative preference biased sentences models methods tend underestimate biases retraining sentences biased disadvantaged groups
76,Capturing Author Self Beliefs in Social Media Language,"['Siddharth Mangalik', 'Adithya V Ganesan', 'Abigail B. Wheeler', 'Nicholas Kerry', 'Jeremy D. W. Clifton', 'H. Schwartz', 'Ryan L. Boyd']",,Capturing Author Self Beliefs Social Media Language
77,Neural Topic Modeling with Large Language Models in the Loop,"['Xiaohao Yang', 'He Zhao', 'Weijie Xu', 'YUANYUAN QI', 'Jueqing Lu', 'Dinh Phung', 'Lan Du']","Topic modeling is a fundamental task in natural language processing, allowing the discovery of latent thematic structures in text corpora. While Large Language Models (LLMs) have demonstrated promising capabilities in topic discovery, their direct application to topic modeling suffers from issues such as incomplete topic coverage, misalignment of topics, and inefficiency. To address these limitations, we propose LLM-ITL, a novel LLM-in-the-loop framework that integrates LLMs with Neural Topic Models (NTMs). In LLM-ITL, global topics and document representations are learned through the NTM. Meanwhile, an LLM refines these topics using an Optimal Transport (OT)-based alignment objective, where the refinement is dynamically adjusted based on the LLM's confidence in suggesting topical words for each set of input words. With the flexibility of being integrated into many existing NTMs, the proposed approach enhances the interpretability of topics while preserving the efficiency of NTMs in learning topics and document representations. Extensive experiments demonstrate that LLM-ITL helps NTMs significantly improve their topic interpretability while maintaining the quality of document representation. Our code and datasets are available at https://github.com/Xiaohao-Yang/LLM-ITL",Neural Topic Modeling Large Language Models Loop Topic modeling fundamental task natural language processing allowing discovery latent thematic structures text corpora Large Language Models LLMs demonstrated promising capabilities topic discovery direct application topic modeling suffers issues incomplete topic coverage misalignment topics inefficiency address limitations propose LLM ITL novel LLM loop framework integrates LLMs Neural Topic Models NTMs LLM ITL global topics document representations learned NTM LLM refines topics using Optimal Transport OT based alignment objective refinement dynamically adjusted based LLM s confidence suggesting topical words set input words flexibility integrated existing NTMs proposed approach enhances interpretability topics preserving efficiency NTMs learning topics document representations Extensive experiments demonstrate LLM ITL helps NTMs significantly improve topic interpretability maintaining quality document representation code datasets available https github com Xiaohao Yang LLM ITL
78,HALoGEN: Fantastic LLM Hallucinations and Where to Find Them,"['Abhilasha Ravichander', 'Shrusti Ghela', 'David Wadden', 'Yejin Choi']","Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type B errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models.",HALoGEN Fantastic LLM Hallucinations Despite impressive ability generate high quality fluent text generative large language models LLMs produce hallucinations statements misaligned established world knowledge provided input context measuring hallucination challenging having humans verify model generations fly expensive time consuming work release HALoGEN comprehensive hallucination benchmark consisting 1 10 923 prompts generative models spanning domains including programming scientific attribution summarization 2 automatic high precision verifiers use case decompose LLM generations atomic units verify unit high quality knowledge source use framework evaluate 150 000 generations 14 language models finding best performing models riddled hallucinations 86 generated atomic facts depending domain define novel error classification LLM hallucinations based likely stem incorrect recollection training data Type errors incorrect knowledge training data Type B errors fabrication Type C errors hope framework provides foundation enable principled study generative models hallucinate advances development trustworthy large language models
79,Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection,"['Shuguo Hu', 'Jun Hu', 'Huaiwen Zhang']","Large Language Models (LLMs) can assist multimodal fake news detection by predicting pseudo labels. However, LLM-generated pseudo labels alone demonstrate poor performance compared to traditional detection methods, making their effective integration non-trivial. In this paper, we propose Global Label Propagation Network with LLM-based Pseudo Labeling (GLPN-LLM) for multimodal fake news detection, which integrates LLM capabilities via label propagation techniques. The global label propagation can utilize LLM-generated pseudo labels, enhancing prediction accuracy by propagating label information among all samples. For label propagation, a mask-based mechanism is designed to prevent label leakage during training by ensuring that training nodes do not propagate their own labels back to themselves. Experimental results on benchmark datasets show that by synergizing LLMs with label propagation, our model achieves superior performance over state-of-the-art baselines.",Synergizing LLMs Global Label Propagation Multimodal Fake News Detection Large Language Models LLMs assist multimodal fake news detection predicting pseudo labels LLM generated pseudo labels demonstrate poor performance compared traditional detection methods making effective integration non trivial paper propose Global Label Propagation Network LLM based Pseudo Labeling GLPN LLM multimodal fake news detection integrates LLM capabilities label propagation techniques global label propagation utilize LLM generated pseudo labels enhancing prediction accuracy propagating label information samples label propagation mask based mechanism designed prevent label leakage training ensuring training nodes propagate labels Experimental results benchmark datasets synergizing LLMs label propagation model achieves superior performance state art baselines
80,"“Yes, My LoRD.” Guiding Language Model Extraction with Locality Reinforced Distillation","['Zi Liang', 'Qingqing Ye', 'Yanyun Wang', 'Sen Zhang', 'Yaxin Xiao', 'RongHua Li', 'Jianliang Xu', 'Haibo Hu']",,Yes LoRD Guiding Language Model Extraction Locality Reinforced Distillation
81,Jailbreak Large Vision-Language Models Through Multi-Modal Linkage,"['Yu Wang', 'Xiaofei Zhou', 'Yichen Wang', 'Geyuan Zhang', 'Tianxing He']","With the significant advancement of Large Vision-Language Models (VLMs), concerns about their potential misuse and abuse have grown rapidly. Previous studies have highlighted VLMs' vulnerability to jailbreak attacks, where carefully crafted inputs can lead the model to produce content that violates ethical and legal standards. However, existing methods struggle against state-of-the-art VLMs like GPT-4o, due to the over-exposure of harmful content and lack of stealthy malicious guidance. In this work, we propose a novel jailbreak attack framework: Multi-Modal Linkage (MML) Attack. Drawing inspiration from cryptography, MML utilizes an encryption-decryption process across text and image modalities to mitigate over-exposure of malicious information. To align the model's output with malicious intent covertly, MML employs a technique called ""evil alignment"", framing the attack within a video game production scenario. Comprehensive experiments demonstrate MML's effectiveness. Specifically, MML jailbreaks GPT-4o with attack success rates of 97.80% on SafeBench, 98.81% on MM-SafeBench and 99.07% on HADES-Dataset. Our code is available at https://github.com/wangyu-ovo/MML.",Jailbreak Large Vision Language Models Multi Modal Linkage significant advancement Large Vision Language Models VLMs concerns potential misuse abuse grown rapidly Previous studies highlighted VLMs vulnerability jailbreak attacks carefully crafted inputs lead model produce content violates ethical legal standards existing methods struggle state art VLMs like GPT 4o exposure harmful content lack stealthy malicious guidance work propose novel jailbreak attack framework Multi Modal Linkage MML Attack Drawing inspiration cryptography MML utilizes encryption decryption process text image modalities mitigate exposure malicious information align model s output malicious intent covertly MML employs technique called evil alignment framing attack video game production scenario Comprehensive experiments demonstrate MML s effectiveness Specifically MML jailbreaks GPT 4o attack success rates 97 80 SafeBench 98 81 MM SafeBench 99 07 HADES Dataset code available https github com wangyu ovo MML
82,"Wait, that’s not an option: LLMs Robustness with Incorrect Multiple-Choice Options","['Gracjan Góral', 'Emilia Wiśnios', 'Piotr Sankowski', 'Paweł Budzianowski']",,Wait s option LLMs Robustness Incorrect Multiple Choice Options
83,The Hidden Attention of Mamba Models,"['Ameen Ali Ali', 'Itamar Zimerman', 'Lior Wolf']",,Hidden Attention Mamba Models
84,KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding,"['Shi Luohe', 'Zuchao Li', 'Lefei Zhang', 'Baoyuan Qi', 'Liu Guoming', 'hai zhao']",,KV Latent Dimensional level KV Cache Reduction Frequency aware Rotary Positional Embedding
85,LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models,"['YAN WANG', 'Ling Ding', 'Tien N Nguyen', 'Shaohua Wang', 'Yanan Zheng']","Large Language Models for code often entail significant computational complexity, which grows significantly with the length of the input code sequence. We propose LeanCode for code simplification to reduce training and prediction time, leveraging code contexts in utilizing attention scores to represent the tokens' importance. We advocate for the selective removal of tokens based on the average context-aware attention scores rather than average scores across all inputs. LeanCode uses the attention scores of `CLS' tokens within the encoder for classification tasks, such as code search. It also employs the encoder-decoder attention scores to determine token significance for sequence-to-sequence tasks like code summarization. Our evaluation shows LeanCode's superiority over the SOTAs DietCode and Slimcode, with improvements of 60% and 16% for code search, and 29% and 27% for code summarization, respectively.",LEANCODE Understanding Models Better Code Simplification Pre trained Large Language Models Large Language Models code entail significant computational complexity grows significantly length input code sequence propose LeanCode code simplification reduce training prediction time leveraging code contexts utilizing attention scores represent tokens importance advocate selective removal tokens based average context aware attention scores average scores inputs LeanCode uses attention scores CLS tokens encoder classification tasks code search employs encoder decoder attention scores determine token significance sequence sequence tasks like code summarization evaluation shows LeanCode s superiority SOTAs DietCode Slimcode improvements 60 16 code search 29 27 code summarization respectively
86,MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset,"['Weiqi Wang', 'Yangqiu Song']","To enable Large Language Models (LLMs) to function as conscious agents with generalizable reasoning capabilities, it is crucial that they possess the reasoning ability to comprehend situational changes (transitions) in distribution triggered by environmental factors or actions from other agents. Despite its fundamental significance, this ability remains underexplored due to the complexity of modeling infinite possible changes in an event and their associated distributions, coupled with the lack of benchmark data with situational transitions. Addressing these gaps, we propose a novel formulation of reasoning with distributional changes as a three-step discriminative process, termed as MetAphysical ReaSoning. We then introduce the first-ever benchmark, MARS, comprising three tasks corresponding to each step. These tasks systematically assess LLMs' capabilities in reasoning the plausibility of (i) changes in actions, (ii) states caused by changed actions, and (iii) situational transitions driven by changes in action. Extensive evaluations with 20 (L)LMs of varying sizes and methods indicate that all three tasks in this process pose significant challenges, even for state-of-the-art LLMs and LMs after fine-tuning. Further analyses reveal potential causes for the underperformance of LLMs and demonstrate that pre-training them on large-scale conceptualization taxonomies can potentially enhance their metaphysical reasoning capabilities. Our data and models are publicly accessible at https://github.com/HKUST-KnowComp/MARS.",MARS Benchmarking Metaphysical Reasoning Abilities Language Models Multi task Evaluation Dataset enable Large Language Models LLMs function conscious agents generalizable reasoning capabilities crucial possess reasoning ability comprehend situational changes transitions distribution triggered environmental factors actions agents Despite fundamental significance ability remains underexplored complexity modeling infinite possible changes event associated distributions coupled lack benchmark data situational transitions Addressing gaps propose novel formulation reasoning distributional changes step discriminative process termed MetAphysical ReaSoning introduce benchmark MARS comprising tasks corresponding step tasks systematically assess LLMs capabilities reasoning plausibility changes actions ii states caused changed actions iii situational transitions driven changes action Extensive evaluations 20 L LMs varying sizes methods indicate tasks process pose significant challenges state art LLMs LMs fine tuning analyses reveal potential causes underperformance LLMs demonstrate pre training large scale conceptualization taxonomies potentially enhance metaphysical reasoning capabilities data models publicly accessible https github com HKUST KnowComp MARS
87,Ask-Before-Detection: Identifying and Mitigating Conformity Bias in LLM-Powered Error Detector for Math Word Problem Solutions,"['Hang Li', 'Tianlong Xu', 'Kaiqi Yang', 'Yucheng Chu', 'Yanling Chen', 'Yichi Song', 'Qingsong Wen', 'Hui Liu']","The rise of large language models (LLMs) offers new opportunities for automatic error detection in education, particularly for math word problems (MWPs). While prior studies demonstrate the promise of LLMs as error detectors, they overlook the presence of multiple valid solutions for a single MWP. Our preliminary analysis reveals a significant performance gap between conventional and alternative solutions in MWPs, a phenomenon we term conformity bias in this work. To mitigate this bias, we introduce the Ask-Before-Detect (AskBD) framework, which generates adaptive reference solutions using LLMs to enhance error detection. Experiments on 200 examples of GSM8K show that AskBD effectively mitigates bias and improves performance, especially when combined with reasoning-enhancing techniques like chain-of-thought prompting.",Ask Detection Identifying Mitigating Conformity Bias LLM Powered Error Detector Math Word Problem Solutions rise large language models LLMs offers new opportunities automatic error detection education particularly math word problems MWPs prior studies demonstrate promise LLMs error detectors overlook presence multiple valid solutions single MWP preliminary analysis reveals significant performance gap conventional alternative solutions MWPs phenomenon term conformity bias work mitigate bias introduce Ask Detect AskBD framework generates adaptive reference solutions using LLMs enhance error detection Experiments 200 examples GSM8K AskBD effectively mitigates bias improves performance especially combined reasoning enhancing techniques like chain thought prompting
88,Real-time Fake News from Adversarial Feedback,"['Sanxing Chen', 'Yukun Huang', 'Bhuwan Dhingra']","We show that existing evaluations for fake news detection based on conventional sources, such as claims on fact-checking websites, result in high accuracies over time for LLM-based detectors -- even after their knowledge cutoffs. This suggests that recent popular fake news from such sources can be easily detected due to pre-training and retrieval corpus contamination or increasingly salient shallow patterns. Instead, we argue that a proper fake news detection dataset should test a model's ability to reason factually about the current world by retrieving and reading related evidence. To this end, we develop a novel pipeline that leverages natural language feedback from a RAG-based detector to iteratively modify real-time news into deceptive fake news that challenges LLMs. Our iterative rewrite decreases the binary classification ROC-AUC by an absolute 17.5 percent for a strong RAG-based GPT-4o detector. Our experiments reveal the important role of RAG in both detecting and generating fake news, as retrieval-free LLM detectors are vulnerable to unseen events and adversarial attacks, while feedback from RAG detection helps discover more deceitful patterns in fake news.",Real time Fake News Adversarial Feedback existing evaluations fake news detection based conventional sources claims fact checking websites result high accuracies time LLM based detectors knowledge cutoffs suggests recent popular fake news sources easily detected pre training retrieval corpus contamination increasingly salient shallow patterns Instead argue proper fake news detection dataset test model s ability reason factually current world retrieving reading related evidence end develop novel pipeline leverages natural language feedback RAG based detector iteratively modify real time news deceptive fake news challenges LLMs iterative rewrite decreases binary classification ROC AUC absolute 17 5 percent strong RAG based GPT 4o detector experiments reveal important role RAG detecting generating fake news retrieval free LLM detectors vulnerable unseen events adversarial attacks feedback RAG detection helps discover deceitful patterns fake news
89,Improve Vision Language Model Chain-of-thought Reasoning,"['Ruohong Zhang', 'Bowen Zhang', 'Yanghao Li', 'Haotian Zhang', 'Zhiqing Sun', 'Zhe Gan', 'Yinfei Yang', 'Ruoming Pang', 'Yiming Yang']","Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.",Improve Vision Language Model Chain thought Reasoning Chain thought CoT reasoning vision language models VLMs crucial improving interpretability trustworthiness current training recipes lack robust CoT reasoning data relying datasets dominated short annotations minimal rationales work training VLM short answers does generalize reasoning tasks require detailed responses address propose fold approach distill rationales GPT 4o model enrich training data fine tune VLMs boosting CoT performance Second apply reinforcement learning calibrate reasoning quality Specifically construct positive correct negative incorrect pairs model generated reasoning chains comparing predictions annotated short answers Using pairwise data apply Direct Preference Optimization algorithm refine model s reasoning abilities experiments demonstrate significant improvements CoT reasoning benchmark datasets better generalization direct answer prediction work emphasizes importance incorporating detailed rationales training leveraging reinforcement learning strengthen reasoning capabilities VLMs
90,On the Mutual Influence of Gender and Occupation in LLM Representations,"['Haozhe An', 'Connor Baumler', 'Abhilasha Sancheti', 'Rachel Rudinger']","We examine LLM representations of gender for first names in various occupational contexts to study how occupations and the gender perception of first names in LLMs influence each other mutually. We find that LLMs' first-name gender representations correlate with real-world gender statistics associated with the name, and are influenced by the co-occurrence of stereotypically feminine or masculine occupations. Additionally, we study the influence of first-name gender representations on LLMs in a downstream occupation prediction task and their potential as an internal metric to identify extrinsic model biases. While feminine first-name embeddings often raise the probabilities for female-dominated jobs (and vice versa for male-dominated jobs), reliably using these internal gender representations for bias detection remains challenging.",Mutual Influence Gender Occupation LLM Representations examine LLM representations gender names various occupational contexts study occupations gender perception names LLMs influence mutually LLMs gender representations correlate real world gender statistics associated influenced occurrence stereotypically feminine masculine occupations Additionally study influence gender representations LLMs downstream occupation prediction task potential internal metric identify extrinsic model biases feminine embeddings raise probabilities female dominated jobs vice versa male dominated jobs reliably using internal gender representations bias detection remains challenging
91,Disentangling Memory and Reasoning Ability in Large Language Models,"['Mingyu Jin', 'Weidi Luo', 'Sitao Cheng', 'Xinyi Wang', 'Wenyue Hua', 'Ruixiang Tang', 'William Yang Wang', 'Yongfeng Zhang']","Large Language Models (LLMs) have demonstrated strong performance in handling complex tasks requiring both extensive knowledge and reasoning abilities. However, the existing LLM inference pipeline operates as an opaque process without explicit separation between knowledge retrieval and reasoning steps, making the model's decision-making process unclear and disorganized. This ambiguity can lead to issues such as hallucinations and knowledge forgetting, which significantly impact the reliability of LLMs in high-stakes domains. In this paper, we propose a new inference paradigm that decomposes the complex inference process into two distinct and clear actions: (1) memory recall: which retrieves relevant knowledge, and (2) reasoning: which performs logical steps based on the recalled knowledge. To facilitate this decomposition, we introduce two special tokens memory and reason, guiding the model to distinguish between steps that require knowledge retrieval and those that involve reasoning. Our experiment results show that this decomposition not only improves model performance but also enhances the interpretability of the inference process, enabling users to identify sources of error and refine model responses effectively. The code is available at https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.",Disentangling Memory Reasoning Ability Large Language Models Large Language Models LLMs demonstrated strong performance handling complex tasks requiring extensive knowledge reasoning abilities existing LLM inference pipeline operates opaque process explicit separation knowledge retrieval reasoning steps making model s decision making process unclear disorganized ambiguity lead issues hallucinations knowledge forgetting significantly impact reliability LLMs high stakes domains paper propose new inference paradigm decomposes complex inference process distinct clear actions 1 memory recall retrieves relevant knowledge 2 reasoning performs logical steps based recalled knowledge facilitate decomposition introduce special tokens memory reason guiding model distinguish steps require knowledge retrieval involve reasoning experiment results decomposition improves model performance enhances interpretability inference process enabling users identify sources error refine model responses effectively code available https github com MingyuJ666 Disentangling Memory Reasoning
92,Open-World Attribute Mining for E-Commerce Products with Multimodal Self-Correction Instruction Tuning,"['Jiaqi Li', 'Yanming Li', 'Xiaoli Shen', 'Chuanyi Zhang', 'Guilin Qi', 'Sheng Bi']",,Open World Attribute Mining E Commerce Products Multimodal Self Correction Instruction Tuning
93,Normalized AOPC: Fixing Misleading Faithfulness Metrics for Feature Attributions Explainability,"['Joakim Edin', 'Andreas Geert Motzfeldt', 'Casper L. Christensen', 'Tuukka Ruotsalo', 'Lars Maaløe', 'Maria Maistro']",,Normalized AOPC Fixing Misleading Faithfulness Metrics Feature Attributions Explainability
94,Takin-VC: Expressive Zero-Shot Voice Conversion via Adaptive Hybrid Content Encoding and Enhanced Timbre Modeling,"['Yang Yuguang', 'Yu Pan', 'Jixun Yao', 'xiang zhang', 'Jianhao Ye', 'Hongbin Zhou', 'Lei Xie', 'Lei Ma', 'Jianjun Zhao']",,Takin VC Expressive Zero Shot Voice Conversion Adaptive Hybrid Content Encoding Enhanced Timbre Modeling
95,LangSAMP: Language-Script Aware Multilingual Pretraining,"['Yihong Liu', 'Haotian Ye', 'Chunlan Ma', 'Mingyang Wang', 'Hinrich Schuetze']","Recent multilingual pretrained language models (mPLMs) often avoid using language embeddings -- learnable vectors assigned to individual languages. However, this places a significant burden on token representations to encode all language-specific information, which may hinder language neutrality. To address this limitation, we propose Language-Script Aware Multilingual Pretraining (LangSAMP), a method that incorporates both language and script embeddings to enhance representation learning. Specifically, we integrate these embeddings into the output of the Transformer blocks before passing the final representations to the language modeling head for prediction. We apply LangSAMP to the continual pretraining of XLM-R on a highly multilingual corpus covering more than 500 languages. The resulting model consistently outperforms the baseline in zero-shot crosslingual transfer across diverse downstream tasks. Extensive analysis reveals that language and script embeddings capture language- and script-specific nuances, which benefits more language-neutral representations, proven by improved pairwise cosine similarity. In our case study, we also show that language and script embeddings can be used to select better source languages for crosslingual transfer. We make our code and models publicly available at https://github.com/cisnlp/LangSAMP.",LangSAMP Language Script Aware Multilingual Pretraining Recent multilingual pretrained language models mPLMs avoid using language embeddings learnable vectors assigned individual languages places significant burden token representations encode language specific information hinder language neutrality address limitation propose Language Script Aware Multilingual Pretraining LangSAMP method incorporates language script embeddings enhance representation learning Specifically integrate embeddings output Transformer blocks passing final representations language modeling head prediction apply LangSAMP continual pretraining XLM R highly multilingual corpus covering 500 languages resulting model consistently outperforms baseline zero shot crosslingual transfer diverse downstream tasks Extensive analysis reveals language script embeddings capture language script specific nuances benefits language neutral representations proven improved pairwise cosine similarity case study language script embeddings used select better source languages crosslingual transfer make code models publicly available https github com cisnlp LangSAMP
96,RelationalCoder: Relational Representation of Complex Tables for Program-Based Processing and Reasoning,"['Haoyu Dong', 'Yue Hu', 'Huailiang Peng', 'Yanan Cao']",,RelationalCoder Relational Representation Complex Tables Program Based Processing Reasoning
97,Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study,"['Bolei Ma', 'Berk Yoztyurk', 'Anna-Carolina Haensch', 'Xinpeng Wang', 'Markus Herklotz', 'Frauke Kreuter', 'Barbara Plank', 'Matthias Aßenmacher']","In recent research, large language models (LLMs) have been increasingly used to investigate public opinions. This study investigates the algorithmic fidelity of LLMs, i.e., the ability to replicate the socio-cultural context and nuanced opinions of human participants. Using open-ended survey data from the German Longitudinal Election Studies (GLES), we prompt different LLMs to generate synthetic public opinions reflective of German subpopulations by incorporating demographic features into the persona prompts. Our results show that Llama performs better than other LLMs at representing subpopulations, particularly when there is lower opinion diversity within those groups. Our findings further reveal that the LLM performs better for supporters of left-leaning parties like The Greens and The Left compared to other parties, and matches the least with the right-party AfD. Additionally, the inclusion or exclusion of specific variables in the prompts can significantly impact the models' predictions. These findings underscore the importance of aligning LLMs to more effectively model diverse public opinions while minimizing political biases and enhancing robustness in representativeness.",Algorithmic Fidelity Large Language Models Generating Synthetic German Public Opinions Case Study recent research large language models LLMs increasingly used investigate public opinions study investigates algorithmic fidelity LLMs e ability replicate socio cultural context nuanced opinions human participants Using open ended survey data German Longitudinal Election Studies GLES prompt different LLMs generate synthetic public opinions reflective German subpopulations incorporating demographic features persona prompts results Llama performs better LLMs representing subpopulations particularly lower opinion diversity groups findings reveal LLM performs better supporters left leaning parties like Greens Left compared parties matches right party AfD Additionally inclusion exclusion specific variables prompts significantly impact models predictions findings underscore importance aligning LLMs effectively model diverse public opinions minimizing political biases enhancing robustness representativeness
98,TUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos,"['Fanheng Kong', 'Jingyuan Zhang', 'Hongzhi Zhang', 'Shi Feng', 'Daling Wang', 'Linhao Yu', 'Xingguang Ji', 'Yu Tian', 'V. W.', 'Fuzheng Zhang']","Videos are unique in their integration of temporal elements, including camera, scene, action, and attribute, along with their dynamic relationships over time. However, existing benchmarks for video understanding often treat these properties separately or narrowly focus on specific aspects, overlooking the holistic nature of video content. To address this, we introduce TUNA, a temporal-oriented benchmark for fine-grained understanding on dense dynamic videos, with two complementary tasks: captioning and QA. Our TUNA features diverse video scenarios and dynamics, assisted by interpretable and robust evaluation criteria. We evaluate several leading models on our benchmark, providing fine-grained performance assessments across various dimensions. This evaluation reveals key challenges in video temporal understanding, such as limited action description, inadequate multi-subject understanding, and insensitivity to camera motion, offering valuable insights for improving video understanding models. The data and code are available at https://friedrichor.github.io/projects/TUNA.",TUNA Comprehensive Fine grained Temporal Understanding Evaluation Dense Dynamic Videos Videos unique integration temporal elements including camera scene action attribute dynamic relationships time existing benchmarks video understanding treat properties separately narrowly focus specific aspects overlooking holistic nature video content address introduce TUNA temporal oriented benchmark fine grained understanding dense dynamic videos complementary tasks captioning QA TUNA features diverse video scenarios dynamics assisted interpretable robust evaluation criteria evaluate leading models benchmark providing fine grained performance assessments various dimensions evaluation reveals key challenges video temporal understanding limited action description inadequate multi subject understanding insensitivity camera motion offering valuable insights improving video understanding models data code available https friedrichor github io projects TUNA
99,Self-Instructed Derived Prompt Generation Meets In-Context Learning: Unlocking New Potential of Black-Box LLMs,"['Zhuo Li', 'Yuhao Du', 'Jinpeng Hu', 'Xiang Wan', 'Anningzhe Gao']","Large language models (LLMs) have shown success in generating high-quality responses. In order to achieve better alignment with LLMs with human preference, various works are proposed based on specific optimization process, which, however, is not suitable to Black-Box LLMs like GPT-4, due to inaccessible parameters. In Black-Box LLMs case, their performance is highly dependent on the quality of the provided prompts. Existing methods to enhance response quality often involve a prompt refinement model, yet these approaches potentially suffer from semantic inconsistencies between the refined and original prompts, and typically overlook the relationship between them. To address these challenges, we introduce a self-instructed in-context learning framework that empowers LLMs to deliver more effective responses by generating reliable derived prompts to construct informative contextual environments. Our approach incorporates a self-instructed reinforcement learning mechanism, enabling direct interaction with the response model during derived prompt generation for better alignment. We then formulate querying as an in-context learning task, using responses from LLMs combined with the derived prompts to establish a contextual demonstration for the original prompt. This strategy ensures alignment with the original query, reduces discrepancies from refined prompts, and maximizes the LLMs' in-context learning capability. Extensive experiments demonstrate that the proposed method not only generates more reliable derived prompts but also significantly enhances LLMs' ability to deliver more effective responses, including Black-Box models such as GPT-4.",Self Instructed Derived Prompt Generation Meets Context Learning Unlocking New Potential Black Box LLMs Large language models LLMs shown success generating high quality responses order achieve better alignment LLMs human preference various works proposed based specific optimization process suitable Black Box LLMs like GPT 4 inaccessible parameters Black Box LLMs case performance highly dependent quality provided prompts Existing methods enhance response quality involve prompt refinement model approaches potentially suffer semantic inconsistencies refined original prompts typically overlook relationship address challenges introduce self instructed context learning framework empowers LLMs deliver effective responses generating reliable derived prompts construct informative contextual environments approach incorporates self instructed reinforcement learning mechanism enabling direct interaction response model derived prompt generation better alignment formulate querying context learning task using responses LLMs combined derived prompts establish contextual demonstration original prompt strategy ensures alignment original query reduces discrepancies refined prompts maximizes LLMs context learning capability Extensive experiments demonstrate proposed method generates reliable derived prompts significantly enhances LLMs ability deliver effective responses including Black Box models GPT 4
100,Binary Classifier Optimization for Large Language Model Alignment,['Seungjae Jung'],"In real-world services such as ChatGPT, aligning models based on user feedback is crucial for improving model performance. However, due to the simplicity and convenience of providing feedback, users typically offer only basic binary signals, such as 'thumbs-up' or 'thumbs-down'. Most existing alignment research, on the other hand, relies on preference-based approaches that require both positive and negative responses as a pair. We propose Binary Classifier Optimization (BCO), a technique that effectively aligns LLMs using only binary feedback. BCO trains a binary classifier, where the logit serves as an implicit reward, effectively minimizing the Direct Preference Optimization (DPO) loss. We demonstrate that the binary cross-entropy loss employed in classifier training acts as an upper bound for the DPO loss. Additionally, a novel reward shift technique further minimizes the gap between the losses. We validate our methodology in two settings: first, on a paired preference dataset, where our method performs on par with DPO; and second, on a Likert-5 scale annotation dataset which stems from real users' queries. Our model consistently demonstrates effective and robust alignment across four base LLMs and three different datasets, showcasing the strength of our approach to learning from binary signals.",Binary Classifier Optimization Large Language Model Alignment real world services ChatGPT aligning models based user feedback crucial improving model performance simplicity convenience providing feedback users typically offer basic binary signals thumbs thumbs existing alignment research hand relies preference based approaches require positive negative responses pair propose Binary Classifier Optimization BCO technique effectively aligns LLMs using binary feedback BCO trains binary classifier logit serves implicit reward effectively minimizing Direct Preference Optimization DPO loss demonstrate binary cross entropy loss employed classifier training acts upper bound DPO loss Additionally novel reward shift technique minimizes gap losses validate methodology settings paired preference dataset method performs par DPO second Likert 5 scale annotation dataset stems real users queries model consistently demonstrates effective robust alignment base LLMs different datasets showcasing strength approach learning binary signals
101,UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs’ Memorization,"['Md Nayem Uddin', 'Amir Saeidi', 'Divij Handa', 'Agastya Seth', 'Tran Cao Son', 'Eduardo Blanco', 'Steven Corman', 'Chitta Baral']",,UnSeenTimeQA Time Sensitive Question Answering LLMs Memorization
102,From Information to Insight: Leveraging LLMs for Open Aspect-Based Educational Summarization,"['Yang Zhong', 'Diane Litman']",,Information Insight Leveraging LLMs Open Aspect Based Educational Summarization
103,"AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset","['Charles Nimo', 'Tobi Olatunji', 'Abraham Toluwase Owodunni', 'Tassallah Abdullahi', 'Emmanuel Ayodele', 'Mardhiyah Sanni', 'Ezinwanne C. Aka', 'Folafunmi Omofoye', 'Foutse Yuehgoh', 'Timothy Faniran', 'Bonaventure F. P. Dossou', 'Moshood O. Yekini', 'Jonas Kemp', 'Katherine A Heller', 'Jude Chidubem Omeke', 'Chidi Asuzu MD', 'Naome A Etori', 'Aïmérou Ndiaye', 'Ifeoma Okoh', 'Evans Doe Ocansey', 'Wendy Kinara', 'Michael Best', 'Irfan Essa', 'Stephen Edward Moore', 'Chris Fourie', 'Mercy Nyamewaa Asiedu']","Recent advancements in large language model(LLM) performance on medical multiple choice question (MCQ) benchmarks have stimulated interest from healthcare providers and patients globally. Particularly in low-and middle-income countries (LMICs) facing acute physician shortages and lack of specialists, LLMs offer a potentially scalable pathway to enhance healthcare access and reduce costs. However, their effectiveness in the Global South, especially across the African continent, remains to be established. In this work, we introduce AfriMed-QA, the first large scale Pan-African English multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open and closed-ended) sourced from over 60 medical schools across 16 countries, covering 32 medical specialties. We further evaluate 30 LLMs across multiple axes including correctness and demographic bias. Our findings show significant performance variation across specialties and geographies, MCQ performance clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general models and smaller edge-friendly LLMs struggle to achieve a passing score. Interestingly, human evaluations show a consistent consumer preference for LLM answers and explanations when compared with clinician answers.",AfriMed QA Pan African Multi Specialty Medical Question Answering Benchmark Dataset Recent advancements large language model LLM performance medical multiple choice question MCQ benchmarks stimulated healthcare providers patients globally Particularly low middle income countries LMICs facing acute physician shortages lack specialists LLMs offer potentially scalable pathway enhance healthcare access reduce costs effectiveness Global South especially African continent remains established work introduce AfriMed QA large scale Pan African English multi specialty medical Question Answering QA dataset 15 000 questions open closed ended sourced 60 medical schools 16 countries covering 32 medical specialties evaluate 30 LLMs multiple axes including correctness demographic bias findings significant performance variation specialties geographies MCQ performance clearly lags USMLE MedQA biomedical LLMs underperform general models smaller edge friendly LLMs struggle achieve passing score Interestingly human evaluations consistent consumer preference LLM answers explanations compared clinician answers
104,Root Defense Strategies: Ensuring Safety of LLM at the Decoding Level,"['Xinyi Zeng', 'Yuying Shang', 'Jiawei Chen', 'Jingyuan Zhang', 'Yu Tian']",,Root Defense Strategies Ensuring Safety LLM Decoding Level
105,In-the-wild Audio Spatialization with Flexible Text-guided Localization,"['Tianrui Pan', 'Jie Liu', 'Zewen Huang', 'Jie Tang', 'Gangshan Wu']","To enhance immersive experiences, binaural audio offers spatial awareness of sounding objects in AR, VR, and embodied AI applications. While existing audio spatialization methods can generally map any available monaural audio to binaural audio signals, they often lack the flexible and interactive control needed in complex multi-object user-interactive environments. To address this, we propose a Text-guided Audio Spatialization (TAS) framework that utilizes flexible text prompts and evaluates our model from unified generation and comprehension perspectives. Due to the limited availability of premium and large-scale stereo data, we construct the SpatialTAS dataset, which encompasses 376,000 simulated binaural audio samples to facilitate the training of our model. Our model learns binaural differences guided by 3D spatial location and relative position prompts, augmented by flipped-channel audio. It outperforms existing methods on both simulated and real-recorded datasets, demonstrating superior generalization and accuracy. Besides, we develop an assessment model based on Llama-3.1-8B, which evaluates the spatial semantic coherence between our generated binaural audio and text prompts through a spatial reasoning task. Results demonstrate that text prompts provide flexible and interactive control to generate binaural audio with excellent quality and semantic consistency in spatial locations. Dataset is available at \href{https://github.com/Alice01010101/TASU}",wild Audio Spatialization Flexible Text guided Localization enhance immersive experiences binaural audio offers spatial awareness sounding objects AR VR embodied AI applications existing audio spatialization methods generally map available monaural audio binaural audio signals lack flexible interactive control needed complex multi object user interactive environments address propose Text guided Audio Spatialization TAS framework utilizes flexible text prompts evaluates model unified generation comprehension perspectives limited availability premium large scale stereo data construct SpatialTAS dataset encompasses 376 000 simulated binaural audio samples facilitate training model model learns binaural differences guided 3D spatial location relative position prompts augmented flipped channel audio outperforms existing methods simulated real recorded datasets demonstrating superior generalization accuracy develop assessment model based Llama 3 1 8B evaluates spatial semantic coherence generated binaural audio text prompts spatial reasoning task Results demonstrate text prompts provide flexible interactive control generate binaural audio excellent quality semantic consistency spatial locations Dataset available href https github com Alice01010101 TASU
106,L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models,"['Hyesung Jeon', 'Yulhwa Kim', 'Jae-Joon Kim']","Due to the high memory and computational costs associated with large language models (LLMs), model compression techniques such as quantization, which reduces inference costs, and parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaptation (LoRA), which reduce training costs, have gained significant popularity. This trend has spurred active research into quantization-aware PEFT techniques, aimed at maintaining model accuracy while minimizing memory overhead during both inference and training. Previous quantization-aware PEFT methods typically apply post-training quantization (PTQ) to pre-trained LLMs, followed by PEFT to recover accuracy loss. Meanwhile, this approach has limitations in recovering the accuracy loss. In this paper, we propose L4Q, a method that integrates Quantization-Aware Training (QAT) with LoRA. By employing a memory-optimized layer design, L4Q significantly reduces QAT's memory overhead, making its training cost comparable to LoRA, while preserving the advantage of QAT in producing fully quantized LLMs with high accuracy. Our experiments demonstrate that this combined approach to quantization and fine-tuning achieves superior accuracy compared to decoupled fine-tuning schemes, particularly in 4-bit and 3-bit quantization, positioning L4Q as an efficient QAT solution. Using the LLaMA and Mistral models with instructional datasets, we showcase L4Q's capabilities in language tasks and few-shot learning.",L4Q Parameter Efficient Quantization Aware Fine Tuning Large Language Models high memory computational costs associated large language models LLMs model compression techniques quantization reduces inference costs parameter efficient fine tuning PEFT methods like Low Rank Adaptation LoRA reduce training costs gained significant popularity trend spurred active research quantization aware PEFT techniques aimed maintaining model accuracy minimizing memory overhead inference training Previous quantization aware PEFT methods typically apply post training quantization PTQ pre trained LLMs followed PEFT recover accuracy loss approach limitations recovering accuracy loss paper propose L4Q method integrates Quantization Aware Training QAT LoRA employing memory optimized layer design L4Q significantly reduces QAT s memory overhead making training cost comparable LoRA preserving advantage QAT producing fully quantized LLMs high accuracy experiments demonstrate combined approach quantization fine tuning achieves superior accuracy compared decoupled fine tuning schemes particularly 4 bit 3 bit quantization positioning L4Q efficient QAT solution Using LLaMA Mistral models instructional datasets showcase L4Q s capabilities language tasks shot learning
107,Second Language (Arabic) Acquisition of LLMs via Progressive Vocabulary Expansion,"['Jianqing Zhu', 'Huang Huang', 'Zhihang Lin', 'Juhao Liang', 'Zhengyang Tang', 'Khalid Almubarak', 'Mosen Alharthi', 'Bang An', 'Juncai He', 'Xiangbo Wu', 'Fei Yu', 'Junying Chen', 'MA Zhuoheng', 'Yuhao Du', 'He Zhang', 'Saied Alshahrani', 'Emad A. Alghamdi', 'Lian Zhang', 'Ruoyu Sun', 'Haizhou Li', 'Benyou Wang', 'Jinchao Xu']","This paper addresses the critical need for democratizing large language models (LLM) in the Arab world, a region that has seen slower progress in developing models comparable to state-of-the-art offerings like GPT-4 or ChatGPT 3.5, due to a predominant focus on mainstream languages (e.g., English and Chinese). One practical objective for an Arabic LLM is to utilize an Arabic-specific vocabulary for the tokenizer that could speed up decoding. However, using a different vocabulary often leads to a degradation of learned knowledge since many words are initially out-of-vocabulary (OOV) when training starts. Inspired by the vocabulary learning during Second Language (Arabic) Acquisition for humans, the released AraLLaMA employs progressive vocabulary expansion, which is implemented by a modified BPE algorithm that progressively extends the Arabic subwords in its dynamic vocabulary during training, thereby balancing the OOV ratio at every stage. The ablation study demonstrated the effectiveness of Progressive Vocabulary Expansion. Moreover, AraLLaMA achieves decent performance comparable to the best Arabic LLMs across a variety of Arabic benchmarks. Models, training data, benchmarks, and codes will be all open-sourced.",Second Language Arabic Acquisition LLMs Progressive Vocabulary Expansion paper addresses critical need democratizing large language models LLM Arab world region seen slower progress developing models comparable state art offerings like GPT 4 ChatGPT 3 5 predominant focus mainstream languages e g English Chinese practical objective Arabic LLM utilize Arabic specific vocabulary tokenizer speed decoding using different vocabulary leads degradation learned knowledge words initially vocabulary OOV training starts Inspired vocabulary learning Second Language Arabic Acquisition humans released AraLLaMA employs progressive vocabulary expansion implemented modified BPE algorithm progressively extends Arabic subwords dynamic vocabulary training balancing OOV ratio stage ablation study demonstrated effectiveness Progressive Vocabulary Expansion AraLLaMA achieves decent performance comparable best Arabic LLMs variety Arabic benchmarks Models training data benchmarks codes open sourced
108,What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs,"['Sangyeop Kim', 'Yohan Lee', 'Yongwoo Song', 'Kimin Lee']","We investigate long-context vulnerabilities in Large Language Models (LLMs) through Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of up to 128K tokens. Through comprehensive analysis with various many-shot attack settings with different instruction styles, shot density, topic, and format, we reveal that context length is the primary factor determining attack effectiveness. Critically, we find that successful attacks do not require carefully crafted harmful content. Even repetitive shots or random dummy text can circumvent model safety measures, suggesting fundamental limitations in long-context processing capabilities of LLMs. The safety behavior of well-aligned models becomes increasingly inconsistent with longer contexts. These findings highlight significant safety gaps in context expansion capabilities of LLMs, emphasizing the need for new safety mechanisms.",Really Matters Shot Attacks Empirical Study Long Context Vulnerabilities LLMs investigate long context vulnerabilities Large Language Models LLMs Shot Jailbreaking MSJ experiments utilize context length 128K tokens comprehensive analysis various shot attack settings different instruction styles shot density topic format reveal context length primary factor determining attack effectiveness Critically successful attacks require carefully crafted harmful content repetitive shots random dummy text circumvent model safety measures suggesting fundamental limitations long context processing capabilities LLMs safety behavior aligned models increasingly inconsistent longer contexts findings highlight significant safety gaps context expansion capabilities LLMs emphasizing need new safety mechanisms
109,ECERC: Evidence-Cause Attention Network for Multi-Modal Emotion Recognition in Conversation,"['Tao Zhang', 'Zhenhua Tan']",,ECERC Evidence Cause Attention Network Multi Modal Emotion Recognition Conversation
110,CompileAgent: Automated Real-World Repo-Level Compilation with Tool-Integrated LLM-based Agent System,"['Li Hu', 'Guoqiang Chen', 'Xiuwei Shang', 'Shaoyin Cheng', 'Benlong Wu', 'LiGangyang', 'Xu Zhu', 'Weiming Zhang', 'Nenghai Yu']","With open-source projects growing in size and complexity, manual compilation becomes tedious and error-prone, highlighting the need for automation to improve efficiency and accuracy. However, the complexity of compilation instruction search and error resolution makes automatic compilation challenging. Inspired by the success of LLM-based agents in various fields, we propose CompileAgent, the first LLM-based agent framework dedicated to repo-level compilation. CompileAgent integrates five tools and a flow-based agent strategy, enabling interaction with software artifacts for compilation instruction search and error resolution. To measure the effectiveness of our method, we design a public repo-level benchmark CompileAgentBench, and we also design two baselines for comparison by combining two compilation-friendly schemes. The performance on this benchmark shows that our method significantly improves the compilation success rate, ranging from 10% to 71%. Meanwhile, we evaluate the performance of CompileAgent under different agent strategies and verify the effectiveness of the flow-based strategy. Additionally, we emphasize the scalability of CompileAgent, further expanding its application prospects.",CompileAgent Automated Real World Repo Level Compilation Tool Integrated LLM based Agent open source projects growing size complexity manual compilation tedious error prone highlighting need automation improve efficiency accuracy complexity compilation instruction search error resolution makes automatic compilation challenging Inspired success LLM based agents various fields propose CompileAgent LLM based agent framework dedicated repo level compilation CompileAgent integrates tools flow based agent strategy enabling interaction software artifacts compilation instruction search error resolution measure effectiveness method design public repo level benchmark CompileAgentBench design baselines comparison combining compilation friendly schemes performance benchmark shows method significantly improves compilation success rate ranging 10 71 evaluate performance CompileAgent different agent strategies verify effectiveness flow based strategy Additionally emphasize scalability CompileAgent expanding application prospects
111,Beyond Demographics: Fine-tuning Large Language Models to Predict Individuals’ Subjective Text Perceptions,"['Matthias Orlikowski', 'Jiaxin Pei', 'Paul Röttger', 'Philipp Cimiano', 'David Jurgens', 'Dirk Hovy']",,Demographics Fine tuning Large Language Models Predict Individuals Subjective Text Perceptions
112,Exploring Forgetting in Large Language Model Pre-Training,"['Chonghua Liao', 'Ruobing Xie', 'Xingwu Sun', 'Haowen Sun', 'Zhanhui Kang']","Catastrophic forgetting remains a formidable obstacle to building an omniscient model in large language models (LLMs). Despite the pioneering research on task-level forgetting in LLM fine-tuning, there is scant focus on forgetting during pre-training. We systematically explored the existence and measurement of forgetting in pre-training, questioning traditional metrics such as perplexity (PPL) and introducing new metrics to better detect entity memory retention. Based on our revised assessment of forgetting metrics, we explored low-cost, straightforward methods to mitigate forgetting during the pre-training phase. Further, we carefully analyzed the learning curves, offering insights into the dynamics of forgetting. Extensive evaluations and analyses on forgetting of pre-training could facilitate future research on LLMs.",Exploring Forgetting Large Language Model Pre Training Catastrophic forgetting remains formidable obstacle building omniscient model large language models LLMs Despite pioneering research task level forgetting LLM fine tuning scant focus forgetting pre training systematically explored existence measurement forgetting pre training questioning traditional metrics perplexity PPL introducing new metrics better detect entity memory retention Based revised assessment forgetting metrics explored low cost straightforward methods mitigate forgetting pre training phase carefully analyzed learning curves offering insights dynamics forgetting Extensive evaluations analyses forgetting pre training facilitate future research LLMs
113,Call for Rigor in Reporting Quality of Instruction Tuning Data,"['Hyeonseok Moon', 'Jaehyung Seo', 'Heuiseok Lim']","Instruction tuning is crucial for adapting large language models (LLMs) to align with user intentions. Numerous studies emphasize the significance of the quality of instruction tuning (IT) data, revealing a strong correlation between IT data quality and the alignment performance of LLMs. In these studies, the quality of IT data is typically assessed by evaluating the performance of LLMs trained with that data. However, we identified a prevalent issue in such practice: hyperparameters for training models are often selected arbitrarily without adequate justification. We observed significant variations in hyperparameters applied across different studies, even when training the same model with the same data. In this study, we demonstrate the potential problems arising from this practice and emphasize the need for careful consideration in verifying data quality. Through our experiments on the quality of LIMA data and a selected set of 1,000 Alpaca data points, we demonstrate that arbitrary hyperparameter decisions can make any arbitrary conclusion.",Rigor Reporting Quality Instruction Tuning Data Instruction tuning crucial adapting large language models LLMs align user intentions Numerous studies emphasize significance quality instruction tuning data revealing strong correlation data quality alignment performance LLMs studies quality data typically assessed evaluating performance LLMs trained data identified prevalent issue practice hyperparameters training models selected arbitrarily adequate justification observed significant variations hyperparameters applied different studies training model data study demonstrate potential problems arising practice emphasize need careful consideration verifying data quality experiments quality LIMA data selected set 1 000 Alpaca data points demonstrate arbitrary hyperparameter decisions make arbitrary conclusion
114,Bias in the Mirror : Are LLMs opinions robust to their own adversarial attacks,"['Virgile Rennard', 'Christos Xypolopoulos', 'Michalis Vazirgiannis']",,Bias Mirror LLMs opinions robust adversarial attacks
115,AndroidLab: Developing and Evaluating Android Agents in A Reproducible Environment,"['Yifan Xu', 'Xiao Liu', 'Xueqiao Sun', 'Siyi Cheng', 'Hao Yu', 'Hanyu Lai', 'Shudan Zhang', 'Dan Zhang', 'Jie Tang', 'Yuxiao Dong']",,AndroidLab Developing Evaluating Android Agents Reproducible Environment
116,Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment,"['Yongxin Huang', 'Kexin Wang', 'Goran Glavaš', 'Iryna Gurevych']","Multilingual sentence encoders (MSEs) are commonly obtained by training multilingual language models to map sentences from different languages into a shared semantic space. As such, they are subject to curse of multilinguality, a loss of monolingual representational accuracy due to parameter sharing. Another limitation of MSEs is the trade-off between different task performance: cross-lingual alignment training distorts the optimal monolingual structure of semantic spaces of individual languages, harming the utility of sentence embeddings in monolingual tasks; cross-lingual tasks, such as cross-lingual semantic similarity and zero-shot transfer for sentence classification, may also require conflicting cross-lingual alignment strategies. In this work, we address both issues by means of modular training of sentence encoders. We first train language-specific monolingual modules to mitigate negative interference between languages (i.e., the curse). We then align all non-English sentence embeddings to the English by training cross-lingual alignment adapters, preventing interference with monolingual specialization from the first step. We train the cross-lingual adapters with two different types of data to resolve the conflicting requirements of different cross-lingual tasks. Monolingual and cross-lingual results on semantic text similarity and relatedness, bitext mining and sentence classification show that our modular solution achieves better and more balanced performance across all the tasks compared to full-parameter training of monolithic multilingual sentence encoders, especially benefiting low-resource languages.",Modular Sentence Encoders Separating Language Specialization Cross Lingual Alignment Multilingual sentence encoders MSEs commonly obtained training multilingual language models map sentences different languages shared semantic space subject curse multilinguality loss monolingual representational accuracy parameter sharing limitation MSEs trade different task performance cross lingual alignment training distorts optimal monolingual structure semantic spaces individual languages harming utility sentence embeddings monolingual tasks cross lingual tasks cross lingual semantic similarity zero shot transfer sentence classification require conflicting cross lingual alignment strategies work address issues means modular training sentence encoders train language specific monolingual modules mitigate negative interference languages e curse align non English sentence embeddings English training cross lingual alignment adapters preventing interference monolingual specialization step train cross lingual adapters different types data resolve conflicting requirements different cross lingual tasks Monolingual cross lingual results semantic text similarity relatedness bitext mining sentence classification modular solution achieves better balanced performance tasks compared parameter training monolithic multilingual sentence encoders especially benefiting low resource languages
117,Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs,"['Yijie Jin', 'Junjie Peng', 'Xuanchao Lin', 'Haochen Yuan', 'Lan Wang', 'Cangzhi Zheng']","Multimodal Sentiment Analysis (MSA) is a rapidly developing field that integrates multimodal information to recognize sentiments, and existing models have made significant progress in this area. The central challenge in MSA is multimodal fusion, which is predominantly addressed by Multimodal Transformers (MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns. In this work, from the perspective of efficiency optimization, we propose and prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and we introduce the graph-structured representation pattern of MulTs. Based on this pattern, we propose an Interlaced Mask (IM) mechanism to design the Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is formally equivalent to MulTs which achieves an efficient weight-sharing mechanism without information disorder through IM, enabling All-Modal-In-One fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called Decomposition is implemented to ensure avoiding additional computational overhead. Moreover, it achieves significantly higher performance than traditional MulTs. To further validate the effectiveness of GsiT itself and the HMHG concept, we integrate them into multiple state-of-the-art models and demonstrate notable performance improvements and parameter reduction on widely used MSA datasets.",Multimodal Transformers Hierarchical Modal wise Heterogeneous Graphs Multimodal Sentiment Analysis MSA rapidly developing field integrates multimodal information recognize sentiments existing models significant progress area central challenge MSA multimodal fusion predominantly addressed Multimodal Transformers MulTs act paradigm MulTs suffer efficiency concerns work perspective efficiency optimization propose prove MulTs hierarchical modal wise heterogeneous graphs HMHGs introduce graph structured representation pattern MulTs Based pattern propose Interlaced Mask IM mechanism design Graph Structured Interlaced Masked Multimodal Transformer GsiT formally equivalent MulTs achieves efficient weight sharing mechanism information disorder IM enabling Modal fusion 1 3 parameters pure MulTs Triton kernel called Decomposition implemented ensure avoiding additional computational overhead achieves significantly higher performance traditional MulTs validate effectiveness GsiT HMHG concept integrate multiple state art models demonstrate notable performance improvements parameter reduction widely used MSA datasets
118,Have We Designed Generalizable Structural Knowledge Promptings? Systematic Evaluation and Rethinking,"['Yichi Zhang', 'Zhuo Chen', 'Lingbing Guo', 'yajing Xu', 'Shaokai Chen', 'Mengshu Sun', 'Binbin Hu', 'Zhiqiang Zhang', 'Lei Liang', 'Wen Zhang', 'Huajun Chen']","Large language models (LLMs) have demonstrated exceptional performance in text generation within current NLP research. However, the lack of factual accuracy is still a dark cloud hanging over the LLM skyscraper. Structural knowledge prompting (SKP) is a prominent paradigm to integrate external knowledge into LLMs by incorporating structural representations, achieving state-of-the-art results in many knowledge-intensive tasks. However, existing methods often focus on specific problems, lacking a comprehensive exploration of the generalization and capability boundaries of SKP. This paper aims to evaluate and rethink the generalization capability of the SKP paradigm from four perspectives including Granularity, Transferability, Scalability, and Universality. To provide a thorough evaluation, we introduce a novel multi-granular, multi-level benchmark called SUBARU, consisting of 9 different tasks with varying levels of granularity and difficulty.",Designed Generalizable Structural Knowledge Promptings Systematic Evaluation Rethinking Large language models LLMs demonstrated exceptional performance text generation current NLP research lack factual accuracy dark cloud hanging LLM skyscraper Structural knowledge prompting SKP prominent paradigm integrate external knowledge LLMs incorporating structural representations achieving state art results knowledge intensive tasks existing methods focus specific problems lacking comprehensive exploration generalization capability boundaries SKP paper aims evaluate rethink generalization capability SKP paradigm perspectives including Granularity Transferability Scalability Universality provide thorough evaluation introduce novel multi granular multi level benchmark called SUBARU consisting 9 different tasks varying levels granularity difficulty
119,"LLäMmlein 🐑: Transparent, Compact and Competitive German-Only Language Models from Scratch","['Jan Pfister', 'Julia Wunderle', 'Andreas Hotho']",,LLäMmlein Transparent Compact Competitive German Language Models Scratch
120,Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues,"['Youngmin Kim', 'Jiwan Chung', 'Jisoo Kim', 'sunghyun lee', 'Sangkyu Lee', 'Junhyeok Kim', 'Cheoljong Yang', 'Youngjae Yu']","Nonverbal communication is integral to human interaction, with gestures, facial expressions, and body language conveying critical aspects of intent and emotion. However, existing large language models (LLMs) fail to effectively incorporate these nonverbal elements, limiting their capacity to create fully immersive conversational experiences. We introduce MARS, a multimodal language model designed to understand and generate nonverbal cues alongside text, bridging this gap in conversational AI. Our key innovation is VENUS, a large-scale dataset comprising annotated videos with time-aligned text, facial expressions, and body language. Leveraging VENUS, we train MARS with a next-token prediction objective, combining text with vector-quantized nonverbal representations to achieve multimodal understanding and generation within a unified framework. Based on various analyses of the VENUS datasets, we validate its substantial scale and high effectiveness. Our quantitative and qualitative results demonstrate that MARS successfully generates text and nonverbal languages, corresponding to conversational input.",Speaking Language Large Scale Multimodal Dataset Learning Nonverbal Cues Video Grounded Dialogues Nonverbal communication integral human interaction gestures facial expressions body language conveying critical aspects intent emotion existing large language models LLMs fail effectively incorporate nonverbal elements limiting capacity create fully immersive conversational experiences introduce MARS multimodal language model designed understand generate nonverbal cues alongside text bridging gap conversational AI key innovation VENUS large scale dataset comprising annotated videos time aligned text facial expressions body language Leveraging VENUS train MARS token prediction objective combining text vector quantized nonverbal representations achieve multimodal understanding generation unified framework Based various analyses VENUS datasets validate substantial scale high effectiveness quantitative qualitative results demonstrate MARS successfully generates text nonverbal languages corresponding conversational input
121,How Much Do Pretrained Language Models Know About Word Senses?,"['Simone Teglia', 'Simone Tedeschi', 'Roberto Navigli']",,Pretrained Language Models Know Word Senses
122,When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations,"['Huaizhi Ge', 'Yiming Li', 'Qifan Wang', 'Yongfeng Zhang', 'Ruixiang Tang']","Large Language Models (LLMs) are known to be vulnerable to backdoor attacks, where triggers embedded in poisoned samples can maliciously alter LLMs' behaviors. In this paper, we move beyond attacking LLMs and instead examine backdoor attacks through the novel lens of natural language explanations. Specifically, we leverage LLMs' generative capabilities to produce human-readable explanations for their decisions, enabling direct comparisons between explanations for clean and poisoned samples. Our results show that backdoored models produce coherent explanations for clean inputs but diverse and logically flawed explanations for poisoned data, a pattern consistent across classification and generation tasks for different backdoor attacks. Further analysis reveals key insights into the explanation generation process. At the token level, explanation tokens associated with poisoned samples only appear in the final few transformer layers. At the sentence level, attention dynamics indicate that poisoned inputs shift attention away from the original input context during explanation generation. These findings enhance our understanding of backdoor mechanisms in LLMs and present a promising framework for detecting vulnerabilities through explainability.",Backdoors Speak Understanding LLM Backdoor Attacks Model Generated Explanations Large Language Models LLMs known vulnerable backdoor attacks triggers embedded poisoned samples maliciously alter LLMs behaviors paper attacking LLMs instead examine backdoor attacks novel lens natural language explanations Specifically leverage LLMs generative capabilities produce human readable explanations decisions enabling direct comparisons explanations clean poisoned samples results backdoored models produce coherent explanations clean inputs diverse logically flawed explanations poisoned data pattern consistent classification generation tasks different backdoor attacks analysis reveals key insights explanation generation process token level explanation tokens associated poisoned samples appear final transformer layers sentence level attention dynamics indicate poisoned inputs shift attention away original input context explanation generation findings enhance understanding backdoor mechanisms LLMs present promising framework detecting vulnerabilities explainability
123,HateDay: Insights from a Global Hate Speech Dataset Representative of a Day on Twitter,"['Manuel Tonneau', 'Diyi Liu', 'Niyati Malhotra', 'Scott A. Hale', 'Samuel Fraiberger', 'Victor Orozco-Olvera', 'Paul Röttger']","To address the global challenge of online hate speech, prior research has developed detection models to flag such content on social media. However, due to systematic biases in evaluation datasets, the real-world effectiveness of these models remains unclear, particularly across geographies. We introduce HateDay, the first global hate speech dataset representative of social media settings, constructed from a random sample of all tweets posted on September 21, 2022 and covering eight languages and four English-speaking countries. Using HateDay, we uncover substantial variation in the prevalence and composition of hate speech across languages and regions. We show that evaluations on academic datasets greatly overestimate real-world detection performance, which we find is very low, especially for non-European languages. Our analysis identifies key drivers of this gap, including models' difficulty to distinguish hate from offensive speech and a mismatch between the target groups emphasized in academic datasets and those most frequently targeted in real-world settings. We argue that poor model performance makes public models ill-suited for automatic hate speech moderation and find that high moderation rates are only achievable with substantial human oversight. Our results underscore the need to evaluate detection systems on data that reflects the complexity and diversity of real-world social media.",HateDay Insights Global Hate Speech Dataset Representative Day Twitter address global challenge online hate speech prior research developed detection models flag content social media systematic biases evaluation datasets real world effectiveness models remains unclear particularly geographies introduce HateDay global hate speech dataset representative social media settings constructed random sample tweets posted September 21 2022 covering languages English speaking countries Using HateDay uncover substantial variation prevalence composition hate speech languages regions evaluations academic datasets greatly overestimate real world detection performance low especially non European languages analysis identifies key drivers gap including models difficulty distinguish hate offensive speech mismatch target groups emphasized academic datasets frequently targeted real world settings argue poor model performance makes public models ill suited automatic hate speech moderation high moderation rates achievable substantial human oversight results underscore need evaluate detection systems data reflects complexity diversity real world social media
124,LegalAgentBench: Evaluating LLM Agents in Legal Domain,"['Haitao Li', 'Junjie Chen', 'Jingli Yang', 'Qingyao Ai', 'Wei Jia', 'Youfeng Liu', 'Kai Lin', 'Yueyue WU', 'Guozhi Yuan', 'Yiran HU', 'Wuyue Wang', 'Yiqun LIU', 'Minlie Huang']","With the increasing intelligence and autonomy of LLM agents, their potential applications in the legal domain are becoming increasingly apparent. However, existing general-domain benchmarks cannot fully capture the complexity and subtle nuances of real-world judicial cognition and decision-making. Therefore, we propose LegalAgentBench, a comprehensive benchmark specifically designed to evaluate LLM Agents in the Chinese legal domain. LegalAgentBench includes 17 corpora from real-world legal scenarios and provides 37 tools for interacting with external knowledge. We designed a scalable task construction framework and carefully annotated 300 tasks. These tasks span various types, including multi-hop reasoning and writing, and range across different difficulty levels, effectively reflecting the complexity of real-world legal scenarios. Moreover, beyond evaluating final success, LegalAgentBench incorporates keyword analysis during intermediate processes to calculate progress rates, enabling more fine-grained evaluation. We evaluated eight popular LLMs, highlighting the strengths, limitations, and potential areas for improvement of existing models and methods. LegalAgentBench sets a new benchmark for the practical application of LLMs in the legal domain, with its code and data available at \url{https://github.com/CSHaitao/LegalAgentBench}.",LegalAgentBench Evaluating LLM Agents Legal Domain increasing intelligence autonomy LLM agents potential applications legal domain increasingly apparent existing general domain benchmarks fully capture complexity subtle nuances real world judicial cognition decision making propose LegalAgentBench comprehensive benchmark specifically designed evaluate LLM Agents Chinese legal domain LegalAgentBench includes 17 corpora real world legal scenarios provides 37 tools interacting external knowledge designed scalable task construction framework carefully annotated 300 tasks tasks span various types including multi hop reasoning writing range different difficulty levels effectively reflecting complexity real world legal scenarios evaluating final success LegalAgentBench incorporates keyword analysis intermediate processes calculate progress rates enabling fine grained evaluation evaluated popular LLMs highlighting strengths limitations potential areas improvement existing models methods LegalAgentBench sets new benchmark practical application LLMs legal domain code data available url https github com CSHaitao LegalAgentBench
125,Inference Compute-Optimal Video Vision Language Models,"['Peiqi Wang', 'ShengYun Peng', 'Xuewen Zhang', 'Hanchao Yu', 'Yibo Yang', 'Lifu Huang', 'Fujun Liu', 'Qifan Wang']","This work investigates the optimal allocation of inference compute across three key scaling factors in video vision language models: language model size, frame count, and the number of visual tokens per frame. While prior works typically focuses on optimizing model efficiency or improving performance without considering resource constraints, we instead identify optimal model configuration under fixed inference compute budgets. We conduct large-scale training sweeps and careful parametric modeling of task performance to identify the inference compute-optimal frontier. Our experiments reveal how task performance depends on scaling factors and finetuning data size, as well as how changes in data size shift the compute-optimal frontier. These findings translate to practical tips for selecting these scaling factors.",Inference Compute Optimal Video Vision Language Models work investigates optimal allocation inference compute key scaling factors video vision language models language model size frame count number visual tokens frame prior works typically focuses optimizing model efficiency improving performance considering resource constraints instead identify optimal model configuration fixed inference compute budgets conduct large scale training sweeps careful parametric modeling task performance identify inference compute optimal frontier experiments reveal task performance depends scaling factors finetuning data size changes data size shift compute optimal frontier findings translate practical tips selecting scaling factors
126,Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models,"['Anirudh Sundar', 'Sinead Williamson', 'Katherine Metcalf', 'Barry-John Theobald', 'Skyler Seto', 'Masha Fedzechkina']","Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions -- a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation. We show that modifying the mLLM's activations changes its embedding space such that cross-lingual alignment is enhanced. Further, we show that the changes to the embedding space translate into improved downstream performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on cross-lingual retrieval.",Steering New Embedding Spaces Analyzing Cross Lingual Alignment Induced Model Interventions Multilingual Language Models Aligned representations languages desired property multilingual large language models mLLMs alignment improve performance cross lingual tasks Typically alignment requires fine tuning model computationally expensive sizable language data available data efficient alternative fine tuning model interventions method manipulating model activations steer generation desired direction analyze effect popular intervention finding experts alignment cross lingual representations mLLMs identify neurons manipulate given language introspect embedding space mLLMs pre post manipulation modifying mLLM s activations changes embedding space cross lingual alignment enhanced changes embedding space translate improved downstream performance retrieval tasks 2x improvements 1 accuracy cross lingual retrieval
127,Digital Gatekeepers: Google’s Role in Curating Hashtags and Subreddits,"['Amrit Poudel', 'Yifan Ding', 'Tim Weninger', 'Jürgen Pfeffer']",,Digital Gatekeepers Google s Role Curating Hashtags Subreddits
128,Behind Closed Words: Creating and Investigating the forePLay Annotated Dataset for Polish Erotic Discourse,"['Anna Kołos', 'Katarzyna Lorenc', 'Emilia Wiśnios', 'Agnieszka Karlińska']","The surge in online content has created an urgent demand for robust detection systems, especially in non-English contexts where current tools demonstrate significant limitations. We present forePLay, a novel Polish language dataset for erotic content detection, featuring over 24k annotated sentences with a multidimensional taxonomy encompassing ambiguity, violence, and social unacceptability dimensions. Our comprehensive evaluation demonstrates that specialized Polish language models achieve superior performance compared to multilingual alternatives, with transformer-based architectures showing particular strength in handling imbalanced categories. The dataset and accompanying analysis establish essential frameworks for developing linguistically-aware content moderation systems, while highlighting critical considerations for extending such capabilities to morphologically complex languages.",Closed Words Creating Investigating forePLay Annotated Dataset Polish Erotic Discourse surge online content created urgent demand robust detection systems especially non English contexts current tools demonstrate significant limitations present forePLay novel Polish language dataset erotic content detection featuring 24k annotated sentences multidimensional taxonomy encompassing ambiguity violence social unacceptability dimensions comprehensive evaluation demonstrates specialized Polish language models achieve superior performance compared multilingual alternatives transformer based architectures showing particular strength handling imbalanced categories dataset accompanying analysis establish essential frameworks developing linguistically aware content moderation systems highlighting critical considerations extending capabilities morphologically complex languages
129,Assessment and manipulation of latent constructs in pre-trained language models using psychometric scales,"['Maor Reuben', 'Ortal Slobodin', 'Idan-Chaim Cohen', 'Aviad Elyashar', 'Orna Braun-Lewensohn', 'Odeya Cohen', 'Rami Puzis']","Human-like personality traits have recently been discovered in large language models, raising the hypothesis that their (known and as yet undiscovered) biases conform with human latent psychological constructs. While large conversational models may be tricked into answering psychometric questionnaires, the latent psychological constructs of thousands of simpler transformers, trained for other tasks, cannot be assessed because appropriate psychometric methods are currently lacking. Here, we show how standard psychological questionnaires can be reformulated into natural language inference prompts, and we provide a code library to support the psychometric assessment of arbitrary models. We demonstrate, using a sample of 88 publicly available models, the existence of human-like mental health-related constructs (including anxiety, depression, and Sense of Coherence) which conform with standard theories in human psychology and show similar correlations and mitigation strategies. The ability to interpret and rectify the performance of language models by using psychological tools can boost the development of more explainable, controllable, and trustworthy models.",Assessment manipulation latent constructs pre trained language models using psychometric scales Human like personality traits recently discovered large language models raising hypothesis known undiscovered biases conform human latent psychological constructs large conversational models tricked answering psychometric questionnaires latent psychological constructs thousands simpler transformers trained tasks assessed appropriate psychometric methods currently lacking standard psychological questionnaires reformulated natural language inference prompts provide code library support psychometric assessment arbitrary models demonstrate using sample 88 publicly available models existence human like mental health related constructs including anxiety depression Sense Coherence conform standard theories human psychology similar correlations mitigation strategies ability interpret rectify performance language models using psychological tools boost development explainable controllable trustworthy models
130,Did Translation Models Get More Robust Without Anyone Even Noticing?,"['Ben Peters', 'Andre Martins']","Neural machine translation (MT) models achieve strong results across a variety of settings, but it is widely believed that they are highly sensitive to ""noisy"" inputs, such as spelling errors, abbreviations, and other formatting issues. In this paper, we revisit this insight in light of recent multilingual MT models and large language models (LLMs) applied to machine translation. Somewhat surprisingly, we show through controlled experiments that these models are far more robust to many kinds of noise than previous models, even when they perform similarly on clean data. This is notable because, even though LLMs have more parameters and more complex training processes than past models, none of the open ones we consider use any techniques specifically designed to encourage robustness. Next, we show that similar trends hold for social media translation experiments -- LLMs are more robust to social media text. We include an analysis of the circumstances in which source correction techniques can be used to mitigate the effects of noise. Altogether, we show that robustness to many types of noise has increased.",Did Translation Models Robust Noticing Neural machine translation MT models achieve strong results variety settings widely believed highly sensitive noisy inputs spelling errors abbreviations formatting issues paper revisit insight light recent multilingual MT models large language models LLMs applied machine translation Somewhat surprisingly controlled experiments models far robust kinds noise previous models perform similarly clean data notable LLMs parameters complex training processes past models open ones consider use techniques specifically designed encourage robustness similar trends hold social media translation experiments LLMs robust social media text include analysis circumstances source correction techniques used mitigate effects noise Altogether robustness types noise increased
131,Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset,"['Dan SU', 'Kezhi Kong', 'Ying Lin', 'Joseph Jennings', 'Brandon Norick', 'Markus Kliegl', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Bryan Catanzaro']",,Transforming Common Crawl Refined Long Horizon Pretraining Dataset
132,Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings,"['Hans William Alexander Hanley', 'Zakir Durumeric']","Contextual large language model embeddings are increasingly utilized for topic modeling and clustering. However, current methods often scale poorly, rely on opaque similarity metrics, and struggle in multilingual settings. In this work, we present a novel, scalable, interpretable, hierarchical, and multilingual approach to clustering news articles and social media data. To do this, we first train multilingual Matryoshka embeddings that can determine story similarity at varying levels of granularity based on which subset of the dimensions of the embeddings is examined. This embedding model achieves state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson $\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering algorithm that leverages the hierarchical nature of Matryoshka embeddings to identify unique news stories, narratives, and themes. We conclude by illustrating how our approach can identify and cluster stories, narratives, and overarching themes within real-world news datasets.",Hierarchical Level Wise News Article Clustering Multilingual Matryoshka Embeddings Contextual large language model embeddings increasingly utilized topic modeling clustering current methods scale poorly rely opaque similarity metrics struggle multilingual settings work present novel scalable interpretable hierarchical multilingual approach clustering news articles social media data train multilingual Matryoshka embeddings determine story similarity varying levels granularity based subset dimensions embeddings examined embedding model achieves state art performance SemEval 2022 Task 8 test dataset Pearson rho 0 816 trained develop efficient hierarchical clustering algorithm leverages hierarchical nature Matryoshka embeddings identify unique news stories narratives themes conclude illustrating approach identify cluster stories narratives overarching themes real world news datasets
133,Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models,"['Tassilo Klein', 'Moin Nabi']","The generation of toxic content by large language models (LLMs) remains a critical challenge for the safe deployment of language technology. We propose a novel framework for implicit knowledge editing and controlled text generation by fine-tuning LLMs with a prototype-based contrastive perplexity objective. Central to our method is the construction of hard negatives - toxic outputs that are generated through adversarial paraphrasing to be semantically similar and model probability to their non-toxic counterparts. By training on these challenging and realistic pairs, our approach ensures robust and stable contrastive optimization. Experimental results in the domain of detoxification demonstrate that our method significantly reduces toxic generation while maintaining strong performance on downstream tasks such as commonsense reasoning and reading comprehension. Our findings highlight the effectiveness of exploiting hard negatives for attribute-aware fine-tuning.",Contrastive Perplexity Controlled Generation Application Detoxifying Large Language Models generation toxic content large language models LLMs remains critical challenge safe deployment language technology propose novel framework implicit knowledge editing controlled text generation fine tuning LLMs prototype based contrastive perplexity objective Central method construction hard negatives toxic outputs generated adversarial paraphrasing semantically similar model probability non toxic counterparts training challenging realistic pairs approach ensures robust stable contrastive optimization Experimental results domain detoxification demonstrate method significantly reduces toxic generation maintaining strong performance downstream tasks commonsense reasoning reading comprehension findings highlight effectiveness exploiting hard negatives attribute aware fine tuning
134,INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent,"['Haohang Li', 'Yupeng Cao', 'Yangyang Yu', 'Shashidhar Reddy Javaji', 'Zhiyang Deng', 'Yueru He', 'Yuechen Jiang', 'Zining Zhu', 'K.P. Subbalakshmi', 'Jimin Huang', 'Lingfei Qian', 'Xueqing Peng', 'Jordan W. Suchow', 'Qianqian Xie']","Recent advancements have underscored the potential of large language model (LLM)-based agents in financial decision-making. Despite this progress, the field currently encounters two main challenges: (1) the lack of a comprehensive LLM agent framework adaptable to a variety of financial tasks, and (2) the absence of standardized benchmarks and consistent datasets for assessing agent performance. To tackle these issues, we introduce \textsc{InvestorBench}, the first benchmark specifically designed for evaluating LLM-based agents in diverse financial decision-making contexts. InvestorBench enhances the versatility of LLM-enabled agents by providing a comprehensive suite of tasks applicable to different financial products, including single equities like stocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we assess the reasoning and decision-making capabilities of our agent framework using thirteen different LLMs as backbone models, across various market environments and tasks. Furthermore, we have curated a diverse collection of open-source, multi-modal datasets and developed a comprehensive suite of environments for financial decision-making. This establishes a highly accessible platform for evaluating financial agents' performance across various scenarios.",INVESTORBENCH Benchmark Financial Decision Making Tasks LLM based Agent Recent advancements underscored potential large language model LLM based agents financial decision making Despite progress field currently encounters main challenges 1 lack comprehensive LLM agent framework adaptable variety financial tasks 2 absence standardized benchmarks consistent datasets assessing agent performance tackle issues introduce textsc InvestorBench benchmark specifically designed evaluating LLM based agents diverse financial decision making contexts InvestorBench enhances versatility LLM enabled agents providing comprehensive suite tasks applicable different financial products including single equities like stocks cryptocurrencies exchange traded funds ETFs Additionally assess reasoning decision making capabilities agent framework using thirteen different LLMs backbone models various market environments tasks Furthermore curated diverse collection open source multi modal datasets developed comprehensive suite environments financial decision making establishes highly accessible platform evaluating financial agents performance various scenarios
135,"Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference","['Benjamin Warner', 'Antoine Chaffin', 'Benjamin Clavié', 'Orion Weller', 'Oskar Hallström', 'Said Taghadouini', 'Alexis Gallagher', 'Raja Biswas', 'Faisal Ladhak', 'Tom Aarsen', 'Griffin Thomas Adams', 'Jeremy Howard', 'Iacopo Poli']","Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.",Smarter Better Faster Longer Modern Bidirectional Encoder Fast Memory Efficient Long Context Finetuning Inference Encoder transformer models BERT offer great performance size tradeoff retrieval classification tasks respect larger decoder models Despite workhorse numerous production pipelines limited Pareto improvements BERT release paper introduce ModernBERT bringing modern model optimizations encoder models representing major Pareto improvement older encoders Trained 2 trillion tokens native 8192 sequence length ModernBERT models exhibit state art results large pool evaluations encompassing diverse classification tasks single multi vector retrieval different domains including code addition strong downstream performance ModernBERT speed memory efficient encoder designed inference common GPUs
136,Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models,"['Zhengyang Shan', 'Emily Diana', 'Jiawei Zhou']","We present a comprehensive evaluation of gender fairness in large language models (LLMs), focusing on their ability to handle both binary and non-binary genders. While previous studies primarily focus on binary gender distinctions, we introduce the Gender Inclusivity Fairness Index (GIFI), a novel and comprehensive metric that quantifies the diverse gender inclusivity of LLMs. GIFI consists of a wide range of evaluations at different levels, from simply probing the model with respect to provided gender pronouns to testing various aspects of model generation and cognitive behaviors under different gender assumptions, revealing biases associated with varying gender identifiers. We conduct extensive evaluations with GIFI on 22 prominent open-source and proprietary LLMs of varying sizes and capabilities, discovering significant variations in LLMs' gender inclusivity. Our study highlights the importance of improving LLMs' inclusivity, providing a critical benchmark for future advancements in gender fairness in generative models.",Gender Inclusivity Fairness Index GIFI Multilevel Framework Evaluating Gender Diversity Large Language Models present comprehensive evaluation gender fairness large language models LLMs focusing ability handle binary non binary genders previous studies primarily focus binary gender distinctions introduce Gender Inclusivity Fairness Index GIFI novel comprehensive metric quantifies diverse gender inclusivity LLMs GIFI consists wide range evaluations different levels simply probing model respect provided gender pronouns testing various aspects model generation cognitive behaviors different gender assumptions revealing biases associated varying gender identifiers conduct extensive evaluations GIFI 22 prominent open source proprietary LLMs varying sizes capabilities discovering significant variations LLMs gender inclusivity study highlights importance improving LLMs inclusivity providing critical benchmark future advancements gender fairness generative models
137,D.Va: Validate Your Demonstration First Before You Use It,"['Qi Zhang', 'Zhiqing Xiao', 'Ruixuan Xiao', 'Lirong Gao', 'Junbo Zhao']",,D Va Validate Demonstration Use
138,Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?,"['Jiwan Chung', 'Janghan Yoon', 'Junhyeong Park', 'Sangeyl Lee', 'Joowon Yang', 'Sooyeon Park', 'Youngjae Yu']","Any-to-any generative models aim to enable seamless interpretation and generation across multiple modalities within a unified framework, yet their ability to preserve relationships across modalities remains uncertain. Do unified models truly achieve cross-modal coherence, or is this coherence merely perceived? To explore this, we introduce ACON, a dataset of 1,000 images (500 newly contributed) paired with captions, editing instructions, and Q&A pairs to evaluate cross-modal transfers rigorously. Using three consistency criteria-cyclic consistency, forward equivariance, and conjugated equivariance-our experiments reveal that any-to-any models do not consistently demonstrate greater cross-modal consistency than specialized models in pointwise evaluations such as cyclic consistency. However, equivariance evaluations uncover weak but observable consistency through structured analyses of the intermediate latent space enabled by multiple editing operations. We release our code and data at https://github.com/JiwanChung/ACON.",Models Consistent Modality Transfers Specialists generative models aim enable seamless interpretation generation multiple modalities unified framework ability preserve relationships modalities remains uncertain unified models truly achieve cross modal coherence coherence merely perceived explore introduce ACON dataset 1 000 images 500 newly contributed paired captions editing instructions Q pairs evaluate cross modal transfers rigorously Using consistency criteria cyclic consistency forward equivariance conjugated equivariance experiments reveal models consistently demonstrate greater cross modal consistency specialized models pointwise evaluations cyclic consistency equivariance evaluations uncover weak observable consistency structured analyses intermediate latent space enabled multiple editing operations release code data https github com JiwanChung ACON
139,MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation,"['Chia-Yuan Chang', 'Zhimeng Jiang', 'Vineeth Rakesh', 'Menghai Pan', 'Chin-Chia Michael Yeh', 'Guanchu Wang', 'Mingzhi Hu', 'Zhichao Xu', 'Yan Zheng', 'Mahashweta Das', 'Na Zou']","Large Language Models (LLMs) are becoming essential tools for various natural language processing tasks but often suffer from generating outdated or incorrect information. Retrieval-Augmented Generation (RAG) addresses this issue by incorporating external, real-time information retrieval to ground LLM responses. However, the existing RAG systems frequently struggle with the quality of retrieval documents, as irrelevant or noisy documents degrade performance, increase computational overhead, and undermine response reliability. To tackle this problem, we propose Multi-Agent Filtering Retrieval-Augmented Generation (MAIN-RAG), a training-free RAG framework that leverages multiple LLM agents to collaboratively filter and score retrieved documents. Specifically, MAIN-RAG introduces an adaptive filtering mechanism that dynamically adjusts the relevance filtering threshold based on score distributions, effectively minimizing noise while maintaining high recall of relevant documents. The proposed approach leverages inter-agent consensus to ensure robust document selection without requiring additional training data or fine-tuning. Experimental results across four QA benchmarks demonstrate that MAIN-RAG consistently outperforms traditional RAG approaches, achieving a 2-11% improvement in answer accuracy while reducing the number of irrelevant retrieved documents. Quantitative analysis further reveals that our approach achieves superior response consistency and answer accuracy over baseline methods, offering a competitive and practical alternative to training-based solutions.",MAIN RAG Multi Agent Filtering Retrieval Augmented Generation Large Language Models LLMs essential tools various natural language processing tasks suffer generating outdated incorrect information Retrieval Augmented Generation RAG addresses issue incorporating external real time information retrieval ground LLM responses existing RAG systems frequently struggle quality retrieval documents irrelevant noisy documents degrade performance increase computational overhead undermine response reliability tackle problem propose Multi Agent Filtering Retrieval Augmented Generation MAIN RAG training free RAG framework leverages multiple LLM agents collaboratively filter score retrieved documents Specifically MAIN RAG introduces adaptive filtering mechanism dynamically adjusts relevance filtering threshold based score distributions effectively minimizing noise maintaining high recall relevant documents proposed approach leverages inter agent consensus ensure robust document selection requiring additional training data fine tuning Experimental results QA benchmarks demonstrate MAIN RAG consistently outperforms traditional RAG approaches achieving 2 11 improvement answer accuracy reducing number irrelevant retrieved documents Quantitative analysis reveals approach achieves superior response consistency answer accuracy baseline methods offering competitive practical alternative training based solutions
140,Unraveling the Mechanics of Learning-Based Demonstration Selection for In-Context Learning,"['Hui Liu', 'Wenya Wang', 'Hao Sun', 'Chris XING TIAN', 'Chenqi Kong', 'Xin Dong', 'Haoliang Li']",,Unraveling Mechanics Learning Based Demonstration Selection Context Learning
141,Direct Prompt Optimization with Continuous Representations,"['Yangkun Wang', 'Zihan Wang', 'Jingbo Shang']",,Direct Prompt Optimization Continuous Representations
142,uMedSum: A Unified Framework for Advancing Medical Abstractive Summarization,"['Aishik Nagar', 'Yutong Liu', 'Andy T. Liu', 'Viktor Schlegel', 'Vijay Prakash Dwivedi', 'Arun-Kumar Kaliya-Perumal', 'GUNA PRATHEEP KALANCHIAM', 'Yili Tang', 'Robby T. Tan']","Medical abstractive summarization faces the challenge of balancing faithfulness and informativeness. Current methods often sacrifice key information for faithfulness or introduce confabulations when prioritizing informativeness. While recent advancements in techniques like in-context learning (ICL) and fine-tuning have improved medical summarization, they often overlook crucial aspects such as faithfulness and informativeness without considering advanced methods like model reasoning and self-improvement. Moreover, the field lacks a unified benchmark, hindering systematic evaluation due to varied metrics and datasets. This paper addresses these gaps by presenting a comprehensive benchmark of six advanced abstractive summarization methods across three diverse datasets using five standardized metrics. Building on these findings, we propose uMedSum, a modular hybrid summarization framework that introduces novel approaches for sequential confabulation removal followed by key missing information addition, ensuring both faithfulness and informativeness. Our work improves upon previous GPT-4-based state-of-the-art (SOTA) medical summarization methods, significantly outperforming them in both quantitative metrics and qualitative domain expert evaluations. Notably, we achieve an average relative performance improvement of 11.8% in reference-free metrics over the previous SOTA. Doctors prefer uMedSum's summaries 6 times more than previous SOTA in difficult cases where there are chances of confabulations or missing information. These results highlight uMedSum's effectiveness and generalizability across various datasets and metrics, marking a significant advancement in medical summarization.",uMedSum Unified Framework Advancing Medical Abstractive Summarization Medical abstractive summarization faces challenge balancing faithfulness informativeness Current methods sacrifice key information faithfulness introduce confabulations prioritizing informativeness recent advancements techniques like context learning ICL fine tuning improved medical summarization overlook crucial aspects faithfulness informativeness considering advanced methods like model reasoning self improvement field lacks unified benchmark hindering systematic evaluation varied metrics datasets paper addresses gaps presenting comprehensive benchmark advanced abstractive summarization methods diverse datasets using standardized metrics Building findings propose uMedSum modular hybrid summarization framework introduces novel approaches sequential confabulation removal followed key missing information addition ensuring faithfulness informativeness work improves previous GPT 4 based state art SOTA medical summarization methods significantly outperforming quantitative metrics qualitative domain expert evaluations Notably achieve average relative performance improvement 11 8 reference free metrics previous SOTA Doctors prefer uMedSum s summaries 6 times previous SOTA difficult cases chances confabulations missing information results highlight uMedSum s effectiveness generalizability various datasets metrics marking significant advancement medical summarization
143,"GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement","['Yifan Yang', 'Zheshu Song', 'Jianheng Zhuo', 'Mingyu Cui', 'Jinpeng Li', 'Bo Yang', 'Yexing Du', 'Ziyang Ma', 'Xunying Liu', 'Ziyuan Wang', 'Ke Li', 'Shuai Fan', 'Kai Yu', 'Wei-Qiang Zhang', 'Guoguo Chen', 'Xie Chen']","The evolution of speech technology has been spurred by the rapid increase in dataset sizes. Traditional speech models generally depend on a large amount of labeled training data, which is scarce for low-resource languages. This paper presents GigaSpeech 2, a large-scale, multi-domain, multilingual speech recognition corpus. It is designed for low-resource languages and does not rely on paired speech and text data. GigaSpeech 2 comprises about 30,000 hours of automatically transcribed speech, including Thai, Indonesian, and Vietnamese, gathered from unlabeled YouTube videos. We also introduce an automated pipeline for data crawling, transcription, and label refinement. Specifically, this pipeline involves Whisper for initial transcription, MMS for forced alignment, and multi-dimensional filtering for data quality assurance. A modified Noisy Student Training is developed to further refine flawed pseudo labels iteratively, thereby enhancing model performance. Experimental results on our manually transcribed evaluation set and two public test sets from Common Voice and FLEURS confirm our corpus's high quality and broad applicability. Notably, ASR models trained on GigaSpeech 2 can reduce the word error rate for Thai, Indonesian, and Vietnamese on our challenging and realistic YouTube test set by 25% to 40% compared to Whisper large-v3, with merely 10% model parameters. Furthermore, our ASR models trained on GigaSpeech 2 yield superior performance compared to commercial services. We hope that our newly introduced corpus and pipeline will open a new avenue for low-resource speech recognition and significantly facilitate research in this area.",GigaSpeech 2 Evolving Large Scale Multi domain ASR Corpus Low Resource Languages Automated Crawling Transcription Refinement evolution speech technology spurred rapid increase dataset sizes Traditional speech models generally depend large labeled training data scarce low resource languages paper presents GigaSpeech 2 large scale multi domain multilingual speech recognition corpus designed low resource languages does rely paired speech text data GigaSpeech 2 comprises 30 000 hours automatically transcribed speech including Thai Indonesian Vietnamese gathered unlabeled YouTube videos introduce automated pipeline data crawling transcription label refinement Specifically pipeline involves Whisper initial transcription MMS forced alignment multi dimensional filtering data quality assurance modified Noisy Student Training developed refine flawed pseudo labels iteratively enhancing model performance Experimental results manually transcribed evaluation set public test sets Common Voice FLEURS confirm corpus s high quality broad applicability Notably ASR models trained GigaSpeech 2 reduce word error rate Thai Indonesian Vietnamese challenging realistic YouTube test set 25 40 compared Whisper large v3 merely 10 model parameters Furthermore ASR models trained GigaSpeech 2 yield superior performance compared commercial services hope newly introduced corpus pipeline open new avenue low resource speech recognition significantly facilitate research area
144,Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents,"['Fanhang Man', 'Huandong Wang', 'Jianjie Fang', 'Zhaoyi Deng', 'Baining Zhao', 'Xinlei Chen', 'Yong Li']","User sentiment on social media reveals the underlying social trends, crises, and needs. Researchers have analyzed users' past messages to trace the evolution of sentiments and reconstruct sentiment dynamics. However, predicting the imminent sentiment of an ongoing event is rarely studied. In this paper, we address the problem of \textbf{sentiment forecasting} on social media to predict the user's future sentiment in response to the development of the event. We extract sentiment-related features to enhance the modeling skill and propose a multi-perspective role-playing framework to simulate the process of human response. Our preliminary results show significant improvement in sentiment forecasting on both microscopic and macroscopic levels.",Context Aware Sentiment Forecasting LLM based Multi Perspective Role Playing Agents User sentiment social media reveals underlying social trends crises needs Researchers analyzed users past messages trace evolution sentiments reconstruct sentiment dynamics predicting imminent sentiment ongoing event rarely studied paper address problem textbf sentiment forecasting social media predict user s future sentiment response development event extract sentiment related features enhance modeling skill propose multi perspective role playing framework simulate process human response preliminary results significant improvement sentiment forecasting microscopic macroscopic levels
145,TARGA: Targeted Synthetic Data Generation for Practical Reasoning over Structured Data,"['Xiang Huang', 'Jiayu Shen', 'Shanshan Huang', 'Sitao Cheng', 'Xiaxia Wang', 'Yuzhong Qu']","Semantic parsing, which converts natural language questions into logic forms, plays a crucial role in reasoning within structured environments. However, existing methods encounter two significant challenges: reliance on extensive manually annotated datasets and limited generalization capability to unseen examples. To tackle these issues, we propose Targeted Synthetic Data Generation (TARGA), a practical framework that dynamically generates high-relevance synthetic data without manual annotation. Starting from the pertinent entities and relations of a given question, we probe for the potential relevant queries through layer-wise expansion and cross-layer combination. Then we generate corresponding natural language questions for these constructed queries to jointly serve as the synthetic demonstrations for in-context learning. Experiments on multiple knowledge base question answering (KBQA) datasets demonstrate that TARGA, using only a 7B-parameter model, substantially outperforms existing non-fine-tuned methods that utilize close-sourced model, achieving notable improvements in F1 scores on GrailQA(+7.7) and KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency, robustness, and generalization capabilities under non-I.I.D. settings.",TARGA Targeted Synthetic Data Generation Practical Reasoning Structured Data Semantic parsing converts natural language questions logic forms plays crucial role reasoning structured environments existing methods encounter significant challenges reliance extensive manually annotated datasets limited generalization capability unseen examples tackle issues propose Targeted Synthetic Data Generation TARGA practical framework dynamically generates high relevance synthetic data manual annotation Starting pertinent entities relations given question probe potential relevant queries layer wise expansion cross layer combination generate corresponding natural language questions constructed queries jointly serve synthetic demonstrations context learning Experiments multiple knowledge base question answering KBQA datasets demonstrate TARGA using 7B parameter model substantially outperforms existing non fine tuned methods utilize close sourced model achieving notable improvements F1 scores GrailQA 7 7 KBQA Agent 12 2 Furthermore TARGA exhibits superior sample efficiency robustness generalization capabilities non D settings
146,AndroidGen: Building an Android Language Agent under Data Scarcity,"['Hanyu Lai', 'Junjie Gao', 'Xiao Liu', 'Yifan Xu', 'Shudan Zhang', 'Yuxiao Dong', 'Jie Tang']","Large language models have opened up a world of possibilities for various NLP tasks, sparking optimism for the future. Despite their potential, LLMs have yet to be widely used as agents on real mobile devices. The main challenge is the need for high-quality data sources. Time constraints and labor intensity often hinder human annotation. On the other hand, existing LLMs exhibit inadequate completion rates and need a robust data filtration strategy. Given these challenges, we develop a framework called AndroidGen to enhance the capabilities of LLM-based agents under data scarcity. In addition, we leverage AndroidGen to collect trajectories given human tasks and train open-source LLMs on these trajectories to develop an open-source mobile agent without manually labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld, AitW, and various popular applications, demonstrating its improvements and revealing potential areas for future improvement. Code, model, and data are available at https://github.com/THUDM/AndroidGen.",AndroidGen Building Android Language Agent Data Scarcity Large language models opened world possibilities various NLP tasks sparking optimism future Despite potential LLMs widely used agents real mobile devices main challenge need high quality data sources Time constraints labor intensity hinder human annotation hand existing LLMs exhibit inadequate completion rates need robust data filtration strategy Given challenges develop framework called AndroidGen enhance capabilities LLM based agents data scarcity addition leverage AndroidGen collect trajectories given human tasks train open source LLMs trajectories develop open source mobile agent manually labeled trajectories extensively evaluate AndroidGen AndroidWorld AitW various popular applications demonstrating improvements revealing potential areas future improvement Code model data available https github com THUDM AndroidGen
147,"Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation","['Mingxuan Xia', 'Haobo Wang', 'Yixuan Li', 'Zewei Yu', 'Jindong Wang', 'Junbo Zhao', 'Runze Wu']","Recently, Large Language Models (LLMs) have demonstrated significant potential for data annotation, markedly reducing the labor costs associated with downstream applications. However, existing methods mostly adopt an aggressive strategy by prompting LLM to determine a single gold label for each unlabeled sample. Due to the inherent uncertainty within LLMs, they often produce incorrect labels for difficult samples, severely compromising the data quality for downstream applications. Motivated by ambiguity aversion in human behaviors, we propose a novel candidate annotation paradigm wherein large language models are encouraged to output all possible labels when incurring uncertainty. To ensure unique labels are provided for downstream tasks, we develop a teacher-student framework CanDist that distills candidate annotations with a Small Language Model (SLM). We further provide a rigorous justification demonstrating that distilling candidate annotations from the teacher LLM offers superior theoretical guarantees compared to directly using single annotations. Extensive experiments across six text classification tasks validate the effectiveness of our proposed method. The source code is available at https://github.com/MingxuanXia/CanDist.",Prompt Candidates Distill Teacher Student Framework LLM driven Data Annotation Recently Large Language Models LLMs demonstrated significant potential data annotation markedly reducing labor costs associated downstream applications existing methods adopt aggressive strategy prompting LLM determine single gold label unlabeled sample inherent uncertainty LLMs produce incorrect labels difficult samples severely compromising data quality downstream applications Motivated ambiguity aversion human behaviors propose novel candidate annotation paradigm large language models encouraged output possible labels incurring uncertainty ensure unique labels provided downstream tasks develop teacher student framework CanDist distills candidate annotations Small Language Model SLM provide rigorous justification demonstrating distilling candidate annotations teacher LLM offers superior theoretical guarantees compared directly using single annotations Extensive experiments text classification tasks validate effectiveness proposed method source code available https github com MingxuanXia CanDist
148,BQA: Body Language Question Answering Dataset for Video Large Language Models,"['Shintaro Ozaki', 'Kazuki Hayashi', 'Miyu Oba', 'Yusuke Sakai', 'Hidetaka Kamigaito', 'Taro Watanabe']","A large part of human communication relies on nonverbal cues such as facial expressions, eye contact, and body language. Unlike language or sign language, such nonverbal communication lacks formal rules, requiring complex reasoning based on commonsense understanding. Enabling current Video Large Language Models (VideoLLMs) to accurately interpret body language is a crucial challenge, as human unconscious actions can easily cause the model to misinterpret their intent. To address this, we propose a dataset, BQA, a body language question answering dataset, to validate whether the model can correctly interpret emotions from short clips of body language comprising 26 emotion labels of videos of body language. We evaluated various VideoLLMs on BQA and revealed that understanding body language is challenging, and our analyses of the wrong answers by VideoLLMs show that certain VideoLLMs made significantly biased answers depending on the age group and ethnicity of the individuals in the video. The dataset is available.",BQA Body Language Question Answering Dataset Video Large Language Models large human communication relies nonverbal cues facial expressions eye contact body language Unlike language sign language nonverbal communication lacks formal rules requiring complex reasoning based commonsense understanding Enabling current Video Large Language Models VideoLLMs accurately interpret body language crucial challenge human unconscious actions easily cause model misinterpret intent address propose dataset BQA body language question answering dataset validate model correctly interpret emotions short clips body language comprising 26 emotion labels videos body language evaluated various VideoLLMs BQA revealed understanding body language challenging analyses wrong answers VideoLLMs certain VideoLLMs significantly biased answers depending age group ethnicity individuals video dataset available
149,A Survey of Post-Training Scaling in Large Language Models,"['Hanyu Lai', 'Xiao Liu', 'Junjie Gao', 'Jiale Cheng', 'Zehan Qi', 'Yifan Xu', 'Shuntian Yao', 'Dan Zhang', 'Jinhua Du', 'Zhenyu Hou', 'Xin Lv', 'Minlie Huang', 'Yuxiao Dong', 'Jie Tang']",,Survey Post Training Scaling Large Language Models
150,Position-aware Automatic Circuit Discovery,"['Tal Haklay', 'Hadas Orgad', 'David Bau', 'Aaron Mueller', 'Yonatan Belinkov']","A widely used strategy to discover and understand language model mechanisms is circuit analysis. A circuit is a minimal subgraph of a model's computation graph that executes a specific task. We identify a gap in existing circuit discovery methods: they assume circuits are position-invariant, treating model components as equally relevant across input positions. This limits their ability to capture cross-positional interactions or mechanisms that vary across positions. To address this gap, we propose two improvements to incorporate positionality into circuits, even on tasks containing variable-length examples. First, we extend edge attribution patching, a gradient-based method for circuit discovery, to differentiate between token positions. Second, we introduce the concept of a dataset schema, which defines token spans with similar semantics across examples, enabling position-aware circuit discovery in datasets with variable length examples. We additionally develop an automated pipeline for schema generation and application using large language models. Our approach enables fully automated discovery of position-sensitive circuits, yielding better trade-offs between circuit size and faithfulness compared to prior work.",Position aware Automatic Circuit Discovery widely used strategy discover understand language model mechanisms circuit analysis circuit minimal subgraph model s computation graph executes specific task identify gap existing circuit discovery methods assume circuits position invariant treating model components equally relevant input positions limits ability capture cross positional interactions mechanisms vary positions address gap propose improvements incorporate positionality circuits tasks containing variable length examples extend edge attribution patching gradient based method circuit discovery differentiate token positions Second introduce concept dataset schema defines token spans similar semantics examples enabling position aware circuit discovery datasets variable length examples additionally develop automated pipeline schema generation application using large language models approach enables fully automated discovery position sensitive circuits yielding better trade offs circuit size faithfulness compared prior work
151,HyperFM: Fact-Centric Multimodal Fusion for Link Prediction over Hyper-Relational Knowledge Graphs,"['Yuhuan Lu', 'Weijian Yu', 'Xin Jing', 'Dingqi Yang']",,HyperFM Fact Centric Multimodal Fusion Link Prediction Hyper Relational Knowledge Graphs
152,Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model,"['Gregor Geigle', 'Florian Schneider', 'Carolin Holtermann', 'Chris Biemann', 'Radu Timofte', 'Anne Lauscher', 'Goran Glavaš']","Most Large Vision-Language Models (LVLMs) to date are trained predominantly on English data, which makes them struggle to understand non-English input and fail to generate output in the desired target language. Existing efforts mitigate these issues by adding multilingual training data, but do so in a largely ad-hoc manner, lacking insight into how different training mixes tip the scale for different groups of languages. In this work, we present a comprehensive investigation into the training strategies for massively multilingual LVLMs. First, we conduct a series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically examining: (1) the number of training languages that can be included without degrading English performance and (2) optimal language distributions of pre-training as well as (3) instruction-tuning data. Further, we (4) investigate how to improve multilingual text-in-image understanding, and introduce a new benchmark for the task. Surprisingly, our analysis reveals that one can (i) include as many as 100 training languages simultaneously (ii) with as little as 25-50\% of non-English data, to greatly improve multilingual performance while retaining strong English performance. We further find that (iii) including non-English OCR data in pre-training and instruction-tuning is paramount for improving multilingual text-in-image understanding. Finally, we put all our findings together and train Centurio, a 100-language LVLM, offering state-of-the-art performance in an evaluation covering 14 tasks and 56 languages.",Centurio Drivers Multilingual Ability Large Vision Language Model Large Vision Language Models LVLMs date trained predominantly English data makes struggle understand non English input fail generate output desired target language Existing efforts mitigate issues adding multilingual training data largely ad hoc manner lacking insight different training mixes tip scale different groups languages work present comprehensive investigation training strategies massively multilingual LVLMs conduct series multi stage experiments spanning 13 downstream vision language tasks 43 languages systematically examining 1 number training languages included degrading English performance 2 optimal language distributions pre training 3 instruction tuning data 4 investigate improve multilingual text image understanding introduce new benchmark task Surprisingly analysis reveals include 100 training languages simultaneously ii little 25 50 non English data greatly improve multilingual performance retaining strong English performance iii including non English OCR data pre training instruction tuning paramount improving multilingual text image understanding Finally findings train Centurio 100 language LVLM offering state art performance evaluation covering 14 tasks 56 languages
153,Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation,"['Dimitris Gkoumas', 'Maria Liakata']","Scientific language models drive research innovation but require extensive fine-tuning on large datasets. This work enhances such models by improving their inference and evaluation capabilities with minimal or no additional training. Focusing on molecule caption generation, we explore post-training synergies between alignment fine-tuning and model merging in a cross-modal setup. We reveal intriguing insights into the behaviour and suitability of such methods while significantly surpassing state-of-the-art models. Moreover, we propose a novel atomic-level evaluation method leveraging off-the-shelf Natural Language Inference (NLI) models for use in the unseen chemical domain. Our experiments demonstrate that our evaluation operates at the right level of granularity, effectively handling multiple content units and subsentence reasoning, while widely adopted NLI methods consistently misalign with assessment criteria.",Enhanced Feedback aligned Mixed LLMs Molecule Caption Generation Fine Grained NLI Evaluation Scientific language models drive research innovation require extensive fine tuning large datasets work enhances models improving inference evaluation capabilities minimal additional training Focusing molecule caption generation explore post training synergies alignment fine tuning model merging cross modal setup reveal intriguing insights behaviour suitability methods significantly surpassing state art models propose novel atomic level evaluation method leveraging shelf Natural Language Inference NLI models use unseen chemical domain experiments demonstrate evaluation operates right level granularity effectively handling multiple content units subsentence reasoning widely adopted NLI methods consistently misalign assessment criteria
154,Ensemble Watermarks for Large Language Models,"['Georg Niess', 'Roman Kern']","As large language models (LLMs) reach human-like fluency, reliably distinguishing AI-generated text from human authorship becomes increasingly difficult. While watermarks already exist for LLMs, they often lack flexibility and struggle with attacks such as paraphrasing. To address these issues, we propose a multi-feature method for generating watermarks that combines multiple distinct watermark features into an ensemble watermark. Concretely, we combine acrostica and sensorimotor norms with the established red-green watermark to achieve a 98% detection rate. After a paraphrasing attack, the performance remains high with 95% detection rate. In comparison, the red-green feature alone as a baseline achieves a detection rate of 49% after paraphrasing. The evaluation of all feature combinations reveals that the ensemble of all three consistently has the highest detection rate across several LLMs and watermark strength settings. Due to the flexibility of combining features in the ensemble, various requirements and trade-offs can be addressed. Additionally, the same detection function can be used without adaptations for all ensemble configurations. This method is particularly of interest to facilitate accountability and prevent societal harm.",Ensemble Watermarks Large Language Models large language models LLMs reach human like fluency reliably distinguishing AI generated text human authorship increasingly difficult watermarks exist LLMs lack flexibility struggle attacks paraphrasing address issues propose multi feature method generating watermarks combines multiple distinct watermark features ensemble watermark Concretely combine acrostica sensorimotor norms established red green watermark achieve 98 detection rate paraphrasing attack performance remains high 95 detection rate comparison red green feature baseline achieves detection rate 49 paraphrasing evaluation feature combinations reveals ensemble consistently highest detection rate LLMs watermark strength settings flexibility combining features ensemble various requirements trade offs addressed Additionally detection function used adaptations ensemble configurations method particularly facilitate accountability prevent societal harm
155,$\mathsf{Con Instruction}$: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities,"['Jiahui Geng', 'Thy Thy Tran', 'Preslav Nakov', 'Iryna Gurevych']",,mathsf Instruction Universal Jailbreaking Multimodal Large Language Models Non Textual Modalities
156,TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for LLM-as-a-Judge,"['Cheng-Han Chiang', 'Hung-yi Lee', 'Michal Lukasik']","The LLM-as-a-judge paradigm uses large language models (LLMs) for automated text evaluation, where a numerical assessment is assigned by an LLM to the input text following scoring rubrics. Existing methods for LLM-as-a-judge use cross-entropy (CE) loss for fine-tuning, which neglects the numeric nature of score prediction. Recent work addresses numerical prediction limitations of LLM fine-tuning through regression-aware fine-tuning, which, however, does not consider chain-of-thought (CoT) reasoning for score prediction. In this paper, we introduce TRACT (Two-stage Regression-Aware fine-tuning with CoT), a method combining CoT reasoning with regression-aware training. TRACT consists of two stages: first, seed LLM is fine-tuned to generate CoTs, which serve as supervision for the second stage fine-tuning. The training objective of TRACT combines the CE loss for learning the CoT reasoning capabilities, and the regression-aware loss for the score prediction. Experiments across four LLM-as-a-judge datasets and two LLMs show that TRACT significantly outperforms existing methods. Extensive ablation studies validate the importance of each component in TRACT.",TRACT Regression Aware Fine tuning Meets Chain Thought Reasoning LLM Judge LLM judge paradigm uses large language models LLMs automated text evaluation numerical assessment assigned LLM input text following scoring rubrics Existing methods LLM judge use cross entropy CE loss fine tuning neglects numeric nature score prediction Recent work addresses numerical prediction limitations LLM fine tuning regression aware fine tuning does consider chain thought CoT reasoning score prediction paper introduce TRACT stage Regression Aware fine tuning CoT method combining CoT reasoning regression aware training TRACT consists stages seed LLM fine tuned generate CoTs serve supervision second stage fine tuning training objective TRACT combines CE loss learning CoT reasoning capabilities regression aware loss score prediction Experiments LLM judge datasets LLMs TRACT significantly outperforms existing methods Extensive ablation studies validate importance component TRACT
157,DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation,"['Hanghui Guo', 'Jia Zhu', 'Shimin Di', 'Weijie Shi', 'Zhangze Chen', 'Jiajie Xu']","Dynamic Retrieval-augmented Generation (RAG) has shown great success in mitigating hallucinations in large language models (LLMs) during generation. However, existing dynamic RAG methods face significant limitations in two key aspects: 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG method, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), which consists of two main components: adaptive cognitive detection and contextual retrieval optimization, specifically designed to determine when retrieval is needed and what to retrieve for LLMs is useful. Experimental results demonstrate that DioR achieves superior performance on all tasks, demonstrating the effectiveness of our work.",DioR Adaptive Cognitive Detection Contextual Retrieval Optimization Dynamic Retrieval Augmented Generation Dynamic Retrieval augmented Generation RAG shown great success mitigating hallucinations large language models LLMs generation existing dynamic RAG methods face significant limitations key aspects 1 Lack effective mechanism control retrieval triggers 2 Lack effective scrutiny retrieval content address limitations propose innovative dynamic RAG method DioR Adaptive Cognitive Detection Contextual Retrieval Optimization consists main components adaptive cognitive detection contextual retrieval optimization specifically designed determine retrieval needed retrieve LLMs useful Experimental results demonstrate DioR achieves superior performance tasks demonstrating effectiveness work
158,Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation,"['Boxuan Lyu', 'Hidetaka Kamigaito', 'Kotaro Funakoshi', 'Manabu Okumura']","Maximum a posteriori decoding, a commonly used method for neural machine translation (NMT), aims to maximize the estimated posterior probability. However, high estimated probability does not always lead to high translation quality. Minimum Bayes Risk (MBR) decoding offers an alternative by seeking hypotheses with the highest expected utility. Inspired by Quality Estimation (QE) reranking which uses the QE model as a ranker we propose source-based MBR (sMBR) decoding, a novel approach that utilizes quasi-sources (generated via paraphrasing or back-translation) as ``support hypotheses'' and a reference-free quality estimation metric as the utility function, marking the first work to solely use sources in MBR decoding. Experiments show that sMBR outperforms QE reranking and the standard MBR decoding. Our findings suggest that sMBR is a promising approach for NMT decoding.",Unveiling Power Source Source based Minimum Bayes Risk Decoding Neural Machine Translation Maximum posteriori decoding commonly used method neural machine translation NMT aims maximize estimated posterior probability high estimated probability does lead high translation quality Minimum Bayes Risk MBR decoding offers alternative seeking hypotheses highest expected utility Inspired Quality Estimation QE reranking uses QE model ranker propose source based MBR sMBR decoding novel approach utilizes quasi sources generated paraphrasing translation support hypotheses reference free quality estimation metric utility function marking work solely use sources MBR decoding Experiments sMBR outperforms QE reranking standard MBR decoding findings suggest sMBR promising approach NMT decoding
159,ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use,"['Junjie Ye', 'Zhengyin Du', 'Xuesong Yao', 'Weijian Lin', 'Yufei Xu', 'Zehui Chen', 'Zaiyuan Wang', 'Sining Zhu', 'Zhiheng Xi', 'Siyu Yuan', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang', 'Jiecao Chen']","Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/datasets/bytedance-research/ToolHop.",ToolHop Query Driven Benchmark Evaluating Large Language Models Multi Hop Tool Use Effective evaluation multi hop tool use critical analyzing understanding reasoning function calling capabilities large language models LLMs progress hindered lack reliable evaluation datasets address present ToolHop dataset comprising 995 user queries 3 912 associated tools specifically designed rigorous evaluation multi hop tool use ToolHop ensures diverse queries meaningful interdependencies locally executable tools detailed feedback verifiable answers novel query driven data construction approach includes tool creation document refinement code generation evaluate 14 LLMs model families e LLaMA3 1 Qwen2 5 Gemini1 5 Claude3 5 GPT uncovering significant challenges handling multi hop tool use scenarios leading model GPT 4o achieves accuracy 49 04 underscoring substantial room improvement analysis reveals variations tool use strategies various families offering actionable insights guide development effective approaches Code data https huggingface datasets bytedance research ToolHop
160,Mixture of insighTful Experts (MoTE): The Synergy of Reasoning Chains and Expert Mixtures in Self-Alignment,"['Zhili Liu', 'Yunhao GOU', 'Kai Chen', 'Lanqing HONG', 'Jiahui Gao', 'Fei Mi', 'Yu Zhang', 'Zhenguo Li', 'Xin Jiang', 'Qun Liu', 'James Kwok']",,Mixture insighTful Experts MoTE Synergy Reasoning Chains Expert Mixtures Self Alignment
161,MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment,"['Weicong Qin', 'Yi Xu', 'Weijie Yu', 'Chenglei Shen', 'Ming He', 'Jianping Fan', 'Xiao Zhang', 'Jun Xu']","Personalized product search aims to retrieve and rank items that match users' preferences and search intent. Despite their effectiveness, existing approaches typically assume that users' query fully captures their real motivation. However, our analysis of a real-world e-commerce platform reveals that users often engage in relevant consultations before searching, indicating they refine intents through consultations based on motivation and need. The implied motivation in consultations is a key enhancing factor for personalized search. This unexplored area comes with new challenges including aligning contextual motivations with concise queries, bridging the category-text gap, and filtering noise within sequence history. To address these, we propose a Motivation-Aware Personalized Search (MAPS) method. It embeds queries and consultations into a unified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE) to prioritize critical semantics, and introduces dual alignment: (1) contrastive learning aligns consultations, reviews, and product features; (2) bidirectional attention integrates motivation-aware embeddings with user preferences. Extensive experiments on real and synthetic data show MAPS outperforms existing methods in both retrieval and ranking tasks.",MAPS Motivation Aware Personalized Search LLM Driven Consultation Alignment Personalized product search aims retrieve rank items match users preferences search intent Despite effectiveness existing approaches typically assume users query fully captures real motivation analysis real world e commerce platform reveals users engage relevant consultations searching indicating refine intents consultations based motivation need implied motivation consultations key enhancing factor personalized search unexplored area comes new challenges including aligning contextual motivations concise queries bridging category text gap filtering noise sequence history address propose Motivation Aware Personalized Search MAPS method embeds queries consultations unified semantic space LLMs utilizes Mixture Attention Experts MoAE prioritize critical semantics introduces dual alignment 1 contrastive learning aligns consultations reviews product features 2 bidirectional attention integrates motivation aware embeddings user preferences Extensive experiments real synthetic data MAPS outperforms existing methods retrieval ranking tasks
162,Aristotle: Mastering Logical Reasoning with A Logic-Complete Decompose-Search-Resolve Framework,"['Jundong Xu', 'Hao Fei', 'Meng Luo', 'Qian Liu', 'Liangming Pan', 'William Yang Wang', 'Preslav Nakov', 'Mong-Li Lee', 'Wynne Hsu']","In the context of large language models (LLMs), current advanced reasoning methods have made impressive strides in various reasoning tasks. However, when it comes to logical reasoning tasks, major challenges remain in both efficacy and efficiency. This is rooted in the fact that these systems fail to fully leverage the inherent structure of logical tasks throughout the reasoning processes such as decomposition, search, and resolution. To address this, we propose a logic-complete reasoning framework, Aristotle, with three key components: Logical Decomposer, Logical Search Router, and Logical Resolver. In our framework, symbolic expressions and logical rules are comprehensively integrated into the entire reasoning process, significantly alleviating the bottlenecks of logical reasoning, i.e., reducing sub-task complexity, minimizing search errors, and resolving logical contradictions. The experimental results on several datasets demonstrate that Aristotle consistently outperforms state-of-the-art reasoning frameworks in both accuracy and efficiency, particularly excelling in complex logical reasoning scenarios. We will open-source all our code at https://github.com/Aiden0526/Aristotle.",Aristotle Mastering Logical Reasoning Logic Complete Decompose Search Resolve Framework context large language models LLMs current advanced reasoning methods impressive strides various reasoning tasks comes logical reasoning tasks major challenges remain efficacy efficiency rooted fact systems fail fully leverage inherent structure logical tasks reasoning processes decomposition search resolution address propose logic complete reasoning framework Aristotle key components Logical Decomposer Logical Search Router Logical Resolver framework symbolic expressions logical rules comprehensively integrated entire reasoning process significantly alleviating bottlenecks logical reasoning e reducing sub task complexity minimizing search errors resolving logical contradictions experimental results datasets demonstrate Aristotle consistently outperforms state art reasoning frameworks accuracy efficiency particularly excelling complex logical reasoning scenarios open source code https github com Aiden0526 Aristotle
163,LADM: Long-context Training Data Selection with Attention-based Dependency Measurement for LLMs,"['Jianghao Chen', 'Junhong Wu', 'Yangyifan Xu', 'Jiajun Zhang']","Long-context modeling has drawn more and more attention in the area of Large Language Models (LLMs). Continual training with long-context data becomes the de-facto method to equip LLMs with the ability to process long inputs. However, it still remains an open challenge to measure the quality of long-context training data. To address this issue, we propose a Long-context data selection framework with Attention-based Dependency Measurement (LADM), which can efficiently identify high-quality long-context data from a large-scale, multi-domain pre-training corpus. LADM leverages the retrieval capabilities of the attention mechanism to capture contextual dependencies, ensuring a comprehensive quality measurement of long-context data. Experimental results show that our LADM framework significantly boosts the performance of LLMs on multiple long-context tasks with only 1B tokens for continual training.",LADM Long context Training Data Selection Attention based Dependency Measurement LLMs Long context modeling drawn attention area Large Language Models LLMs Continual training long context data facto method equip LLMs ability process long inputs remains open challenge measure quality long context training data address issue propose Long context data selection framework Attention based Dependency Measurement LADM efficiently identify high quality long context data large scale multi domain pre training corpus LADM leverages retrieval capabilities attention mechanism capture contextual dependencies ensuring comprehensive quality measurement long context data Experimental results LADM framework significantly boosts performance LLMs multiple long context tasks 1B tokens continual training
164,Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training,"['Yuanfan Li', 'Zhaohan Zhang', 'Chengzhengxu Li', 'Chao Shen', 'Xiaoming Liu']","Machine-generated Text (MGT) detection is crucial for regulating and attributing online texts. While the existing MGT detectors achieve strong performance, they remain vulnerable to simple perturbations and adversarial attacks. To build an effective defense against malicious perturbations, we view MGT detection from a threat modeling perspective, that is, analyzing the model's vulnerability from an adversary's point of view and exploring effective mitigations. To this end, we introduce an adversarial framework for training a robust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The GREATER consists of two key components: an adversary GREATER-A and a detector GREATER-D. The GREATER-D learns to defend against the adversarial attack from GREATER-A and generalizes the defense to other attacks. GREATER-A identifies and perturbs the critical tokens in embedding space, along with greedy search and pruning to generate stealthy and disruptive adversarial examples. Besides, we update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D to generalize its defense to different attacks and varying attack intensities. Our experimental results across 10 text perturbation strategies and 6 adversarial attacks show that our GREATER-D reduces the Attack Success Rate (ASR) by 0.67% compared with SOTA defense methods while our GREATER-A is demonstrated to be more effective and efficient than SOTA attack approaches. Codes and dataset are available in https://github.com/Liyuuuu111/GREATER.",Iron Sharpens Iron Defending Attacks Machine Generated Text Detection Adversarial Training Machine generated Text MGT detection crucial regulating attributing online texts existing MGT detectors achieve strong performance remain vulnerable simple perturbations adversarial attacks build effective defense malicious perturbations view MGT detection threat modeling perspective analyzing model s vulnerability adversary s point view exploring effective mitigations end introduce adversarial framework training robust MGT detector named GREedy Adversary PromoTed DefendER GREATER GREATER consists key components adversary GREATER detector GREATER D GREATER D learns defend adversarial attack GREATER generalizes defense attacks GREATER identifies perturbs critical tokens embedding space greedy search pruning generate stealthy disruptive adversarial examples update GREATER GREATER D synchronously encouraging GREATER D generalize defense different attacks varying attack intensities experimental results 10 text perturbation strategies 6 adversarial attacks GREATER D reduces Attack Success Rate ASR 0 67 compared SOTA defense methods GREATER demonstrated effective efficient SOTA attack approaches Codes dataset available https github com Liyuuuu111 GREATER
165,Cultural Learning-Based Culture Adaptation of Language Models,"['Chen Cecilia Liu', 'Anna Korhonen', 'Iryna Gurevych']","Adapting large language models (LLMs) to diverse cultural values is a challenging task, as existing LLMs often reflect the values of specific groups by default, and potentially causing harm to others. In this paper, we present CLCA, a novel framework for enhancing LLM alignment with cultural values based on cultural learning. The framework leverages simulated social interactions to generate conversations in which LLMs engage in role-playing within culturally adapted social scenarios, capturing implicit cultural norms for model fine-tuning. CLCA improves cultural value alignment across various model architectures measured using World Value Survey data, demonstrating the effectiveness of our proposed approach. Our results provide early evidence that understanding intent and social interactions can enhance cultural value adaptation in LLMs, highlighting the promise of training approaches based on cultural learning.",Cultural Learning Based Culture Adaptation Language Models Adapting large language models LLMs diverse cultural values challenging task existing LLMs reflect values specific groups default potentially causing harm paper present CLCA novel framework enhancing LLM alignment cultural values based cultural learning framework leverages simulated social interactions generate conversations LLMs engage role playing culturally adapted social scenarios capturing implicit cultural norms model fine tuning CLCA improves cultural value alignment various model architectures measured using World Value Survey data demonstrating effectiveness proposed approach results provide early evidence understanding intent social interactions enhance cultural value adaptation LLMs highlighting promise training approaches based cultural learning
166,A-TASC: Asian TED-Based Automatic Subtitling Corpus,"['Yuhan Zhou', 'Naoki Yoshinaga']",,TASC Asian TED Based Automatic Subtitling Corpus
167,Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training,"['Youliang Yuan', 'Wenxiang Jiao', 'Wenxuan Wang', 'Jen-tse Huang', 'Jiahao Xu', 'Tian Liang', 'Pinjia He', 'Zhaopeng Tu']","This study addresses a critical gap in safety tuning practices for Large Language Models (LLMs) by identifying and tackling a refusal position bias within safety tuning data, which compromises the models' ability to appropriately refuse generating unsafe content. We introduce a novel approach, Decoupled Refusal Training (DeRTa), designed to empower LLMs to refuse compliance to harmful prompts at any response position, significantly enhancing their safety capabilities. DeRTa incorporates two novel components: (1) Maximum Likelihood Estimation (MLE) with Harmful Response Prefix, which trains models to recognize and avoid unsafe content by appending a segment of harmful response to the beginning of a safe response, and (2) Reinforced Transition Optimization (RTO), which equips models with the ability to transition from potential harm to safety refusal consistently throughout the harmful response sequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model families across six attack scenarios, demonstrates that our method not only improves model safety without compromising performance but also surpasses baseline methods in defending against attacks.",Refuse Feel Unsafe Improving Safety LLMs Decoupled Refusal Training study addresses critical gap safety tuning practices Large Language Models LLMs identifying tackling refusal position bias safety tuning data compromises models ability appropriately refuse generating unsafe content introduce novel approach Decoupled Refusal Training DeRTa designed empower LLMs refuse compliance harmful prompts response position significantly enhancing safety capabilities DeRTa incorporates novel components 1 Maximum Likelihood Estimation MLE Harmful Response Prefix trains models recognize avoid unsafe content appending segment harmful response beginning safe response 2 Reinforced Transition Optimization RTO equips models ability transition potential harm safety refusal consistently harmful response sequence empirical evaluation conducted using LLaMA3 Mistral model families attack scenarios demonstrates method improves model safety compromising performance surpasses baseline methods defending attacks
168,Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from LLMs,"['Yuchen Fu', 'Zifeng Cheng', 'Zhiwei Jiang', 'Zhonghui Wang', 'Yafeng Yin', 'Zhengliang Li', 'Qing Gu']","Extracting sentence embeddings from large language models (LLMs) is a promising direction, as LLMs have demonstrated stronger semantic understanding capabilities. Previous studies typically focus on prompt engineering to elicit sentence embeddings from LLMs by prompting the model to encode sentence information into the embedding of the last token. However, LLMs are mostly decoder-only models with causal attention and the earlier tokens in the sentence cannot attend to the latter tokens, resulting in biased encoding of sentence information and cascading effects on the final decoded token. To this end, we propose a novel Token Prepending (TP) technique that prepends each layer's decoded sentence embedding to the beginning of the sentence in the next layer's input, allowing earlier tokens to attend to the complete sentence information under the causal attention mechanism. The proposed TP technique is a plug-and-play and training-free technique, which means it can be seamlessly integrated with various prompt-based sentence embedding methods and autoregressive LLMs. Extensive experiments on various Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our proposed TP technique can significantly improve the performance of existing prompt-based sentence embedding methods across different LLMs, while incurring negligible additional inference cost.",Token Prepending Training Free Approach Eliciting Better Sentence Embeddings LLMs Extracting sentence embeddings large language models LLMs promising direction LLMs demonstrated stronger semantic understanding capabilities Previous studies typically focus prompt engineering elicit sentence embeddings LLMs prompting model encode sentence information embedding token LLMs decoder models causal attention earlier tokens sentence attend tokens resulting biased encoding sentence information cascading effects final decoded token end propose novel Token Prepending TP technique prepends layer s decoded sentence embedding beginning sentence layer s input allowing earlier tokens attend complete sentence information causal attention mechanism proposed TP technique plug play training free technique means seamlessly integrated various prompt based sentence embedding methods autoregressive LLMs Extensive experiments various Semantic Textual Similarity STS tasks downstream classification tasks demonstrate proposed TP technique significantly improve performance existing prompt based sentence embedding methods different LLMs incurring negligible additional inference cost
169,"No Questions are Stupid, but some are Poorly Posed: Understanding Poorly-Posed Information-Seeking Questions","['Neha Srikanth', 'Rachel Rudinger', 'Jordan Lee Boyd-Graber']",,Questions Stupid Poorly Posed Understanding Poorly Posed Information Seeking Questions
170,Understanding Common Ground Misalignment in Goal-Oriented Dialog: A Case-Study with Ubuntu Chat Logs,"['Rupak Sarkar', 'Neha Srikanth', 'Taylor Hudson', 'Rachel Rudinger', 'Claire Bonial', 'Philip Resnik']","While it is commonly accepted that maintaining common ground plays a role in conversational success, little prior research exists connecting conversational grounding to success in task-oriented conversations. We study failures of grounding in the Ubuntu IRC dataset, where participants use text-only communication to resolve technical issues. We find that disruptions in conversational flow often stem from a misalignment in common ground, driven by a divergence in beliefs and assumptions held by participants. These disruptions, which we call conversational friction, significantly correlate with task success. We find that although LLMs can identify overt cases of conversational friction, they struggle with subtler and more context-dependent instances requiring pragmatic or domain-specific reasoning.",Understanding Common Ground Misalignment Goal Oriented Dialog Case Study Ubuntu Chat Logs commonly accepted maintaining common ground plays role conversational success little prior research exists connecting conversational grounding success task oriented conversations study failures grounding Ubuntu IRC dataset participants use text communication resolve technical issues disruptions conversational flow stem misalignment common ground driven divergence beliefs assumptions held participants disruptions conversational friction significantly correlate task success LLMs identify overt cases conversational friction struggle subtler context dependent instances requiring pragmatic domain specific reasoning
171,"Grounded, or a Good Guesser? A Per-Question Balanced Dataset to Separate Blind from Grounded Models for Embodied Question Answering","['Miles Shelton', 'Nate Wingerd', 'Kritim K Rijal', 'Ayush Garg', 'Adelina Gutic', 'Brett Barnes', 'Catherine Finegan-Dollak']",,Grounded Good Guesser Question Balanced Dataset Separate Blind Grounded Models Embodied Question Answering
172,Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models,"['Olga Loginova', 'Oleksandr Bezrukov', 'Ravi Shekhar', 'Alexey Kravets']","Evaluating Video Language Models (VLMs) is a challenging task. Due to its transparency, Multiple-Choice Question Answering (MCQA) is widely used to measure the performance of these models through accuracy. However, existing MCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to selection bias, when models disproportionately favor certain answer options based on positional patterns observed during training. In this work, we conduct a comprehensive empirical analysis of several VLM architectures across major datasets designed to assess complex video-focused reasoning. We identify where the bias is most pronounced and demonstrate to what extent model responses reflect genuine understanding of video content and related questions, as opposed to reliance on arbitrary patterns or superficial cues, such as answer position. By decomposing the MCQA task and adapting fairness bias metrics to VLMs, we introduce a post-processing calibration technique BOLD to balance this bias. Our results show that reducing selection bias improves not only debiasing metrics but also overall model performance, including Accuracy and F1 Mean score. Our method, by suppressing ""blind guessing"", offers a more cost- and time-effective approach to mitigating selection bias compared to existing techniques. This study represents the first focused investigation of selection bias in video-to-text LLM-powered models.",Addressing Blind Guessing Calibration Selection Bias Multiple Choice Question Answering Video Language Models Evaluating Video Language Models VLMs challenging task transparency Multiple Choice Question Answering MCQA widely used measure performance models accuracy existing MCQA benchmarks fail capture reasoning capabilities VLMs selection bias models disproportionately favor certain answer options based positional patterns observed training work conduct comprehensive empirical analysis VLM architectures major datasets designed assess complex video focused reasoning identify bias pronounced demonstrate extent model responses reflect genuine understanding video content related questions opposed reliance arbitrary patterns superficial cues answer position decomposing MCQA task adapting fairness bias metrics VLMs introduce post processing calibration technique BOLD balance bias results reducing selection bias improves debiasing metrics overall model performance including Accuracy F1 Mean score method suppressing blind guessing offers cost time effective approach mitigating selection bias compared existing techniques study represents focused investigation selection bias video text LLM powered models
173,Towards Reward Fairness in RLHF: From a Resource Allocation Perspective,"['Sheng Ouyang', 'Yulan Hu', 'Ge Chen', 'Qingyang Li', 'Fuzheng Zhang', 'Yong Liu']","Rewards serve as proxies for human preferences and play a crucial role in Reinforcement Learning from Human Feedback (RLHF). However, if these rewards are inherently imperfect, exhibiting various biases, they can adversely affect the alignment of large language models (LLMs). In this paper, we collectively define the various biases present in rewards as the problem of reward unfairness. We propose a bias-agnostic method to address the issue of reward fairness from a resource allocation perspective, without specifically designing for each type of bias, yet effectively mitigating them. Specifically, we model preference learning as a resource allocation problem, treating rewards as resources to be allocated while considering the trade-off between utility and fairness in their distribution. We propose two methods, Fairness Regularization and Fairness Coefficient, to achieve fairness in rewards. We apply our methods in both verification and reinforcement learning scenarios to obtain a fairness reward model and a policy model, respectively. Experiments conducted in these scenarios demonstrate that our approach aligns LLMs with human preferences in a more fair manner.",Reward Fairness RLHF Resource Allocation Perspective Rewards serve proxies human preferences play crucial role Reinforcement Learning Human Feedback RLHF rewards inherently imperfect exhibiting various biases adversely affect alignment large language models LLMs paper collectively define various biases present rewards problem reward unfairness propose bias agnostic method address issue reward fairness resource allocation perspective specifically designing type bias effectively mitigating Specifically model preference learning resource allocation problem treating rewards resources allocated considering trade utility fairness distribution propose methods Fairness Regularization Fairness Coefficient achieve fairness rewards apply methods verification reinforcement learning scenarios obtain fairness reward model policy model respectively Experiments conducted scenarios demonstrate approach aligns LLMs human preferences fair manner
174,Taming LLMs with Gradient Grouping,"['Siyuan Li', 'Juanxi Tian', 'Zedong Wang', 'Xin Jin', 'Zicheng Liu', 'Wentao Zhang', 'Dan Xu']",,Taming LLMs Gradient Grouping
175,LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews,"['Sukannya Purkayastha', 'Zhuang Li', 'Anne Lauscher', 'Lizhen Qu', 'Iryna Gurevych']","Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of `quick' heuristics, referred to as lazy thinking, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve the peer-reviewing process. However, there is limited NLP research on this issue, and no real-world dataset exists to support the development of detection tools. This work introduces LazyReview, a dataset of peer-review sentences annotated with fine-grained lazy thinking categories. Our analysis reveals that Large Language Models (LLMs) struggle to detect these instances in a zero-shot setting. However, instruction-based fine-tuning on our dataset significantly boosts performance by 10-20 performance points, highlighting the importance of high-quality training data. Furthermore, a controlled experiment demonstrates that reviews revised with lazy thinking feedback are more comprehensive and actionable than those written without such feedback. We will release our dataset and the enhanced guidelines that can be used to train junior reviewers in the community. (Code available here: https://github.com/UKPLab/acl2025-lazy-review)",LazyReview Dataset Uncovering Lazy Thinking NLP Peer Reviews Peer review cornerstone quality control scientific publishing increasing workload unintended use quick heuristics referred lazy thinking emerged recurring issue compromising review quality Automated methods detect heuristics help improve peer reviewing process limited NLP research issue real world dataset exists support development detection tools work introduces LazyReview dataset peer review sentences annotated fine grained lazy thinking categories analysis reveals Large Language Models LLMs struggle detect instances zero shot setting instruction based fine tuning dataset significantly boosts performance 10 20 performance points highlighting importance high quality training data Furthermore controlled experiment demonstrates reviews revised lazy thinking feedback comprehensive actionable written feedback release dataset enhanced guidelines used train junior reviewers community Code available https github com UKPLab acl2025 lazy review
176,Revisiting Common Assumptions about Arabic Dialects in NLP,"['Amr Keleg', 'Sharon Goldwater', 'Walid Magdy']","Arabic has diverse dialects, where one dialect can be substantially different from the others. In the NLP literature, some assumptions about these dialects are widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable regional dialects"") and are manifested in different computational tasks such as Arabic Dialect Identification (ADI). However, these assumptions are not quantitatively verified. We identify four of these assumptions and examine them by extending and analyzing a multi-label dataset, where the validity of each sentence in 11 different country-level dialects is manually assessed by speakers of these dialects. Our analysis indicates that the four assumptions oversimplify reality, and some of them are not always accurate. This in turn might be hindering further progress in different Arabic NLP tasks.",Revisiting Common Assumptions Arabic Dialects NLP Arabic diverse dialects dialect substantially different NLP literature assumptions dialects widely adopted e g Arabic dialects grouped distinguishable regional dialects manifested different computational tasks Arabic Dialect Identification ADI assumptions quantitatively verified identify assumptions examine extending analyzing multi label dataset validity sentence 11 different country level dialects manually assessed speakers dialects analysis indicates assumptions oversimplify reality accurate turn hindering progress different Arabic NLP tasks
177,Retrieve to Explain: Evidence-driven Predictions for Explainable Drug Target Identification,"['Ravi Patel', 'Angus Brayne', 'Rogier Hintzen', 'Daniel Jaroslawicz', 'Georgiana Neculae', 'Dane S. Corneil']","Language models hold incredible promise for enabling scientific discovery by synthesizing massive research corpora. Many complex scientific research questions have multiple plausible answers, each supported by evidence of varying strength. However, existing language models lack the capability to quantitatively and faithfully compare answer plausibility in terms of supporting evidence. To address this, we introduce Retrieve to Explain (R2E), a retrieval-based model that scores and ranks all possible answers to a research question based on evidence retrieved from a document corpus. The architecture represents each answer only in terms of its supporting evidence, with the answer itself masked. This allows us to extend feature attribution methods such as Shapley values, to transparently attribute answer scores to supporting evidence at inference time. The architecture also allows incorporation of new evidence without retraining, including non-textual data modalities templated into natural language. We developed R2E for the challenging scientific discovery task of drug target identification, a human-in-the-loop process where failures are extremely costly and explainability paramount. When predicting whether drug targets will subsequently be confirmed as efficacious in clinical trials, R2E not only matches non-explainable literature-based models but also surpasses a genetics-based target identification approach used throughout the pharmaceutical industry.",Retrieve Explain Evidence driven Predictions Explainable Drug Target Identification Language models hold incredible promise enabling scientific discovery synthesizing massive research corpora complex scientific research questions multiple plausible answers supported evidence varying strength existing language models lack capability quantitatively faithfully compare answer plausibility terms supporting evidence address introduce Retrieve Explain R2E retrieval based model scores ranks possible answers research question based evidence retrieved document corpus architecture represents answer terms supporting evidence answer masked allows extend feature attribution methods Shapley values transparently attribute answer scores supporting evidence inference time architecture allows incorporation new evidence retraining including non textual data modalities templated natural language developed R2E challenging scientific discovery task drug target identification human loop process failures extremely costly explainability paramount predicting drug targets subsequently confirmed efficacious clinical trials R2E matches non explainable literature based models surpasses genetics based target identification approach used pharmaceutical industry
178,Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas,"['Nishant Balepur', 'Vishakh Padmakumar', 'Fumeng Yang', 'Shi Feng', 'Rachel Rudinger', 'Jordan Lee Boyd-Graber']","LLMs are aligned to follow input instructions by learning which of two responses users prefer for a prompt. However, such preference data do not convey why users prefer responses that are chosen or rejected, so LLMs trained on these datasets cannot tailor responses to varied user needs. To surface these parameters of personalization, we apply abductive reasoning to preference data, inferring needs and interests of users, i.e., personas, that may prefer either response. We test this idea in two steps: Persona Inference (PI), abductively inferring personas of users who prefer chosen or rejected outputs, and Persona Tailoring (PT), training models to tailor outputs to personas from PI. We show: 1) LLMs infer personas accurately explaining why different users may prefer both chosen or rejected outputs; 2) Training on preference data augmented with PI personas via PT boosts personalization and generalizes to supporting user-written personas; and 3) Rejected response personas form harder personalization evaluations, showing PT better aids users with uncommon preferences versus typical alignment methods. We argue for an abductive view of preferences for personalization, asking not only which response is better but when, why, and for whom.",Boat Does Float Improving Personalization Preference Tuning Inferred User Personas LLMs aligned follow input instructions learning responses users prefer prompt preference data convey users prefer responses chosen rejected LLMs trained datasets tailor responses varied user needs surface parameters personalization apply abductive reasoning preference data inferring needs interests users e personas prefer response test idea steps Persona Inference PI abductively inferring personas users prefer chosen rejected outputs Persona Tailoring PT training models tailor outputs personas PI 1 LLMs infer personas accurately explaining different users prefer chosen rejected outputs 2 Training preference data augmented PI personas PT boosts personalization generalizes supporting user written personas 3 Rejected response personas form harder personalization evaluations showing PT better aids users uncommon preferences versus typical alignment methods argue abductive view preferences personalization asking response better
179,Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above,"['Nishant Balepur', 'Rachel Rudinger', 'Jordan Lee Boyd-Graber']","Multiple choice question answering (MCQA) is popular for LLM evaluation due to its simplicity and human-like testing, but we argue for its reform. We first reveal flaws in MCQA's format, as it struggles to: 1) test generation/subjectivity; 2) match LLM use cases; and 3) fully test knowledge. We instead advocate for generative formats based on human testing, where LLMs construct and explain answers, better capturing user needs and knowledge while remaining easy to score. We then show even when MCQA is a useful format, its datasets suffer from: leakage; unanswerability; shortcuts; and saturation. In each issue, we give fixes from education, like rubrics to guide MCQ writing; scoring methods to bridle guessing; and Item Response Theory to build harder MCQs. Lastly, we discuss LLM errors in MCQA, robustness, biases, and unfaithful explanations, showing how our prior solutions better measure or address these issues. While we do not need to desert MCQA, we encourage more efforts in refining the task based on educational testing, advancing evaluations.",Best Describes Multiple Choice Evaluation LLMs Forced B Flawed C Fixable D Multiple choice question answering MCQA popular LLM evaluation simplicity human like testing argue reform reveal flaws MCQA s format struggles 1 test generation subjectivity 2 match LLM use cases 3 fully test knowledge instead advocate generative formats based human testing LLMs construct explain answers better capturing user needs knowledge remaining easy score MCQA useful format datasets suffer leakage unanswerability shortcuts saturation issue fixes education like rubrics guide MCQ writing scoring methods bridle guessing Item Response Theory build harder MCQs Lastly discuss LLM errors MCQA robustness biases unfaithful explanations showing prior solutions better measure address issues need desert MCQA encourage efforts refining task based educational testing advancing evaluations
180,Detection of Human and Machine-Authored Fake News in Urdu,"['Muhammad Zain Ali', 'Yuxia Wang', 'Bernhard Pfahringer', 'Tony C Smith']","The rise of social media has amplified the spread of fake news, now further complicated by large language models (LLMs) like ChatGPT, which ease the generation of highly convincing, error-free misinformation, making it increasingly challenging for the public to discern truth from falsehood. Traditional fake news detection methods relying on linguistic cues also becomes less effective. Moreover, current detectors primarily focus on binary classification and English texts, often overlooking the distinction between machine-generated true vs. fake news and the detection in low-resource languages. To this end, we updated detection schema to include machine-generated news with focus on the Urdu language. We further propose a hierarchical detection strategy to improve the accuracy and robustness. Experiments show its effectiveness across four datasets in various settings.",Detection Human Machine Authored Fake News Urdu rise social media amplified spread fake news complicated large language models LLMs like ChatGPT ease generation highly convincing error free misinformation making increasingly challenging public discern truth falsehood Traditional fake news detection methods relying linguistic cues effective current detectors primarily focus binary classification English texts overlooking distinction machine generated true vs fake news detection low resource languages end updated detection schema include machine generated news focus Urdu language propose hierarchical detection strategy improve accuracy robustness Experiments effectiveness datasets various settings
181,An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals.,"['Yangyang Zhao', 'Ben Niu', 'Libo Qin', 'Shihan Wang']",,Efficient Task Oriented Dialogue Policy Evolutionary Reinforcement Learning Injected Elite Individuals
182,SR-LLM: Rethinking the Structured Representation in Large Language Model,"['Jiahuan Zhang', 'Tianheng Wang', 'Ziyi Huang', 'Yulong Wu', 'HANQING WU', 'DongbaiChen', 'Linfeng Song', 'Yue Zhang', 'guozheng rao', 'Kaicheng Yu']",,SR LLM Rethinking Structured Representation Large Language Model
183,Learning Sparsity for Effective and Efficient Music Performance Question Answering,"['Xingjian Diao', 'Tianzhen Yang', 'Chunhui Zhang', 'Weiyi Wu', 'Ming Cheng', 'Jiang Gui']","Music performances, characterized by dense and continuous audio as well as seamless audio-visual integration, present unique challenges for multimodal scene understanding and reasoning. Recent Music Performance Audio-Visual Question Answering (Music AVQA) datasets have been proposed to reflect these challenges, highlighting the continued need for more effective integration of audio-visual representations in complex question answering. However, existing Music AVQA methods often rely on dense and unoptimized representations, leading to inefficiencies in the isolation of key information, the reduction of redundancy, and the prioritization of critical samples. To address these challenges, we introduce Sparsify, a sparse learning framework specifically designed for Music AVQA. It integrates three sparsification strategies into an end-to-end pipeline and achieves state-of-the-art performance on the Music AVQA datasets. In addition, it reduces training time by 28.32% compared to its fully trained dense counterpart while maintaining accuracy, demonstrating clear efficiency gains. To further improve data efficiency, we propose a key-subset selection algorithm that selects and uses approximately 25% of MUSIC-AVQA v2.0 training data and retains 70-80% of full-data performance across models.",Learning Sparsity Effective Efficient Music Performance Question Answering Music performances characterized dense continuous audio seamless audio visual integration present unique challenges multimodal scene understanding reasoning Recent Music Performance Audio Visual Question Answering Music AVQA datasets proposed reflect challenges highlighting continued need effective integration audio visual representations complex question answering existing Music AVQA methods rely dense unoptimized representations leading inefficiencies isolation key information reduction redundancy prioritization critical samples address challenges introduce Sparsify sparse learning framework specifically designed Music AVQA integrates sparsification strategies end end pipeline achieves state art performance Music AVQA datasets addition reduces training time 28 32 compared fully trained dense counterpart maintaining accuracy demonstrating clear efficiency gains improve data efficiency propose key subset selection algorithm selects uses approximately 25 MUSIC AVQA v2 0 training data retains 70 80 data performance models
184,Taming Language Models for Text-attributed Graph Learning with Decoupled Aggregation,"['Chuang Zhou', 'Zhu Wang', 'Shengyuan Chen', 'Jiahe Du', 'Qiyuan Zheng', 'Zhaozhuo Xu', 'Xiao Huang']",,Taming Language Models Text attributed Graph Learning Decoupled Aggregation
185,Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering,"['Zifeng Cheng', 'Zhonghui Wang', 'Yuchen Fu', 'Zhiwei Jiang', 'Yafeng Yin', 'Cong Wang', 'Qing Gu']","Extracting sentence embeddings from large language models (LLMs) is a practical direction, as it requires neither additional data nor fine-tuning. Previous studies usually focus on prompt engineering to guide LLMs to encode the core semantic information of the sentence into the embedding of the last token. However, the last token in these methods still encodes an excess of non-essential information, such as stop words, limiting its encoding capacity. To this end, we propose a Contrastive Prompting (CP) method that introduces an extra auxiliary prompt to elicit better sentence embedding. By contrasting with the auxiliary prompt, CP can steer existing prompts to encode the core semantics of the sentence, rather than non-essential information. CP is a plug-and-play inference-time intervention method that can be combined with various prompt-based methods. Extensive experiments on Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our method can improve the performance of existing prompt-based methods across different LLMs. Our code will be released at https://github.com/zifengcheng/CP.",Contrastive Prompting Enhances Sentence Embeddings LLMs Inference Time Steering Extracting sentence embeddings large language models LLMs practical direction requires additional data fine tuning Previous studies usually focus prompt engineering guide LLMs encode core semantic information sentence embedding token token methods encodes excess non essential information stop words limiting encoding capacity end propose Contrastive Prompting CP method introduces extra auxiliary prompt elicit better sentence embedding contrasting auxiliary prompt CP steer existing prompts encode core semantics sentence non essential information CP plug play inference time intervention method combined various prompt based methods Extensive experiments Semantic Textual Similarity STS tasks downstream classification tasks demonstrate method improve performance existing prompt based methods different LLMs code released https github com zifengcheng CP
186,Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence,"['Jinghan He', 'Kuan Zhu', 'Haiyun Guo', 'Junfeng Fang', 'Zhenglin Hua', 'Yuheng Jia', 'Ming Tang', 'Tat-Seng Chua', 'Jinqiao Wang']","Large vision-language models (LVLMs) have made substantial progress in integrating large language models (LLMs) with visual inputs, enabling advanced multimodal reasoning. Despite their success, a persistent challenge is hallucination-where generated text fails to accurately reflect visual content-undermining both accuracy and reliability. Existing methods focus on alignment training or decoding refinements but primarily address symptoms at the generation stage without probing the underlying causes. In this work, we investigate the internal mechanisms driving hallucination in LVLMs, with an emphasis on the multi-head attention module. Specifically, we introduce Vision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of attention head outputs to visual context. Based on this, our findings reveal the presence of vision-aware attention heads that are more attuned to visual information; however, the model's overreliance on its prior language patterns is closely related to hallucinations. Building on these insights, we propose Vision-aware Head Reinforcement (VHR), a training-free approach to mitigate hallucination by enhancing the role of vision-aware attention heads. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art approaches in mitigating hallucinations, while maintaining high efficiency with negligible additional time overhead.",Cracking Code Hallucination LVLMs Vision aware Head Divergence Large vision language models LVLMs substantial progress integrating large language models LLMs visual inputs enabling advanced multimodal reasoning Despite success persistent challenge hallucination generated text fails accurately reflect visual content undermining accuracy reliability Existing methods focus alignment training decoding refinements primarily address symptoms generation stage probing underlying causes work investigate internal mechanisms driving hallucination LVLMs emphasis multi head attention module Specifically introduce Vision aware Head Divergence VHD metric quantifies sensitivity attention head outputs visual context Based findings reveal presence vision aware attention heads attuned visual information model s overreliance prior language patterns closely related hallucinations Building insights propose Vision aware Head Reinforcement VHR training free approach mitigate hallucination enhancing role vision aware attention heads Extensive experiments demonstrate method achieves superior performance compared state art approaches mitigating hallucinations maintaining high efficiency negligible additional time overhead
187,Hierarchical Document Refinement for Long-context Retrieval-augmented Generation,"['Jiajie Jin', 'Xiaoxi Li', 'Guanting Dong', 'Yuyao Zhang', 'Yutao Zhu', 'Yongkang Wu', 'Zhonghua Li', 'YE QI', 'Zhicheng Dou']","Real-world RAG applications often encounter long-context input scenarios, where redundant information and noise results in higher inference costs and reduced performance. To address these challenges, we propose LongRefiner, an efficient plug-and-play refiner that leverages the inherent structural characteristics of long documents. LongRefiner employs dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model. Experiments on seven QA datasets demonstrate that LongRefiner achieves competitive performance in various scenarios while using 10x fewer computational costs and latency compared to the best baseline. Further analysis validates that LongRefiner is scalable, efficient, and effective, providing practical insights for real-world long-text RAG applications. Our code is available at https://github.com/ignorejjj/LongRefiner.",Hierarchical Document Refinement Long context Retrieval augmented Generation Real world RAG applications encounter long context input scenarios redundant information noise results higher inference costs reduced performance address challenges propose LongRefiner efficient plug play refiner leverages inherent structural characteristics long documents LongRefiner employs dual level query analysis hierarchical document structuring adaptive refinement multi task learning single foundation model Experiments seven QA datasets demonstrate LongRefiner achieves competitive performance various scenarios using 10x fewer computational costs latency compared best baseline analysis validates LongRefiner scalable efficient effective providing practical insights real world long text RAG applications code available https github com ignorejjj LongRefiner
188,Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations,"['Chaoyi Xiang', 'Chunhua Liu', 'Simon De Deyne', 'Lea Frermann']","As the impact of large language models increases, understanding the moral values they reflect becomes ever more important. Assessing the nature of moral values as understood by these models via direct prompting is challenging due to potential leakage of human norms into model training data, and their sensitivity to prompt formulation. Instead, we propose to use word associations, which have been shown to reflect moral reasoning in humans, as low-level underlying representations to obtain a more robust picture of LLMs' moral reasoning. We study moral differences in associations from western English-speaking communities and LLMs trained predominantly on English data. First, we create a large dataset of LLM-generated word associations, resembling an existing data set of human word associations. Next, we propose a novel method to propagate moral values based on seed words derived from Moral Foundation Theory through the human and LLM-generated association graphs. Finally, we compare the resulting moral conceptualizations, highlighting detailed but systematic differences between moral values emerging from English speakers and LLM associations.",Comparing Moral Values Western English speaking societies LLMs Word Associations impact large language models increases understanding moral values reflect important Assessing nature moral values understood models direct prompting challenging potential leakage human norms model training data sensitivity prompt formulation Instead propose use word associations shown reflect moral reasoning humans low level underlying representations obtain robust picture LLMs moral reasoning study moral differences associations western English speaking communities LLMs trained predominantly English data create large dataset LLM generated word associations resembling existing data set human word associations propose novel method propagate moral values based seed words derived Moral Foundation Theory human LLM generated association graphs Finally compare resulting moral conceptualizations highlighting detailed systematic differences moral values emerging English speakers LLM associations
189,TEACH: A Contrastive Knowledge Adaptive Distillation Framework for Ancient Chinese Understanding,"['Yuting Wei', 'Qi Meng', 'Yuanxing Xu', 'Bin Wu']",,TEACH Contrastive Knowledge Adaptive Distillation Framework Ancient Chinese Understanding
190,RAG-Critic: Leveraging Automated Critic-Guided Agentic Workflow for Retrieval Augmented Generation,"['Guanting Dong', 'Jiajie Jin', 'Xiaoxi Li', 'Yutao Zhu', 'Zhicheng Dou', 'Ji-Rong Wen']",,RAG Critic Leveraging Automated Critic Guided Agentic Workflow Retrieval Augmented Generation
191,Progressive Multimodal Reasoning via Active Retrieval,"['Guanting Dong', 'Chenghao Zhang', 'Mengjie Deng', 'Yutao Zhu', 'Zhicheng Dou', 'Ji-Rong Wen']","Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning.",Progressive Multimodal Reasoning Active Retrieval Multi step multimodal reasoning tasks pose significant challenges multimodal large language models MLLMs finding effective ways enhance performance scenarios remains unresolved issue paper propose AR MCTS universal framework designed progressively improve reasoning capabilities MLLMs Active Retrieval AR Monte Carlo Tree Search MCTS approach begins development unified retrieval module retrieves key supporting insights solving complex reasoning problems hybrid modal retrieval corpus bridge gap automated multimodal reasoning verification employ MCTS algorithm combined active retrieval mechanism enables automatic generation step wise annotations strategy dynamically retrieves key insights reasoning step moving traditional beam search sampling improve diversity reliability reasoning space Additionally introduce process reward model aligns progressively support automatic verification multimodal reasoning tasks Experimental results complex multimodal reasoning benchmarks confirm effectiveness AR MCTS framework enhancing performance various multimodal models analysis demonstrates AR MCTS optimize sampling diversity accuracy yielding reliable multimodal reasoning
192,Pre-training Distillation for Large Language Models: A Design Space Exploration,"['Hao Peng', 'Xin Lv', 'Yushi Bai', 'Zijun Yao', 'Jiajie Zhang', 'Lei Hou', 'Juanzi Li']","Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses generated by the teacher model. In this paper, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD). We first conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, validating the effectiveness of PD. Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits. We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while a larger teacher LLM does not necessarily guarantee better results. We hope our exploration of the design space will inform future practices in pre-training distillation.",Pre training Distillation Large Language Models Design Space Exploration Knowledge distillation KD aims transfer knowledge large teacher model smaller student model Previous work applying KD field large language models LLMs typically focused post training phase student LLM learns directly instructions corresponding responses generated teacher model paper extend KD pre training phase LLMs named pre training distillation PD conduct preliminary experiment using GLM 4 9B teacher LLM distill 1 9B parameter student LLM validating effectiveness PD Considering key impact factors distillation systematically explore design space pre training distillation aspects logits processing loss selection scaling law offline online logits conduct extensive experiments explore design space pre training distillation better configurations interesting conclusions larger student LLMs generally benefiting pre training distillation larger teacher LLM does necessarily guarantee better results hope exploration design space inform future practices pre training distillation
193,Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions,"['Pu Jian', 'Donglei Yu', 'Jiajun Zhang', 'Shuo Ren', 'Wen Yang']",,Teaching Vision Language Models Ask Resolving Ambiguity Visual Questions
194,LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks,"['Yushi Bai', 'Shangqing Tu', 'Jiajie Zhang', 'Hao Peng', 'Xiaozhi Wang', 'Xin Lv', 'Shulin Cao', 'Jiazheng Xu', 'Lei Hou', 'Yuxiao Dong', 'Jie Tang', 'Juanzi Li']","This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at https://longbench2.github.io.",LongBench v2 Deeper Understanding Reasoning Realistic Long context Multitasks paper introduces LongBench v2 benchmark designed assess ability LLMs handle long context problems requiring deep understanding reasoning real world multitasks LongBench v2 consists 503 challenging multiple choice questions contexts ranging 8k 2M words major task categories single document QA multi document QA long context learning long dialogue history understanding code repository understanding long structured data understanding ensure breadth practicality collect data nearly 100 highly educated individuals diverse professional backgrounds employ automated manual review processes maintain high quality difficulty resulting human experts achieving 53 7 accuracy 15 minute time constraint evaluation reveals best performing model directly answers questions achieves 50 1 accuracy contrast o1 preview model includes longer reasoning achieves 57 7 surpassing human baseline 4 results highlight importance enhanced reasoning ability scaling inference time compute tackle long context challenges LongBench v2 project available https longbench2 github io
195,Battling against Tough Resister: Strategy Planning with Adversarial Game for Non-collaborative Dialogues,"['Haiyang Wang', 'Zhiliang Tian', 'Yuchen Pan', 'Xin Song', 'Xin Niu', 'Minlie Huang', 'Bin Zhou']",,Battling Tough Resister Strategy Planning Adversarial Game Non collaborative Dialogues
196,Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts,"['Youcheng Huang', 'Chen Huang', 'Duanyu Feng', 'Wenqiang Lei', 'Jiancheng Lv']","Understanding the inner workings of Large Language Models (LLMs) is a critical research frontier. Prior research has shown that a single LLM's concept representations can be captured as steering vectors (SVs), enabling the control of LLM behavior (e.g., towards generating harmful content). Our work takes a novel approach by exploring the intricate relationships between concept representations across different LLMs, drawing an intriguing parallel to Plato's Allegory of the Cave. In particular, we introduce a linear transformation method to bridge these representations and present three key findings: 1) Concept representations across different LLMs can be effectively aligned using simple linear transformations, enabling efficient cross-model transfer and behavioral control via SVs. 2) This linear transformation generalizes across concepts, facilitating alignment and control of SVs representing different concepts across LLMs. 3) A weak-to-strong transferability exists between LLM concept representations, whereby SVs extracted from smaller LLMs can effectively control the behavior of larger LLMs.",Cross model Transferability Large Language Models Platonic Representations Concepts Understanding inner workings Large Language Models LLMs critical research frontier Prior research shown single LLM s concept representations captured steering vectors SVs enabling control LLM behavior e g generating harmful content work takes novel approach exploring intricate relationships concept representations different LLMs drawing intriguing parallel Plato s Allegory Cave particular introduce linear transformation method bridge representations present key findings 1 Concept representations different LLMs effectively aligned using simple linear transformations enabling efficient cross model transfer behavioral control SVs 2 linear transformation generalizes concepts facilitating alignment control SVs representing different concepts LLMs 3 weak strong transferability exists LLM concept representations SVs extracted smaller LLMs effectively control behavior larger LLMs
197,FoldMoE: Efficient Long Sequence MoE Training via Attention-MoE Pipelining,"['Guichao Zhu', 'Lintian Lei', 'Yuhao QING', 'Yichao Fu', 'Fanxin Li', 'Dong HUANG', 'Zekai Sun', 'Heming Cui']",,FoldMoE Efficient Long Sequence MoE Training Attention MoE Pipelining
198,LongReward: Improving Long-context Large Language Models with AI Feedback,"['Jiajie Zhang', 'Zhongni Hou', 'Xin Lv', 'Shulin Cao', 'Zhenyu Hou', 'Yilin Niu', 'Lei Hou', 'Yuxiao Dong', 'Ling Feng', 'Juanzi Li']","Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinforcement learning (RL) with appropriate reward signals can further enhance models' capacities. However, how to obtain reliable rewards in long-context scenarios remains unexplored. To this end, we propose LongReward, a novel method that utilizes an off-the-shelf LLM to provide rewards for long-context model responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, each with a carefully designed assessment pipeline. By combining LongReward and offline RL algorithm DPO, we are able to effectively improve long-context SFT models. Our experiments indicate that LongReward not only significantly improves models' long-context performance but also enhances their ability to follow short instructions. We also find that long-context DPO with LongReward and conventional short-context DPO can be used together without hurting either one's performance.",LongReward Improving Long context Large Language Models AI Feedback significant advancements achieved developing long context large language models LLMs compromised quality LLM synthesized data supervised fine tuning SFT affects long context performance SFT models leads inherent limitations principle reinforcement learning RL appropriate reward signals enhance models capacities obtain reliable rewards long context scenarios remains unexplored end propose LongReward novel method utilizes shelf LLM provide rewards long context model responses human valued dimensions helpfulness logicality faithfulness completeness carefully designed assessment pipeline combining LongReward offline RL algorithm DPO able effectively improve long context SFT models experiments indicate LongReward significantly improves models long context performance enhances ability follow short instructions long context DPO LongReward conventional short context DPO used hurting s performance
199,"Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles","['Yuxi Xia', 'Pedro Henrique Luz de Araujo', 'Klim Zaporojets', 'Benjamin Roth']","Calibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.",Influences LLM Calibration Study Response Agreement Loss Functions Prompt Styles Calibration alignment model confidence prediction accuracy critical reliable deployment large language models LLMs Existing works neglect measure generalization methods prompt styles different sizes LLMs address define controlled experimental setting covering 12 LLMs prompt styles additionally investigate incorporating response agreement multiple LLMs appropriate loss function improve calibration performance Concretely build Calib n novel framework trains auxiliary model confidence estimation aggregates responses multiple LLMs capture inter model agreement optimize calibration integrate focal AUC surrogate losses alongside binary cross entropy Experiments datasets demonstrate response agreement focal loss improve calibration baselines shot prompts effective auxiliary model based methods auxiliary models demonstrate robust calibration performance accuracy variations outperforming LLMs internal probabilities verbalized confidences insights deepen understanding influence factors LLM calibration supporting reliable deployment diverse applications
200,UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench,"['Boxi Yu', 'Yuxuan Zhu', 'Pinjia He', 'Daniel Kang']","The advent of Large Language Models (LLMs) has spurred the development of coding agents for real-world code generation. As a widely used benchmark for evaluating the code generation capabilities of these agents, SWE-Bench uses real-world problems based on GitHub issues and their corresponding pull requests. However, the manually written test cases included in these pull requests are often insufficient, allowing generated patches to pass the tests without resolving the underlying issue. To address this challenge, we introduce UTGenerator, an LLM-driven test case generator that automatically analyzes codebases and dependencies to generate test cases for real-world Python projects. Building on UTGenerator, we propose UTBoost, a comprehensive framework for test case augmentation. In our evaluation, we identified 36 task instances with insufficient test cases and uncovered 345 erroneous patches incorrectly labeled as passed in the original SWE Bench. These corrections, impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard entries, yield 18 and 11 ranking changes, respectively.",UTBoost Rigorous Evaluation Coding Agents SWE Bench advent Large Language Models LLMs spurred development coding agents real world code generation widely used benchmark evaluating code generation capabilities agents SWE Bench uses real world problems based GitHub issues corresponding pull requests manually written test cases included pull requests insufficient allowing generated patches pass tests resolving underlying issue address challenge introduce UTGenerator LLM driven test case generator automatically analyzes codebases dependencies generate test cases real world Python projects Building UTGenerator propose UTBoost comprehensive framework test case augmentation evaluation identified 36 task instances insufficient test cases uncovered 345 erroneous patches incorrectly labeled passed original SWE Bench corrections impacting 40 9 SWE Bench Lite 24 4 SWE Bench Verified leaderboard entries yield 18 11 ranking changes respectively
201,Towards Better Evaluation for Generated Patent Claims,"['Lekang Jiang', 'Pascal A. Scherz', 'Stefan Goetz']","Patent claims define the scope of protection and establish the legal boundaries of an invention. Drafting these claims is a complex and time-consuming process that usually requires the expertise of skilled patent attorneys, which can form a large access barrier for many small enterprises. To solve these challenges, researchers have investigated the use of large language models (LLMs) for automating patent claim generation. However, existing studies highlight inconsistencies between automated evaluation metrics and human expert assessments. To bridge this gap, we introduce Patent-CE, the first comprehensive benchmark for evaluating patent claims. Patent-CE includes comparative claim evaluations annotated by patent experts, focusing on five key criteria: feature completeness, conceptual clarity, terminology consistency, logical linkage, and overall quality. Additionally, we propose PatClaimEval, a novel multi-dimensional evaluation method specifically designed for patent claims. Our experiments demonstrate that PatClaimEval achieves the highest correlation with human expert evaluations across all assessment criteria among all tested metrics. This research provides the groundwork for more accurate evaluations of automated patent claim generation systems.",Better Evaluation Generated Patent Claims Patent claims define scope protection establish legal boundaries invention Drafting claims complex time consuming process usually requires expertise skilled patent attorneys form large access barrier small enterprises solve challenges researchers investigated use large language models LLMs automating patent claim generation existing studies highlight inconsistencies automated evaluation metrics human expert assessments bridge gap introduce Patent CE comprehensive benchmark evaluating patent claims Patent CE includes comparative claim evaluations annotated patent experts focusing key criteria feature completeness conceptual clarity terminology consistency logical linkage overall quality Additionally propose PatClaimEval novel multi dimensional evaluation method specifically designed patent claims experiments demonstrate PatClaimEval achieves highest correlation human expert evaluations assessment criteria tested metrics research provides groundwork accurate evaluations automated patent claim generation systems
202,Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT Refinement in LLMs,"['Haritz Puerto', 'Tilek Chubakov', 'Xiaodan Zhu', 'Harish Tayyar Madabushi', 'Iryna Gurevych']","Requiring a large language model (LLM) to generate intermediary reasoning steps, known as Chain of Thought (CoT), has been shown to be an effective way of boosting performance. Previous approaches have focused on generating multiple independent CoTs, combining them through ensembling or other post-hoc strategies to enhance reasoning. In this work, we introduce a novel approach where LLMs are fine-tuned to generate a sequence of Diverse Chains of Thought (DCoT) within a single inference step, which is fundamentally different from prior work that primarily operate on parallel CoT generations. DCoT allows LLMs to gain the ability to perform within-inference refinement of reasoning chains without requiring external feedback. Through a rigorous set of experiments spanning a wide range of tasks that require various reasoning types, we show that fine-tuning on DCoT improves performance over the CoT baseline across model families and scales (1.3B to 70B). These improvements are particularly impactful for tasks with a large result state space, such as those involving numeric answers. Our work is also significant because both quantitative analyses and manual evaluations reveal the observed gains stem from the models' ability to refine an initial reasoning chain by generating a second, improved chain within the same inference step, demonstrating previously elusive self-improvement. Our code and data are publicly available at https://github.com/UKPLab/acl2025-diverse-cot.",Fine Tuning Diverse Reasoning Chains Drives Inference CoT Refinement LLMs Requiring large language model LLM generate intermediary reasoning steps known Chain Thought CoT shown effective way boosting performance Previous approaches focused generating multiple independent CoTs combining ensembling post hoc strategies enhance reasoning work introduce novel approach LLMs fine tuned generate sequence Diverse Chains Thought DCoT single inference step fundamentally different prior work primarily operate parallel CoT generations DCoT allows LLMs gain ability perform inference refinement reasoning chains requiring external feedback rigorous set experiments spanning wide range tasks require various reasoning types fine tuning DCoT improves performance CoT baseline model families scales 1 3B 70B improvements particularly impactful tasks large result state space involving numeric answers work significant quantitative analyses manual evaluations reveal observed gains stem models ability refine initial reasoning chain generating second improved chain inference step demonstrating previously elusive self improvement code data publicly available https github com UKPLab acl2025 diverse cot
203,Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis,"['Kejian Zhu', 'Shangqing Tu', 'Zhuoran Jin', 'Lei Hou', 'Juanzi Li', 'Jun Zhao']","The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient ($\rho$) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation",Establishing Trustworthy LLM Evaluation Shortcut Neuron Analysis development large language models LLMs depends trustworthy evaluation current evaluations rely public benchmarks prone data contamination issues significantly compromise fairness Previous researches focused constructing dynamic benchmarks address contamination continuously building new benchmarks costly cyclical work aim tackle contamination analyzing mechanisms contaminated models experiments discover overestimation contaminated models likely parameters acquiring shortcut solutions training propose novel method identifying shortcut neurons comparative causal analysis Building introduce evaluation method called shortcut neuron patching suppress shortcut neurons Experiments validate effectiveness approach mitigating contamination Additionally evaluation results exhibit strong linear correlation MixEval recently released trustworthy benchmark achieving Spearman coefficient rho exceeding 0 95 high correlation indicates method closely reveals true capabilities models trustworthy conduct experiments demonstrate generalizability method various benchmarks hyperparameter settings Code https github com GaryStack Trustworthy Evaluation
204,Do Large Language Models have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs,"['Yanzhu Guo', 'Simone Conia', 'Zelin Zhou', 'Min Li', 'Saloni Potdar', 'Henry Xiao']",,Large Language Models English Accent Evaluating Improving Naturalness Multilingual LLMs
205,Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning,"['Zhu Xu', 'Zhiqiang Zhao', 'Zihan Zhang', 'Yuchi Liu', 'Quanwei Shen', 'Fei Liu', 'Yu Kuang', 'Jian He', 'Conglin Liu']","Tokenization methods like Byte-Pair Encoding (BPE) enhance computational efficiency in large language models (LLMs) but often obscure internal character structures within tokens. This limitation hinders LLMs' ability to predict precise character positions, which is crucial in tasks like Chinese Spelling Correction (CSC) where identifying the positions of misspelled characters accelerates correction processes. We propose Token Internal Position Awareness (TIPA), a method that significantly improves models' ability to capture character positions within tokens by training them on reverse character prediction tasks using the tokenizer's vocabulary. Experiments demonstrate that TIPA enhances position prediction accuracy in LLMs, enabling more precise identification of target characters in original text. Furthermore, when applied to downstream tasks that do not require exact position prediction, TIPA still boosts performance in tasks needing character-level information, validating its versatility and effectiveness.",Enhancing Character Level Understanding LLMs Token Internal Structure Learning Tokenization methods like Byte Pair Encoding BPE enhance computational efficiency large language models LLMs obscure internal character structures tokens limitation hinders LLMs ability predict precise character positions crucial tasks like Chinese Spelling Correction CSC identifying positions misspelled characters accelerates correction processes propose Token Internal Position Awareness TIPA method significantly improves models ability capture character positions tokens training reverse character prediction tasks using tokenizer s vocabulary Experiments demonstrate TIPA enhances position prediction accuracy LLMs enabling precise identification target characters original text Furthermore applied downstream tasks require exact position prediction TIPA boosts performance tasks needing character level information validating versatility effectiveness
206,Conformity in Large Language Models,"['Xiaochen Zhu', 'Caiqi Zhang', 'Tom Stafford', 'Nigel Collier', 'Andreas Vlachos']",,Conformity Large Language Models
207,Interpret and Improve In-Context Learning via the Lens of Input-Label Mappings,"['Chenghao Sun', 'Zhen Huang', 'Yonggang Zhang', 'Le Lu', 'Houqiang Li', 'Xinmei Tian', 'Xu Shen', 'Jieping Ye']",,Interpret Improve Context Learning Lens Input Label Mappings
208,Positional Overload: Positional Debiasing and Context Window Extension for Large Language Models using Set Encoding,"['Lukas Kinder', 'Lukas Edman', 'Alexander Fraser', 'Tobias Käfer']",,Positional Overload Positional Debiasing Context Window Extension Large Language Models using Set Encoding
209,FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling,"['Weilin Zhao', 'Tengyu Pan', 'Xu Han', 'Yudi Zhang', 'Sun Ao', 'Yuxiang Huang', 'Kaihuo Zhang', 'Weilun Zhao', 'Yuxuan Li', 'Jie Zhou', 'Hao Zhou', 'Jianyong Wang', 'Maosong Sun', 'Zhiyuan Liu']","Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12$\times$ speedup over the state-of-the-art speculative sampling method EAGLE-2. Code available at https://github.com/thunlp/FR-Spec.",FR Spec Accelerating Large Vocabulary Language Models Frequency Ranked Speculative Sampling Speculative sampling emerged important technique accelerating auto regressive generation process large language models LLMs utilizing draft verify mechanism produce multiple tokens forward pass state art speculative sampling methods use single layer language modeling LM head draft model achieve impressive layer compression efficiency gains substantially reduced large vocabulary LLMs Llama 3 8B vocabulary 128k tokens address present FR Spec frequency ranked speculative sampling framework optimizes draft candidate selection vocabulary space compression constraining draft search frequency prioritized token subset method reduces LM Head computation overhead 75 ensuring equivalence final output distribution Experiments multiple datasets demonstrate average 1 12 times speedup state art speculative sampling method EAGLE 2 Code available https github com thunlp FR Spec
210,VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism,"['Congzhi Zhang', 'Jiawei Peng', 'Zhenglin Wang', 'Yilong Lai', 'Haowen Sun', 'Heng Chang', 'Fei Ma', 'Weijiang Yu']","Large Vision-Language Models (LVLMs) have shown exceptional performance in multimodal tasks, but their effectiveness in complex visual reasoning is still constrained, especially when employing Chain-of-Thought prompting techniques. In this paper, we propose VReST, a novel training-free approach that enhances Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms. VReST meticulously traverses the reasoning landscape by establishing a search tree, where each node encapsulates a reasoning step, and each path delineates a comprehensive reasoning sequence. Our innovative multimodal Self-Reward mechanism assesses the quality of reasoning steps by integrating the utility of sub-questions, answer correctness, and the relevance of vision-language clues, all without the need for additional models. VReST surpasses current prompting methods and secures state-of-the-art performance across three multimodal mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy of test-time scaling laws in multimodal tasks, offering a promising direction for future research.",VReST Enhancing Reasoning Large Vision Language Models Tree Search Self Reward Mechanism Large Vision Language Models LVLMs shown exceptional performance multimodal tasks effectiveness complex visual reasoning constrained especially employing Chain Thought prompting techniques paper propose VReST novel training free approach enhances Reasoning LVLMs Monte Carlo Tree Search Self Reward mechanisms VReST meticulously traverses reasoning landscape establishing search tree node encapsulates reasoning step path delineates comprehensive reasoning sequence innovative multimodal Self Reward mechanism assesses quality reasoning steps integrating utility sub questions answer correctness relevance vision language clues need additional models VReST surpasses current prompting methods secures state art performance multimodal mathematical reasoning benchmarks Furthermore substantiates efficacy test time scaling laws multimodal tasks offering promising direction future research
211,Past Meets Present: Creating Historical Analogy with Large Language Models,"['Nianqi Li', 'Siyu Yuan', 'Jiangjie Chen', 'Jiaqing Liang', 'Feng Wei', 'Zujie Liang', 'Deqing Yang', 'Yanghua Xiao']","Historical analogies, which compare known past events with contemporary but unfamiliar events, are important abilities that help people make decisions and understand the world. However, research in applied history suggests that people have difficulty finding appropriate analogies. And previous studies in the AI community have also overlooked historical analogies. To fill this gap, in this paper, we focus on the historical analogy acquisition task, which aims to acquire analogous historical events for a given event. We explore retrieval and generation methods for acquiring historical analogies based on different large language models (LLMs). Furthermore, we propose a self-reflection method to mitigate hallucinations and stereotypes when LLMs generate historical analogies. Through human evaluations and our specially designed automatic multi-dimensional assessment, we find that LLMs generally have a good potential for historical analogies. And the performance of the models can be further improved by using our self-reflection method.",Past Meets Present Creating Historical Analogy Large Language Models Historical analogies compare known past events contemporary unfamiliar events important abilities help people make decisions understand world research applied history suggests people difficulty finding appropriate analogies previous studies AI community overlooked historical analogies gap paper focus historical analogy acquisition task aims acquire analogous historical events given event explore retrieval generation methods acquiring historical analogies based different large language models LLMs Furthermore propose self reflection method mitigate hallucinations stereotypes LLMs generate historical analogies human evaluations specially designed automatic multi dimensional assessment LLMs generally good potential historical analogies performance models improved using self reflection method
212,Meta-Reflection: A Feedback-Free Reflection Learning Framework,"['Yaoke Wang', 'Yun Zhu', 'XintongBao', 'Wenqiao Zhang', 'Suyang Dai', 'kehan chen', 'Wenqiang Li', 'Gang Huang', 'Siliang Tang', 'Yueting Zhuang']",,Meta Reflection Feedback Free Reflection Learning Framework
213,Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon,"['Chen Zhang', 'Zhiyuan Liao', 'Yansong Feng']","Despite substantial research efforts evaluating how well large language models~(LLMs) handle global cultural diversity, the mechanisms behind their cultural knowledge acquisition, particularly in multilingual settings, remain unclear. We study this question by investigating how cultural knowledge transfers across languages during language adaptation of LLMs. We introduce an interpretable framework for studying this transfer, ensuring training data transparency and controlling transfer effects. Through a study of four non-Anglophonic cultures, we observe bidirectional cultural transfer between English and other high-resource languages, while low-resource languages primarily transfer knowledge to English with limited reverse flow. To explain this asymmetric phenomenon, we propose a frequency-based hypothesis: cultural knowledge appearing more frequently in the pretraining data transfers more easily, which is supported by empirical analysis of the training corpora.",Cross Lingual Transfer Cultural Knowledge Asymmetric Phenomenon Despite substantial research efforts evaluating large language models LLMs handle global cultural diversity mechanisms cultural knowledge acquisition particularly multilingual settings remain unclear study question investigating cultural knowledge transfers languages language adaptation LLMs introduce interpretable framework studying transfer ensuring training data transparency controlling transfer effects study non Anglophonic cultures observe bidirectional cultural transfer English high resource languages low resource languages primarily transfer knowledge English limited reverse flow explain asymmetric phenomenon propose frequency based hypothesis cultural knowledge appearing frequently pretraining data transfers easily supported empirical analysis training corpora
214,Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books,"['Chen Zhang', 'Jiuheng Lin', 'Xiao Liu', 'Zekai Zhang', 'Yansong Feng']","While large language models (LLMs) have shown promise in translating extremely low-resource languages using resources like dictionaries, the effectiveness of grammar books remains debated. This paper investigates the role of grammar books in translating extremely low-resource languages by decomposing it into two key steps: grammar rule retrieval and application. To facilitate the study, we introduce ZhuangRules, a modularized dataset of grammar rules and their corresponding test sentences. Our analysis reveals that rule retrieval constitutes a primary bottleneck in grammar-based translation. Moreover, although LLMs can apply simple rules for translation when explicitly provided, they encounter difficulties in handling more complex rules. To address these challenges, we propose representing grammar rules as code functions, considering their similarities in structure and the benefit of code in facilitating LLM reasoning. Our experiments show that using code rules significantly boosts both rule retrieval and application, ultimately resulting in a 13.1% BLEU improvement in translation.",Read Steps Translating Extremely Low Resource Languages Code Augmented Grammar Books large language models LLMs shown promise translating extremely low resource languages using resources like dictionaries effectiveness grammar books remains debated paper investigates role grammar books translating extremely low resource languages decomposing key steps grammar rule retrieval application facilitate study introduce ZhuangRules modularized dataset grammar rules corresponding test sentences analysis reveals rule retrieval constitutes primary bottleneck grammar based translation LLMs apply simple rules translation explicitly provided encounter difficulties handling complex rules address challenges propose representing grammar rules code functions considering similarities structure benefit code facilitating LLM reasoning experiments using code rules significantly boosts rule retrieval application ultimately resulting 13 1 BLEU improvement translation
215,Confidence v.s. Critique: A Decomposition of Self-Correction Capability for LLMs,"['Zhe Yang', 'Yichang Zhang', 'Yudong Wang', 'Ziyao Xu', 'Junyang Lin', 'Zhifang Sui']","Large Language Models (LLMs) can correct their self-generated responses, but a decline in accuracy after self-correction is also witnessed. To have a deeper understanding of self-correction, we endeavor to decompose, evaluate, and analyze the self-correction behaviors of LLMs. By enumerating and analyzing answer correctness before and after self-correction, we decompose the self-correction capability into confidence (being confident to correct answers) and critique (turning wrong answers to correct) capabilities, and propose two metrics from a probabilistic perspective to measure these 2 capabilities, along with another metric for overall self-correction capability evaluation. Based on our decomposition and evaluation metrics, we conduct extensive experiments and draw some empirical conclusions. For example, we find different models can exhibit distinct behaviors: some models are confident while others are more critical. We also find the trade-off between the two capabilities (i.e. improving one can lead to a decline in the other) when manipulating model self-correction behavior by prompts or in-context learning. Further, we find a simple yet efficient strategy to improve self-correction capability by transforming Supervision Fine-Tuning (SFT) data format, and our strategy outperforms vanilla SFT in both capabilities and achieves much higher accuracy after self-correction. Our code will be publicly available on GitHub.",Confidence v s Critique Decomposition Self Correction Capability LLMs Large Language Models LLMs correct self generated responses decline accuracy self correction witnessed deeper understanding self correction endeavor decompose evaluate analyze self correction behaviors LLMs enumerating analyzing answer correctness self correction decompose self correction capability confidence confident correct answers critique turning wrong answers correct capabilities propose metrics probabilistic perspective measure 2 capabilities metric overall self correction capability evaluation Based decomposition evaluation metrics conduct extensive experiments draw empirical conclusions example different models exhibit distinct behaviors models confident critical trade capabilities e improving lead decline manipulating model self correction behavior prompts context learning simple efficient strategy improve self correction capability transforming Supervision Fine Tuning SFT data format strategy outperforms vanilla SFT capabilities achieves higher accuracy self correction code publicly available GitHub
216,"Automating Legal Concept Interpretation with LLMs: Retrieval, Generation, and Evaluation","['Kangcheng Luo', 'Quzhe Huang', 'Cong Jiang', 'Yansong Feng']",,Automating Legal Concept Interpretation LLMs Retrieval Generation Evaluation
217,Visual Evidence Prompting Mitigates Hallucinations in Large Vision-Language Models,"['Wei Li', 'Zhen Huang', 'Houqiang Li', 'Le Lu', 'Yang Lu', 'Xinmei Tian', 'Xu Shen', 'Jieping Ye']",,Visual Evidence Prompting Mitigates Hallucinations Large Vision Language Models
218,Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration,"['Shao Zhang', 'Xihuai Wang', 'Wenhao Zhang', 'Chaoran Li', 'Junru Song', 'Tingyu Li', 'Lin Qiu', 'Xuezhi Cao', 'Xunliang Cai', 'Wen Yao', 'Weinan Zhang', 'Xinbing Wang', 'Ying Wen']","Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. DPT-Agent can effectively help LLMs convert correct slow thinking and reasoning into executable actions, thereby improving performance. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.",Leveraging Dual Process Theory Language Agent Framework Real time Simultaneous Human AI Collaboration Agents built large language models LLMs excelled turn turn human AI collaboration struggle simultaneous tasks requiring real time interaction Latency issues challenge inferring variable human strategies hinder ability make autonomous decisions explicit instructions experiments current independent 1 2 methods validate necessity using Dual Process Theory DPT real time tasks propose DPT Agent novel language agent framework integrates 1 2 efficient real time simultaneous human AI collaboration DPT Agent s 1 uses Finite state Machine FSM code policy fast intuitive controllable decision making DPT Agent s 2 integrates Theory Mind ToM asynchronous reflection infer human intentions perform reasoning based autonomous decisions demonstrate effectiveness DPT Agent experiments rule based agents human collaborators showing significant improvements mainstream LLM based frameworks DPT Agent effectively help LLMs convert correct slow thinking reasoning executable actions improving performance best knowledge DPT Agent language agent framework achieves successful real time simultaneous human AI collaboration autonomously Code DPT Agent https github com sjtu marl DPT Agent
219,TokAlign: Efficient Vocabulary Adaptation via Token Alignment,"['Chong Li', 'Jiajun Zhang', 'Chengqing Zong']","Tokenization serves as a foundational step for Large Language Models (LLMs) to process text. In new domains or languages, the inefficiency of the tokenizer will slow down the training and generation of LLM. The mismatch in vocabulary also hinders deep knowledge transfer between LLMs like token-level distillation. To mitigate this gap, we propose an efficient method named TokAlign to replace the vocabulary of LLM from the token co-occurrences view, and further transfer the token-level knowledge between models. It first aligns the source vocabulary to the target one by learning a one-to-one mapping matrix for token IDs. Model parameters, including embeddings, are rearranged and progressively fine-tuned for the new vocabulary. Our method significantly improves multilingual text compression rates and vocabulary initialization for LLMs, decreasing the perplexity from 3.4$\text{e}^2$ of strong baseline methods to 1.2$\text{e}^2$ after initialization. Experimental results on models across multiple parameter scales demonstrate the effectiveness and generalization of TokAlign, which costs as few as 5k steps to restore the performance of the vanilla model. After unifying vocabularies between LLMs, token-level distillation can remarkably boost (+4.4% than sentence-level distillation) the base model, costing only 235M tokens.",TokAlign Efficient Vocabulary Adaptation Token Alignment Tokenization serves foundational step Large Language Models LLMs process text new domains languages inefficiency tokenizer slow training generation LLM mismatch vocabulary hinders deep knowledge transfer LLMs like token level distillation mitigate gap propose efficient method named TokAlign replace vocabulary LLM token occurrences view transfer token level knowledge models aligns source vocabulary target learning mapping matrix token IDs Model parameters including embeddings rearranged progressively fine tuned new vocabulary method significantly improves multilingual text compression rates vocabulary initialization LLMs decreasing perplexity 3 4 text e 2 strong baseline methods 1 2 text e 2 initialization Experimental results models multiple parameter scales demonstrate effectiveness generalization TokAlign costs 5k steps restore performance vanilla model unifying vocabularies LLMs token level distillation remarkably boost 4 4 sentence level distillation base model costing 235M tokens
220,AceEdit: Advancing Continuous Knowledge Editing For Large Language Models,"['Qi Li', 'Xiaowen Chu']",,AceEdit Advancing Continuous Knowledge Editing Large Language Models
221,The Impact of Token Granularity on the Predictive Power of Language Model Surprisal,"['Byung-Doh Oh', 'William Schuler']","Word-by-word language model surprisal is often used to model the incremental processing of human readers, which raises questions about how various choices in language modeling influence its predictive power. One factor that has been overlooked in cognitive modeling is the granularity of subword tokens, which explicitly encodes information about word length and frequency, and ultimately influences the quality of vector representations that are learned. This paper presents experiments that manipulate the token granularity and evaluate its impact on the ability of surprisal to account for processing difficulty of naturalistic text and garden-path constructions. Experiments with naturalistic reading times reveal a substantial influence of token granularity on surprisal, with tokens defined by a vocabulary size of 8,000 resulting in surprisal that is most predictive. In contrast, on garden-path constructions, language models trained on coarser-grained tokens generally assigned higher surprisal to critical regions, suggesting a greater sensitivity to garden-path effects than previously reported. Taken together, these results suggest a large role of token granularity on the quality of language model surprisal for cognitive modeling.",Impact Token Granularity Predictive Power Language Model Surprisal Word word language model surprisal used model incremental processing human readers raises questions various choices language modeling influence predictive power factor overlooked cognitive modeling granularity subword tokens explicitly encodes information word length frequency ultimately influences quality vector representations learned paper presents experiments manipulate token granularity evaluate impact ability surprisal account processing difficulty naturalistic text garden path constructions Experiments naturalistic reading times reveal substantial influence token granularity surprisal tokens defined vocabulary size 8 000 resulting surprisal predictive contrast garden path constructions language models trained coarser grained tokens generally assigned higher surprisal critical regions suggesting greater sensitivity garden path effects previously reported Taken results suggest large role token granularity quality language model surprisal cognitive modeling
222,Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models,"['Xiaochen Zhu', 'Georgi Karadzhov', 'Chenxi Whitehouse', 'Andreas Vlachos']","Diffusion models have shown promise in text generation, but often struggle with generating long, coherent, and contextually accurate text. Token-level diffusion doesn't model word-order dependencies explicitly and operates on short, fixed output windows, while passage-level diffusion struggles with learning robust representations for long-form text. To address these challenges, we propose Segment-Level Diffusion (SLD), a framework that enhances diffusion-based text generation through text segmentation, robust representation training with adversarial and contrastive learning, and improved latent-space guidance. By segmenting long-form outputs into multiple latent representations and decoding them with an autoregressive decoder, SLD simplifies diffusion predictions and improves scalability. Experiments on four datasets demonstrate that, when compared to other diffusion and autoregressive baselines SLD achieves competitive or superior fluency, coherence, and contextual compatibility in automatic and human evaluations.",Segment Level Diffusion Framework Controllable Long Form Generation Diffusion Language Models Diffusion models shown promise text generation struggle generating long coherent contextually accurate text Token level diffusion doesn t model word order dependencies explicitly operates short fixed output windows passage level diffusion struggles learning robust representations long form text address challenges propose Segment Level Diffusion SLD framework enhances diffusion based text generation text segmentation robust representation training adversarial contrastive learning improved latent space guidance segmenting long form outputs multiple latent representations decoding autoregressive decoder SLD simplifies diffusion predictions improves scalability Experiments datasets demonstrate compared diffusion autoregressive baselines SLD achieves competitive superior fluency coherence contextual compatibility automatic human evaluations
223,BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering,"['Taolin Zhang', 'Dongyang Li', 'Qizhou Chen', 'Chengyu Wang', 'Xiaofeng He']","Multi-hop question answering (QA) involves finding multiple relevant passages and performing step-by-step reasoning to answer complex questions. Previous works on multi-hop QA employ specific methods from different modeling perspectives based on large language models (LLMs), regardless of the question types. In this paper, we first conduct an in-depth analysis of public multi-hop QA benchmarks, dividing the questions into four types and evaluating five types of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step, Iterative-step, Sub-step, and Adaptive-step. We find that different types of multi-hop questions have varying degrees of sensitivity to different types of methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to address multi-hop QA by specifically focusing on the correspondence between question types and methods, where each type of method is regarded as an ''operator'' by prompting LLMs differently. The first level of BELLE includes multiple agents that debate to obtain an executive plan of combined ''operators'' to address the multi-hop QA task comprehensively. During the debate, in addition to the basic roles of affirmative debater, negative debater, and judge, at the second level, we further leverage fast and slow debaters to monitor whether changes in viewpoints are reasonable. Extensive experiments demonstrate that BELLE significantly outperforms strong baselines in various datasets. Additionally, the model consumption of BELLE is higher cost-effectiveness than that of single models in more complex multi-hop QA scenarios.",BELLE Bi Level Multi Agent Reasoning Framework Multi Hop Question Answering Multi hop question answering QA involves finding multiple relevant passages performing step step reasoning answer complex questions Previous works multi hop QA employ specific methods different modeling perspectives based large language models LLMs regardless question types paper conduct depth analysis public multi hop QA benchmarks dividing questions types evaluating types cutting edge methods multi hop QA Chain Thought CoT Single step Iterative step Sub step Adaptive step different types multi hop questions varying degrees sensitivity different types methods propose Bi levEL muLti agEnt reasoning BELLE framework address multi hop QA specifically focusing correspondence question types methods type method regarded operator prompting LLMs differently level BELLE includes multiple agents debate obtain executive plan combined operators address multi hop QA task comprehensively debate addition basic roles affirmative debater negative debater judge second level leverage fast slow debaters monitor changes viewpoints reasonable Extensive experiments demonstrate BELLE significantly outperforms strong baselines various datasets Additionally model consumption BELLE higher cost effectiveness single models complex multi hop QA scenarios
224,Dynamic and Generalizable Process Reward Modeling,"['Zhangyue Yin', 'Qiushi Sun', 'Zhiyuan Zeng', 'Qinyuan Cheng', 'Xipeng Qiu', 'Xuanjing Huang']",,Dynamic Generalizable Process Reward Modeling
225,AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness,"['Zixin Chen', 'Hongzhan Lin', 'Kaixin Li', 'Ziyang Luo', 'Zhen Ye', 'Guang Chen', 'Zhiyong Huang', 'Jing Ma']","The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme.",AdamMeme Adaptively Probe Reasoning Capacity Multimodal Large Language Models Harmfulness proliferation multimodal memes social media era demands multimodal Large Language Models mLLMs effectively understand meme harmfulness Existing benchmarks assessing mLLMs harmful meme understanding rely accuracy based model agnostic evaluations using static datasets benchmarks limited ability provide date thorough assessments online memes evolve dynamically address propose AdamMeme flexible agent based evaluation framework adaptively probes reasoning capabilities mLLMs deciphering meme harmfulness multi agent collaboration AdamMeme provides comprehensive evaluations iteratively updating meme data challenging samples exposing specific limitations mLLMs interpret harmfulness Extensive experiments framework systematically reveals varying performance different target mLLMs offering depth fine grained analyses model specific weaknesses code available https github com Lbotirx AdamMeme
226,Towards Text-Image Interleaved Retrieval,"['Xin Zhang', 'Ziqi Dai', 'Yongqi Li', 'Yanzhao Zhang', 'Dingkun Long', 'Pengjun Xie', 'Meishan Zhang', 'Jun Yu', 'Wenjie Li', 'Min Zhang']","Current multimodal information retrieval studies mainly focus on single-image inputs, which limits real-world applications involving multiple images and text-image interleaved content. In this work, we introduce the text-image interleaved retrieval (TIIR) task, where the query and document are interleaved text-image sequences, and the model is required to understand the semantics from the interleaved context for effective retrieval. We construct a TIIR benchmark based on naturally interleaved wikiHow tutorials, where a specific pipeline is designed to generate interleaved queries. To explore the task, we adapt several off-the-shelf retrievers and build a dense baseline by interleaved multimodal large language model (MLLM). We then propose a novel Matryoshka Multimodal Embedder (MME), which compresses the number of visual tokens at different granularity, to address the challenge of excessive visual tokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption of existing models does not consistently yield effective results. Our MME achieves significant improvements over the baseline by substantially fewer visual tokens. We provide extensive analysis and will release the dataset and code to facilitate future research.",Text Image Interleaved Retrieval Current multimodal information retrieval studies mainly focus single image inputs limits real world applications involving multiple images text image interleaved content work introduce text image interleaved retrieval TIIR task query document interleaved text image sequences model required understand semantics interleaved context effective retrieval construct TIIR benchmark based naturally interleaved wikiHow tutorials specific pipeline designed generate interleaved queries explore task adapt shelf retrievers build dense baseline interleaved multimodal large language model MLLM propose novel Matryoshka Multimodal Embedder MME compresses number visual tokens different granularity address challenge excessive visual tokens MLLM based TIIR models Experiments demonstrate simple adaption existing models does consistently yield effective results MME achieves significant improvements baseline substantially fewer visual tokens provide extensive analysis release dataset code facilitate future research
227,Large Margin Representation Learning for Robust Cross-lingual Named Entity Recognition,"['Guangcheng Zhu', 'Ruixuan Xiao', 'Zhen Zhu', 'Gengyu Lyu', 'Junbo Zhao', 'Haobo Wang']",,Large Margin Representation Learning Robust Cross lingual Named Entity Recognition
228,An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning,"['Wei Sun', 'Qianlong Du', 'Fuwei Cui', 'Jiajun Zhang']","Enhancing the mathematical reasoning capabilities of Large Language Models (LLMs) is of great scientific and practical significance. Researchers typically employ process-supervised reward models (PRMs) to guide the reasoning process, effectively improving the models' reasoning abilities. However, existing methods for constructing process supervision training data, such as manual annotation and per-step Monte Carlo estimation, are often costly or suffer from poor quality. To address these challenges, this paper introduces a framework called EpicPRM, which annotates each intermediate reasoning step based on its quantified contribution and uses an adaptive binary search algorithm to enhance both annotation precision and efficiency. Using this approach, we efficiently construct a high-quality process supervision training dataset named Epic50k, consisting of 50k annotated intermediate steps. Compared to other publicly available datasets, the PRM trained on Epic50k demonstrates significantly superior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.",Efficient Precise Training Data Construction Framework Process supervised Reward Model Mathematical Reasoning Enhancing mathematical reasoning capabilities Large Language Models LLMs great scientific practical significance Researchers typically employ process supervised reward models PRMs guide reasoning process effectively improving models reasoning abilities existing methods constructing process supervision training data manual annotation step Monte Carlo estimation costly suffer poor quality address challenges paper introduces framework called EpicPRM annotates intermediate reasoning step based quantified contribution uses adaptive binary search algorithm enhance annotation precision efficiency Using approach efficiently construct high quality process supervision training dataset named Epic50k consisting 50k annotated intermediate steps Compared publicly available datasets PRM trained Epic50k demonstrates significantly superior performance Getting Epic50k https github com xiaolizh1 EpicPRM
229,QAEncoder: Towards Aligned Representation Learning in Question Answering Systems,"['Zhengren Wang', 'Qinhan Yu', 'Shida Wei', 'Zhiyu li', 'Feiyu Xiong', 'Xiaoxing Wang', 'Simin Niu', 'Hao Liang', 'Wentao Zhang']","Modern QA systems entail retrieval-augmented generation (RAG) for accurate and trustworthy responses. However, the inherent gap between user queries and relevant documents hinders precise matching. We introduce QAEncoder, a training-free approach to bridge this gap. Specifically, QAEncoder estimates the expectation of potential queries in the embedding space as a robust surrogate for the document embedding, and attaches document fingerprints to effectively distinguish these embeddings. Extensive experiments across diverse datasets, languages, and embedding models confirmed QAEncoder's alignment capability, which offers a simple-yet-effective solution with zero additional index storage, retrieval latency, training costs, or catastrophic forgetting and hallucination issues. The repository is publicly available at https://github.com/IAAR-Shanghai/QAEncoder.",QAEncoder Aligned Representation Learning Question Answering Systems Modern QA systems entail retrieval augmented generation RAG accurate trustworthy responses inherent gap user queries relevant documents hinders precise matching introduce QAEncoder training free approach bridge gap Specifically QAEncoder estimates expectation potential queries embedding space robust surrogate document embedding attaches document fingerprints effectively distinguish embeddings Extensive experiments diverse datasets languages embedding models confirmed QAEncoder s alignment capability offers simple effective solution zero additional index storage retrieval latency training costs catastrophic forgetting hallucination issues repository publicly available https github com IAAR Shanghai QAEncoder
230,Game Development as Human-LLM Interaction,"['Jiale Hong', 'Hongqiu Wu', 'hai zhao']","Game development is a highly specialized task that relies on a complex game engine powered by complex programming languages, preventing many gaming enthusiasts from handling it. This paper introduces the Chat Game Engine (ChatGE) powered by LLM, which allows everyone to develop a custom game using natural language through Human-LLM interaction. To enable an LLM to function as a ChatGE, we instruct it to perform the following processes in each turn: (1) $P_{script}$: configure the game script segment based on the user's input; (2) $P_{code}$: generate the corresponding code snippet based on the game script segment; (3) $P_{utter}$: interact with the user, including guidance and feedback. We propose a data synthesis pipeline based on LLM to generate game script-code pairs and interactions from a few manually crafted seed data. We propose a three-stage progressive training strategy to transfer the dialogue-based LLM to our ChatGE smoothly. We construct a ChatGE for poker games as a case study and comprehensively evaluate it from two perspectives: interaction quality and code correctness.",Game Development Human LLM Interaction Game development highly specialized task relies complex game engine powered complex programming languages preventing gaming enthusiasts handling paper introduces Chat Game Engine ChatGE powered LLM allows develop custom game using natural language Human LLM interaction enable LLM function ChatGE instruct perform following processes turn 1 P_ script configure game script segment based user s input 2 P_ code generate corresponding code snippet based game script segment 3 P_ utter interact user including guidance feedback propose data synthesis pipeline based LLM generate game script code pairs interactions manually crafted seed data propose stage progressive training strategy transfer dialogue based LLM ChatGE smoothly construct ChatGE poker games case study comprehensively evaluate perspectives interaction quality code correctness
231,Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis of L1-Dependent Biases,"['Rena Wei Gao', 'Xuetong Wu', 'Tatsuki Kuribayashi', 'Mingrui Ye', 'Siya Qi', 'Carsten Roever', 'Yuanxing Liu', 'Zheng Yuan', 'Jey Han Lau']","This study evaluates Large Language Models' (LLMs) ability to simulate non-native-like English use observed in human second language (L2) learners interfered with by their native first language (L1). In dialogue-based interviews, we prompt LLMs to mimic L2 English learners with specific L1s (e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to real L2 learner data. Our analysis examines L1-driven linguistic biases, such as reference word usage and avoidance behaviors, using information-theoretic and distributional density measures. Results show that modern LLMs (e.g., Qwen2.5, LLAMA3.3, DeepseekV3, GPT-4o) replicate L1-dependent patterns observed in human L2 data, with distinct influences from various languages (e.g., Japanese, Korean, and Mandarin significantly affect tense agreement, and Urdu influences noun-verb collocations). Our results reveal the potential of LLMs for L2 dialogue generation and evaluation for future educational applications.",LLMs Simulate L2 English Dialogue Information Theoretic Analysis L1 Dependent Biases study evaluates Large Language Models LLMs ability simulate non native like English use observed human second language L2 learners interfered native language L1 dialogue based interviews prompt LLMs mimic L2 English learners specific L1s e g Japanese Thai Urdu seven languages comparing outputs real L2 learner data analysis examines L1 driven linguistic biases reference word usage avoidance behaviors using information theoretic distributional density measures Results modern LLMs e g Qwen2 5 LLAMA3 3 DeepseekV3 GPT 4o replicate L1 dependent patterns observed human L2 data distinct influences various languages e g Japanese Korean Mandarin significantly affect tense agreement Urdu influences noun verb collocations results reveal potential LLMs L2 dialogue generation evaluation future educational applications
232,DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking,"['Zhuoqun Li', 'Haiyang Yu', 'Xuanang Chen', 'Hongyu Lin', 'Yaojie Lu', 'Fei Huang', 'Xianpei Han', 'Yongbin Li', 'Le Sun']","Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system's ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications.",DeepSolution Boosting Complex Engineering Solution Design Tree based Exploration Bi point Thinking Designing solutions complex engineering challenges crucial human production activities previous research retrieval augmented generation RAG field sufficiently addressed tasks related design complex engineering solutions gap introduce new benchmark SolutionBench evaluate s ability generate complete feasible solutions engineering problems multiple complex constraints advance design complex engineering solutions propose novel SolutionRAG leverages tree based exploration bi point thinking mechanism generate reliable solutions Extensive experimental results demonstrate SolutionRAG achieves state art SOTA performance SolutionBench highlighting potential enhance automation reliability complex engineering solution design real world applications
233,Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility,"['Suet-Ying Lam', 'Qingcheng Zeng', 'Jingyi Wu', 'Rob Voigt']","Whether large language models (LLMs) process language similarly to humans has been the subject of much theoretical and practical debate. We examine this question through the lens of the production-interpretation distinction found in human sentence processing and evaluate the extent to which instruction-tuned LLMs replicate this distinction. Using an empirically documented asymmetry between pronoun production and interpretation in humans for implicit causality verbs as a testbed, we find that some LLMs do quantitatively and qualitatively reflect human-like asymmetries between production and interpretation. We demonstrate that whether this behavior holds depends upon both model size-with larger models more likely to reflect human-like patterns and the choice of meta-linguistic prompts used to elicit the behavior. Our codes and results are available at https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025.",Leveraging Human Production Interpretation Asymmetries Test LLM Cognitive Plausibility large language models LLMs process language similarly humans subject theoretical practical debate examine question lens production interpretation distinction human sentence processing evaluate extent instruction tuned LLMs replicate distinction Using empirically documented asymmetry pronoun production interpretation humans implicit causality verbs testbed LLMs quantitatively qualitatively reflect human like asymmetries production interpretation demonstrate behavior holds depends model size larger models likely reflect human like patterns choice meta linguistic prompts used elicit behavior codes results available https github com LingMechLab Production Interpretation_Asymmetries_ACL2025
234,SurveyPilot: an Agentic Framework for Automated Human Opinion Collection from Social Media,"['Viet Thanh Pham', 'Lizhen Qu', 'Zhuang Li', 'Suraj Sharma', 'Gholamreza Haffari']",,SurveyPilot Agentic Framework Automated Human Opinion Collection Social Media
235,Sharper and Faster mean Better: Towards More Efficient Vision-Language Model for Hour-scale Long Video Understanding,"['Daoze Zhang', 'Yuze Zhao', 'Jintao Huang', 'Yingda Chen']",,Sharper Faster mean Better Efficient Vision Language Model Hour scale Long Video Understanding
236,Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and Committee Discussions,"['Ruochen Zhao', 'Wenxuan Zhang', 'Yew Ken Chia', 'Weiwen Xu', 'Deli Zhao', 'Lidong Bing']","As LLMs continuously evolve, there is an urgent need for a reliable evaluation method that delivers trustworthy results promptly. Currently, static benchmarks suffer from inflexibility and unreliability, leading users to prefer human voting platforms like Chatbot Arena. However, human evaluations require significant manual effort. To address this, we propose the Auto-Arena, an innovative framework that automates the entire evaluation process using LLM-powered agents. Firstly, an LLM examiner generates questions. Then, two LLM candidates engage in a multi-round peer battle based on individual questions, aiming at revealing their true performance differences. Finally, a committee of LLM judges collaboratively discusses and decides the winner, reducing bias and enhancing fairness. During the peer battles, we observe intriguing scenarios where the LLM candidates display competitive behaviors and even learn from the opponents. In our extensive experiments involving 15 recent LLMs, Auto-Arena shows a 92.14% correlation with human preferences, surpassing all previous expert-annotated benchmarks without any manual efforts. As a result, Auto-Arena offers a promising alternative to current human evaluation platforms for evaluating LLMs automatically.",Auto Arena Automating LLM Evaluations Agent Peer Battles Committee Discussions LLMs continuously evolve urgent need reliable evaluation method delivers trustworthy results promptly Currently static benchmarks suffer inflexibility unreliability leading users prefer human voting platforms like Chatbot Arena human evaluations require significant manual effort address propose Auto Arena innovative framework automates entire evaluation process using LLM powered agents Firstly LLM examiner generates questions LLM candidates engage multi round peer battle based individual questions aiming revealing true performance differences Finally committee LLM judges collaboratively discusses decides winner reducing bias enhancing fairness peer battles observe intriguing scenarios LLM candidates display competitive behaviors learn opponents extensive experiments involving 15 recent LLMs Auto Arena shows 92 14 correlation human preferences surpassing previous expert annotated benchmarks manual efforts result Auto Arena offers promising alternative current human evaluation platforms evaluating LLMs automatically
237,How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian,"['Andrea Pedrotti', 'Giulia Rambelli', 'Caterina Villani', 'Marianna Bolognesi']","People can categorize the same entity at multiple taxonomic levels, such as basic (bear), superordinate (animal), and subordinate (grizzly bear). While prior research has focused on basic-level categories, this study is the first attempt to examine the organization of categories by analyzing exemplars produced at the subordinate level. We present a new Italian psycholinguistic dataset of human-generated exemplars for 187 concrete words. We then use these data to evaluate whether textual and vision LLMs produce meaningful exemplars that align with human category organization across three key tasks: exemplar generation, category induction, and typicality judgment. Our findings show a low alignment between humans and LLMs, consistent with previous studies. However, their performance varies notably across different semantic domains. Ultimately, this study highlights both the promises and the constraints of using AI-generated exemplars to support psychological and linguistic research.",Humans LLMs Organize Conceptual Knowledge Exploring Subordinate Categories Italian People categorize entity multiple taxonomic levels basic bear superordinate animal subordinate grizzly bear prior research focused basic level categories study attempt examine organization categories analyzing exemplars produced subordinate level present new Italian psycholinguistic dataset human generated exemplars 187 concrete words use data evaluate textual vision LLMs produce meaningful exemplars align human category organization key tasks exemplar generation category induction typicality judgment findings low alignment humans LLMs consistent previous studies performance varies notably different semantic domains Ultimately study highlights promises constraints using AI generated exemplars support psychological linguistic research
238,PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models,"['Jiaqi Zhao', 'Miao Zhang', 'Ming Wang', 'Yuzhang Shang', 'Kaihao Zhang', 'Weili Guan', 'Yaowei Wang', 'Min Zhang']","Large Language Models (LLMs) suffer severe performance degradation when facing extremely low-bit (sub 2-bit) quantization. Several existing sub 2-bit post-training quantization (PTQ) methods utilize a mix-precision scheme by leveraging an unstructured fine-grained mask to explicitly distinguish salient weights, while which introduces an extra 1-bit or more per weight. To explore the real limit of PTQ, we propose an extremely low-bit PTQ method called PTQ1.61, which enables weight quantization to 1.61-bit for the first time. Specifically, we first introduce a one-dimensional structured mask with negligibly additional 0.0002-bit per weight based on input activations from the perspective of reducing the upper bound of quantization error to allocate corresponding salient weight channels to 4-bit. For non-salient channels binarization, an efficient block-wise scaling factors optimization framework is then presented to take implicit row-wise correlations and angular biases into account. Different from prior works that concentrate on adjusting quantization methodologies, we further propose a novel paradigm called quantization preprocessing, where we argue that transforming the weight distribution of the pretrained model before quantization can alleviate the difficulty in per-channel extremely low-bit PTQ. Extensive experiments indicate our PTQ1.61 achieves state-of-the-art performance in extremely low-bit quantization. Codes are available at https://github.com/zjq0455/PTQ1.61.",PTQ1 61 Push Real Limit Extremely Low Bit Post Training Quantization Methods Large Language Models Large Language Models LLMs suffer severe performance degradation facing extremely low bit sub 2 bit quantization existing sub 2 bit post training quantization PTQ methods utilize mix precision scheme leveraging unstructured fine grained mask explicitly distinguish salient weights introduces extra 1 bit weight explore real limit PTQ propose extremely low bit PTQ method called PTQ1 61 enables weight quantization 1 61 bit time Specifically introduce dimensional structured mask negligibly additional 0 0002 bit weight based input activations perspective reducing upper bound quantization error allocate corresponding salient weight channels 4 bit non salient channels binarization efficient block wise scaling factors optimization framework presented implicit row wise correlations angular biases account Different prior works concentrate adjusting quantization methodologies propose novel paradigm called quantization preprocessing argue transforming weight distribution pretrained model quantization alleviate difficulty channel extremely low bit PTQ Extensive experiments indicate PTQ1 61 achieves state art performance extremely low bit quantization Codes available https github com zjq0455 PTQ1 61
239,ProtoLens: Advancing Prototype Learning for Fine-Grained Interpretability in Text Classification,"['Bowen Wei', 'Ziwei Zhu']",,ProtoLens Advancing Prototype Learning Fine Grained Interpretability Text Classification
240,Fine-grained Video Dubbing Duration Alignment with Segment Supervised Preference Optimization,"['Chaoqun Cui', 'Liangbin Huang', 'Shijing Wang', 'Zhe Tong', 'Zhaolong Huang', 'Xiao Zeng', 'Xiaofeng Liu']",,Fine grained Video Dubbing Duration Alignment Segment Supervised Preference Optimization
241,Sparse Latents Steer Retrieval-Augmented Generation,"['Chunlei Xin', 'Shuheng Zhou', 'Huijia Zhu', 'Weiqiang Wang', 'Xuanang Chen', 'Xinyan Guan', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun']",,Sparse Latents Steer Retrieval Augmented Generation
242,Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution’s Characteristics,"['Lorenzo Jaime Yu Flores', 'Ori Ernst', 'Jackie CK Cheung']",,Improving Calibration Confidence Scores Text Generation Using Output Distribution s Characteristics
243,Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders,"['Boyi Deng', 'Yu Wan', 'Baosong Yang', 'Yidan Zhang', 'Fuli Feng']","The mechanisms behind multilingual capabilities in Large Language Models (LLMs) have been examined using neuron-based or internal-activation-based methods. However, these methods often face challenges such as superposition and layer-wise activation variance, which limit their reliability. Sparse Autoencoders (SAEs) offer a more nuanced analysis by decomposing the activations of LLMs into a sparse linear combination of SAE features. We introduce a novel metric to assess the monolinguality of features obtained from SAEs, discovering that some features are strongly related to specific languages. Additionally, we show that ablating these SAE features only significantly reduces abilities in one language of LLMs, leaving others almost unaffected. Interestingly, we find some languages have multiple synergistic SAE features, and ablating them together yields greater improvement than ablating individually. Moreover, we leverage these SAE-derived language-specific features to enhance steering vectors, achieving control over the language generated by LLMs. The code is publicly available at https://github.com/Aatrox103/multilingual-llm-features.",Unveiling Language Specific Features Large Language Models Sparse Autoencoders mechanisms multilingual capabilities Large Language Models LLMs examined using neuron based internal activation based methods methods face challenges superposition layer wise activation variance limit reliability Sparse Autoencoders SAEs offer nuanced analysis decomposing activations LLMs sparse linear combination SAE features introduce novel metric assess monolinguality features obtained SAEs discovering features strongly related specific languages Additionally ablating SAE features significantly reduces abilities language LLMs leaving unaffected Interestingly languages multiple synergistic SAE features ablating yields greater improvement ablating individually leverage SAE derived language specific features enhance steering vectors achieving control language generated LLMs code publicly available https github com Aatrox103 multilingual llm features
244,SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model,"['Xun Liang', 'Simin Niu', 'Zhiyu li', 'Sensen Zhang', 'Hanyu Wang', 'Feiyu Xiong', 'Zhaoxin Fan', 'Bo Tang', 'Jihao Zhao', 'Jiawei Yang', 'Shichao Song', 'Mengwei Wang']","The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.",SafeRAG Benchmarking Security Retrieval Augmented Generation Large Language Model indexing retrieval generation paradigm retrieval augmented generation RAG highly successful solving knowledge intensive tasks integrating external knowledge large language models LLMs incorporation external unverified knowledge increases vulnerability LLMs attackers perform attack tasks manipulating knowledge paper introduce benchmark named SafeRAG designed evaluate RAG security classify attack tasks silver noise inter context conflict soft ad white Denial Service construct RAG security evaluation dataset e SafeRAG dataset primarily manually task utilize SafeRAG dataset simulate various attack scenarios RAG encounter Experiments conducted 14 representative RAG components demonstrate RAG exhibits significant vulnerability attack tasks apparent attack task easily bypass existing retrievers filters advanced LLMs resulting degradation RAG service quality Code available https github com IAAR Shanghai SafeRAG
245,AnRe: Analogical Replay for Temporal Knowledge Graph Forecasting,"['Guo Tang', 'Zheng Chu', 'Wenxiang Zheng', 'Junjia Xiang', 'Yizhuo Li', 'Weihao Zhang', 'Ming Liu', 'Bing Qin']",,AnRe Analogical Replay Temporal Knowledge Graph Forecasting
246,Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?,"['Zhiyuan Zeng', 'Qinyuan Cheng', 'Zhangyue Yin', 'Yunhua Zhou', 'Xipeng Qiu']","The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches.",Revisiting Test Time Scaling o1 like Models Truly Possess Test Time Scaling Capabilities advent test time scaling large language models LLMs exemplified OpenAI s o1 series advanced reasoning capabilities scaling computational resource allocation inference successors like QwQ Deepseek R1 R1 LIMO replicate advancements models truly possess test time scaling capabilities remains underexplored study longer CoTs o1 like models consistently enhance accuracy fact correct solutions shorter incorrect ones questions investigation shows phenomenon closely related models self revision capabilities longer CoTs contain self revisions lead performance degradation compare sequential parallel scaling strategies QwQ R1 LIMO finding parallel scaling achieves better coverage scalability Based insights propose Shortest Majority Vote method combines parallel scaling strategies CoT length characteristics significantly improving models test time scalability compared conventional majority voting approaches
247,Text is All You Need: LLM-enhanced Incremental Social Event Detection,"['Zitai Qiu', 'Congbo Ma', 'Jia Wu', 'Jian Yang']",,Text Need LLM enhanced Incremental Social Event Detection
248,Multimodal Pragmatic Jailbreak on Text-to-image Models,"['Tong Liu', 'Zhixin Lai', 'Jiawen Wang', 'Gengyuan Zhang', 'Shuo Chen', 'Philip Torr', 'Vera Demberg', 'Volker Tresp', 'Jindong Gu']","Diffusion models have recently achieved remarkable advancements in terms of image quality and fidelity to textual prompts. Concurrently, the safety of such generative models has become an area of growing concern. This work introduces a novel type of jailbreak, which triggers T2I models to generate the image with visual text, where the image and the text, although considered to be safe in isolation, combine to form unsafe content. To systematically explore this phenomenon, we propose a dataset to evaluate the current diffusion-based text-to-image (T2I) models under such jailbreak. We benchmark nine representative T2I models, including two closed-source commercial models. Experimental results reveal a concerning tendency to produce unsafe content: all tested models suffer from such type of jailbreak, with rates of unsafe generation ranging from around 10\% to 70\% where DALLE 3 demonstrates almost the highest unsafety. In real-world scenarios, various filters such as keyword blocklists, customized prompt filters, and NSFW image filters, are commonly employed to mitigate these risks. We evaluate the effectiveness of such filters against our jailbreak and found that, while these filters may be effective for single modality detection, they fail to work against our jailbreak. We also investigate the underlying reason for such jailbreaks, from the perspective of text rendering capability and training data. Our work provides a foundation for further development towards more secure and reliable T2I models. Project page at https://multimodalpragmatic.github.io/.",Multimodal Pragmatic Jailbreak Text image Models Diffusion models recently achieved remarkable advancements terms image quality fidelity textual prompts Concurrently safety generative models area growing concern work introduces novel type jailbreak triggers T2I models generate image visual text image text considered safe isolation combine form unsafe content systematically explore phenomenon propose dataset evaluate current diffusion based text image T2I models jailbreak benchmark representative T2I models including closed source commercial models Experimental results reveal concerning tendency produce unsafe content tested models suffer type jailbreak rates unsafe generation ranging 10 70 DALLE 3 demonstrates highest unsafety real world scenarios various filters keyword blocklists customized prompt filters NSFW image filters commonly employed mitigate risks evaluate effectiveness filters jailbreak filters effective single modality detection fail work jailbreak investigate underlying reason jailbreaks perspective text rendering capability training data work provides foundation development secure reliable T2I models Project page https multimodalpragmatic github io
249,Principled Understanding of Generalization for Generative Transformer Models in Arithmetic Reasoning Tasks,"['Xingcheng Xu', 'Zibo Zhao', 'Haipeng Zhang', 'Yanqing Yang']","Transformer-based models excel in various tasks but their generalization capabilities, especially in arithmetic reasoning, remain incompletely understood. Arithmetic tasks provide a controlled framework to explore these capabilities, yet performance anomalies persist, such as inconsistent effectiveness in multiplication and erratic generalization in modular addition (e.g., modulo 100 vs. 101). This paper develops a unified theoretical framework for understanding the generalization behaviors of transformers in arithmetic tasks, focusing on length generalization. Through detailed analysis of addition, multiplication, and modular operations, we reveal that translation invariance in addition aligns with relative positional encoding for robust generalization, while base mismatch in modular operations disrupts this alignment. Experiments across GPT-family models validate our framework, confirming its ability to predict generalization behaviors. Our work highlights the importance of task structure and training data distribution for achieving data-efficient and structure-aware training, providing a systematic approach to understanding of length generalization in transformers.",Principled Understanding Generalization Generative Transformer Models Arithmetic Reasoning Tasks Transformer based models excel various tasks generalization capabilities especially arithmetic reasoning remain incompletely understood Arithmetic tasks provide controlled framework explore capabilities performance anomalies persist inconsistent effectiveness multiplication erratic generalization modular addition e g modulo 100 vs 101 paper develops unified theoretical framework understanding generalization behaviors transformers arithmetic tasks focusing length generalization detailed analysis addition multiplication modular operations reveal translation invariance addition aligns relative positional encoding robust generalization base mismatch modular operations disrupts alignment Experiments GPT family models validate framework confirming ability predict generalization behaviors work highlights importance task structure training data distribution achieving data efficient structure aware training providing systematic approach understanding length generalization transformers
250,Discourse Relation-Enhanced Neural Coherence Modeling,"['Wei Liu', 'Michael Strube']",,Discourse Relation Enhanced Neural Coherence Modeling
251,Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models,"['Kuofeng Gao', 'Shu-Tao Xia', 'Ke Xu', 'Philip Torr', 'Jindong Gu']","Large Audio-Language Models (LALMs) have unclocked audio dialogue capabilities, where audio dialogues are a direct exchange of spoken language between LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs in back-and-forth audio dialogues with humans. This progression not only underscores the potential of LALMs but also broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we firstly propose the evaluation of ambiguity handling in audio dialogues that expresses different intentions beyond the same literal meaning of sentences, e.g., ""Really!?"" with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 13 LALMs, our analysis reveals that there is still considerable room for improvement in the audio dialogue understanding abilities of existing LALMs. In particular, they struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones.",Benchmarking Open ended Audio Dialogue Understanding Large Audio Language Models Large Audio Language Models LALMs unclocked audio dialogue capabilities audio dialogues direct exchange spoken language LALMs humans Recent advances GPT 4o enabled LALMs forth audio dialogues humans progression underscores potential LALMs broadens applicability wide range practical scenarios supported audio dialogues given advancements comprehensive benchmark evaluate performance LALMs open ended audio dialogue understanding remains absent currently address gap propose Audio Dialogue Understanding Benchmark ADU Bench consists 4 benchmark datasets assess open ended audio dialogue ability LALMs 3 general scenarios 12 skills 9 multilingual languages 4 categories ambiguity handling Notably firstly propose evaluation ambiguity handling audio dialogues expresses different intentions literal meaning sentences e g Really different intonations summary ADU Bench includes 20 000 open ended audio dialogues assessment LALMs extensive experiments conducted 13 LALMs analysis reveals considerable room improvement audio dialogue understanding abilities existing LALMs particular struggle mathematical symbols formulas understanding human behavior roleplay comprehending multiple languages handling audio dialogue ambiguities different phonetic elements intonations pause positions homophones
252,from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors,"['Yu Yan', 'Sheng Sun', 'Zenghao Duan', 'Teli Liu', 'Min Liu', 'Zhiyi yin', 'Qi Li', 'LeiJingyu']","Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our study, we introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR) to induce the LLM to calibrate malicious metaphors for jailbreaking. Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed. Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content. Experimental results demonstrate that AVATAR can effectively and transferable jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs.",Benign import Toxic Jailbreaking Language Model Adversarial Metaphors Current studies exposed risk Large Language Models LLMs generating harmful content jailbreak attacks overlook direct generation harmful content scratch difficult inducing LLM calibrate benign content harmful forms study introduce novel attack framework exploits AdVersArial meTAphoR AVATAR induce LLM calibrate malicious metaphors jailbreaking Specifically answer harmful queries AVATAR adaptively identifies set benign logically related metaphors initial seed driven metaphors target LLM induced reason calibrate metaphorical content jailbroken directly outputting harmful responses calibrating residuals metaphorical professional harmful content Experimental results demonstrate AVATAR effectively transferable jailbreak LLMs achieve state art attack success rate multiple advanced LLMs
253,ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework,"['Hengyuan Zhang', 'Chenming Shang', 'Sizhe Wang', 'Dongdong Zhang', 'Feng Yao', 'Renliang Sun', 'Yiyao Yu', 'Yujiu Yang', 'Furu Wei']",,ShifCon Enhancing Non Dominant Language Capabilities Shift based Contrastive Framework
254,MorphMark: Flexible Adaptive Watermarking for Large Language Models,"['Zongqi Wang', 'Tianle Gu', 'Baoyuan Wu', 'Yujiu Yang']","Watermarking by altering token sampling probabilities based on red-green list is a promising method for tracing the origin of text generated by large language models (LLMs). However, existing watermark methods often struggle with a fundamental dilemma: improving watermark effectiveness (the detectability of the watermark) often comes at the cost of reduced text quality. This trade-off limits their practical application. To address this challenge, we first formalize the problem within a multi-objective trade-off analysis framework. Within this framework, we identify a key factor that influences the dilemma. Unlike existing methods, where watermark strength is typically treated as a fixed hyperparameter, our theoretical insights lead to the development of MorphMarka method that adaptively adjusts the watermark strength in response to changes in the identified factor, thereby achieving an effective resolution of the dilemma. In addition, MorphMark also prioritizes flexibility since it is a model-agnostic and model-free watermark method, thereby offering a practical solution for real-world deployment, particularly in light of the rapid evolution of AI models. Extensive experiments demonstrate that MorphMark achieves a superior resolution of the effectiveness-quality dilemma, while also offering greater flexibility and time and space efficiency.",MorphMark Flexible Adaptive Watermarking Large Language Models Watermarking altering token sampling probabilities based red green list promising method tracing origin text generated large language models LLMs existing watermark methods struggle fundamental dilemma improving watermark effectiveness detectability watermark comes cost reduced text quality trade limits practical application address challenge formalize problem multi objective trade analysis framework framework identify key factor influences dilemma Unlike existing methods watermark strength typically treated fixed hyperparameter theoretical insights lead development MorphMarka method adaptively adjusts watermark strength response changes identified factor achieving effective resolution dilemma addition MorphMark prioritizes flexibility model agnostic model free watermark method offering practical solution real world deployment particularly light rapid evolution AI models Extensive experiments demonstrate MorphMark achieves superior resolution effectiveness quality dilemma offering greater flexibility time space efficiency
255,A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression,"['Chenlong Deng', 'Zhisong Zhang', 'Kelong Mao', 'Shuaiyi Li', 'Xinting Huang', 'Dong Yu', 'Zhicheng Dou']","In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to compression? Through extensive experiments, we show that while gist-based compression can achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, it faces challenges in tasks like synthetic recall. Furthermore, we identify three key failure patterns: lost by the boundary, lost if surprise, and lost along the way. To mitigate these issues, we propose two effective strategies: fine-grained autoencoding, which enhances the reconstruction of original token information, and segment-wise token importance estimation, which adjusts optimization based on token dependencies. Our work provides valuable insights into the understanding of gist token-based context compression and offers practical strategies for improving compression capabilities.",Silver Bullet Compromise Attention Comprehensive Study Gist Token based Context Compression work provide thorough investigation gist based context compression methods improve long context processing large language models focus key questions 1 methods replace attention models 2 potential failure patterns arise compression extensive experiments gist based compression achieve near lossless performance tasks like retrieval augmented generation long document QA faces challenges tasks like synthetic recall Furthermore identify key failure patterns lost boundary lost surprise lost way mitigate issues propose effective strategies fine grained autoencoding enhances reconstruction original token information segment wise token importance estimation adjusts optimization based token dependencies work provides valuable insights understanding gist token based context compression offers practical strategies improving compression capabilities
256,On the Limit of Language Models as Planning Formalizers,"['Cassie Huang', 'Li Zhang']","Large Language Models have been found to create plans that are neither executable nor verifiable in grounded environments. An emerging line of work demonstrates success in using the LLM as a formalizer to generate a formal representation of the planning domain in some language, such as Planning Domain Definition Language (PDDL). This formal representation can be deterministically solved to find a plan. We systematically evaluate this methodology while bridging some major gaps. While previous work only generates a partial PDDL representation, given templated, and therefore unrealistic environment descriptions, we generate the complete representation given descriptions of various naturalness levels. Among an array of observations critical to improve LLMs' formal planning abilities, we note that most large enough models can effectively formalize descriptions as PDDL, outperforming those directly generating plans, while being robust to lexical perturbation. As the descriptions become more natural-sounding, we observe a decrease in performance and provide detailed error analysis.",Limit Language Models Planning Formalizers Large Language Models create plans executable verifiable grounded environments emerging line work demonstrates success using LLM formalizer generate formal representation planning domain language Planning Domain Definition Language PDDL formal representation deterministically solved plan systematically evaluate methodology bridging major gaps previous work generates partial PDDL representation given templated unrealistic environment descriptions generate complete representation given descriptions various naturalness levels array observations critical improve LLMs formal planning abilities note large models effectively formalize descriptions PDDL outperforming directly generating plans robust lexical perturbation descriptions natural sounding observe decrease performance provide detailed error analysis
257,Learning to Generate Structured Output with Schema Reinforcement Learning,"['Yaxi Lu', 'Haolun Li', 'Xin Cong', 'Zhong Zhang', 'Yesai Wu', 'Yankai Lin', 'Zhiyuan Liu', 'Fangming Liu', 'Maosong Sun']","This study investigates the structured generation capabilities of large language models (LLMs), focusing on producing valid JSON outputs against a given schema. Despite the widespread use of JSON in integrating language models with programs, there is a lack of comprehensive analysis and benchmarking of these capabilities. We explore various aspects of JSON generation, such as structure understanding, escaping, and natural language description, to determine how to assess and enable LLMs to generate valid responses. Building upon this, we propose SchemaBench features around 40K different JSON schemas to obtain and assess models' abilities in generating valid JSON. We find that the latest LLMs are still struggling to generate a valid JSON string. Moreover, we demonstrate that incorporating reinforcement learning with a Fine-grained Schema Validator can further enhance models' understanding of JSON schema, leading to improved performance. Our models demonstrate significant improvement in both generating JSON outputs and downstream tasks.",Learning Generate Structured Output Schema Reinforcement Learning study investigates structured generation capabilities large language models LLMs focusing producing valid JSON outputs given schema Despite widespread use JSON integrating language models programs lack comprehensive analysis benchmarking capabilities explore various aspects JSON generation structure understanding escaping natural language description determine assess enable LLMs generate valid responses Building propose SchemaBench features 40K different JSON schemas obtain assess models abilities generating valid JSON latest LLMs struggling generate valid JSON string demonstrate incorporating reinforcement learning Fine grained Schema Validator enhance models understanding JSON schema leading improved performance models demonstrate significant improvement generating JSON outputs downstream tasks
258,On the Robustness of RAG Systems in Educational Question Answering under Knowledge Discrepancies,"['Tianshi Zheng', 'Weihan Li', 'Jiaxin Bai', 'Weiqi Wang', 'Yangqiu Song']",,Robustness RAG Systems Educational Question Answering Knowledge Discrepancies
259,Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning,"['Peichao Lai', 'Zhengfeng Zhang', 'Wentao Zhang', 'Fangcheng Fu', 'Bin CUI']","Recently, using large language models (LLMs) for data augmentation has led to considerable improvements in unsupervised sentence embedding models. However, existing methods encounter two primary challenges: limited data diversity and high data noise. Current approaches often neglect fine-grained knowledge, such as entities and quantities, leading to insufficient diversity. Besides, unsupervised data frequently lacks discriminative information, and the generated synthetic samples may introduce noise. In this paper, we propose a pipeline-based data augmentation method via LLMs and introduce the Gaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model to enhance unsupervised sentence embeddings. To tackle the issue of low data diversity, our pipeline utilizes knowledge graphs (KGs) to extract entities and quantities, enabling LLMs to generate more diverse samples. To address high data noise, the GCSE model uses a Gaussian-decayed function to limit the impact of false hard negative samples, enhancing the model's discriminative capability. Experimental results show that our approach achieves state-of-the-art performance in semantic textual similarity (STS) tasks, using fewer data samples and smaller LLMs, demonstrating its efficiency and robustness across various models.",Enhancing Unsupervised Sentence Embeddings Knowledge Driven Data Augmentation Gaussian Decayed Contrastive Learning Recently using large language models LLMs data augmentation led considerable improvements unsupervised sentence embedding models existing methods encounter primary challenges limited data diversity high data noise Current approaches neglect fine grained knowledge entities quantities leading insufficient diversity unsupervised data frequently lacks discriminative information generated synthetic samples introduce noise paper propose pipeline based data augmentation method LLMs introduce Gaussian decayed gradient assisted Contrastive Sentence Embedding GCSE model enhance unsupervised sentence embeddings tackle issue low data diversity pipeline utilizes knowledge graphs KGs extract entities quantities enabling LLMs generate diverse samples address high data noise GCSE model uses Gaussian decayed function limit impact false hard negative samples enhancing model s discriminative capability Experimental results approach achieves state art performance semantic textual similarity STS tasks using fewer data samples smaller LLMs demonstrating efficiency robustness various models
260,Improve Safety Training of Large Language Models with Safety-Critical Singular Vectors Localization,"['Peijian Gu', 'Quan Wang', 'Zhendong Mao']",,Improve Safety Training Large Language Models Safety Critical Singular Vectors Localization
261,WarriorCoder: Learning from Expert Battles to Augment Code Large Language Models,"['Huawen Feng', 'Pu Zhao', 'Qingfeng Sun', 'Can Xu', 'Fangkai Yang', 'Lu Wang', 'Qianli Ma', 'Qingwei Lin', 'Saravan Rajmohan', 'Dongmei Zhang', 'Qi Zhang']","Despite recent progress achieved by code large language models (LLMs), their remarkable abilities are largely dependent on fine-tuning on the high-quality data, posing challenges for data collection and annotation. To address this, current methods often design various data flywheels to collect complex code instructions, enabling models to handle more intricate tasks. However, these approaches typically rely on off-the-shelf datasets and data augmentation from a limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which restricts the diversity of the constructed data and makes it prone to systemic biases. In this paper, we propose WarriorCoder, a novel paradigm learns from expert battles to address these limitations. Specifically, we create an arena where leading expert code LLMs challenge each other, with evaluations conducted by impartial judges. This competitive framework generates novel training data from scratch, leveraging the strengths of all participants. Experimental results show that WarriorCoder achieves state-of-the-art performance compared to previous models of the same size, even without relying on proprietary LLMs.",WarriorCoder Learning Expert Battles Augment Code Large Language Models Despite recent progress achieved code large language models LLMs remarkable abilities largely dependent fine tuning high quality data posing challenges data collection annotation address current methods design various data flywheels collect complex code instructions enabling models handle intricate tasks approaches typically rely shelf datasets data augmentation limited set proprietary LLMs e g Claude GPT4 restricts diversity constructed data makes prone systemic biases paper propose WarriorCoder novel paradigm learns expert battles address limitations Specifically create arena leading expert code LLMs challenge evaluations conducted impartial judges competitive framework generates novel training data scratch leveraging strengths participants Experimental results WarriorCoder achieves state art performance compared previous models size relying proprietary LLMs
262,A Triple-View Framework for Fine-Grained Emotion Classification with Clustering-Guided Contrastive Learning,"['Junqing Gong', 'Binhan Yang', 'Wei Shen']",,Triple View Framework Fine Grained Emotion Classification Clustering Guided Contrastive Learning
263,Quantification of Large Language Model Distillation,"['Sunbowen Lee', 'Junting Zhou', 'Chang Ao', 'Kaige Li', 'Xeron Du', 'Sirui He', 'Haihong Wu', 'Tianci Liu', 'Jiaheng Liu', 'Hamid Alinejad-Rokny', 'Min Yang', 'Yitao Liang', 'Zhoufutu Wen', 'Shiwen Ni']","Model distillation is a fundamental technique in building large language models (LLMs), transferring knowledge from a teacher model to a student model. However, distillation can lead to model homogenization, reducing diversity among models and impairing their ability to robustly handle complex or novel tasks. These limitations underscore the need to systematically quantify the distillation process and its impact. In this work, we propose a framework to evaluate and quantify model distillation. Our method addresses two key aspects: (1) Identifying identity cognition contradictions to assess discrepancies in how models perceive and represent identity-related information, and (2) Analyzing multi-granularity response similarities across models to measure the extent of homogenization. Experimental results demonstrate two key insights: (1) Well-known closed-source and open-source LLMs usually exhibit high distillation degrees, except for Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees compared to aligned LLMs. By offering a systematic approach to improve the transparency of LLM data distillation, we call for LLMs with more independent development and more transparent technical reports to improve LLMs' robustness and safety. The code and data are available under https://github.com/Aegis1863/LLMs-Distillation-Quantification.",Quantification Large Language Model Distillation Model distillation fundamental technique building large language models LLMs transferring knowledge teacher model student model distillation lead model homogenization reducing diversity models impairing ability robustly handle complex novel tasks limitations underscore need systematically quantify distillation process impact work propose framework evaluate quantify model distillation method addresses key aspects 1 Identifying identity cognition contradictions assess discrepancies models perceive represent identity related information 2 Analyzing multi granularity response similarities models measure extent homogenization Experimental results demonstrate key insights 1 known closed source open source LLMs usually exhibit high distillation degrees Claude Doubao Gemini 2 Base LLMs higher distillation degrees compared aligned LLMs offering systematic approach improve transparency LLM data distillation LLMs independent development transparent technical reports improve LLMs robustness safety code data available https github com Aegis1863 LLMs Distillation Quantification
264,Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models,"['Zihan Qiu', 'Zeyu Huang', 'Bo Zheng', 'Kaiyue Wen', 'Zekun Wang', 'Rui Men', 'Ivan Titov', 'Dayiheng Liu', 'Jingren Zhou', 'Junyang Lin']","This paper revisits the implementation of $\textbf{L}$oad-$\textbf{b}$alancing $\textbf{L}$oss (LBL) when training Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as $N_E \sum_{i=1}^{N_E} f_i p_i$, where $N_E$ is the total number of experts, $f_i$ represents the frequency of expert $i$ being selected, and $p_i$ denotes the average gating score of the expert $i$. Existing MoE training frameworks usually employ the parallel training strategy so that $f_i$ and the LBL are calculated within a $\textbf{micro-batch}$ and then averaged across parallel groups. In essence, a micro-batch for training billion-scale LLMs normally contains very few sequences. So, the micro-batch LBL is almost at the sequence level, and the router is pushed to distribute the token evenly within each sequence. Under this strict constraint, even tokens from a domain-specific sequence ($\textit{e.g.}$, code) are uniformly routed to all experts, thereby inhibiting expert specialization. In this work, we propose calculating LBL using a $\textbf{global-batch}$ to loose this constraint. Because a global-batch contains much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level. Specifically, we introduce an extra communication step to synchronize $f_i$ across micro-batches and then use it to calculate the LBL. Through experiments on training MoEs-based LLMs (up to $\textbf{42.8B}$ total parameters and $\textbf{400B}$ tokens), we surprisingly find that the global-batch LBL strategy yields excellent performance gains in both pre-training perplexity and downstream tasks. Our analysis reveals that the global-batch LBL also greatly improves the domain specialization of MoE experts.",Demons Implementing Load Balancing Loss Training Specialized Mixture Expert Models paper revisits implementation textbf L oad textbf b alancing textbf L oss LBL training Mixture Experts MoEs models Specifically LBL MoEs defined N_E sum_ 1 N_E f_i p_i N_E total number experts f_i represents frequency expert selected p_i denotes average gating score expert Existing MoE training frameworks usually employ parallel training strategy f_i LBL calculated textbf micro batch averaged parallel groups essence micro batch training billion scale LLMs normally contains sequences micro batch LBL sequence level router pushed distribute token evenly sequence strict constraint tokens domain specific sequence textit e g code uniformly routed experts inhibiting expert specialization work propose calculating LBL using textbf global batch loose constraint global batch contains diverse sequences micro batch encourage load balance corpus level Specifically introduce extra communication step synchronize f_i micro batches use calculate LBL experiments training MoEs based LLMs textbf 42 8B total parameters textbf 400B tokens surprisingly global batch LBL strategy yields excellent performance gains pre training perplexity downstream tasks analysis reveals global batch LBL greatly improves domain specialization MoE experts
265,Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models,"['Jinyang Wu', 'Shuai Zhang', 'Feihu Che', 'Mingkuan Feng', 'Pengpeng Shao', 'Jianhua Tao']",,Pandora s Box Aladdin s Lamp Comprehensive Analysis Revealing Role RAG Noise Large Language Models
266,Stepwise Reasoning Disruption Attack of LLMs,"['Jingyu Peng', 'Maolin Wang', 'Xiangyu Zhao', 'Kai Zhang', 'Wanyu Wang', 'Pengyue Jia', 'Qidong Liu', 'Ruocheng Guo', 'Qi Liu']",,Stepwise Reasoning Disruption Attack LLMs
267,Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge,"['Qiyuan Zhang', 'Yufei Wang', 'Yuxin Jiang', 'Liangyou Li', 'Chuhan Wu', 'Yasheng Wang', 'Xin Jiang', 'Lifeng Shang', 'Ruiming Tang', 'Fuyuan Lyu', 'Chen Ma']","LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales.",Crowd Comparative Reasoning Unlocking Comprehensive Evaluations LLM Judge LLM Judge generates chain thought CoT judgments widely adopted auto evaluation method reliability compromised CoT reasoning s inability capture comprehensive deeper details leading incomplete outcomes Existing methods mainly rely majority voting criteria expansion insufficient address limitation CoT propose Crowd based Comparative Evaluation introduces additional crowd responses compare candidate responses exposing deeper comprehensive details candidate responses process effectively guides LLM Judge provide detailed CoT judgment Extensive experiments demonstrate approach enhances evaluation reliability achieving average accuracy gain 6 7 benchmarks method produces higher quality CoTs facilitate judge distillation exhibit superior performance rejection sampling supervised fine tuning SFT referred crowd rejection sampling enabling efficient SFT analysis confirms CoTs generated comprehensive higher quality evaluation accuracy improves inference scales
268,Lost in Multilinguality: Dissecting Cross-lingual Factual Inconsistency in Transformer Language Models,"['Mingyang Wang', 'Heike Adel', 'Lukas Lange', 'Yihong Liu', 'Ercong Nie', 'Jannik Strötgen', 'Hinrich Schuetze']","Multilingual language models (MLMs) store factual knowledge across languages but often struggle to provide consistent responses to semantically equivalent prompts in different languages. While previous studies point out this cross-lingual inconsistency issue, the underlying causes remain unexplored. In this work, we use mechanistic interpretability methods to investigate cross-lingual inconsistencies in MLMs. We find that MLMs encode knowledge in a language-independent concept space through most layers, and only transition to language-specific spaces in the final layers. Failures during the language transition often result in incorrect predictions in the target language, even when the answers are correct in other languages. To mitigate this inconsistency issue, we propose a linear shortcut method that bypasses computations in the final layers, enhancing both prediction accuracy and cross-lingual consistency. Our findings shed light on the internal mechanisms of MLMs and provide a lightweight, effective strategy for producing more consistent factual outputs.",Lost Multilinguality Dissecting Cross lingual Factual Inconsistency Transformer Language Models Multilingual language models MLMs store factual knowledge languages struggle provide consistent responses semantically equivalent prompts different languages previous studies point cross lingual inconsistency issue underlying causes remain unexplored work use mechanistic interpretability methods investigate cross lingual inconsistencies MLMs MLMs encode knowledge language independent concept space layers transition language specific spaces final layers Failures language transition result incorrect predictions target language answers correct languages mitigate inconsistency issue propose linear shortcut method bypasses computations final layers enhancing prediction accuracy cross lingual consistency findings shed light internal mechanisms MLMs provide lightweight effective strategy producing consistent factual outputs
269,Optimizing Decomposition for Optimal Claim Verification,"['Yining Lu', 'Noah Ziems', 'Hy Dang', 'Meng Jiang']","Current research on the \textit{Decompose-Then-Verify} paradigm for evaluating the factuality of long-form text typically treats decomposition and verification in isolation, overlooking their interactions and potential misalignment. We find that existing decomposition policies, typically hand-crafted demonstrations, do not align well with downstream verifiers in terms of atomicity -- a novel metric quantifying information density -- leading to suboptimal verification results. We formulate finding the optimal decomposition policy for optimal verification as a bilevel optimization problem. To approximate a solution for this strongly NP-hard problem, we propose dynamic decomposition, a reinforcement learning framework that leverages verifier feedback to learn a policy for dynamically decomposing claims to verifier-preferred atomicity. Experimental results show that dynamic decomposition outperforms existing decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on average across varying verifiers, datasets, and atomcities of input claims.",Optimizing Decomposition Optimal Claim Verification Current research textit Decompose Verify paradigm evaluating factuality long form text typically treats decomposition verification isolation overlooking interactions potential misalignment existing decomposition policies typically hand crafted demonstrations align downstream verifiers terms atomicity novel metric quantifying information density leading suboptimal verification results formulate finding optimal decomposition policy optimal verification bilevel optimization problem approximate solution strongly NP hard problem propose dynamic decomposition reinforcement learning framework leverages verifier feedback learn policy dynamically decomposing claims verifier preferred atomicity Experimental results dynamic decomposition outperforms existing decomposition policies improving verification confidence 0 07 accuracy 0 12 0 1 scale average varying verifiers datasets atomcities input claims
270,GradOT: Training-free Gradient-persevering Offsite-tuning for Large Language Models,"['Kai Yao', 'Zhaorui Tan', 'Penglei Gao', 'Lichun Li', 'Kaixin Wu', 'Yinggui Wang', 'Yuan Zhao', 'Yixin Ji', 'Jianke Zhu', 'Wei Wang']",,GradOT Training free Gradient persevering Offsite tuning Large Language Models
271,Knowledge Boundary of Large Language Models: A Survey,"['Moxin Li', 'Yong Zhao', 'Wenxuan Zhang', 'Shuaiyi Li', 'Wenya Xie', 'See-Kiong Ng', 'Tat-Seng Chua', 'Yang Deng']",,Knowledge Boundary Large Language Models Survey
272,Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning,"['Hai-Long Sun', 'Zhun Sun', 'Houwen Peng', 'Han-Jia Ye']","Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4 points vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems.",Mitigating Visual Forgetting Visual Conditioning Multi modal Long CoT Reasoning Recent advancements Large Language Models LLMs demonstrated enhanced reasoning capabilities evolving Chain Thought CoT prompting advanced product oriented solutions like OpenAI o1 implementation model noticed multimodal tasks requiring visual input e g geometry problems Multimodal LLMs MLLMs struggle maintain focus visual information words MLLMs suffer gradual decline attention visual information reasoning progresses causing text relied outputs investigate ablate image inputs long chain reasoning Concretely truncate reasoning process midway complete reasoning process input image removed observe 2 accuracy drop MathVista s test hard subset revealing model s textual outputs dominate following reasoning process Motivated propose Visual Conditioning TVC strategy shifts image input critical reasoning stages compresses redundant visual tokens dynamic pruning methodology helps model retain attention visual components reasoning approach achieves state art performance average mathematical reasoning benchmarks 3 4 points vs previous sota demonstrating effectiveness TVC enhancing multimodal reasoning systems
273,MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System,"['Jihao Zhao', 'Zhiyuan Ji', 'Zhaoxin Fan', 'Hanyu Wang', 'Simin Niu', 'Bo Tang', 'Feiyu Xiong', 'Zhiyu li']","Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline. This paper initially introduces a dual-metric evaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable the direct quantification of chunking quality. Leveraging this assessment method, we highlight the inherent limitations of traditional and semantic chunking in handling complex contextual nuances, thereby substantiating the necessity of integrating LLMs into chunking process. To address the inherent trade-off between computational efficiency and chunking precision in LLM-based approaches, we devise the granularity-aware Mixture-of-Chunkers (MoC) framework, which consists of a three-stage processing mechanism. Notably, our objective is to guide the chunker towards generating a structured list of chunking regular expressions, which are subsequently employed to extract chunks from the original text. Extensive experiments demonstrate that both our proposed metrics and the MoC framework effectively settle challenges of the chunking task, revealing the chunking kernel while enhancing the performance of the RAG system.",MoC Mixtures Text Chunking Learners Retrieval Augmented Generation Retrieval Augmented Generation RAG serving viable complement large language models LLMs overlooks crucial aspect text chunking pipeline paper initially introduces dual metric evaluation method comprising Boundary Clarity Chunk Stickiness enable direct quantification chunking quality Leveraging assessment method highlight inherent limitations traditional semantic chunking handling complex contextual nuances substantiating necessity integrating LLMs chunking process address inherent trade computational efficiency chunking precision LLM based approaches devise granularity aware Mixture Chunkers MoC framework consists stage processing mechanism Notably objective guide chunker generating structured list chunking regular expressions subsequently employed extract chunks original text Extensive experiments demonstrate proposed metrics MoC framework effectively settle challenges chunking task revealing chunking kernel enhancing performance RAG
274,Mitigating Selection Bias with Node Pruning and Auxiliary Options,"['Hyeong Kyu Choi', 'Weijie Xu', 'Chi Xue', 'Stephanie Eckman', 'Chandan K. Reddy']","Large language models (LLMs) often exhibit systematic preferences for certain answer choices when responding to multiple-choice questions-a behavior known as selection bias. This bias reduces the accuracy and reliability of LLM outputs, limiting their usefulness in decision-critical applications. While prior work has focused on adjusting model inputs or outputs to mitigate this issue, our work takes a fundamentally different approach by identifying and removing the internal sources of bias. We introduce two methods: Bias Node Pruning (BNP), which prunes parameters that contribute to selection bias, and Auxiliary Option Injection (AOI), which introduces an additional answer choice to reduce bias in both white-box and black-box settings. To address the shortcomings of existing evaluation metrics, we propose Choice Kullback-Leibler Divergence (CKLD), a new metric that captures distributional imbalances in model predictions. Experiments on three LLMs across multiple datasets demonstrate that our methods consistently improve answer accuracy while reducing selection bias, providing a robust solution for both open- and closed-source models.",Mitigating Selection Bias Node Pruning Auxiliary Options Large language models LLMs exhibit systematic preferences certain answer choices responding multiple choice questions behavior known selection bias bias reduces accuracy reliability LLM outputs limiting usefulness decision critical applications prior work focused adjusting model inputs outputs mitigate issue work takes fundamentally different approach identifying removing internal sources bias introduce methods Bias Node Pruning BNP prunes parameters contribute selection bias Auxiliary Option Injection AOI introduces additional answer choice reduce bias white box black box settings address shortcomings existing evaluation metrics propose Choice Kullback Leibler Divergence CKLD new metric captures distributional imbalances model predictions Experiments LLMs multiple datasets demonstrate methods consistently improve answer accuracy reducing selection bias providing robust solution open closed source models
275,Dually Self-Improved Counterfactual Data Augmentation Using Large Language Model,"['Luhao Zhang', 'Xinyu Zhang', 'Linmei Hu', 'Dandan Song', 'Liqiang Nie']",,Dually Self Improved Counterfactual Data Augmentation Using Large Language Model
276,RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented Generation,"['Shi-Qi Yan', 'Quan Liu', 'Zhen-Hua Ling']","While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing external knowledge, its generation process heavily depends on the quality and accuracy of the retrieved context. Large language models (LLMs) struggle to evaluate the correctness of non-parametric knowledge retrieved externally when it differs from internal memorization, leading to knowledge conflicts during response generation. To this end, we introduce the Retrieval Preference Optimization (RPO), a lightweight and effective alignment method to adaptively leverage multi-source knowledge based on retrieval relevance. An implicit representation of retrieval relevance is derived and incorporated into the reward model to integrate retrieval evaluation and response generation into a single model, solving the problem that previous methods necessitate the additional procedure to assess the retrieval quality. Notably, RPO is the only RAG-dedicated alignment approach that quantifies the awareness of retrieval relevance in training, overcoming mathematical obstacles. Experiments on four datasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any extra component, exhibiting its robust generalization.",RPO Retrieval Preference Optimization Robust Retrieval Augmented Generation Retrieval Augmented Generation RAG exhibited promise utilizing external knowledge generation process heavily depends quality accuracy retrieved context Large language models LLMs struggle evaluate correctness non parametric knowledge retrieved externally differs internal memorization leading knowledge conflicts response generation end introduce Retrieval Preference Optimization RPO lightweight effective alignment method adaptively leverage multi source knowledge based retrieval relevance implicit representation retrieval relevance derived incorporated reward model integrate retrieval evaluation response generation single model solving problem previous methods necessitate additional procedure assess retrieval quality Notably RPO RAG dedicated alignment approach quantifies awareness retrieval relevance training overcoming mathematical obstacles Experiments datasets demonstrate RPO outperforms RAG 4 10 accuracy extra component exhibiting robust generalization
277,Improving Parallel Sentence Mining for Low-Resource and Endangered Languages,"['Shu Okabe', 'Katharina Hämmerl', 'Alexander Fraser']",,Improving Parallel Sentence Mining Low Resource Endangered Languages
278,Learning to Reason from Feedback at Test-Time,"['Yanyang Li', 'Michael Lyu', 'Liwei Wang']","Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OpTune, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OpTune achieve superior scalability and performance.",Learning Reason Feedback Test Time Solving complex tasks single attempt challenging large language models LLMs Iterative interaction environment feedback required achieve success making effective feedback utilization critical topic Existing approaches struggle length generalization rely naive retries leveraging prior information paper introduce FTTT novel paradigm formulates feedback utilization optimization problem test time Additionally propose learnable test time optimizer OpTune effectively exploit feedback Experiments LLMs reasoning datasets demonstrate FTTT OpTune achieve superior scalability performance
279,$\textit{L-CiteEval}$: A Suite for Evaluating Fidelity of Long-context Models,"['Zecheng Tang', 'Keyan Zhou', 'Juntao Li', 'Baibei Ji', 'jianye hou', 'Min Zhang']",,textit L CiteEval Suite Evaluating Fidelity Long context Models
280,$SECRET$: Semi-supervised Clinical Trial Document Similarity Search,"['Trisha Das', 'Afrah Shafquat', 'Mandis Beigi', 'Jacob Aptekar', 'Jimeng Sun']",,SECRET Semi supervised Clinical Trial Document Similarity Search
281,Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models’ Uncertainty?,"['Jiayu Liu', 'Qing Zong', 'Weiqi Wang', 'Yangqiu Song']",,Revisiting Epistemic Markers Confidence Estimation Markers Accurately Reflect Large Language Models Uncertainty
282,Geometric Signatures of Compositionality Across a Language Model’s Lifetime,"['Jin Hwa Lee', 'Thomas Jiralerspong', 'Lei Yu', 'Yoshua Bengio', 'Emily Cheng']",,Geometric Signatures Compositionality Language Model s Lifetime
283,Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine,"['Maxime Griot', 'Jean Vanderdonckt', 'Demet YUKSEL', 'Coralie Hemptinne']","Large Language Models (LLMs) such as ChatGPT demonstrate significant potential in the medical domain and are often evaluated using multiple-choice questions (MCQs) modeled on exams like the USMLE. However, such benchmarks may overestimate true clinical understanding by rewarding pattern recognition and test-taking heuristics. To investigate this, we created a fictional medical benchmark centered on an imaginary organ, the Glianorex, allowing us to separate memorized knowledge from reasoning ability. We generated textbooks and MCQs in English and French using leading LLMs, then evaluated proprietary, open-source, and domain-specific models in a zero-shot setting. Despite the fictional content, models achieved an average score of 64%, while physicians scored only 27%. Fine-tuned medical models outperformed base models in English but not in French. Ablation and interpretability analyses revealed that models frequently relied on shallow cues, test-taking strategies, and hallucinated reasoning to identify the correct choice. These results suggest that standard MCQ-based evaluations may not effectively measure clinical reasoning and highlight the need for more robust, clinically meaningful assessment methods for LLMs.",Pattern Recognition Medical Knowledge Problem Multiple Choice Questions Medicine Large Language Models LLMs ChatGPT demonstrate significant potential medical domain evaluated using multiple choice questions MCQs modeled exams like USMLE benchmarks overestimate true clinical understanding rewarding pattern recognition test taking heuristics investigate created fictional medical benchmark centered imaginary organ Glianorex allowing separate memorized knowledge reasoning ability generated textbooks MCQs English French using leading LLMs evaluated proprietary open source domain specific models zero shot setting Despite fictional content models achieved average score 64 physicians scored 27 Fine tuned medical models outperformed base models English French Ablation interpretability analyses revealed models frequently relied shallow cues test taking strategies hallucinated reasoning identify correct choice results suggest standard MCQ based evaluations effectively measure clinical reasoning highlight need robust clinically meaningful assessment methods LLMs
284,People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text,"['Jenna Russell', 'Marzena Karpinska', 'Mohit Iyyer']","In this paper, we study how well humans can detect text generated by commercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300 non-fiction English articles, label them as either human-written or AI-generated, and provide paragraph-length explanations for their decisions. Our experiments show that annotators who frequently use LLMs for writing tasks excel at detecting AI-generated text, even without any specialized training or feedback. In fact, the majority vote among five such ""expert"" annotators misclassifies only 1 of 300 articles, significantly outperforming most commercial and open-source detectors we evaluated even in the presence of evasion tactics like paraphrasing and humanization. Qualitative analysis of the experts' free-form explanations shows that while they rely heavily on specific lexical clues ('AI vocabulary'), they also pick up on more complex phenomena within the text (e.g., formality, originality, clarity) that are challenging to assess for automatic detectors. We release our annotated dataset and code to spur future research into both human and automated detection of AI-generated text.",People frequently use ChatGPT writing tasks accurate robust detectors AI generated text paper study humans detect text generated commercial LLMs GPT 4o Claude o1 hire annotators read 300 non fiction English articles label human written AI generated provide paragraph length explanations decisions experiments annotators frequently use LLMs writing tasks excel detecting AI generated text specialized training feedback fact majority vote expert annotators misclassifies 1 300 articles significantly outperforming commercial open source detectors evaluated presence evasion tactics like paraphrasing humanization Qualitative analysis experts free form explanations shows rely heavily specific lexical clues AI vocabulary pick complex phenomena text e g formality originality clarity challenging assess automatic detectors release annotated dataset code spur future research human automated detection AI generated text
285,YuLan-Mini: Pushing the Limits of Open Data-efficient Language Model,"['Hu Yiwen', 'Song Huatong', 'Jie Chen', 'Jia Deng', 'jiapeng wang', 'Kun Zhou', 'Yutao Zhu', 'Jinhao Jiang', 'zican Dong', 'YANG LU', 'Xu Miao', 'Xin Zhao', 'Ji-Rong Wen']",,YuLan Mini Pushing Limits Open Data efficient Language Model
286,"Your Model is Overconfident, and Other Lies We Tell Ourselves","['Timothee Mickus', 'Aman Sinha', 'Raúl Vázquez']","The difficulty intrinsic to a given example, rooted in its inherent ambiguity, is a key yet often overlooked factor in evaluating neural NLP models. We investigate the interplay and divergence among various metrics for assessing intrinsic difficulty, including annotator dissensus, training dynamics, and model confidence. Through a comprehensive analysis using 29 models on three datasets, we reveal that while correlations exist among these metrics, their relationships are neither linear nor monotonic. By disentangling these dimensions of uncertainty, we aim to refine our understanding of data complexity and its implications for evaluating and improving NLP models.",Model Overconfident Lies Tell difficulty intrinsic given example rooted inherent ambiguity key overlooked factor evaluating neural NLP models investigate interplay divergence various metrics assessing intrinsic difficulty including annotator dissensus training dynamics model confidence comprehensive analysis using 29 models datasets reveal correlations exist metrics relationships linear monotonic disentangling dimensions uncertainty aim refine understanding data complexity implications evaluating improving NLP models
287,Bridging the Language Gaps in Large Language Models with Inference-Time Cross-Lingual Intervention,"['Weixuan Wang', 'Minghao Wu', 'Barry Haddow', 'Alexandra Birch']","Large Language Models (LLMs) have shown remarkable capabilities in natural language processing but exhibit significant performance gaps among different languages. Most existing approaches to address these disparities rely on pretraining or fine-tuning, which are resource-intensive. To overcome these limitations without incurring significant costs, we propose Inference-Time Cross-Lingual Intervention (INCLINE), a novel framework that enhances LLM performance on low-performing (source) languages by aligning their internal representations with those of high-performing (target) languages during inference. INCLINE initially learns alignment matrices using parallel sentences from source and target languages through a Least-Squares optimization, and then applies these matrices during inference to transform the low-performing language representations toward the high-performing language space. Extensive experiments on nine benchmarks with five LLMs demonstrate that INCLINE significantly improves performance across diverse tasks and languages, compared to recent strong baselines. Our analysis demonstrates that INCLINE is highly cost-effective and applicable to a wide range of applications. In addition, we release the code to foster research along this line: https://github.com/weixuan-wang123/INCLINE.",Bridging Language Gaps Large Language Models Inference Time Cross Lingual Intervention Large Language Models LLMs shown remarkable capabilities natural language processing exhibit significant performance gaps different languages existing approaches address disparities rely pretraining fine tuning resource intensive overcome limitations incurring significant costs propose Inference Time Cross Lingual Intervention INCLINE novel framework enhances LLM performance low performing source languages aligning internal representations high performing target languages inference INCLINE initially learns alignment matrices using parallel sentences source target languages Squares optimization applies matrices inference transform low performing language representations high performing language space Extensive experiments benchmarks LLMs demonstrate INCLINE significantly improves performance diverse tasks languages compared recent strong baselines analysis demonstrates INCLINE highly cost effective applicable wide range applications addition release code foster research line https github com weixuan wang123 INCLINE
288,Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models,"['Kyeonghyun Kim', 'Jinhee Jang', 'Juhwan Choi', 'Yoonji Lee', 'Kyohoon Jin', 'YoungBin Kim']","Large language models (LLMs) are renowned for their extensive linguistic knowledge and strong generalization capabilities, but their high computational demands make them unsuitable for resource-constrained environments. In contrast, small language models (SLMs) are computationally efficient but often lack the broad generalization capacity of LLMs. To bridge this gap, we propose PiFi, a novel framework that combines the strengths of both LLMs and SLMs to achieve high performance while maintaining efficiency. PiFi integrates a single frozen layer from an LLM into a SLM and fine-tunes the combined model for specific tasks, boosting performance without a significant increase in computational cost. We show that PiFi delivers consistent performance improvements across a range of natural language processing tasks, including both natural language understanding and generation. Moreover, our findings demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing generalization to unseen domains and facilitating the transfer of linguistic abilities.",Plug Fine tuning Bridging Gap Small Language Models Large Language Models Large language models LLMs renowned extensive linguistic knowledge strong generalization capabilities high computational demands make unsuitable resource constrained environments contrast small language models SLMs computationally efficient lack broad generalization capacity LLMs bridge gap propose PiFi novel framework combines strengths LLMs SLMs achieve high performance maintaining efficiency PiFi integrates single frozen layer LLM SLM fine tunes combined model specific tasks boosting performance significant increase computational cost PiFi delivers consistent performance improvements range natural language processing tasks including natural language understanding generation findings demonstrate PiFi s ability effectively leverage LLM knowledge enhancing generalization unseen domains facilitating transfer linguistic abilities
289,"What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma","['Han Meng', 'Yancan Chen', 'Yunan Li', 'YITIAN YANG', 'Jungup Lee', 'Renwen Zhang', 'Yi-Chieh Lee']","Mental-health stigma remains a pervasive social problem that hampers treatment-seeking and recovery. Existing resources for training neural models to finely classify such stigma are limited, relying primarily on social-media or synthetic data without theoretical underpinnings. To remedy this gap, we present an expert-annotated, theory-informed corpus of human-chatbot interviews, comprising 4,141 snippets from 684 participants with documented socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural models and empirically unpack the challenges of stigma detection. This dataset can facilitate research on computationally detecting, neutralizing, and counteracting mental-health stigma. Our corpus is openly available at https://github.com/HanMeng2004/Mental-Health-Stigma-Interview-Corpus.",Stigma Attributed Theory Grounded Expert Annotated Interview Corpus Demystifying Mental Health Stigma Mental health stigma remains pervasive social problem hampers treatment seeking recovery Existing resources training neural models finely classify stigma limited relying primarily social media synthetic data theoretical underpinnings remedy gap present expert annotated theory informed corpus human chatbot interviews comprising 4 141 snippets 684 participants documented socio cultural backgrounds experiments benchmark state art neural models empirically unpack challenges stigma detection dataset facilitate research computationally detecting neutralizing counteracting mental health stigma corpus openly available https github com HanMeng2004 Mental Health Stigma Interview Corpus
290,ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors,"['Yuguo Yin', 'Yuxin Xie', 'Wenyuan Yang', 'Dongchao Yang', 'Jinghan Ru', 'Xianwei Zhuang', 'Liming Liang', 'Yuexian Zou']","Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to retrieve audio clips or multilingual texts from databases. However, existing ML-ATR schemes suffer from inconsistencies for instance similarity matching across languages. We theoretically analyze the inconsistency in terms of both multilingual modal alignment direction error and weight error, and propose the theoretical weight error upper bound for quantifying the inconsistency. Based on the analysis of the weight error upper bound, we find that the inconsistency problem stems from the data distribution error caused by random sampling of languages. We propose a consistent ML-ATR scheme using 1-to-k contrastive learning and audio-English co-anchor contrastive learning, aiming to mitigate the negative impact of data distribution error on recall and consistency in ML-ATR. Experimental results on the translated AudioCaps and Clotho datasets show that our scheme achieves state-of-the-art performance on recall and consistency metrics for eight mainstream languages, including English. Our code will be available at https://github.com/ATRI-ACL/ATRI-ACL.",ATRI Mitigating Multilingual Audio Text Retrieval Inconsistencies Reducing Data Distribution Errors Multilingual audio text retrieval ML ATR challenging task aims retrieve audio clips multilingual texts databases existing ML ATR schemes suffer inconsistencies instance similarity matching languages theoretically analyze inconsistency terms multilingual modal alignment direction error weight error propose theoretical weight error upper bound quantifying inconsistency Based analysis weight error upper bound inconsistency problem stems data distribution error caused random sampling languages propose consistent ML ATR scheme using 1 k contrastive learning audio English anchor contrastive learning aiming mitigate negative impact data distribution error recall consistency ML ATR Experimental results translated AudioCaps Clotho datasets scheme achieves state art performance recall consistency metrics mainstream languages including English code available https github com ATRI ACL ATRI ACL
291,Enhancing Transformers for Generalizable First-Order Logical Entailment,"['Tianshi Zheng', 'Jiazheng Wang', 'Zihao Wang', 'Jiaxin Bai', 'Hang Yin', 'Zheye Deng', 'Yangqiu Song', 'Jianxin Li']","Transformers, as the fundamental deep learning architecture, have demonstrated great capability in reasoning. This paper studies the generalizable first-order logical reasoning ability of transformers with their parameterized knowledge and how to improve it. Transformers' capability of first-order reasoning is further captured by whether they can conduct first-order logical entailment, which is quantitatively measured by their performance in answering knowledge graph queries. We establish the connections between (1) two types of distribution shifts studied in out-of-distribution generalization and (2) unseen knowledge and query settings discussed in the task of knowledge graph query answering, which makes it possible to characterize the fine-grained generalizability. Results on our comprehensive dataset showed that transformers \textit{outperform} previous methods designed particularly for this task and provided detailed empirical evidence about the impact of the input query syntax, token embedding, and transformer architectures on their reasoning capability. Interestingly, our results revealed the mismatch of positional encoding and other design choices of transformer architectures in previous practices. Motivated by this, we propose TEGA, a logic-aware architecture that significantly improves the performance in generalizable first-order logical entailment.",Enhancing Transformers Generalizable Order Logical Entailment Transformers fundamental deep learning architecture demonstrated great capability reasoning paper studies generalizable order logical reasoning ability transformers parameterized knowledge improve Transformers capability order reasoning captured conduct order logical entailment quantitatively measured performance answering knowledge graph queries establish connections 1 types distribution shifts studied distribution generalization 2 unseen knowledge query settings discussed task knowledge graph query answering makes possible characterize fine grained generalizability Results comprehensive dataset showed transformers textit outperform previous methods designed particularly task provided detailed empirical evidence impact input query syntax token embedding transformer architectures reasoning capability Interestingly results revealed mismatch positional encoding design choices transformer architectures previous practices Motivated propose TEGA logic aware architecture significantly improves performance generalizable order logical entailment
292,Self-Taught Agentic Long Context Understanding,"['Yufan Zhuang', 'Xiaodong Yu', 'Jialian Wu', 'Ximeng Sun', 'Ze Wang', 'Jiang Liu', 'Yusheng Su', 'Jingbo Shang', 'Zicheng Liu', 'Emad Barsoum']","Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.",Self Taught Agentic Long Context Understanding Answering complex long context questions remains major challenge large language models LLMs requires effective question clarifications context retrieval propose Agentic Long Context Understanding AgenticLU framework designed enhance LLM s understanding queries integrating targeted self clarification contextual grounding agentic workflow core AgenticLU Chain Clarifications CoC models refine understanding self generated clarification questions corresponding contextual groundings scaling inference tree search node represents CoC step achieve 97 8 answer recall NarrativeQA search depth branching factor amortize high cost search process training leverage preference pairs step obtained CoC workflow perform stage model finetuning 1 supervised finetuning learn effective decomposition strategies 2 direct preference optimization enhance reasoning quality enables AgenticLU models generate clarifications retrieve relevant context effectively efficiently single inference pass Extensive experiments seven long context tasks demonstrate AgenticLU significantly outperforms state art prompting methods specialized long context LLMs achieving robust multi hop reasoning sustaining consistent performance context length grows
293,Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model Training,"['Shahrad Mohammadzadeh', 'Juan David Guerra', 'Marco Bonizzato', 'Reihaneh Rabbany', 'Golnoosh Farnadi']","As large language models (LLMs) are increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations - outputs that are factually inaccurate or irrelevant to user input - have grown. Our research investigates the relationship between the training process and the emergence of hallucinations to address a key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M - 12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce Sensitivity Dropout (SenD), a novel training protocol designed to mitigate hallucinations by reducing variance during training. SenD achieves this by deterministically dropping embedding indices with significant variability, referred to as Sensitive Embedding Indices. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore at 2x speed. This efficient metric is integrated into our protocol, allowing SenD to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to Wikipedia, Medical, and LegalBench domains.",Hallucination Detox Sensitivity Dropout SenD Large Language Model Training large language models LLMs increasingly deployed various industries concerns regarding reliability particularly hallucinations outputs factually inaccurate irrelevant user input grown research investigates relationship training process emergence hallucinations address key gap existing research focuses primarily post hoc detection mitigation strategies Using models Pythia suite 70M 12B parameters hallucination detection metrics analyze hallucination trends training explore LLM internal dynamics introduce Sensitivity Dropout SenD novel training protocol designed mitigate hallucinations reducing variance training SenD achieves deterministically dropping embedding indices significant variability referred Sensitive Embedding Indices addition develop unsupervised hallucination detection metric Efficient EigenScore EES approximates traditional EigenScore 2x speed efficient metric integrated protocol allowing SenD computationally scalable effective reducing hallucinations empirical evaluation demonstrates approach improves LLM reliability test time 40 compared normal training providing efficient method improve factual accuracy adapting LLMs Wikipedia Medical LegalBench domains
294,OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis,"['Qiushi Sun', 'Kanzhi Cheng', 'Zichen Ding', 'Chuanyang Jin', 'Yian Wang', 'Fangzhi Xu', 'Zhenyu Wu', 'Chengyou Jia', 'Liheng Chen', 'Zhoumianze Liu', 'Ben Kao', 'Guohao Li', 'Junxian He', 'Yu Qiao', 'Zhiyong Wu']","Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at https://qiushisun.github.io/OS-Genesis-Home/.",OS Genesis Automating GUI Agent Trajectory Construction Reverse Task Synthesis Graphical User Interface GUI agents powered Vision Language Models VLMs demonstrated human like computer control capability Despite utility advancing digital automation critical bottleneck persists collecting high quality trajectory data training Common practices collecting data rely human supervision synthetic data generation executing pre defined tasks resource intensive unable guarantee data quality methods suffer limited data diversity significant gaps synthetic data real world environments address challenges propose OS Genesis novel GUI data synthesis pipeline reverses conventional trajectory collection process Instead relying pre defined tasks OS Genesis enables agents perceive environments perform step wise interactions retrospectively derive high quality tasks enable trajectory level exploration trajectory reward model employed ensure quality generated trajectories demonstrate training GUI agents OS Genesis significantly improves performance highly challenging online benchmarks depth analysis validates OS Genesis s efficiency superior data quality diversity compared existing synthesis methods codes data checkpoints available https qiushisun github io OS Genesis Home
295,CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter,"['Yepeng Weng', 'Dianwen Mei', 'Huishi Qiu', 'Xujie Chen', 'Li Liu', 'Jiang Tian', 'Zhongchao Shi']","Speculative decoding is a powerful technique that accelerates Large Language Model (LLM) inference by leveraging a lightweight speculative draft model. However, existing designs suffers in performance due to misalignment between training and inference. Recent methods have tried to solve this issue by adopting a multi-step training strategy, but the complex inputs of different training steps make it harder for the draft model to converge. To address this, we propose CORAL, a novel framework that improves both accuracy and efficiency in speculative drafting. CORAL introduces Cross-Step Representation Alignment, a method that enhances consistency across multiple training steps, significantly improving speculative drafting performance. Additionally, we identify the LM head as a major bottleneck in the inference speed of the draft model. We introduce a weight-grouping mechanism that selectively activates a subset of LM head parameters during inference, substantially reducing the latency of the draft model. We evaluate CORAL on three LLM families and three benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that CORAL effectively mitigates training-inference misalignment and delivers significant speedup for modern LLMs with large vocabularies.",CORAL Learning Consistent Representations Multi step Training Lighter Speculative Drafter Speculative decoding powerful technique accelerates Large Language Model LLM inference leveraging lightweight speculative draft model existing designs suffers performance misalignment training inference Recent methods tried solve issue adopting multi step training strategy complex inputs different training steps make harder draft model converge address propose CORAL novel framework improves accuracy efficiency speculative drafting CORAL introduces Cross Step Representation Alignment method enhances consistency multiple training steps significantly improving speculative drafting performance Additionally identify LM head major bottleneck inference speed draft model introduce weight grouping mechanism selectively activates subset LM head parameters inference substantially reducing latency draft model evaluate CORAL LLM families benchmark datasets achieving speedup ratios 2 50x 4 07x outperforming state art methods EAGLE 2 HASS results demonstrate CORAL effectively mitigates training inference misalignment delivers significant speedup modern LLMs large vocabularies
296,ConSim: Measuring Concept-Based Explanations’ Effectiveness with Automated Simulatability,"['Antonin Poché', 'Alon Jacovi', 'Agustin Martin Picard', 'Victor Boutin', 'Fanny Jourdan']",,ConSim Measuring Concept Based Explanations Effectiveness Automated Simulatability
297,Decoding Reading Goals from Eye Movements,"['Omer Shubi', 'Cfir Avraham Hadar', 'Yevgeni Berzak']","Readers can have different goals with respect to the text that they are reading. Can these goals be decoded from their eye movements over the text? In this work, we examine for the first time whether it is possible to distinguish between two types of common reading goals: information seeking and ordinary reading for comprehension. Using large-scale eye tracking data, we address this task with a wide range of models that cover different architectural and data representation strategies, and further introduce a new model ensemble. We find that transformer-based models with scanpath representations coupled with language modeling solve it most successfully, and that accurate predictions can be made in real time, long before the participant finished reading the text. We further introduce a new method for model performance analysis based on mixed effect modeling. Combining this method with rich textual annotations reveals key properties of textual items and participants that contribute to the difficulty of the task, and improves our understanding of the variability in eye movement patterns across the two reading regimes.",Decoding Reading Goals Eye Movements Readers different goals respect text reading goals decoded eye movements text work examine time possible distinguish types common reading goals information seeking ordinary reading comprehension Using large scale eye tracking data address task wide range models cover different architectural data representation strategies introduce new model ensemble transformer based models scanpath representations coupled language modeling solve successfully accurate predictions real time long participant finished reading text introduce new method model performance analysis based mixed effect modeling Combining method rich textual annotations reveals key properties textual items participants contribute difficulty task improves understanding variability eye movement patterns reading regimes
298,Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Space,"['Si Wu', 'Sebastian Bruch']","Imageability (potential of text to evoke a mental image) and concreteness (perceptibility of text) are two psycholinguistic properties that link visual and semantic spaces. It is little surprise that computational methods that estimate them do so using parallel visual and semantic spaces, such as collections of image-caption pairs or multi-modal models. In this paper, we work on the supposition that text itself in an image-caption dataset offers sufficient signals to accurately estimate these properties. We hypothesize, in particular, that the peakedness of the neighborhood of a word in the semantic embedding space reflects its degree of imageability and concreteness. We then propose an unsupervised, distribution-free measure, which we call Neighborhood Stability Measure (NSM), that quantifies the sharpness of peaks. Extensive experiments show that NSM correlates more strongly with ground-truth ratings than existing unsupervised methods, and is a strong predictor of these properties for classification. Our code and data are available on GitHub (https://github.com/Artificial-Memory-Lab/imageability).",Uncovering Visual Semantic Psycholinguistic Properties Distributional Structure Text Embedding Space Imageability potential text evoke mental image concreteness perceptibility text psycholinguistic properties link visual semantic spaces little surprise computational methods estimate using parallel visual semantic spaces collections image caption pairs multi modal models paper work supposition text image caption dataset offers sufficient signals accurately estimate properties hypothesize particular peakedness neighborhood word semantic embedding space reflects degree imageability concreteness propose unsupervised distribution free measure Neighborhood Stability Measure NSM quantifies sharpness peaks Extensive experiments NSM correlates strongly ground truth ratings existing unsupervised methods strong predictor properties classification code data available GitHub https github com Artificial Memory Lab imageability
299,GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent,"['Bin Xie', 'Rui Shao', 'Gongwei Chen', 'Kaiwen Zhou', 'Yinchuan Li', 'Jie Liu', 'Min Zhang', 'Liqiang Nie']","GUI automation faces critical challenges in dynamic environments. MLLMs suffer from two key issues: misinterpreting UI components and outdated knowledge. Traditional fine-tuning methods are costly for app-specific knowledge updates. We propose GUI-explorer, a training-free GUI agent that incorporates two fundamental mechanisms: (1) Autonomous Exploration of Function-aware Trajectory. To comprehensively cover all application functionalities, we design a Function-aware Task Goal Generator that automatically constructs exploration goals by analyzing GUI structural information (e.g., screenshots and activity hierarchies). This enables systematic exploration to collect diverse trajectories. (2) Unsupervised Mining of Transition-aware Knowledge. To establish precise screen-operation logic, we develop a Transition-aware Knowledge Extractor that extracts effective screen-operation logic through unsupervised analysis the state transition of structured interaction triples (observation, action, outcome). This eliminates the need for human involvement in knowledge extraction. With a task success rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows significant improvements over SOTA agents. It requires no parameter updates for new apps. GUI-explorer is open-sourced and publicly available at https://github.com/JiuTian-VL/GUI-explorer.",GUI explorer Autonomous Exploration Mining Transition aware Knowledge GUI Agent GUI automation faces critical challenges dynamic environments MLLMs suffer key issues misinterpreting UI components outdated knowledge Traditional fine tuning methods costly app specific knowledge updates propose GUI explorer training free GUI agent incorporates fundamental mechanisms 1 Autonomous Exploration Function aware Trajectory comprehensively cover application functionalities design Function aware Task Goal Generator automatically constructs exploration goals analyzing GUI structural information e g screenshots activity hierarchies enables systematic exploration collect diverse trajectories 2 Unsupervised Mining Transition aware Knowledge establish precise screen operation logic develop Transition aware Knowledge Extractor extracts effective screen operation logic unsupervised analysis state transition structured interaction triples observation action outcome eliminates need human involvement knowledge extraction task success rate 53 7 SPA Bench 47 4 AndroidWorld GUI explorer shows significant improvements SOTA agents requires parameter updates new apps GUI explorer open sourced publicly available https github com JiuTian VL GUI explorer
300,P$^2$ Law: Scaling Law for Post-Training After Model Pruning,"['Xiaodong Chen', 'Yuxuan Hu', 'Xiaokang Zhang', 'Yanling Wang', 'Cuiping Li', 'Hong Chen', 'Jing Zhang']","Pruning has become a widely adopted technique for reducing the hardware requirements of large language models (LLMs). To recover model performance after pruning, post-training is commonly employed to mitigate the resulting performance degradation. While post-training benefits from larger datasets, once the dataset size is already substantial, increasing the training data provides only limited performance gains. To balance post-training cost and model performance, it is necessary to explore the optimal amount of post-training data.Through extensive experiments on the Llama-3 and Qwen-2.5 series models, pruned using various common pruning methods, we uncover the scaling \textbf{Law} for \textbf{P}ost-training after model \textbf{P}runing, referred to as the P$^2$ Law.This law identifies four key factors for predicting the pruned model's post-training loss: the model size before pruning, the number of post-training tokens, the pruning rate, and the model's loss before pruning. Moreover, P$^2$ Law can generalize to larger dataset sizes, larger model sizes, and higher pruning rates, offering valuable insights for the post-training of pruned LLMs.",P 2 Law Scaling Law Post Training Model Pruning Pruning widely adopted technique reducing hardware requirements large language models LLMs recover model performance pruning post training commonly employed mitigate resulting performance degradation post training benefits larger datasets dataset size substantial increasing training data provides limited performance gains balance post training cost model performance necessary explore optimal post training data extensive experiments Llama 3 Qwen 2 5 series models pruned using various common pruning methods uncover scaling textbf Law textbf P ost training model textbf P runing referred P 2 Law law identifies key factors predicting pruned model s post training loss model size pruning number post training tokens pruning rate model s loss pruning P 2 Law generalize larger dataset sizes larger model sizes higher pruning rates offering valuable insights post training pruned LLMs
301,Making FETCH! Happen: Finding Emergent Dog Whistles Through Common Habitats,"['Kuleen Sasse', 'Carlos Alejandro Aguirre', 'Isabel Cachola', 'Sharon Levy', 'Mark Dredze']","WARNING: This paper contains content that maybe upsetting or offensive to some readers. Dog whistles are coded expressions with dual meanings: one intended for the general public (outgroup) and another that conveys a specific message to an intended audience (ingroup). Often, these expressions are used to convey controversial political opinions while maintaining plausible deniability and slip by content moderation filters. Identification of dog whistles relies on curated lexicons, which have trouble keeping up to date. We introduce FETCH!, a task for finding novel dog whistles in massive social media corpora. We find that state-of-the-art systems fail to achieve meaningful results across three distinct social media case studies. We present EarShot, a strong baseline system that combines the strengths of vector databases and Large Language Models (LLMs) to efficiently and effectively identify new dog whistles.",Making FETCH Happen Finding Emergent Dog Whistles Common Habitats WARNING paper contains content maybe upsetting offensive readers Dog whistles coded expressions dual meanings intended general public outgroup conveys specific message intended audience ingroup expressions used convey controversial political opinions maintaining plausible deniability slip content moderation filters Identification dog whistles relies curated lexicons trouble keeping date introduce FETCH task finding novel dog whistles massive social media corpora state art systems fail achieve meaningful results distinct social media case studies present EarShot strong baseline combines strengths vector databases Large Language Models LLMs efficiently effectively identify new dog whistles
302,Lost in the Context: Insufficient and Distracted Attention to Contexts in Preference Modeling,"['Shihan Dou', 'Jiayi Chen', 'Chenhao Huang', 'Feng Chen', 'Wei Chengzhi', 'Huiyuan Zheng', 'Shichun Liu', 'Yan Liu', 'Chenxiao Liu', 'Chao Xin', 'Lin Yan', 'Zongzhang Zhang', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang']",,Lost Context Insufficient Distracted Attention Contexts Preference Modeling
303,Entailment-Preserving First-order Logic Representations in Natural Language Entailment,"['Jinu Lee', 'Qi Liu', 'Runzhi Ma', 'Vincent Han', 'Ziqi Wang', 'Heng Ji', 'Julia Hockenmaier']","First-order logic (FOL) can represent the logical entailment semantics of natural language (NL) sentences, but determining natural language entailment using FOL remains a challenge. To address this, we propose the Entailment-Preserving FOL representations (EPF) task and introduce reference-free evaluation metrics for EPF, the Entailment-Preserving Rate (EPR) family. In EPF, one should generate FOL representations from multi-premise natural language entailment data (e.g. EntailmentBank) so that the automatic prover's result preserves the entailment labels. Experiments show that existing methods for NL-to-FOL translation struggle in EPF. To this extent, we propose a training method specialized for the task, iterative learning-to-rank, which directly optimizes the model's EPR score through a novel scoring function and a learning-to-rank objective. Our method achieves a 1.8-2.7% improvement in EPR and a 17.4-20.6% increase in EPR@16 compared to diverse baselines in three datasets. Further analyses reveal that iterative learning-to-rank effectively suppresses the arbitrariness of FOL representation by reducing the diversity of predicate signatures, and maintains strong performance across diverse inference types and out-of-domain data.",Entailment Preserving order Logic Representations Natural Language Entailment order logic FOL represent logical entailment semantics natural language NL sentences determining natural language entailment using FOL remains challenge address propose Entailment Preserving FOL representations EPF task introduce reference free evaluation metrics EPF Entailment Preserving Rate EPR family EPF generate FOL representations multi premise natural language entailment data e g EntailmentBank automatic prover s result preserves entailment labels Experiments existing methods NL FOL translation struggle EPF extent propose training method specialized task iterative learning rank directly optimizes model s EPR score novel scoring function learning rank objective method achieves 1 8 2 7 improvement EPR 17 4 20 6 increase EPR 16 compared diverse baselines datasets analyses reveal iterative learning rank effectively suppresses arbitrariness FOL representation reducing diversity predicate signatures maintains strong performance diverse inference types domain data
304,Enhancing Multimodal Continual Instruction Tuning with BranchLoRA,"['Duzhen Zhang', 'Yong Ren', 'Zhong-Zhi Li', 'Yahan Yu', 'Jiahua Dong', 'Chenxing Li', 'Zhilong Ji', 'Jinfeng Bai']","Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal Large Language Models (MLLMs) to continually align with human intent across sequential tasks. Existing approaches often rely on the Mixture-of-Experts (MoE) LoRA framework to preserve previous instruction alignments. However, these methods are prone to Catastrophic Forgetting (CF), as they aggregate all LoRA blocks via simple summation, which compromises performance over time. In this paper, we identify a critical parameter inefficiency in the MoELoRA framework within the MCIT context. Based on this insight, we propose BranchLoRA, an asymmetric framework to enhance both efficiency and performance. To mitigate CF, we introduce a flexible tuning-freezing mechanism within BranchLoRA, enabling branches to specialize in intra-task knowledge while fostering inter-task collaboration. Moreover, we incrementally incorporate task-specific routers to ensure an optimal branch distribution over time, rather than favoring the most recent task. To streamline inference, we introduce a task selector that automatically routes test inputs to the appropriate router without requiring task identity. Extensive experiments on the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms MoELoRA and maintains its superiority across various MLLM sizes.",Enhancing Multimodal Continual Instruction Tuning BranchLoRA Multimodal Continual Instruction Tuning MCIT aims finetune Multimodal Large Language Models MLLMs continually align human intent sequential tasks Existing approaches rely Mixture Experts MoE LoRA framework preserve previous instruction alignments methods prone Catastrophic Forgetting CF aggregate LoRA blocks simple summation compromises performance time paper identify critical parameter inefficiency MoELoRA framework MCIT context Based insight propose BranchLoRA asymmetric framework enhance efficiency performance mitigate CF introduce flexible tuning freezing mechanism BranchLoRA enabling branches specialize intra task knowledge fostering inter task collaboration incrementally incorporate task specific routers ensure optimal branch distribution time favoring recent task streamline inference introduce task selector automatically routes test inputs appropriate router requiring task identity Extensive experiments latest MCIT benchmark demonstrate BranchLoRA significantly outperforms MoELoRA maintains superiority various MLLM sizes
305,Enhancing Automated Interpretability with Output-Centric Feature Descriptions,"['Yoav Gur-Arieh', 'Roy Mayan', 'Chen Agassy', 'Atticus Geiger', 'Mor Geva']","Automated interpretability pipelines generate natural language descriptions for the concepts represented by features in large language models (LLMs), such as plants or the first word in a sentence. These descriptions are derived using inputs that activate the feature, which may be a dimension or a direction in the model's representation space. However, identifying activating inputs is costly, and the mechanistic role of a feature in model behavior is determined both by how inputs cause a feature to activate and by how feature activation affects outputs. Using steering evaluations, we reveal that current pipelines provide descriptions that fail to capture the causal effect of the feature on outputs. To fix this, we propose efficient, output-centric methods for automatically generating feature descriptions. These methods use the tokens weighted higher after feature stimulation or the highest weight tokens after applying the vocabulary ""unembedding"" head directly to the feature. Our output-centric descriptions better capture the causal effect of a feature on model outputs than input-centric descriptions, but combining the two leads to the best performance on both input and output evaluations. Lastly, we show that output-centric descriptions can be used to find inputs that activate features previously thought to be ""dead"".",Enhancing Automated Interpretability Output Centric Feature Descriptions Automated interpretability pipelines generate natural language descriptions concepts represented features large language models LLMs plants word sentence descriptions derived using inputs activate feature dimension direction model s representation space identifying activating inputs costly mechanistic role feature model behavior determined inputs cause feature activate feature activation affects outputs Using steering evaluations reveal current pipelines provide descriptions fail capture causal effect feature outputs fix propose efficient output centric methods automatically generating feature descriptions methods use tokens weighted higher feature stimulation highest weight tokens applying vocabulary unembedding head directly feature output centric descriptions better capture causal effect feature model outputs input centric descriptions combining leads best performance input output evaluations Lastly output centric descriptions used inputs activate features previously thought dead
306,Towards Effective and Efficient Continual Pre-training of Large Language Models,"['Jie Chen', 'Zhipeng Chen', 'jiapeng wang', 'Kun Zhou', 'Yutao Zhu', 'Jinhao Jiang', 'Yingqian Min', 'Xin Zhao', 'Zhicheng Dou', 'Jiaxin Mao', 'Yankai Lin', 'Ruihua Song', 'Jun Xu', 'Xu Chen', 'Rui Yan', 'Zhewei Wei', 'Di Hu', 'Wenbing Huang', 'Ji-Rong Wen']",,Effective Efficient Continual Pre training Large Language Models
307,Efficient Universal Goal Hijacking with Semantics-guided Prompt Organization,"['Yihao Huang', 'Chong Wang', 'Xiaojun Jia', 'Qing Guo', 'Felix Juefei-Xu', 'Jian Zhang', 'Yang Liu', 'Geguang Pu']","Universal goal hijacking is a kind of prompt injection attack that forces LLMs to return a target malicious response for arbitrary normal user prompts. The previous methods achieve high attack performance while being too cumbersome and time-consuming. Also, they have concentrated solely on optimization algorithms, overlooking the crucial role of the prompt. To this end, we propose a method called POUGH that incorporates an efficient optimization algorithm and two semantics-guided prompt organization strategies. Specifically, our method starts with a sampling strategy to select representative prompts from a candidate pool, followed by a ranking strategy that prioritizes them. Given the sequentially ranked prompts, our method employs an iterative optimization algorithm to generate a fixed suffix that can concatenate to arbitrary user prompts for universal goal hijacking. Experiments conducted on four popular LLMs and ten types of target responses verified the effectiveness.",Efficient Universal Goal Hijacking Semantics guided Prompt Organization Universal goal hijacking kind prompt injection attack forces LLMs return target malicious response arbitrary normal user prompts previous methods achieve high attack performance cumbersome time consuming concentrated solely optimization algorithms overlooking crucial role prompt end propose method called POUGH incorporates efficient optimization algorithm semantics guided prompt organization strategies Specifically method starts sampling strategy select representative prompts candidate pool followed ranking strategy prioritizes Given sequentially ranked prompts method employs iterative optimization algorithm generate fixed suffix concatenate arbitrary user prompts universal goal hijacking Experiments conducted popular LLMs types target responses verified effectiveness
308,mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding,"['Anwen Hu', 'Haiyang Xu', 'Liang Zhang', 'Jiabo Ye', 'Ming Yan', 'Ji Zhang', 'Qin Jin', 'Fei Huang', 'Jingren Zhou']",,mPLUG DocOwl2 High resolution Compressing OCR free Multi page Document Understanding
309,What Makes a Good Natural Language Prompt?,"['Do Xuan Long', 'Duy Dinh', 'Ngoc-Hai Nguyen', 'Kenji Kawaguchi', 'Nancy F. Chen', 'Shafiq Joty', 'Min-Yen Kan']","As large language models (LLMs) have progressed towards more human-like and human--AI communications have become prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying more than 150 prompting-related papers from leading NLP and AI conferences from 2022 to 2025 and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. We then empirically explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Finally, we discover that instruction-tuning on property-enhanced prompts can result in better reasoning models. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human--AI communication and opening new prompting research directions.",Makes Good Natural Language Prompt large language models LLMs progressed human like human AI communications prevalent prompting emerged decisive component limited conceptual consensus exactly quantifies natural language prompts attempt address question conducting meta analysis surveying 150 prompting related papers leading NLP AI conferences 2022 2025 blogs propose property human centric framework evaluating prompt quality encompassing 21 properties categorized dimensions examine existing studies assess impact LLMs revealing imbalanced support models tasks substantial research gaps analyze correlations properties high quality natural language prompts deriving prompting recommendations empirically explore multi property prompt enhancements reasoning tasks observing single property enhancements greatest impact Finally discover instruction tuning property enhanced prompts result better reasoning models findings establish foundation property centric prompt evaluation optimization bridging gaps human AI communication opening new prompting research directions
310,"Limited-Resource Adapters Are Regularizers, Not Linguists","['Marcell Fekete', 'Nathaniel Romney Robinson', 'Ernests Lavrinovics', 'Djeride Jean-Baptise', 'Raj Dabre', 'Johannes Bjerva', 'Heather Lent']","Cross-lingual transfer from related high-resource languages is a well-established strategy to enhance low-resource language technologies. Prior work has shown that adapters show promise for, e.g., improving low-resource machine translation (MT). In this work, we investigate an adapter souping method combined with cross-attention fine-tuning of a pre-trained MT model to leverage language transfer for three low-resource Creole languages, which exhibit relatedness to different language groups across distinct linguistic dimensions. Our approach improves performance substantially over baselines. However, we find that linguistic relatedness -- or even a lack thereof -- does not covary meaningfully with adapter performance. Surprisingly, our cross-attention fine-tuning approach appears equally effective with randomly initialized adapters, implying that the benefit of adapters in this setting lies in parameter regularization, and not in meaningful information transfer. We provide analysis supporting this regularization hypothesis. Our findings underscore the reality that neural language processing involves many success factors, and that not all neural methods leverage linguistic knowledge in intuitive ways.",Limited Resource Adapters Regularizers Linguists Cross lingual transfer related high resource languages established strategy enhance low resource language technologies Prior work shown adapters promise e g improving low resource machine translation MT work investigate adapter souping method combined cross attention fine tuning pre trained MT model leverage language transfer low resource Creole languages exhibit relatedness different language groups distinct linguistic dimensions approach improves performance substantially baselines linguistic relatedness lack thereof does covary meaningfully adapter performance Surprisingly cross attention fine tuning approach appears equally effective randomly initialized adapters implying benefit adapters setting lies parameter regularization meaningful information transfer provide analysis supporting regularization hypothesis findings underscore reality neural language processing involves success factors neural methods leverage linguistic knowledge intuitive ways
311,X-TURING: Towards an Enhanced and Efficient Turing Test for Long-Term Dialogue Agents,"['Weiqi Wu', 'Hongqiu Wu', 'hai zhao']","The Turing test examines whether AIs exhibit human-like behaviour in natural language conversations. The traditional setting limits each participant to one message at a time and requires constant human participation. This fails to reflect a natural conversational style and hinders the evaluation of dialogue agents based on Large Language Models (LLMs) in complex and prolonged interactions. This paper proposes \textbf{\textsc{X-Turing}}, which enhances the original test with a \textit{burst dialogue} pattern, allowing more dynamic exchanges using consecutive messages. It further reduces human workload by iteratively generating dialogues that simulate the long-term interaction between the agent and a human to compose the majority of the test process. With the \textit{pseudo-dialogue} history, the agent then engages in a shorter dialogue with a real human, which is paired with a human-human conversation on the same topic to be judged using questionnaires. We introduce the \textit{X-Turn Pass-Rate} metric to assess the human likeness of LLMs across varying durations. While LLMs like GPT-4 initially perform well, achieving pass rates of 51.9\% and 38.9\% during 3 turns and 10 turns of dialogues respectively, their performance drops as the dialogue progresses, which underscores the difficulty in maintaining consistency in the long term.",X TURING Enhanced Efficient Turing Test Long Term Dialogue Agents Turing test examines AIs exhibit human like behaviour natural language conversations traditional setting limits participant message time requires constant human participation fails reflect natural conversational style hinders evaluation dialogue agents based Large Language Models LLMs complex prolonged interactions paper proposes textbf textsc X Turing enhances original test textit burst dialogue pattern allowing dynamic exchanges using consecutive messages reduces human workload iteratively generating dialogues simulate long term interaction agent human compose majority test process textit pseudo dialogue history agent engages shorter dialogue real human paired human human conversation topic judged using questionnaires introduce textit X Turn Pass Rate metric assess human likeness LLMs varying durations LLMs like GPT 4 initially perform achieving pass rates 51 9 38 9 3 turns 10 turns dialogues respectively performance drops dialogue progresses underscores difficulty maintaining consistency long term
312,Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with UniMoral,"['Shivani Kumar', 'David Jurgens']","Moral reasoning is a complex cognitive process shaped by individual experiences and cultural contexts and presents unique challenges for computational analysis. While natural language processing (NLP) offers promising tools for studying this phenomenon, current research lacks cohesion, employing discordant datasets and tasks that examine isolated aspects of moral reasoning. We bridge this gap with UniMoral, a unified dataset integrating psychologically grounded and social-media-derived moral dilemmas annotated with labels for action choices, ethical principles, contributing factors, and consequences, alongside annotators' moral and cultural profiles. Recognizing the cultural relativity of moral reasoning, UniMoral spans six languages, Arabic, Chinese, English, Hindi, Russian, and Spanish, capturing diverse socio-cultural contexts. We demonstrate UniMoral's utility through a benchmark evaluations of three large language models (LLMs) across four tasks: action prediction, moral typology classification, factor attribution analysis, and consequence generation. Key findings reveal that while implicitly embedded moral contexts enhance the moral reasoning capability of LLMs, there remains a critical need for increasingly specialized approaches to further advance moral reasoning in these models.",Rules Meant Broken Understanding Multilingual Moral Reasoning Computational Pipeline UniMoral Moral reasoning complex cognitive process shaped individual experiences cultural contexts presents unique challenges computational analysis natural language processing NLP offers promising tools studying phenomenon current research lacks cohesion employing discordant datasets tasks examine isolated aspects moral reasoning bridge gap UniMoral unified dataset integrating psychologically grounded social media derived moral dilemmas annotated labels action choices ethical principles contributing factors consequences alongside annotators moral cultural profiles Recognizing cultural relativity moral reasoning UniMoral spans languages Arabic Chinese English Hindi Russian Spanish capturing diverse socio cultural contexts demonstrate UniMoral s utility benchmark evaluations large language models LLMs tasks action prediction moral typology classification factor attribution analysis consequence generation Key findings reveal implicitly embedded moral contexts enhance moral reasoning capability LLMs remains critical need increasingly specialized approaches advance moral reasoning models
313,Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models,"['Zheyuan Liu', 'Guangyao Dou', 'Xiangchi Yuan', 'Chunhui Zhang', 'Zhaoxuan Tan', 'Meng Jiang']","Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.",Modality Aware Neuron Pruning Unlearning Multimodal Large Language Models Generative models Large Language Models LLMs Multimodal Large Language Models MLLMs trained massive datasets lead memorize inadvertently reveal sensitive information raising ethical privacy concerns prior works explored issue context LLMs presents unique challenge MLLMs entangled nature knowledge modalities making comprehensive unlearning difficult address challenge propose Modality Aware Neuron Unlearning MANU novel unlearning framework MLLMs designed selectively clip neurons based relative importance targeted forget data curated different modalities Specifically MANU consists stages important neuron selection selective pruning stage identifies collects influential neurons modalities relative targeted forget knowledge second stage dedicated pruning selected neurons MANU effectively isolates removes neurons contribute forget data modality preserving integrity retained knowledge experiments conducted various MLLM architectures illustrate MANU achieve balanced comprehensive unlearning modality largely affecting overall model utility
314,NGQA: A Nutritional Graph Question Answering Benchmark for Personalized Health-aware Nutritional Reasoning,"['Zheyuan Zhang', 'Yiyang Li', 'Nhi Ha Lan Le', 'Zehong Wang', 'Tianyi Ma', 'Vincent Galassi', 'Keerthiram Murugesan', 'Nuno Moniz', 'Werner Geyer', 'Nitesh V Chawla', 'Chuxu Zhang', 'Yanfang Ye']","Diet plays a critical role in human health, yet tailoring dietary reasoning to individual health conditions remains a major challenge. Nutrition Question Answering (QA) has emerged as a popular method for addressing this problem. However, current research faces two critical limitations. On one hand, the absence of datasets involving user-specific medical information severely limits \textit{personalization}. This challenge is further compounded by the wide variability in individual health needs. On the other hand, while large language models (LLMs), a popular solution for this task, demonstrate strong reasoning abilities, they struggle with the domain-specific complexities of personalized healthy dietary reasoning, and existing benchmarks fail to capture these challenges. To address these gaps, we introduce the Nutritional Graph Question Answering (NGQA) benchmark, the first graph question answering dataset designed for personalized nutritional health reasoning. NGQA leverages data from the National Health and Nutrition Examination Survey (NHANES) and the Food and Nutrient Database for Dietary Studies (FNDDS) to evaluate whether a food is healthy for a specific user, supported by explanations of the key contributing nutrients. The benchmark incorporates three question complexity settings and evaluates reasoning across three downstream tasks. Extensive experiments with LLM backbones and baseline models demonstrate that the NGQA benchmark effectively challenges existing models. In sum, NGQA addresses a critical real-world problem while advancing GraphQA research with a novel domain-specific benchmark.",NGQA Nutritional Graph Question Answering Benchmark Personalized Health aware Nutritional Reasoning Diet plays critical role human health tailoring dietary reasoning individual health conditions remains major challenge Nutrition Question Answering QA emerged popular method addressing problem current research faces critical limitations hand absence datasets involving user specific medical information severely limits textit personalization challenge compounded wide variability individual health needs hand large language models LLMs popular solution task demonstrate strong reasoning abilities struggle domain specific complexities personalized healthy dietary reasoning existing benchmarks fail capture challenges address gaps introduce Nutritional Graph Question Answering NGQA benchmark graph question answering dataset designed personalized nutritional health reasoning NGQA leverages data National Health Nutrition Examination Survey NHANES Food Nutrient Database Dietary Studies FNDDS evaluate food healthy specific user supported explanations key contributing nutrients benchmark incorporates question complexity settings evaluates reasoning downstream tasks Extensive experiments LLM backbones baseline models demonstrate NGQA benchmark effectively challenges existing models sum NGQA addresses critical real world problem advancing GraphQA research novel domain specific benchmark
315,ReLearn: Unlearning via Learning for Large Language Models,"['Haoming Xu', 'Ningyuan Zhao', 'Liming Yang', 'Sendong Zhao', 'Shumin Deng', 'Mengru Wang', 'Bryan Hooi', 'Nay Oo', 'Huajun Chen', 'Ningyu Zhang']",,ReLearn Unlearning Learning Large Language Models
316,Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling,"['Pritom Saha Akash', 'Kevin Chen-Chuan Chang']","Topic modeling plays a vital role in uncovering hidden semantic structures within text corpora, but existing models struggle in low-resource settings where limited target-domain data leads to unstable and incoherent topic inference. We address this challenge by formally introducing domain adaptation for low-resource topic modeling, where a high-resource source domain informs a low-resource target domain without overwhelming it with irrelevant content. We establish a finite-sample generalization bound showing that effective knowledge transfer depends on robust performance in both domains, minimizing latent-space discrepancy, and preventing overfitting to the data. Guided by these insights, we propose DALTA (Domain-Aligned Latent Topic Adaptation), a new framework that employs a shared encoder for domain-invariant features, specialized decoders for domain-specific nuances, and adversarial alignment to selectively transfer relevant information. Experiments on diverse low-resource datasets demonstrate that DALTA consistently outperforms state-of-the-art methods in terms of topic coherence, stability, and transferability.",Understanding Cross Domain Adaptation Low Resource Topic Modeling Topic modeling plays vital role uncovering hidden semantic structures text corpora existing models struggle low resource settings limited target domain data leads unstable incoherent topic inference address challenge formally introducing domain adaptation low resource topic modeling high resource source domain informs low resource target domain overwhelming irrelevant content establish finite sample generalization bound showing effective knowledge transfer depends robust performance domains minimizing latent space discrepancy preventing overfitting data Guided insights propose DALTA Domain Aligned Latent Topic Adaptation new framework employs shared encoder domain invariant features specialized decoders domain specific nuances adversarial alignment selectively transfer relevant information Experiments diverse low resource datasets demonstrate DALTA consistently outperforms state art methods terms topic coherence stability transferability
317,UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models,"['Boyang XUE', 'Fei Mi', 'Qi Zhu', 'Hongru WANG', 'Rui Wang', 'Sheng Wang', 'Erxin Yu', 'Xuming Hu', 'Kam-Fai Wong']","Despite demonstrating impressive capabilities, Large Language Models (LLMs) still often struggle to accurately express the factual knowledge they possess, especially in cases where the LLMs' knowledge boundaries are ambiguous. To improve LLMs' factual expressions, we propose the UAlign framework, which leverages Uncertainty estimations to represent knowledge boundaries, and then explicitly incorporates these representations as input features into prompts for LLMs to Align with factual knowledge. First, we prepare the dataset on knowledge question-answering (QA) samples by calculating two uncertainty estimations, including confidence score and semantic entropy, to represent the knowledge boundaries for LLMs. Subsequently, using the prepared dataset, we train a reward model that incorporates uncertainty estimations and then employ the Proximal Policy Optimization (PPO) algorithm for factuality alignment on LLMs. Experimental results indicate that, by integrating uncertainty representations in LLM alignment, the proposed UAlign can significantly enhance the LLMs' capacities to confidently answer known questions and refuse unknown questions on both in-domain and out-of-domain tasks, showing reliability improvements and good generalizability over various prompt- and training-based baselines.",UAlign Leveraging Uncertainty Estimations Factuality Alignment Large Language Models Despite demonstrating impressive capabilities Large Language Models LLMs struggle accurately express factual knowledge possess especially cases LLMs knowledge boundaries ambiguous improve LLMs factual expressions propose UAlign framework leverages Uncertainty estimations represent knowledge boundaries explicitly incorporates representations input features prompts LLMs Align factual knowledge prepare dataset knowledge question answering QA samples calculating uncertainty estimations including confidence score semantic entropy represent knowledge boundaries LLMs Subsequently using prepared dataset train reward model incorporates uncertainty estimations employ Proximal Policy Optimization PPO algorithm factuality alignment LLMs Experimental results indicate integrating uncertainty representations LLM alignment proposed UAlign significantly enhance LLMs capacities confidently answer known questions refuse unknown questions domain domain tasks showing reliability improvements good generalizability various prompt training based baselines
318,CoT-Valve: Length-Compressible Chain-of-Thought Tuning,"['Xinyin Ma', 'Guangnian Wan', 'Runpeng Yu', 'Gongfan Fang', 'Xinchao Wang']",,CoT Valve Length Compressible Chain Thought Tuning
319,HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation,"['Jie Ouyang', 'Tingyue Pan', 'Mingyue Cheng', 'Ruiran Yan', 'Yucong Luo', 'Jiaying Lin', 'Qi Liu']","While Retrieval-Augmented Generation (RAG) has emerged as an effective approach for addressing the knowledge outdating problem in Large Language Models (LLMs), it still faces a critical challenge: the prevalence of outdated information in knowledge bases. Current research primarily focuses on incorporating up-to-date information, yet the impact of outdated information coexisting in retrieval sources remains inadequately addressed. To bridge this gap, we introduce HoH, the first benchmark specifically designed to evaluate the impact of outdated information on RAG. Our benchmark leverages token-level diff algorithms combined with LLM pipelines to efficiently create a large-scale QA dataset that accurately captures the evolution of temporal knowledge in real-world facts. Through comprehensive experiments, we reveal that outdated information significantly degrades RAG performance in two critical ways: (1) it substantially reduces response accuracy by distracting models from correct information, and (2) it can mislead models into generating potentially harmful outputs, even when current information is available. Current RAG approaches struggle with both retrieval and generation aspects when handling outdated information. These findings highlight the urgent need for innovative solutions to address the temporal challenges in RAG. Our code and data are available at: https://github.com/0russwest0/HoH.",HoH Dynamic Benchmark Evaluating Impact Outdated Information Retrieval Augmented Generation Retrieval Augmented Generation RAG emerged effective approach addressing knowledge outdating problem Large Language Models LLMs faces critical challenge prevalence outdated information knowledge bases Current research primarily focuses incorporating date information impact outdated information coexisting retrieval sources remains inadequately addressed bridge gap introduce HoH benchmark specifically designed evaluate impact outdated information RAG benchmark leverages token level diff algorithms combined LLM pipelines efficiently create large scale QA dataset accurately captures evolution temporal knowledge real world facts comprehensive experiments reveal outdated information significantly degrades RAG performance critical ways 1 substantially reduces response accuracy distracting models correct information 2 mislead models generating potentially harmful outputs current information available Current RAG approaches struggle retrieval generation aspects handling outdated information findings highlight urgent need innovative solutions address temporal challenges RAG code data available https github com 0russwest0 HoH
320,Uncertainty Propagation on LLM Agent,"['Qiwei Zhao', 'Dong Li', 'Yanchi Liu', 'Wei Cheng', 'Yiyou Sun', 'Mika Oishi', 'Takao Osaki', 'Katsushi Matsuda', 'Huaxiu Yao', 'Chen Zhao', 'Haifeng Chen', 'Xujiang Zhao']",,Uncertainty Propagation LLM Agent
321,Beyond Position: the emergence of wavelet-like properties in Transformers,"['Valeria Ruscio', 'Umberto Nanni', 'Fabrizio Silvestri']","This paper studies how Transformer models with Rotary Position Embeddings (RoPE) develop emergent, wavelet-like properties that compensate for the positional encoding's theoretical limitations. Through an analysis spanning model scales, architectures, and training checkpoints, we show that attention heads evolve to implement multi-resolution processing analogous to wavelet transforms. We demonstrate that this scale-invariant behavior is unique to RoPE, emerges through distinct evolutionary phases during training, and statistically adheres to the fundamental uncertainty principle. Our findings suggest that the effectiveness of modern Transformers stems from their remarkable ability to spontaneously develop optimal, multi-resolution decompositions to address inherent architectural constraints.",Position emergence wavelet like properties Transformers paper studies Transformer models Rotary Position Embeddings RoPE develop emergent wavelet like properties compensate positional encoding s theoretical limitations analysis spanning model scales architectures training checkpoints attention heads evolve implement multi resolution processing analogous wavelet transforms demonstrate scale invariant behavior unique RoPE emerges distinct evolutionary phases training statistically adheres fundamental uncertainty principle findings suggest effectiveness modern Transformers stems remarkable ability spontaneously develop optimal multi resolution decompositions address inherent architectural constraints
322,Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs,"['Giovanni Servedio', 'Alessandro De Bellis', 'Dario Di Palma', 'Vito Walter Anelli', 'Tommaso Di Noia']","Factual hallucinations are a major challenge for Large Language Models (LLMs). They undermine reliability and user trust by generating inaccurate or fabricated content. Recent studies suggest that when generating false statements, the internal states of LLMs encode information about truthfulness. However, these studies often rely on synthetic datasets that lack realism, which limits generalization when evaluating the factual accuracy of text generated by the model itself. In this paper, we challenge the findings of previous work by investigating truthfulness encoding capabilities, leading to the generation of a more realistic and challenging dataset. Specifically, we extend previous work by introducing: (1) a strategy for sampling plausible true-false factoid sentences from tabular data and (2) a procedure for generating realistic, LLM-dependent true-false datasets from Question Answering collections. Our analysis of two open-source LLMs reveals that while the findings from previous studies are partially validated, generalization to LLM-generated datasets remains challenging. This study lays the groundwork for future research on factuality in LLMs and offers practical guidelines for more effective evaluation.",Hidden States Hiding Testing Limits Factuality Encoding Capabilities LLMs Factual hallucinations major challenge Large Language Models LLMs undermine reliability user trust generating inaccurate fabricated content Recent studies suggest generating false statements internal states LLMs encode information truthfulness studies rely synthetic datasets lack realism limits generalization evaluating factual accuracy text generated model paper challenge findings previous work investigating truthfulness encoding capabilities leading generation realistic challenging dataset Specifically extend previous work introducing 1 strategy sampling plausible true false factoid sentences tabular data 2 procedure generating realistic LLM dependent true false datasets Question Answering collections analysis open source LLMs reveals findings previous studies partially validated generalization LLM generated datasets remains challenging study lays groundwork future research factuality LLMs offers practical guidelines effective evaluation
323,Disentangling Biased Knowledge from Reasoning in Large Language Models via Machine Unlearning,"['Zheyuan Liu', 'Suraj Maharjan', 'Fanyou Wu', 'Rahil Parikh', 'Belhassen Bayar', 'Srinivasan H. Sengamedu', 'Meng Jiang']",,Disentangling Biased Knowledge Reasoning Large Language Models Machine Unlearning
324,LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing,"['Dario Di Palma', 'Alessandro De Bellis', 'Giovanni Servedio', 'Vito Walter Anelli', 'Fedelucio Narducci', 'Tommaso Di Noia']","Large Language Models (LLMs) have rapidly become central to NLP, demonstrating their ability to adapt to various tasks through prompting techniques, including sentiment analysis. However, we still have a limited understanding of how these models capture sentiment-related information. This study probes the hidden layers of Llama models to pinpoint where sentiment features are most represented and to assess how this affects sentiment analysis. Using probe classifiers, we analyze sentiment encoding across layers and scales, identifying the layers and pooling methods that best capture sentiment signals. Our results show that sentiment information is most concentrated in mid-layers for binary polarity tasks, with detection accuracy increasing up to 14% over prompting techniques. Additionally, we find that in decoder-only models, the last token is not consistently the most informative for sentiment encoding. Finally, this approach enables sentiment tasks to be performed with memory requirements reduced by an average of 57%. These insights contribute to a broader understanding of sentiment in LLMs, suggesting layer-specific probing as an effective approach for sentiment tasks beyond prompting, with potential to enhance model utility and reduce memory requirements.",LLaMAs Feelings Unveiling Sentiment Emotion Representations LLaMA Models Probing Large Language Models LLMs rapidly central NLP demonstrating ability adapt various tasks prompting techniques including sentiment analysis limited understanding models capture sentiment related information study probes hidden layers Llama models pinpoint sentiment features represented assess affects sentiment analysis Using probe classifiers analyze sentiment encoding layers scales identifying layers pooling methods best capture sentiment signals results sentiment information concentrated mid layers binary polarity tasks detection accuracy increasing 14 prompting techniques Additionally decoder models token consistently informative sentiment encoding Finally approach enables sentiment tasks performed memory requirements reduced average 57 insights contribute broader understanding sentiment LLMs suggesting layer specific probing effective approach sentiment tasks prompting potential enhance model utility reduce memory requirements
325,CxGGEC: Construction-Guided Grammatical Error Correction,"['Yayu Cao', 'Tianxiang Wang', 'Lvxiaowei Xu', 'Zhenyao Wang', 'Ming Cai']",,CxGGEC Construction Guided Grammatical Error Correction
326,Beyond Sequences: Two-dimensional Representation and Dependency Encoding for Code Generation,"['Xiangyu Zhang', 'Yu Zhou', 'Guang Yang', 'Wei Cheng', 'Taolue Chen']",,Sequences dimensional Representation Dependency Encoding Code Generation
327,HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs,"['Qing Li', 'Jiahui Geng', 'Zongxiong Chen', 'Derui Zhu', 'Yuxia Wang', 'Congbo Ma', 'Chenyang Lyu', 'Fakhri Karray']","In recent years, large language models (LLMs) have made remarkable advancements, yet hallucination, where models produce inaccurate or non-factual statements, remains a significant challenge for real-world deployment. Although current classification-based methods, such as SAPLMA, are highly efficient in mitigating hallucinations, they struggle when non-factual information arises in the early or mid-sequence of outputs, reducing their reliability. To address these issues, we propose Hallucination Detection-Neural Differential Equations (HD-NDEs), a novel method that systematically assesses the truthfulness of statements by capturing the full dynamics of LLMs within their latent space. Our approaches apply neural differential equations (Neural DEs) to model the dynamic system in the latent space of LLMs. Then, the sequence in the latent space is mapped to the classification space for truth assessment. The extensive experiments across five datasets and six widely used LLMs demonstrate the effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art techniques.",HD NDEs Neural Differential Equations Hallucination Detection LLMs recent years large language models LLMs remarkable advancements hallucination models produce inaccurate non factual statements remains significant challenge real world deployment current classification based methods SAPLMA highly efficient mitigating hallucinations struggle non factual information arises early mid sequence outputs reducing reliability address issues propose Hallucination Detection Neural Differential Equations HD NDEs novel method systematically assesses truthfulness statements capturing dynamics LLMs latent space approaches apply neural differential equations Neural DEs model dynamic latent space LLMs sequence latent space mapped classification space truth assessment extensive experiments datasets widely used LLMs demonstrate effectiveness HD NDEs especially achieving 14 improvement AUC ROC True False dataset compared state art techniques
328,What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations,"['Dongqi Liu', 'Chenxi Whitehouse', 'Xi Yu', 'Louis Mahon', 'Rohit Saxena', 'Zheng Zhao', 'Yifu QIU', 'Mirella Lapata', 'Vera Demberg']","Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of our dataset. This study aims to pave the way for future research on scientific video-to-text summarization.",Talk Video Text Summarization Dataset Scientific Presentations Transforming recorded videos concise accurate textual summaries growing challenge multimodal learning paper introduces VISTA dataset specifically designed video text summarization scientific domains VISTA contains 18 599 recorded AI conference presentations paired corresponding paper abstracts benchmark performance state art large models apply plan based framework better capture structured nature abstracts human automated evaluations confirm explicit planning enhances summary quality factual consistency considerable gap remains models human performance highlighting challenges dataset study aims pave way future research scientific video text summarization
329,NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering,"['Ruisheng Cao', 'Hanchong Zhang', 'Tiancheng Huang', 'Zhangyi Kang', 'Yuxin Zhang', 'Liangtai Sun', 'Hanqi Li', 'Yuxun Miao', 'Shuai Fan', 'Lu Chen', 'Kai Yu']","The increasing number of academic papers poses significant challenges for researchers to efficiently acquire key details. While retrieval augmented generation (RAG) shows great promise in large language model (LLM) based automated question answering, previous works often isolate neural and symbolic retrieval despite their complementary strengths. Moreover, conventional single-view chunking neglects the rich structure and layout of PDFs, e.g., sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural symbolic retrieval framework which combines both paradigms in an interactive process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG organizes semi-structured PDF content into both the relational database and vectorstore, enabling LLM agents to iteratively gather context until sufficient to generate answers. Experiments on three full PDF-based QA datasets, including a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the vector-based RAG and various structured baselines, highlighting its capacity to unify both retrieval schemes and utilize multiple views. Code and data are publicly available at https://github.com/X-LANCE/NeuSym-RAG.",NeuSym RAG Hybrid Neural Symbolic Retrieval Multiview Structuring PDF Question Answering increasing number academic papers poses significant challenges researchers efficiently acquire key details retrieval augmented generation RAG shows great promise large language model LLM based automated question answering previous works isolate neural symbolic retrieval despite complementary strengths conventional single view chunking neglects rich structure layout PDFs e g sections tables work propose NeuSym RAG hybrid neural symbolic retrieval framework combines paradigms interactive process leveraging multi view chunking schema based parsing NeuSym RAG organizes semi structured PDF content relational database vectorstore enabling LLM agents iteratively gather context sufficient generate answers Experiments PDF based QA datasets including self annotated AIRQA REAL NeuSym RAG stably defeats vector based RAG various structured baselines highlighting capacity unify retrieval schemes utilize multiple views Code data publicly available https github com X LANCE NeuSym RAG
330,ProvBench: A Benchmark of Legal Provision Recommendation for Contract Auto-Reviewing,"['Xiuxuan Shen', 'Zhongyuan Jiang', 'Junsan Zhang', 'Junxiao Han', 'Yao Wan', 'Chengjie Guo', 'Bingcheng Liu', 'Jie Wu', 'Renxiang Li', 'Philip S. Yu']",,ProvBench Benchmark Legal Provision Recommendation Contract Auto Reviewing
331,F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching,"['Yushen CHEN', 'Zhikang Niu', 'Ziyang Ma', 'Keqi Deng', 'Chunhui Wang', 'JianZhao', 'Kai Yu', 'Xie Chen']","This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model's performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our F5-TTS exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. We have released all codes and checkpoints to promote community development, at https://SWivid.github.io/F5-TTS/.",F5 TTS Fairytaler Fakes Fluent Faithful Speech Flow Matching paper introduces F5 TTS fully non autoregressive text speech based flow matching Diffusion Transformer DiT requiring complex designs duration model text encoder phoneme alignment text input simply padded filler tokens length input speech denoising performed speech generation originally proved feasible E2 TTS original design E2 TTS makes hard follow slow convergence low robustness address issues model input ConvNeXt refine text representation making easy align speech propose inference time Sway Sampling strategy significantly improves model s performance efficiency sampling strategy flow step easily applied existing flow matching based models retraining design allows faster training achieves inference RTF 0 15 greatly improved compared state art diffusion based TTS models Trained public 100K hours multilingual dataset F5 TTS exhibits highly natural expressive zero shot ability seamless code switching capability speed control efficiency released codes checkpoints promote community development https SWivid github io F5 TTS
332,LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks,"['Anna Bavaresco', 'Raffaella Bernardi', 'Leonardo Bertolazzi', 'Desmond Elliott', 'Raquel Fernández', 'Albert Gatt', 'Esam Ghaleb', 'Mario Giulianelli', 'Michael Hanna', 'Alexander Koller', 'Andre Martins', 'Philipp Mondorf', 'Vera Neplenbroek', 'Sandro Pezzelle', 'Barbara Plank', 'David Schlangen', 'Alessandro Suglia', 'Aditya K Surikuchi', 'Ece Takmaz', 'Alberto Testoni']","There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show substantial variance across models and datasets. Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated. We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.",LLMs instead Human Judges Large Scale Empirical Study 20 NLP Evaluation Tasks increasing trend evaluating NLP models LLMs instead human judgments raising questions validity evaluations reproducibility case proprietary models provide JUDGE BENCH extensible collection 20 NLP datasets human annotations covering broad range evaluated properties types data comprehensively evaluate 11 current LLMs covering open weight proprietary models ability replicate annotations evaluations substantial variance models datasets Models reliable evaluators tasks overall display substantial variability depending property evaluated expertise level human judges language human model generated conclude LLMs carefully validated human judgments used evaluators
333,AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation,"['Xiechi Zhang', 'Zetian Ouyang', 'Linlin Wang', 'Gerard de Melo', 'Zhu Cao', 'Xiaoling Wang', 'Ya Zhang', 'Yanfeng Wang', 'Liang He']","With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to measure quality, significantly overlook the importance of medical terminology. While human evaluation tends to be more reliable, it can be very costly and may as well suffer from inaccuracies due to limits in human expertise and motivation. Although there are some evaluation methods based on LLMs, their usability in the medical field is limited due to their proprietary nature or lack of expertise. To tackle these challenges, we present AutoMedEval, an open-sourced automatic evaluation model with 13B parameters specifically engineered to measure the question-answering proficiency of medical LLMs. The overarching objective of AutoMedEval is to assess the quality of responses produced by diverse models, aspiring to significantly reduce the dependence on human evaluation. Specifically, we propose a hierarchical training method involving curriculum instruction tuning and an iterative knowledge introspection mechanism, enabling AutoMedEval to acquire professional medical assessment capabilities with limited instructional data. Human evaluations indicate that AutoMedEval surpasses other baselines in terms of correlation with human judgments.",AutoMedEval Harnessing Language Models Automatic Medical Capability Evaluation proliferation large language models LLMs medical domain increasing demand improved evaluation techniques assess capabilities traditional metrics like F1 ROUGE rely token overlaps measure quality significantly overlook importance medical terminology human evaluation tends reliable costly suffer inaccuracies limits human expertise motivation evaluation methods based LLMs usability medical field limited proprietary nature lack expertise tackle challenges present AutoMedEval open sourced automatic evaluation model 13B parameters specifically engineered measure question answering proficiency medical LLMs overarching objective AutoMedEval assess quality responses produced diverse models aspiring significantly reduce dependence human evaluation Specifically propose hierarchical training method involving curriculum instruction tuning iterative knowledge introspection mechanism enabling AutoMedEval acquire professional medical assessment capabilities limited instructional data Human evaluations indicate AutoMedEval surpasses baselines terms correlation human judgments
334,CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis,"['Bohan Zhang', 'Xiaokang Zhang', 'Jing Zhang', 'Jifan Yu', 'Sijia Luo', 'Jie Tang']","Current inference scaling methods, such as Self-consistency and Best-of-N, have proven effective in improving the accuracy of LLMs on complex reasoning tasks. However, these methods rely heavily on the quality of candidate responses and are unable to produce correct answers when all candidates are incorrect. In this paper, we propose a novel inference scaling strategy, CoT-based Synthesizer, which leverages CoT reasoning to synthesize superior answers by analyzing complementary information from multiple candidate responses, even when all candidate responses are flawed. To enable a lightweight and cost-effective implementation, we introduce an automated data generation pipeline that creates diverse training data. This allows smaller LLMs trained on this data to improve the inference accuracy of larger models, including API-based LLMs. Experimental results across four benchmark datasets with seven policy models demonstrate that our method significantly enhances performance, with gains of 11.8% for Llama3-8B and 10.3% for GPT-4o on the MATH dataset. The corresponding training data and code are publicly available on https://github.com/RUCKBReasoning/CoT-based-Synthesizer.",CoT based Synthesizer Enhancing LLM Performance Answer Synthesis Current inference scaling methods Self consistency Best N proven effective improving accuracy LLMs complex reasoning tasks methods rely heavily quality candidate responses unable produce correct answers candidates incorrect paper propose novel inference scaling strategy CoT based Synthesizer leverages CoT reasoning synthesize superior answers analyzing complementary information multiple candidate responses candidate responses flawed enable lightweight cost effective implementation introduce automated data generation pipeline creates diverse training data allows smaller LLMs trained data improve inference accuracy larger models including API based LLMs Experimental results benchmark datasets seven policy models demonstrate method significantly enhances performance gains 11 8 Llama3 8B 10 3 GPT 4o MATH dataset corresponding training data code publicly available https github com RUCKBReasoning CoT based Synthesizer
335,Efficiently Identifying Watermarked Segments in Mixed-Source Texts,"['Xuandong Zhao', 'Chenwen Liao', 'Yu-Xiang Wang', 'Lei Li']","Text watermarks in large language models (LLMs) are increasingly used to detect synthetic text, mitigating misuse cases like fake news and academic dishonesty. While existing watermarking detection techniques primarily focus on classifying entire documents as watermarked or not, they often neglect the common scenario of identifying individual watermark segments within longer, mixed-source documents. Drawing inspiration from plagiarism detection systems, we propose two novel methods for partial watermark detection. First, we develop a geometry cover detection framework aimed at determining whether there is a watermark segment in long text. Second, we introduce an adaptive online learning algorithm to pinpoint the precise location of watermark segments within the text. Evaluated on three popular watermarking techniques (KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves high accuracy, significantly outperforming baseline methods. Moreover, our framework is adaptable to other watermarking techniques, offering new insights for precise watermark detection. Our code is publicly available at https://github.com/XuandongZhao/llm-watermark-location",Efficiently Identifying Watermarked Segments Mixed Source Texts Text watermarks large language models LLMs increasingly used detect synthetic text mitigating misuse cases like fake news academic dishonesty existing watermarking detection techniques primarily focus classifying entire documents watermarked neglect common scenario identifying individual watermark segments longer mixed source documents Drawing inspiration plagiarism detection systems propose novel methods partial watermark detection develop geometry cover detection framework aimed determining watermark segment long text Second introduce adaptive online learning algorithm pinpoint precise location watermark segments text Evaluated popular watermarking techniques KGW Watermark Unigram Watermark Gumbel Watermark approach achieves high accuracy significantly outperforming baseline methods framework adaptable watermarking techniques offering new insights precise watermark detection code publicly available https github com XuandongZhao llm watermark location
336,FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings,"['Tong Liu', 'Xiao Yu', 'Wenxuan Zhou', 'Jindong Gu', 'Volker Tresp']","Efficient preference optimization algorithms such as Direct Preference Optimization (DPO) have become a popular approach in aligning large language models (LLMs) with human preferences. These algorithms implicitly treat the LLM as a reward model, and focus on training it to correct misranked preference pairs. However, recent work~\citep{chen2024preference} empirically finds that DPO training \textit{rarely improves these misranked preference pairs}, despite its gradient emphasizing on these cases. We introduce FocalPO, a DPO variant that instead \textit{down-weighs} misranked preference pairs and prioritizes enhancing the model's understanding of pairs that it can already rank correctly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this by adding a modulating factor to dynamically scale DPO loss. Our experiment demonstrates that FocalPO surpasses DPO and its variants on popular benchmarks like Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the introduced hyperparameter fixed. Additionally, we empirically reveals how FocalPO affects training on correct and incorrect sample groups, further underscoring its effectiveness.",FocalPO Enhancing Preference Optimizing Focusing Correct Preference Rankings Efficient preference optimization algorithms Direct Preference Optimization DPO popular approach aligning large language models LLMs human preferences algorithms implicitly treat LLM reward model focus training correct misranked preference pairs recent work citep chen2024preference empirically finds DPO training textit rarely improves misranked preference pairs despite gradient emphasizing cases introduce FocalPO DPO variant instead textit weighs misranked preference pairs prioritizes enhancing model s understanding pairs rank correctly Inspired Focal Loss used vision tasks FocalPO achieves adding modulating factor dynamically scale DPO loss experiment demonstrates FocalPO surpasses DPO variants popular benchmarks like Alpaca Eval 2 0 using Mistral Base 7B Llama 3 Instruct 8B introduced hyperparameter fixed Additionally empirically reveals FocalPO affects training correct incorrect sample groups underscoring effectiveness
337,Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks,"['Fangru Lin', 'Shaoguang Mao', 'Emanuele La Malfa', 'Valentin Hofmann', 'Adrian de Wynter', 'Xun Wang', 'Si-Qing Chen', 'Michael J. Wooldridge', 'Janet B. Pierrehumbert', 'Furu Wei']","Language is not monolithic. While benchmarks, including those designed for multiple languages, are often used as proxies to evaluate the performance of Large Language Models (LLMs), they tend to overlook the nuances of within-language variation and thus fail to model the experience of speakers of non-standard dialects. Focusing on African American Vernacular English (AAVE), we present the first study aimed at objectively assessing the fairness and robustness of LLMs in handling dialects across canonical reasoning tasks, including algorithm, math, logic, and integrated reasoning. We introduce ReDial (Reasoning with Dialect Queries), a benchmark containing 1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE speakers, including experts with computer science backgrounds, to rewrite seven popular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate widely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model families. Our findings reveal that almost all of these widely used models show significant brittleness and unfairness to queries in AAVE. Our work establishes a systematic and objective framework for analyzing LLM bias in dialectal queries. Moreover, it highlights how mainstream LLMs provide unfair service to dialect speakers in reasoning tasks, laying a critical foundation for future research.",Assessing Dialect Fairness Robustness Large Language Models Reasoning Tasks Language monolithic benchmarks including designed multiple languages used proxies evaluate performance Large Language Models LLMs tend overlook nuances language variation fail model experience speakers non standard dialects Focusing African American Vernacular English AAVE present study aimed objectively assessing fairness robustness LLMs handling dialects canonical reasoning tasks including algorithm math logic integrated reasoning introduce ReDial Reasoning Dialect Queries benchmark containing 1 2K parallel query pairs Standardized English AAVE hire AAVE speakers including experts computer science backgrounds rewrite seven popular benchmarks HumanEval GSM8K ReDial evaluate widely used LLMs including GPT Claude Llama Mistral Phi model families findings reveal widely used models significant brittleness unfairness queries AAVE work establishes systematic objective framework analyzing LLM bias dialectal queries highlights mainstream LLMs provide unfair service dialect speakers reasoning tasks laying critical foundation future research
338,Towards a More Generalized Approach in Open Relation Extraction,"['Qing Wang', 'Yuepei Li', 'Qiao Qiao', 'Kang Zhou', 'Qi Li']","Open Relation Extraction (OpenRE) seeks to identify and extract novel relational facts between named entities from unlabeled data without pre-defined relation schemas. Traditional OpenRE methods typically assume that the unlabeled data consists solely of novel relations or is pre-divided into known and novel instances. However, in real-world scenarios, novel relations are arbitrarily distributed. In this paper, we propose a generalized OpenRE setting that considers unlabeled data as a mixture of both known and novel instances. To address this, we propose MixORE, a two-phase framework that integrates relation classification and clustering to jointly learn known and novel relations. Experiments on three benchmark datasets demonstrate that MixORE consistently outperforms competitive baselines in known relation classification and novel relation clustering. Our findings contribute to the advancement of generalized OpenRE research and real-world applications.",Generalized Approach Open Relation Extraction Open Relation Extraction OpenRE seeks identify extract novel relational facts named entities unlabeled data pre defined relation schemas Traditional OpenRE methods typically assume unlabeled data consists solely novel relations pre divided known novel instances real world scenarios novel relations arbitrarily distributed paper propose generalized OpenRE setting considers unlabeled data mixture known novel instances address propose MixORE phase framework integrates relation classification clustering jointly learn known novel relations Experiments benchmark datasets demonstrate MixORE consistently outperforms competitive baselines known relation classification novel relation clustering findings contribute advancement generalized OpenRE research real world applications
339,Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back Home,"['Viktor Moskvoretskii', 'Maria Marina', 'Mikhail Salnikov', 'Nikolay Ivanov', 'Sergey Pletenev', 'Daria Galimzianova', 'Nikita Krayko', 'Vasily Konovalov', 'Irina Nikishina', 'Alexander Panchenko']","Retrieval Augmented Generation (RAG) improves correctness of Question Answering (QA) and addresses hallucinations in Large Language Models (LLMs), yet greatly increase computational costs. Besides, RAG is not always needed as may introduce irrelevant information. Recent adaptive retrieval methods integrate LLMs' intrinsic knowledge with external information appealing to LLM self-knowledge, but they often neglect efficiency evaluations and comparisons with uncertainty estimation techniques. We bridge this gap by conducting a comprehensive analysis of 35 adaptive retrieval methods, including 8 recent approaches and 27 uncertainty estimation techniques, across 6 datasets using 10 metrics for QA performance, self-knowledge, and efficiency. Our findings show that uncertainty estimation techniques often outperform complex pipelines in terms of efficiency and self-knowledge, while maintaining comparable QA performance.",Adaptive Retrieval Self Knowledge Bringing Uncertainty Home Retrieval Augmented Generation RAG improves correctness Question Answering QA addresses hallucinations Large Language Models LLMs greatly increase computational costs RAG needed introduce irrelevant information Recent adaptive retrieval methods integrate LLMs intrinsic knowledge external information appealing LLM self knowledge neglect efficiency evaluations comparisons uncertainty estimation techniques bridge gap conducting comprehensive analysis 35 adaptive retrieval methods including 8 recent approaches 27 uncertainty estimation techniques 6 datasets using 10 metrics QA performance self knowledge efficiency findings uncertainty estimation techniques outperform complex pipelines terms efficiency self knowledge maintaining comparable QA performance
340,Evaluating Language Models as Synthetic Data Generators,"['Seungone Kim', 'Juyoung Suk', 'Xiang Yue', 'Vijay Viswanathan', 'Seongyun Lee', 'Yizhong Wang', 'Kiril Gashteovski', 'Carolin Lawrence', 'Sean Welleck', 'Graham Neubig']",,Evaluating Language Models Synthetic Data Generators
341,Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?,"['Yuyao Ge', 'Shenghua Liu', 'Baolong Bi', 'Yiwei Wang', 'Lingrui Mei', 'Wenjie Feng', 'Lizhe Chen', 'Xueqi Cheng']","Large language models (LLMs) have achieved significant success in reasoning tasks, including mathematical reasoning and logical deduction. Among these reasoning tasks, graph problems stand out due to their complexity and unique structural characteristics, attracting considerable attention from researchers. Previous studies have explored LLMs' graph reasoning abilities through various techniques, such as different encoding methods for graph structures and the use of carefully designed prompts. However, a critical factor has been mostly overlooked: the prompt sequential order in which graph descriptions are presented to the models. In this study, we present the first comprehensive analysis of how the order of graph descriptions impacts LLM performance. Specifically, we comprehensively evaluate four graph description orders across six graph problems using six mainstream LLMs. The results reveal that: (1) ordered graph descriptions significantly improve LLMs' comprehension of graph structures; (2) the robustness of LLMs to graph description order varies across different tasks; and (3) the impact of graph order on performance is closely related to the inherent characteristics of tasks. This study provides a critical advancement in the application of LLMs for solving graph-related problems, paving the way for future research to optimize model performance through strategic graph description ordering.",Graph Descriptive Order Affect Solving Graph Problems LLMs Large language models LLMs achieved significant success reasoning tasks including mathematical reasoning logical deduction reasoning tasks graph problems stand complexity unique structural characteristics attracting considerable attention researchers Previous studies explored LLMs graph reasoning abilities various techniques different encoding methods graph structures use carefully designed prompts critical factor overlooked prompt sequential order graph descriptions presented models study present comprehensive analysis order graph descriptions impacts LLM performance Specifically comprehensively evaluate graph description orders graph problems using mainstream LLMs results reveal 1 ordered graph descriptions significantly improve LLMs comprehension graph structures 2 robustness LLMs graph description order varies different tasks 3 impact graph order performance closely related inherent characteristics tasks study provides critical advancement application LLMs solving graph related problems paving way future research optimize model performance strategic graph description ordering
342,Learning to Rewrite: Generalized LLM-Generated Text Detection,"['Wei Hao', 'Ran Li', 'Weiliang Zhao', 'Junfeng Yang', 'Chengzhi Mao']","Large language models (LLMs) present significant risks when used to generate non-factual content and spread disinformation at scale. Detecting such LLM-generated content is crucial, yet current detectors often struggle to generalize in open-world contexts. We introduce Learning2Rewrite, a novel framework for detecting AI-generated text with exceptional generalization to unseen domains. Our method leverages the insight that LLMs inherently modify AI-generated content less than human-written text when tasked with rewriting. By training LLMs to minimize alterations on AI-generated inputs, we amplify this disparity, yielding a more distinguishable and generalizable edit distance across diverse text distributions. Extensive experiments on data from 21 independent domains and four major LLMs (GPT-3.5, GPT-4, Gemini, and Llama-3) demonstrate that our detector outperforms state-of-the-art detection methods by up to 23.04% in AUROC for in-distribution tests, 37.26% for out-of-distribution tests, and 48.66% under adversarial attacks. Our unique training objective ensures better generalizability compared to directly training for classification, when leveraging the same amount of parameters. Our findings suggest that reinforcing LLMs' inherent rewriting tendencies offers a robust and scalable solution for detecting AI-generated text.",Learning Rewrite Generalized LLM Generated Text Detection Large language models LLMs present significant risks used generate non factual content spread disinformation scale Detecting LLM generated content crucial current detectors struggle generalize open world contexts introduce Learning2Rewrite novel framework detecting AI generated text exceptional generalization unseen domains method leverages insight LLMs inherently modify AI generated content human written text tasked rewriting training LLMs minimize alterations AI generated inputs amplify disparity yielding distinguishable generalizable edit distance diverse text distributions Extensive experiments data 21 independent domains major LLMs GPT 3 5 GPT 4 Gemini Llama 3 demonstrate detector outperforms state art detection methods 23 04 AUROC distribution tests 37 26 distribution tests 48 66 adversarial attacks unique training objective ensures better generalizability compared directly training classification leveraging parameters findings suggest reinforcing LLMs inherent rewriting tendencies offers robust scalable solution detecting AI generated text
343,Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search,"['Linhao Yu', 'Xingguang Ji', 'Yahui Liu', 'Fanheng Kong', 'Chenxi Sun', 'Jingyuan Zhang', 'Hongzhi Zhang', 'V. W.', 'Fuzheng Zhang', 'Deyi Xiong']","Video captioning can be used to assess the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, existing benchmarks and evaluation protocols suffer from crucial issues, such as inadequate or homogeneous creation of key points, exorbitant cost of data creation, and limited evaluation scopes. To address these issues, we propose an automatic framework, named AutoCaption, which leverages Monte Carlo Tree Search (MCTS) to construct numerous and diverse descriptive sentences (\textit{i.e.}, key points) that thoroughly represent video content in an iterative way. This iterative captioning strategy enables the continuous enhancement of video details such as actions, objects' attributes, environment details, etc. We apply AutoCaption to curate MCTS-VCB, a fine-grained video caption benchmark covering video details, thereby enabling a comprehensive evaluation of MLLMs on the video captioning task. We evaluate more than 20 open- and closed-source MLLMs of varying sizes on MCTS-VCB. Results show that MCTS-VCB can effectively and comprehensively evaluate the video captioning capability, with Gemini-1.5-Pro achieving the highest F1 score of 71.2. Interestingly, we fine-tune InternVL2.5-8B with the AutoCaption-generated data, which helps the model achieve an overall improvement of 25.0% on MCTS-VCB and 16.3% on DREAM-1K, further demonstrating the effectiveness of AutoCaption. The code and data are available at https://github.com/tjunlp-lab/MCTS-VCB.",Evaluating Multimodal Large Language Models Video Captioning Monte Carlo Tree Search Video captioning used assess video understanding capabilities Multimodal Large Language Models MLLMs existing benchmarks evaluation protocols suffer crucial issues inadequate homogeneous creation key points exorbitant cost data creation limited evaluation scopes address issues propose automatic framework named AutoCaption leverages Monte Carlo Tree Search MCTS construct numerous diverse descriptive sentences textit e key points thoroughly represent video content iterative way iterative captioning strategy enables continuous enhancement video details actions objects attributes environment details apply AutoCaption curate MCTS VCB fine grained video caption benchmark covering video details enabling comprehensive evaluation MLLMs video captioning task evaluate 20 open closed source MLLMs varying sizes MCTS VCB Results MCTS VCB effectively comprehensively evaluate video captioning capability Gemini 1 5 Pro achieving highest F1 score 71 2 Interestingly fine tune InternVL2 5 8B AutoCaption generated data helps model achieve overall improvement 25 0 MCTS VCB 16 3 DREAM 1K demonstrating effectiveness AutoCaption code data available https github com tjunlp lab MCTS VCB
344,GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs,"['Maxim Zhelnin', 'Viktor Moskvoretskii', 'Egor Shvetsov', 'Maria Krylova', 'Venediktov Egor', 'Zuev Aleksandr', 'Evgeny Burnaev']","Parameter Efficient Fine-Tuning (PEFT) methods have gained popularity and democratized the usage of Large Language Models (LLMs). Recent studies have shown that a small subset of weights significantly impacts performance. Based on this observation, we introduce a novel PEFT method, called Gaussian noise Injected Fine Tuning of Salient Weights (GIFT-SW). Our method updates only salient columns, while injecting Gaussian noise into non-salient ones. To identify these columns, we developeda generalized sensitivity metric that extends and unifies metrics from previous studies. Experiments with LLaMA models demonstrate that GIFT-SW outperforms full fine-tuning and modern PEFT methods under the same computational budget. Moreover, GIFT-SW offers practical advantages to recover performance of models subjected to mixed-precision quantization with keeping salient weights in full precision.",GIFT SW Gaussian noise Injected Fine Tuning Salient Weights LLMs Parameter Efficient Fine Tuning PEFT methods gained popularity democratized usage Large Language Models LLMs Recent studies shown small subset weights significantly impacts performance Based observation introduce novel PEFT method called Gaussian noise Injected Fine Tuning Salient Weights GIFT SW method updates salient columns injecting Gaussian noise non salient ones identify columns developeda generalized sensitivity metric extends unifies metrics previous studies Experiments LLaMA models demonstrate GIFT SW outperforms fine tuning modern PEFT methods computational budget GIFT SW offers practical advantages recover performance models subjected mixed precision quantization keeping salient weights precision
345,Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis,"['Hong Huang', 'Dapeng Wu']","Large language models (LLMs) have made exciting achievements across various domains, yet their deployment on resource-constrained personal devices remains hindered by the prohibitive computational and memory demands of task-specific fine-tuning. While quantization offers a pathway to efficiency, existing methods struggle to balance performance and overhead, either incurring high computational/memory costs or failing to address activation outliers, a critical bottleneck in quantized fine-tuning. To address these challenges, we propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning, certain activation outlier channels retain stable spatial positions across training iterations. Building on OSSH, we propose Quaff, a Quantized parameter-efficient fine-tuning framework for LLMs, optimizing low-precision activation representations through targeted momentum scaling. Quaff dynamically suppresses outliers exclusively in invariant channels using lightweight operations, eliminating full-precision weight storage and global rescaling while reducing quantization errors. Extensive experiments across ten benchmarks validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory savings over full-precision fine-tuning while improving accuracy by 0.6% on the Phi-3 model, reconciling the triple trade-off between efficiency, performance, and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080 Super) without sacrificing model utility, Quaff democratizes personalized LLM deployment. The code is available at https://github.com/Little0o0/Quaff.git.",Quaff Quantized Parameter Efficient Fine Tuning Outlier Spatial Stability Hypothesis Large language models LLMs exciting achievements various domains deployment resource constrained personal devices remains hindered prohibitive computational memory demands task specific fine tuning quantization offers pathway efficiency existing methods struggle balance performance overhead incurring high computational memory costs failing address activation outliers critical bottleneck quantized fine tuning address challenges propose Outlier Spatial Stability Hypothesis OSSH fine tuning certain activation outlier channels retain stable spatial positions training iterations Building OSSH propose Quaff Quantized parameter efficient fine tuning framework LLMs optimizing low precision activation representations targeted momentum scaling Quaff dynamically suppresses outliers exclusively invariant channels using lightweight operations eliminating precision weight storage global rescaling reducing quantization errors Extensive experiments benchmarks validate OSSH demonstrate Quaff s efficacy Specifically GPQA reasoning benchmark Quaff achieves 1 73x latency reduction 30 memory savings precision fine tuning improving accuracy 0 6 Phi 3 model reconciling triple trade efficiency performance deployability enabling consumer grade GPU fine tuning e g RTX 2080 Super sacrificing model utility Quaff democratizes personalized LLM deployment code available https github com Little0o0 Quaff git
346,Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models,"['Atsuyuki Miyai', 'Jingkang Yang', 'Jingyang Zhang', 'Yifei Ming', 'Qing Yu', 'Go Irie', 'Yixuan Li', 'Hai Helen Li', 'Ziwei Liu', 'Kiyoharu Aizawa']","This paper introduces a novel task to evaluate the robust understanding capability of Large Multimodal Models (LMMs), termed $\textbf{Unsolvable Problem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely used to assess the understanding capability of LMMs, but it does not guarantee that LMMs truly comprehend the answer. UPD assesses the LMM's ability to withhold answers when encountering unsolvable problems of MCQA, verifying whether the model truly understands the answer. UPD encompasses three problems: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD), covering unsolvable cases like answer-lacking or incompatible choices and image-question mismatches. For the evaluation, we introduce the MM-UPD Bench, a benchmark for assessing performance across various ability dimensions. Our experiments reveal that even most LMMs, which demonstrate adequate performance on existing benchmarks, struggle significantly with MM-UPD, underscoring a novel aspect of trustworthiness that current benchmarks have overlooked. A detailed analysis shows that LMMs have different bottlenecks and chain-of-thought and self-reflection improved performance for LMMs with the bottleneck in their LLM capability. We hope our insights will enhance the broader understanding and development of more reliable LMMs. The code is available at https://github.com/AtsuMiyai/UPD.",Unsolvable Problem Detection Robust Understanding Evaluation Large Multimodal Models paper introduces novel task evaluate robust understanding capability Large Multimodal Models LMMs termed textbf Unsolvable Problem Detection UPD Multiple choice question answering MCQA widely used assess understanding capability LMMs does guarantee LMMs truly comprehend answer UPD assesses LMM s ability withhold answers encountering unsolvable problems MCQA verifying model truly understands answer UPD encompasses problems Absent Answer Detection AAD Incompatible Answer Set Detection IASD Incompatible Visual Question Detection IVQD covering unsolvable cases like answer lacking incompatible choices image question mismatches evaluation introduce MM UPD Bench benchmark assessing performance various ability dimensions experiments reveal LMMs demonstrate adequate performance existing benchmarks struggle significantly MM UPD underscoring novel aspect trustworthiness current benchmarks overlooked detailed analysis shows LMMs different bottlenecks chain thought self reflection improved performance LMMs bottleneck LLM capability hope insights enhance broader understanding development reliable LMMs code available https github com AtsuMiyai UPD
347,AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models,"['Yuhang Wu', 'Wenmeng Yu', 'Yean Cheng', 'Yan Wang', 'Xiaohan Zhang', 'Jiazheng Xu', 'Ming Ding', 'Yuxiao Dong']","Evaluating the alignment capabilities of large Vision-Language Models (VLMs) is essential for determining their effectiveness as helpful assistants. However, existing benchmarks primarily focus on basic abilities using nonverbal methods, such as yes-no and multiple-choice questions. In this paper, we address this gap by introducing AlignMMBench, which provides more nuanced evaluations of alignment capabilities and is the first benchmark specifically designed for Chinese visual contexts. This benchmark is meticulously curated from real-world scenarios and internet sources, encompassing thirteen specific tasks across three categories, and includes both single-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite strategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer pairs. To facilitate the evaluation pipeline, we develop CritiqueVLM, a rule-calibrated evaluator that exceeds GPT-4's evaluation ability. Additionally, we measure the ""alignment score"", a quantitative metric designed to assess the robustness and stability of models across diverse prompts. Finally, we evaluate the performance of representative VLMs on AlignMMBench, offering insights into the capabilities and limitations of different VLM architectures. The evaluation code and data are available at https://github.com/THUDM/AlignMMBench.",AlignMMBench Evaluating Chinese Multimodal Alignment Large Vision Language Models Evaluating alignment capabilities large Vision Language Models VLMs essential determining effectiveness helpful assistants existing benchmarks primarily focus basic abilities using nonverbal methods yes multiple choice questions paper address gap introducing AlignMMBench provides nuanced evaluations alignment capabilities benchmark specifically designed Chinese visual contexts benchmark meticulously curated real world scenarios internet sources encompassing thirteen specific tasks categories includes single turn multi turn dialogue scenarios Incorporating prompt rewrite strategy AlignMMBench encompasses 1 054 images 4 978 question answer pairs facilitate evaluation pipeline develop CritiqueVLM rule calibrated evaluator exceeds GPT 4 s evaluation ability Additionally measure alignment score quantitative metric designed assess robustness stability models diverse prompts Finally evaluate performance representative VLMs AlignMMBench offering insights capabilities limitations different VLM architectures evaluation code data available https github com THUDM AlignMMBench
348,Biased LLMs can Influence Political Decision-Making,"['Jillian Fisher', 'Shangbin Feng', 'Robert Aron', 'Thomas Richardson', 'Yejin Choi', 'Daniel W Fisher', 'Jennifer Pan', 'Yulia Tsvetkov', 'Katharina Reinecke']",,Biased LLMs Influence Political Decision Making
349,LexTempus: Enhancing Temporal Generalizability of Legal Language Models Through Dynamic Mixture of Experts,"['Santosh T.Y.S.S', 'Tuan-Quang Vuong']",,LexTempus Enhancing Temporal Generalizability Legal Language Models Dynamic Mixture Experts
350,That is Unacceptable: the Moral Foundations of Canceling,"['Soda Marem Lo', 'Oscar Araque', 'Rajesh Sharma', 'Marco Antonio Stranisci']","Canceling is a morally-driven phenomenon that hinders the development of safe social media platforms and contributes to ideological polarization. To address this issue we present the Canceling Attitudes Detection (CADE) dataset, an annotated corpus of canceling incidents aimed at exploring the factors of disagreements in evaluating people canceling attitudes on social media. Specifically, we study the impact of annotators' morality in their perception of canceling, showing that morality is an independent axis for the explanation of disagreement on this phenomenon. Annotator's judgments heavily depend on the type of controversial events and involved celebrities. This shows the need to develop more event-centric datasets to better understand how harms are perpetrated in social media and to develop more aware technologies for their detection.",Unacceptable Moral Foundations Canceling Canceling morally driven phenomenon hinders development safe social media platforms contributes ideological polarization address issue present Canceling Attitudes Detection CADE dataset annotated corpus canceling incidents aimed exploring factors disagreements evaluating people canceling attitudes social media Specifically study impact annotators morality perception canceling showing morality independent axis explanation disagreement phenomenon Annotator s judgments heavily depend type controversial events involved celebrities shows need develop event centric datasets better understand harms perpetrated social media develop aware technologies detection
351,FloorPlan-LLaMa: Aligning Architects’ Feedback and Domain Knowledge in Architectural Floor Plan Generation,"['Jun Yin', 'Pengyu Zeng', 'Haoyuan Sun', 'Yuqin Dai', 'Han Zheng', 'Miao Zhang', 'Yachao Zhang', 'Shuai Lu']",,FloorPlan LLaMa Aligning Architects Feedback Domain Knowledge Architectural Floor Plan Generation
352,TheoremExplainAgent: Towards Video-based Multimodal Explanations for LLM Theorem Understanding,"['Max Ku', 'Thomas Chong', 'Jonathan Leung', 'Krish Shah', 'Alvin Yu', 'Wenhu Chen']","Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.",TheoremExplainAgent Video based Multimodal Explanations LLM Theorem Understanding Understanding domain specific theorems requires just text based reasoning effective communication structured visual explanations crucial deeper comprehension large language models LLMs demonstrate strong performance text based theorem reasoning ability generate coherent pedagogically meaningful visual explanations remains open challenge work introduce TheoremExplainAgent agentic approach generating long form theorem explanation videos 5 minutes using Manim animations systematically evaluate multimodal theorem explanations propose TheoremExplainBench benchmark covering 240 theorems multiple STEM disciplines 5 automated evaluation metrics results reveal agentic planning essential generating detailed long form videos o3 mini agent achieves success rate 93 8 overall score 0 77 quantitative qualitative studies videos produced exhibit minor issues visual element layout Furthermore multimodal explanations expose deeper reasoning flaws text based explanations fail reveal highlighting importance multimodal explanations
353,FineReason: Evaluating and Improving LLMs’ Deliberate Reasoning through Reflective Puzzle Solving,"['Guizhen Chen', 'Weiwen Xu', 'Hao Zhang', 'Hou Pong Chan', 'Chaoqun Liu', 'Lidong Bing', 'Deli Zhao', 'Anh Tuan Luu', 'Yu Rong']",,FineReason Evaluating Improving LLMs Deliberate Reasoning Reflective Puzzle Solving
354,The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs,"['Sergey Berezin', 'Reza Farahbakhsh', 'Noel Crespi']","We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the model's prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignments and underscore the urgent need for more sophisticated defence strategies. Warning: this paper contains examples of unethical inquiries used solely for research purposes.",TIP Iceberg Revealing Hidden Class Task Prompt Adversarial Attacks LLMs present novel class jailbreak adversarial attacks LLMs termed Task Prompt TIP attacks approach embeds sequence sequence tasks e g cipher decoding riddles code execution model s prompt indirectly generate prohibited inputs systematically assess effectiveness attacks introduce PHRYGE benchmark demonstrate techniques successfully circumvent safeguards state art language models including GPT 4o LLaMA 3 2 findings highlight critical weaknesses current LLM safety alignments underscore urgent need sophisticated defence strategies Warning paper contains examples unethical inquiries used solely research purposes
355,Identifying Reliable Evaluation Metrics for Scientific Text Revision,"['Leane Jourdan', 'Nicolas Hernandez', 'Florian Boudin', 'Richard Dufour']","Evaluating text revision in scientific writing remains a challenge, as traditional metrics such as ROUGE and BERTScore primarily focus on similarity rather than capturing meaningful improvements. In this work, we analyse and identify the limitations of these metrics and explore alternative evaluation methods that better align with human judgments. We first conduct a manual annotation study to assess the quality of different revisions. Then, we investigate reference-free evaluation metrics from related NLP domains. Additionally, we examine LLM-as-a-judge approaches, analysing their ability to assess revisions with and without a gold reference. Our results show that LLMs effectively assess instruction-following but struggle with correctness, while domain-specific metrics provide complementary insights. We find that a hybrid approach combining LLM-as-a-judge evaluation and task-specific metrics offers the most reliable assessment of revision quality.",Identifying Reliable Evaluation Metrics Scientific Text Revision Evaluating text revision scientific writing remains challenge traditional metrics ROUGE BERTScore primarily focus similarity capturing meaningful improvements work analyse identify limitations metrics explore alternative evaluation methods better align human judgments conduct manual annotation study assess quality different revisions investigate reference free evaluation metrics related NLP domains Additionally examine LLM judge approaches analysing ability assess revisions gold reference results LLMs effectively assess instruction following struggle correctness domain specific metrics provide complementary insights hybrid approach combining LLM judge evaluation task specific metrics offers reliable assessment revision quality
356,Can Language Models Reason about Individualistic Human Values and Preferences?,"['Liwei Jiang', 'Taylor Sorensen', 'Sydney Levine', 'Yejin Choi']","Recent calls for pluralistic alignment emphasize that AI systems should address the diverse needs of all people. Yet, efforts in this space often require sorting people into fixed buckets of pre-specified diversity-defining dimensions (e.g., demographics), risking smoothing out individualistic variations or even stereotyping. To achieve an authentic representation of diversity that respects individuality, we propose individualistic alignment. While individualistic alignment can take various forms, we introduce IndieValueCatalog, a dataset transformed from the influential World Values Survey (WVS), to study language models (LMs) on the specific challenge of individualistic value reasoning. Given a sample of an individual's value-expressing statements, models are tasked with predicting this person's value judgments in novel cases. With IndieValueCatalog, we reveal critical limitations in frontier LMs, which achieve only 55 % to 65% accuracy in predicting individualistic values. Moreover, our results highlight that a precise description of individualistic values cannot be approximated only with demographic information. We also identify a partiality of LMs in reasoning about global individualistic values, as measured by our proposed Value Inequity Index ({\sigma}Inequity). Finally, we train a series of IndieValueReasoners to reveal new patterns and dynamics into global human values.",Language Models Reason Individualistic Human Values Preferences Recent calls pluralistic alignment emphasize AI systems address diverse needs people efforts space require sorting people fixed buckets pre specified diversity defining dimensions e g demographics risking smoothing individualistic variations stereotyping achieve authentic representation diversity respects individuality propose individualistic alignment individualistic alignment various forms introduce IndieValueCatalog dataset transformed influential World Values Survey WVS study language models LMs specific challenge individualistic value reasoning Given sample individual s value expressing statements models tasked predicting person s value judgments novel cases IndieValueCatalog reveal critical limitations frontier LMs achieve 55 65 accuracy predicting individualistic values results highlight precise description individualistic values approximated demographic information identify partiality LMs reasoning global individualistic values measured proposed Value Inequity Index sigma Inequity Finally train series IndieValueReasoners reveal new patterns dynamics global human values
357,BERT-like Models for Slavic Morpheme Segmentation,"['Dmitry Morozov', 'Lizaveta Astapenka', 'Anna Glazkova', 'Timur Garipov', 'Olga Lyashevskaya']",,BERT like Models Slavic Morpheme Segmentation
358,Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling,"['Xianzhen Luo', 'Yixuan Wang', 'Qingfu Zhu', 'Zhiming Zhang', 'Xuanyu Zhang', 'Qing Yang', 'Dongliang Xu']","Massive parameters of LLMs have made inference latency a fundamental bottleneck. Speculative decoding represents a lossless approach to accelerate inference through a guess-and-verify paradigm. Some methods rely on additional architectures to guess draft tokens, which need extra training before use. Alternatively, retrieval-based training-free techniques build libraries from pre-existing corpora or by n-gram generation. However, they face challenges like large storage requirements, time-consuming retrieval, and limited adaptability. Observing that candidate tokens generated during the decoding process are likely to reoccur in future sequences, we propose Token Recycling. It stores candidate tokens in an adjacency matrix and employs a breadth-first-search (BFS)-like algorithm to construct a draft tree, which is then validated through tree attention. New candidate tokens from the decoding process are then used to update the matrix. Token Recycling requires \textless2MB of additional storage and achieves approximately 2x speedup across all sizes of LLMs. It significantly outperforms existing train-free methods by 30\% and even a widely recognized training method by 25\%.",Turning Trash Treasure Accelerating Inference Large Language Models Token Recycling Massive parameters LLMs inference latency fundamental bottleneck Speculative decoding represents lossless approach accelerate inference guess verify paradigm methods rely additional architectures guess draft tokens need extra training use Alternatively retrieval based training free techniques build libraries pre existing corpora n gram generation face challenges like large storage requirements time consuming retrieval limited adaptability Observing candidate tokens generated decoding process likely reoccur future sequences propose Token Recycling stores candidate tokens adjacency matrix employs breadth search BFS like algorithm construct draft tree validated tree attention New candidate tokens decoding process used update matrix Token Recycling requires textless2MB additional storage achieves approximately 2x speedup sizes LLMs significantly outperforms existing train free methods 30 widely recognized training method 25
359,Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering,"['Xinyu Tang', 'Xiaolei Wang', 'Zhihao Lv', 'Yingqian Min', 'Xin Zhao', 'Binbin Hu', 'Ziqi Liu', 'Zhiqiang Zhang']","Recent advancements in long chain-of-thoughts(long CoTs) have significantly improved the reasoning capabilities of large language models(LLMs). Existing work finds that the capability of long CoT reasoning can be efficiently elicited by tuning on only a few examples and can easily transfer to other tasks. This motivates us to investigate whether long CoT reasoning is a general capability for LLMs. In this work, we conduct an empirical analysis for this question from the perspective of representation. We find that LLMs do encode long CoT reasoning as a general capability, with a clear distinction from vanilla CoTs. Furthermore, domain-specific representations are also required for the effective transfer of long CoT reasoning. Inspired by these findings, we propose GLoRE, a novel representation engineering method to unleash the general long CoT reasoning capabilities of LLMs. Extensive experiments demonstrate the effectiveness and efficiency of GLoRE in both in-domain and cross-domain scenarios.",Unlocking General Long Chain Thought Reasoning Capabilities Large Language Models Representation Engineering Recent advancements long chain thoughts long CoTs significantly improved reasoning capabilities large language models LLMs Existing work finds capability long CoT reasoning efficiently elicited tuning examples easily transfer tasks motivates investigate long CoT reasoning general capability LLMs work conduct empirical analysis question perspective representation LLMs encode long CoT reasoning general capability clear distinction vanilla CoTs Furthermore domain specific representations required effective transfer long CoT reasoning Inspired findings propose GLoRE novel representation engineering method unleash general long CoT reasoning capabilities LLMs Extensive experiments demonstrate effectiveness efficiency GLoRE domain cross domain scenarios
360,Drift: Enhancing LLM Faithfulness in Rationale Generation via Dual-Reward Probabilistic Inference,"['Jiazheng Li', 'Hanqi Yan', 'Yulan He']",,Drift Enhancing LLM Faithfulness Rationale Generation Dual Reward Probabilistic Inference
361,Fairness through Difference Awareness: Measuring $\textit{Desired}$ Group Discrimination in LLMs,"['Angelina Wang', 'Michelle Phan', 'Daniel E. Ho', 'Sanmi Koyejo']",,Fairness Difference Awareness Measuring textit Desired Group Discrimination LLMs
362,MergePrint: Merge-Resistant Fingerprints for Robust Black-box Ownership Verification of Large Language Models,"['Shojiro Yamabe', 'Futa Kai Waseda', 'Tsubasa Takahashi', 'Koki Wataoka']","Protecting the intellectual property of Large Language Models (LLMs) has become increasingly critical due to the high cost of training. Model merging, which integrates multiple expert models into a single multi-task model, introduces a novel risk of unauthorized use of LLMs due to its efficient merging process. While fingerprinting techniques have been proposed for verifying model ownership, their resistance to model merging remains unexplored. To address this gap, we propose a novel fingerprinting method, MergePrint, which embeds robust fingerprints capable of surviving model merging. MergePrint enables black-box ownership verification, where owners only need to check if a model produces target outputs for specific fingerprint inputs, without accessing model weights or intermediate outputs. By optimizing against a pseudo-merged model that simulates merged behavior, MergePrint ensures fingerprints that remain detectable after merging. Additionally, to minimize performance degradation, we pre-optimize the fingerprint inputs. MergePrint pioneers a practical solution for black-box ownership verification, protecting LLMs from misappropriation via merging, while also excelling in resistance to broader model theft threats.",MergePrint Merge Resistant Fingerprints Robust Black box Ownership Verification Large Language Models Protecting intellectual property Large Language Models LLMs increasingly critical high cost training Model merging integrates multiple expert models single multi task model introduces novel risk unauthorized use LLMs efficient merging process fingerprinting techniques proposed verifying model ownership resistance model merging remains unexplored address gap propose novel fingerprinting method MergePrint embeds robust fingerprints capable surviving model merging MergePrint enables black box ownership verification owners need check model produces target outputs specific fingerprint inputs accessing model weights intermediate outputs optimizing pseudo merged model simulates merged behavior MergePrint ensures fingerprints remain detectable merging Additionally minimize performance degradation pre optimize fingerprint inputs MergePrint pioneers practical solution black box ownership verification protecting LLMs misappropriation merging excelling resistance broader model theft threats
363,Dynamic Scaling of Unit Tests for Code Reward Modeling,"['Zeyao Ma', 'Xiaokang Zhang', 'Jing Zhang', 'Jifan Yu', 'Sijia Luo', 'Jie Tang']","Current large language models (LLMs) often struggle to produce accurate responses on the first attempt for complex reasoning tasks like code generation. Prior research tackles this challenge by generating multiple candidate solutions and validating them with LLM-generated unit tests. The execution results of unit tests serve as reward signals to identify correct solutions. As LLMs always confidently make mistakes, these unit tests are not reliable, thereby diminishing the quality of reward signals. Motivated by the observation that scaling the number of solutions improves LLM performance, we explore the impact of scaling unit tests to enhance reward signal quality. Our pioneer experiment reveals a positive correlation between the number of unit tests and reward signal quality, with greater benefits observed in more challenging problems. Based on these insights, we propose CodeRM-8B, a lightweight yet effective unit test generator that enables efficient and high-quality unit test scaling. Additionally, we implement a dynamic scaling mechanism that adapts the number of unit tests based on problem difficulty, further improving efficiency. Experimental results show that our approach significantly improves performance across various models on three benchmarks (e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on HumanEval Plus).",Dynamic Scaling Unit Tests Code Reward Modeling Current large language models LLMs struggle produce accurate responses attempt complex reasoning tasks like code generation Prior research tackles challenge generating multiple candidate solutions validating LLM generated unit tests execution results unit tests serve reward signals identify correct solutions LLMs confidently make mistakes unit tests reliable diminishing quality reward signals Motivated observation scaling number solutions improves LLM performance explore impact scaling unit tests enhance reward signal quality pioneer experiment reveals positive correlation number unit tests reward signal quality greater benefits observed challenging problems Based insights propose CodeRM 8B lightweight effective unit test generator enables efficient high quality unit test scaling Additionally implement dynamic scaling mechanism adapts number unit tests based problem difficulty improving efficiency Experimental results approach significantly improves performance various models benchmarks e g gains 18 43 Llama3 8B 3 42 GPT 4o mini HumanEval Plus
364,UniConv: Unifying Retrieval and Response Generation for Large Language Model in Conversation,"['Fengran Mo', 'Yifan Gao', 'Chuan Meng', 'Xin Liu', 'Zhuofeng Wu', 'Kelong Mao', 'Zhengyang Wang', 'Pei Chen', 'Zheng Li', 'Xian Li', 'Bing Yin', 'Meng Jiang']",,UniConv Unifying Retrieval Response Generation Large Language Model Conversation
365,Tracking Life’s Ups and Downs: Mining Life Events from Social Media Posts for Mental Health Analysis,"['Minghao Lv', 'Siyuan Chen', 'Haoan Jin', 'Minghao Yuan', 'Qianqian Ju', 'Yujia Peng', 'Kenny Q. Zhu', 'Mengyue Wu']",,Tracking Life s Ups Downs Mining Life Events Social Media Posts Mental Health Analysis
366,Towards Simultaneous and Independent Zero-shot Speaker Cloning and Zero-shot Language Style Control,"['Shengpeng Ji', 'Qian Chen', 'Wen Wang', 'Jialong Zuo', 'Minghui Fang', 'Ziyue Jiang', 'Hai Huang', 'Zehan Wang', 'Xize Cheng', 'Siqi Zheng', 'Zhou Zhao']",,Simultaneous Independent Zero shot Speaker Cloning Zero shot Language Style Control
367,PIC: Unlocking Long-Form Text Generation Capabilities of Large Language Models via Position ID Compression,"['Haoran Que', 'Wenge Rong']",,PIC Unlocking Long Form Text Generation Capabilities Large Language Models Position ID Compression
368,Towards Effective Extraction and Evaluation of Factual Claims,"['Dasha Metropolitansky', 'Jonathan Larson']","A common strategy for fact-checking long-form content generated by Large Language Models (LLMs) is extracting simple claims that can be verified independently. Since inaccurate or incomplete claims compromise fact-checking results, ensuring claim quality is critical. However, the lack of a standardized evaluation framework impedes assessment and comparison of claim extraction methods. To address this gap, we propose a framework for evaluating claim extraction in the context of fact-checking along with automated, scalable, and replicable methods for applying this framework, including novel approaches for measuring coverage and decontextualization. We also introduce Claimify, an LLM-based claim extraction method, and demonstrate that it outperforms existing methods under our evaluation framework. A key feature of Claimify is its ability to handle ambiguity and extract claims only when there is high confidence in the correct interpretation of the source text.",Effective Extraction Evaluation Factual Claims common strategy fact checking long form content generated Large Language Models LLMs extracting simple claims verified independently inaccurate incomplete claims compromise fact checking results ensuring claim quality critical lack standardized evaluation framework impedes assessment comparison claim extraction methods address gap propose framework evaluating claim extraction context fact checking automated scalable replicable methods applying framework including novel approaches measuring coverage decontextualization introduce Claimify LLM based claim extraction method demonstrate outperforms existing methods evaluation framework key feature Claimify ability handle ambiguity extract claims high confidence correct interpretation source text
369,Beyond Facts: Evaluating Intent Hallucination in Large Language Models,"['Yijie Hao', 'Haofei Yu', 'Jiaxuan You']","When exposed to complex queries containing multiple conditions, today's large language models (LLMs) tend to produce responses that only partially satisfy the query while neglecting certain conditions. We therefore introduce the concept of Intent Hallucination. In this phenomenon, LLMs either omit (neglecting to address certain parts) or misinterpret (responding to invented query parts) elements of the given query, leading to intent hallucinated generation. To systematically evaluate intent hallucination, we introduce FAITHQA, a novel benchmark for intent hallucination that contains 20,068 problems, covering both query-only and retrieval-augmented generation (RAG) setups with varying topics and difficulty. FAITHQA is the first hallucination benchmark that goes beyond factual verification, tailored to identify the fundamental cause of intent hallucination. By evaluating various LLMs on FAITHQA, we find that (1) intent hallucination is a common issue even for state-of-the-art models, and (2) the phenomenon stems from omission or misinterpretation of LLMs. To facilitate future research, we introduce an automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting intent hallucination. Human evaluation results demonstrate that CONSTRAINT SCORE is closer to human performance for intent hallucination compared to baselines.",Facts Evaluating Intent Hallucination Large Language Models exposed complex queries containing multiple conditions today s large language models LLMs tend produce responses partially satisfy query neglecting certain conditions introduce concept Intent Hallucination phenomenon LLMs omit neglecting address certain parts misinterpret responding invented query parts elements given query leading intent hallucinated generation systematically evaluate intent hallucination introduce FAITHQA novel benchmark intent hallucination contains 20 068 problems covering query retrieval augmented generation RAG setups varying topics difficulty FAITHQA hallucination benchmark goes factual verification tailored identify fundamental cause intent hallucination evaluating various LLMs FAITHQA 1 intent hallucination common issue state art models 2 phenomenon stems omission misinterpretation LLMs facilitate future research introduce automatic LLM generation evaluation metric CONSTRAINT SCORE detecting intent hallucination Human evaluation results demonstrate CONSTRAINT SCORE closer human performance intent hallucination compared baselines
370,A Systematic Study of Compositional Syntactic Transformer Language Models,"['Yida Zhao', 'Hao Xve', 'Xiang Hu', 'Kewei Tu']","Syntactic language models (SLMs) enhance Transformers by incorporating syntactic biases through the modeling of linearized syntactic parse trees alongside surface sentences. This paper focuses on compositional SLMs that are based on constituency parse trees and contain explicit bottom-up composition of constituent representations. We identify key aspects of design choices in existing compositional SLMs and propose a unified framework encompassing both existing models and novel variants. We conduct a comprehensive empirical evaluation of all the variants in our framework across language modeling, syntactic generalization, summarization, dialogue, and inference efficiency. Based on the experimental results, we make multiple recommendations on the design of compositional SLMs. Our code is released at https://github.com/zhaoyd1/compositional_SLMs.",Systematic Study Compositional Syntactic Transformer Language Models Syntactic language models SLMs enhance Transformers incorporating syntactic biases modeling linearized syntactic parse trees alongside surface sentences paper focuses compositional SLMs based constituency parse trees contain explicit composition constituent representations identify key aspects design choices existing compositional SLMs propose unified framework encompassing existing models novel variants conduct comprehensive empirical evaluation variants framework language modeling syntactic generalization summarization dialogue inference efficiency Based experimental results make multiple recommendations design compositional SLMs code released https github com zhaoyd1 compositional_SLMs
371,M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine Translation Evaluation,"['Zhaopeng Feng', 'Jiayuan Su', 'Jiamei Zheng', 'Jiahan Ren', 'Yan Zhang', 'Jian Wu', 'Hongwei Wang', 'Zuozhu Liu']",,M MAD Multidimensional Multi Agent Debate Advanced Machine Translation Evaluation
372,SongComposer: A Large Language Model for Lyric and Melody Generation in Song Composition,"['Shuangrui Ding', 'Zihan Liu', 'Xiaoyi Dong', 'Pan Zhang', 'Rui Qian', 'Junhao Huang', 'Conghui He', 'Dahua Lin', 'Jiaqi Wang']","Creating lyrics and melodies for the vocal track in a symbolic format, known as song composition, demands expert musical knowledge of melody, an advanced understanding of lyrics, and precise alignment between them. Despite achievements in sub-tasks such as lyric generation, lyric-to-melody, and melody-to-lyric, etc, a unified model for song composition has not yet been achieved. In this paper, we introduce SongComposer, a pioneering step towards a unified song composition model that can readily create symbolic lyrics and melodies following instructions. SongComposer is a music-specialized large language model (LLM) that, for the first time, integrates the capability of simultaneously composing lyrics and melodies into LLMs by leveraging three key innovations: 1) a flexible tuple format for word-level alignment of lyrics and melodies, 2) an extended tokenizer vocabulary for song notes, with scalar initialization based on musical knowledge to capture rhythm, and 3) a multi-stage pipeline that captures musical structure, starting with motif-level melody patterns and progressing to phrase-level structure for improved coherence. Extensive experiments demonstrate that SongComposer outperforms advanced LLMs, including GPT-4, in tasks such as lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation. Moreover, we will release SongCompose, a large-scale dataset for training, containing paired lyrics and melodies in Chinese and English.",SongComposer Large Language Model Lyric Melody Generation Song Composition Creating lyrics melodies vocal track symbolic format known song composition demands expert musical knowledge melody advanced understanding lyrics precise alignment Despite achievements sub tasks lyric generation lyric melody melody lyric unified model song composition achieved paper introduce SongComposer pioneering step unified song composition model readily create symbolic lyrics melodies following instructions SongComposer music specialized large language model LLM time integrates capability simultaneously composing lyrics melodies LLMs leveraging key innovations 1 flexible tuple format word level alignment lyrics melodies 2 extended tokenizer vocabulary song notes scalar initialization based musical knowledge capture rhythm 3 multi stage pipeline captures musical structure starting motif level melody patterns progressing phrase level structure improved coherence Extensive experiments demonstrate SongComposer outperforms advanced LLMs including GPT 4 tasks lyric melody generation melody lyric generation song continuation text song creation release SongCompose large scale dataset training containing paired lyrics melodies Chinese English
373,Personalized Text Generation with Contrastive Activation Steering,"['Jinghao Zhang', 'Yuting Liu', 'Wenjie Wang', 'Qiang Liu', 'Shu Wu', 'Liang Wang', 'Tat-Seng Chua']","Personalized text generation aims to infer users' writing style preferences from their historical texts and generate outputs that faithfully reflect these stylistic characteristics. Existing solutions primarily adopt two paradigms: retrieval-augmented generation (RAG) and parameter-efficient fine-tuning (PEFT). While these approaches have advanced the field, they suffer from two critical limitations: (1) the entanglement of content semantics and stylistic patterns in historical texts impedes accurate modeling of user-specific writing preferences; and (2) scalability challenges arising from both RAG's inference latency by retrieval operations and PEFT's parameter storage requirements for per user model. To overcome these limitations, we propose StyleVector, a training-free framework that disentangles and represents personalized writing style as a vector in LLM's activation space, enabling style-steered generation during inference without requiring costly retrieval or parameter storage. Comprehensive experiments demonstrate that our framework achieves a significant 8% relative improvement in personalized generation while reducing storage requirements by 1700 times over PEFT method.",Personalized Text Generation Contrastive Activation Steering Personalized text generation aims infer users writing style preferences historical texts generate outputs faithfully reflect stylistic characteristics Existing solutions primarily adopt paradigms retrieval augmented generation RAG parameter efficient fine tuning PEFT approaches advanced field suffer critical limitations 1 entanglement content semantics stylistic patterns historical texts impedes accurate modeling user specific writing preferences 2 scalability challenges arising RAG s inference latency retrieval operations PEFT s parameter storage requirements user model overcome limitations propose StyleVector training free framework disentangles represents personalized writing style vector LLM s activation space enabling style steered generation inference requiring costly retrieval parameter storage Comprehensive experiments demonstrate framework achieves significant 8 relative improvement personalized generation reducing storage requirements 1700 times PEFT method
374,Gumbel Reranking: Differentiable End-to-End Reranker Optimization,"['Siyuan Huang', 'Zhiyuan Ma', 'Jintao Du', 'Changhua Meng', 'Weiqiang Wang', 'Jingwen Leng', 'Minyi Guo', 'Zhouhan Lin']","RAG systems rely on rerankers to identify relevant documents. However, fine-tuning these models remains challenging due to the scarcity of annotated query-document pairs. Existing distillation-based approaches suffer from training-inference misalignment and fail to capture interdependencies among candidate documents. To overcome these limitations, we reframe the reranking process as an attention-mask problem and propose Gumbel Reranking, an end-to-end training framework for rerankers aimed at minimizing the training-inference gap. In our approach, reranker optimization is reformulated as learning a stochastic, document-wise Top-$k$ attention mask using the Gumbel Trick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end optimization by minimizing the overall language loss. Experiments across various settings consistently demonstrate performance gains, including a 10.4\% improvement in recall on HotpotQA for distinguishing indirectly relevant documents.",Gumbel Reranking Differentiable End End Reranker Optimization RAG systems rely rerankers identify relevant documents fine tuning models remains challenging scarcity annotated query document pairs Existing distillation based approaches suffer training inference misalignment fail capture interdependencies candidate documents overcome limitations reframe reranking process attention mask problem propose Gumbel Reranking end end training framework rerankers aimed minimizing training inference gap approach reranker optimization reformulated learning stochastic document wise k attention mask using Gumbel Trick Relaxed k Sampling formulation enables end end optimization minimizing overall language loss Experiments various settings consistently demonstrate performance gains including 10 4 improvement recall HotpotQA distinguishing indirectly relevant documents
375,Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback,"['Lester James Validad Miranda', 'Yizhong Wang', 'Yanai Elazar', 'Sachin Kumar', 'Valentina Pyatkin', 'Faeze Brahman', 'Noah A. Smith', 'Hannaneh Hajishirzi', 'Pradeep Dasigi']","Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, collecting human preferences is expensive and time-consuming, with highly variable annotation quality. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations, offering a cost-effective and scalable alternative, albeit susceptible to other biases and errors. In this work, we introduce HyPER, a Hybrid Preference routER that defers an annotation to either humans or LMs, achieving better annotation quality while reducing the cost of human-only annotation. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we (1) train a performance prediction model (PPM) to predict a reward model's (RM) performance on an arbitrary combination of human and LM annotations and (2) employ a routing strategy that selects a combination that maximizes the predicted performance. We train the PPM on MultiPref, a new preference dataset with 10k instances paired with humans and LM labels. We show that the selected hybrid mixture of synthetic and direct human preferences using HyPER achieves better RM performance compared to using either one exclusively by 7-13% on RewardBench and generalizes across unseen preference datasets and other base models. We also observe the same trend in other benchmarks using Best-of-N reranking, where the hybrid mix has 2-3% better performance. Finally, we analyze features from HyPER and find that prompts with moderate safety concerns or complexity benefit the most from human feedback.",Hybrid Preferences Learning Route Instances Human vs AI Feedback Learning human feedback enabled alignment language models LMs human preferences collecting human preferences expensive time consuming highly variable annotation quality appealing alternative distill preferences LMs source synthetic annotations offering cost effective scalable alternative albeit susceptible biases errors work introduce HyPER Hybrid Preference routER defers annotation humans LMs achieving better annotation quality reducing cost human annotation formulate optimization problem given preference dataset evaluation metric 1 train performance prediction model PPM predict reward model s RM performance arbitrary combination human LM annotations 2 employ routing strategy selects combination maximizes predicted performance train PPM MultiPref new preference dataset 10k instances paired humans LM labels selected hybrid mixture synthetic direct human preferences using HyPER achieves better RM performance compared using exclusively 7 13 RewardBench generalizes unseen preference datasets base models observe trend benchmarks using Best N reranking hybrid mix 2 3 better performance Finally analyze features HyPER prompts moderate safety concerns complexity benefit human feedback
376,SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open Domain Event Detection,"['Yi-Fan Lu', 'Xian-Ling Mao', 'Tian Lan', 'Tong Zhang', 'Yu-Shi Zhu', 'Heyan Huang']","Automatic evaluation for Open Domain Event Detection (ODED) is a highly challenging task, because ODED is characterized by a vast diversity of un-constrained output labels from various domains. Nearly all existing evaluation methods for ODED usually first construct evaluation benchmarks with limited labels and domain coverage, and then evaluate ODED methods using metrics based on token-level label matching rules. However, this kind of evaluation framework faces two issues: (1) The limited evaluation benchmarks lack representatives of the real world, making it difficult to accurately reflect the performance of various ODED methods in real-world scenarios; (2) Evaluation metrics based on token-level matching rules fail to capture semantic similarity between predictions and golden labels. To address these two problems above, we propose a scalable and reliable Semantic-level Evaluation framework for Open domain Event detection (SEOE) by constructing a more representative evaluation benchmark and introducing a semantic evaluation metric. Specifically, our proposed framework first constructs a scalable evaluation benchmark that currently includes 564 event types covering 7 major domains, with a cost-effective supplementary annotation strategy to ensure the benchmark's representativeness. The strategy also allows for the supplement of new event types and domains in the future. Then, the proposed SEOE leverages large language models (LLMs) as automatic evaluation agents to compute a semantic F1-score, incorporating fine-grained definitions of semantically similar labels to enhance the reliability of the evaluation. Extensive experiments validate the representatives of the benchmark and the reliability of the semantic evaluation metric. Existing ODED methods are thoroughly evaluated, and the error patterns of predictions are analyzed, revealing several insightful findings.",SEOE Scalable Reliable Semantic Evaluation Framework Open Domain Event Detection Automatic evaluation Open Domain Event Detection ODED highly challenging task ODED characterized vast diversity constrained output labels various domains Nearly existing evaluation methods ODED usually construct evaluation benchmarks limited labels domain coverage evaluate ODED methods using metrics based token level label matching rules kind evaluation framework faces issues 1 limited evaluation benchmarks lack representatives real world making difficult accurately reflect performance various ODED methods real world scenarios 2 Evaluation metrics based token level matching rules fail capture semantic similarity predictions golden labels address problems propose scalable reliable Semantic level Evaluation framework Open domain Event detection SEOE constructing representative evaluation benchmark introducing semantic evaluation metric Specifically proposed framework constructs scalable evaluation benchmark currently includes 564 event types covering 7 major domains cost effective supplementary annotation strategy ensure benchmark s representativeness strategy allows supplement new event types domains future proposed SEOE leverages large language models LLMs automatic evaluation agents compute semantic F1 score incorporating fine grained definitions semantically similar labels enhance reliability evaluation Extensive experiments validate representatives benchmark reliability semantic evaluation metric Existing ODED methods thoroughly evaluated error patterns predictions analyzed revealing insightful findings
377,The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project,"['Angelina Aspra Aquino', 'Lester James Validad Miranda', 'Elsie Marie T. Or']","This paper presents UD-NewsCrawl, the largest Tagalog treebank to date, containing 15.6k trees manually annotated according to the Universal Dependencies framework. We detail our treebank development process, including data collection, pre-processing, manual annotation, and quality assurance procedures. We provide baseline evaluations using multiple transformer-based models to assess the performance of state-of-the-art dependency parsers on Tagalog. We also highlight challenges in the syntactic analysis of Tagalog given its distinctive grammatical properties, and discuss its implications for the annotation of this treebank. We anticipate that UD-NewsCrawl and our baseline model implementations will serve as valuable resources for advancing computational linguistics research in underrepresented languages like Tagalog.",UD NewsCrawl Treebank Reflections Challenges Large scale Tagalog Syntactic Annotation Project paper presents UD NewsCrawl largest Tagalog treebank date containing 15 6k trees manually annotated according Universal Dependencies framework treebank development process including data collection pre processing manual annotation quality assurance procedures provide baseline evaluations using multiple transformer based models assess performance state art dependency parsers Tagalog highlight challenges syntactic analysis Tagalog given distinctive grammatical properties discuss implications annotation treebank anticipate UD NewsCrawl baseline model implementations serve valuable resources advancing computational linguistics research underrepresented languages like Tagalog
378,DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation,"['Jennifer Chen', 'Aidar Myrzakhan', 'Yaxin Luo', 'Hassaan Muhammad Khan', 'Sondos Mahmoud Bsharat', 'Zhiqiang Shen']","Retrieval-Augmented Generation (RAG) methods have proven highly effective for tasks requiring factual consistency and robust knowledge retrieval. However, large-scale RAG systems consume significant computational resources and are prone to generating hallucinated content from Humans. In this work, we introduce $\texttt{DRAG}$, a novel framework for distilling RAG knowledge from large-scale Language Models (LLMs) into small LMs (SLMs). Our approach leverages evidence- and knowledge graph-based distillation, ensuring that the distilled model retains critical factual knowledge while significantly reducing model size and computational cost. By aligning the smaller model's predictions with a structured knowledge graph and ranked evidence, $\texttt{DRAG}$ effectively mitigates hallucinations and improves factual accuracy. We further present a case demonstrating how our framework mitigates user privacy risks and introduce a corresponding benchmark. Experimental evaluations on multiple benchmarks demonstrate that our method outperforms the prior competitive RAG methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving high-level efficiency and reliability. With $\texttt{DRAG}$, we provide a practical and resource-efficient roadmap to deploying enhanced retrieval and generation capabilities in small-sized LLMs.",DRAG Distilling RAG SLMs LLMs Transfer Knowledge Mitigate Hallucination Evidence Graph based Distillation Retrieval Augmented Generation RAG methods proven highly effective tasks requiring factual consistency robust knowledge retrieval large scale RAG systems consume significant computational resources prone generating hallucinated content Humans work introduce texttt DRAG novel framework distilling RAG knowledge large scale Language Models LLMs small LMs SLMs approach leverages evidence knowledge graph based distillation ensuring distilled model retains critical factual knowledge significantly reducing model size computational cost aligning smaller model s predictions structured knowledge graph ranked evidence texttt DRAG effectively mitigates hallucinations improves factual accuracy present case demonstrating framework mitigates user privacy risks introduce corresponding benchmark Experimental evaluations multiple benchmarks demonstrate method outperforms prior competitive RAG methods like MiniRAG SLMs 27 7 using models preserving high level efficiency reliability texttt DRAG provide practical resource efficient roadmap deploying enhanced retrieval generation capabilities small sized LLMs
379,G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems,"['Shilong Wang', 'Guibin Zhang', 'Miao Yu', 'Guancheng Wan', 'Fanci Meng', 'chongye guo', 'Kun Wang', 'Yang Wang']","Large Language Model (LLM)-based Multi-agent Systems (MAS) have demonstrated remarkable capabilities in various complex tasks, ranging from collaborative problem-solving to autonomous decision-making. However, as these systems become increasingly integrated into critical applications, their vulnerability to adversarial attacks, misinformation propagation, and unintended behaviors have raised significant concerns. To address this challenge, we introduce G-Safeguard, a topology-guided security lens and treatment for robust LLM-MAS, which leverages graph neural networks to detect anomalies on the multi-agent utterance graph and employ topological intervention for attack remediation. Extensive experiments demonstrate that G-Safeguard: (I) exhibits significant effectiveness under various attack strategies, recovering over 40% of the performance for prompt injection; (II) is highly adaptable to diverse LLM backbones and large-scale MAS; (III) can seamlessly combine with mainstream MAS with security guarantees. The code is available at https://github.com/wslong20/G-safeguard.",G Safeguard Topology Guided Security Lens Treatment LLM based Multi agent Systems Large Language Model LLM based Multi agent Systems MAS demonstrated remarkable capabilities various complex tasks ranging collaborative problem solving autonomous decision making systems increasingly integrated critical applications vulnerability adversarial attacks misinformation propagation unintended behaviors raised significant concerns address challenge introduce G Safeguard topology guided security lens treatment robust LLM MAS leverages graph neural networks detect anomalies multi agent utterance graph employ topological intervention attack remediation Extensive experiments demonstrate G Safeguard exhibits significant effectiveness various attack strategies recovering 40 performance prompt injection II highly adaptable diverse LLM backbones large scale MAS III seamlessly combine mainstream MAS security guarantees code available https github com wslong20 G safeguard
380,Deontological Keyword Bias: The Impact of Modal Verbs on Normative Judgments of Language Models,"['Bumjin Park', 'Leejinsil', 'Jaesik Choi']",,Deontological Keyword Bias Impact Modal Verbs Normative Judgments Language Models
381,LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning,"['Weijie Shi', 'Han Zhu', 'Jiaming Ji', 'Mengze Li', 'Jipeng Zhang', 'Ruiyuan Zhang', 'Jia Zhu', 'Jiajie Xu', 'Sirui Han', 'Yike Guo']","Legal judgment prediction (LJP) aims to function as a judge by making final rulings based on case claims and facts, which plays a vital role in the judicial domain for supporting court decision-making and improving judicial efficiency. However, existing methods often struggle with logical errors when conducting complex legal reasoning. We propose LegalReasoner, which enhances LJP reliability through step-wise verification and correction of the reasoning process. Specifically, it first identifies dispute points to decompose complex cases, and then conducts step-wise reasoning while employing a process verifier to validate each step's logic from correctness, progressiveness, and potential perspectives. When errors are detected, expert-designed attribution and resolution strategies are applied for correction. To fine-tune LegalReasoner, we release the LegalHK dataset, containing 58,130 Hong Kong court cases with detailed annotations of dispute points, step-by-step reasoning chains, and process verification labels. Experiments demonstrate that LegalReasoner significantly improves concordance with court decisions from 72.37 to 80.27 on LLAMA-3.1-70B. The data is available at https://huggingface.co/datasets/weijiezz/LegalHK.",LegalReasoner Step wised Verification Correction Legal Judgment Reasoning Legal judgment prediction LJP aims function judge making final rulings based case claims facts plays vital role judicial domain supporting court decision making improving judicial efficiency existing methods struggle logical errors conducting complex legal reasoning propose LegalReasoner enhances LJP reliability step wise verification correction reasoning process Specifically identifies dispute points decompose complex cases conducts step wise reasoning employing process verifier validate step s logic correctness progressiveness potential perspectives errors detected expert designed attribution resolution strategies applied correction fine tune LegalReasoner release LegalHK dataset containing 58 130 Hong Kong court cases detailed annotations dispute points step step reasoning chains process verification labels Experiments demonstrate LegalReasoner significantly improves concordance court decisions 72 37 80 27 LLAMA 3 1 70B data available https huggingface datasets weijiezz LegalHK
382,Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context,"['Maggie Mi', 'Aline Villavicencio', 'Nafise Sadat Moosavi']","Human processing of idioms relies on understanding the contextual sentences in which idioms occur, as well as language-intrinsic features such as frequency and speaker-intrinsic factors like familiarity. While LLMs have shown high performance on idiomaticity detection tasks, this success may be attributed to reasoning shortcuts in existing datasets. To this end, we construct a novel, controlled contrastive dataset designed to test whether LLMs can effectively use context to disambiguate idiomatic meaning. Additionally, we explore how collocational frequency and sentence probability influence model performance. Our findings reveal that LLMs often fail to resolve idiomaticity when it is required to attend to the surrounding context, and that models perform better on sentences that have higher likelihood. The collocational frequency of expressions also impacts performance. We make our code and dataset publicly available.",Rolling DICE Idiomaticity LLMs Fail Grasp Context Human processing idioms relies understanding contextual sentences idioms occur language intrinsic features frequency speaker intrinsic factors like familiarity LLMs shown high performance idiomaticity detection tasks success attributed reasoning shortcuts existing datasets end construct novel controlled contrastive dataset designed test LLMs effectively use context disambiguate idiomatic meaning Additionally explore collocational frequency sentence probability influence model performance findings reveal LLMs fail resolve idiomaticity required attend surrounding context models perform better sentences higher likelihood collocational frequency expressions impacts performance make code dataset publicly available
383,ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code Generation,"['Xuanle Zhao', 'Xianzhen Luo', 'Qi Shi', 'Chi Chen', 'Shuo Wang', 'Zhiyuan Liu', 'Maosong Sun']","Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in chart understanding tasks. However, interpreting charts with textual descriptions often leads to information loss, as it fails to fully capture the dense information embedded in charts. In contrast, parsing charts into code provides lossless representations that can effectively contain all critical details. Although existing open-source MLLMs have achieved success in chart understanding tasks, they still face two major challenges when applied to chart-to-code tasks: (1) Low executability and poor restoration of chart details in the generated code and (2) Lack of large-scale and diverse training data. To address these challenges, we propose \textbf{ChartCoder}, the first dedicated chart-to-code MLLM, which leverages Code LLMs as the language backbone to enhance the executability of the generated code. Furthermore, we introduce \textbf{Chart2Code-160k}, the first large-scale and diverse dataset for chart-to-code generation, and propose the \textbf{Snippet-of-Thought (SoT)} method, which transforms direct chart-to-code generation data into step-by-step generation. Experiments demonstrate that ChartCoder, with only 7B parameters, surpasses existing open-source MLLMs on chart-to-code benchmarks, achieving superior chart restoration and code excitability. Our code is available at https://github.com/thunlp/ChartCoder.",ChartCoder Advancing Multimodal Large Language Model Chart Code Generation Multimodal Large Language Models MLLMs demonstrated remarkable capabilities chart understanding tasks interpreting charts textual descriptions leads information loss fails fully capture dense information embedded charts contrast parsing charts code provides lossless representations effectively contain critical details existing open source MLLMs achieved success chart understanding tasks face major challenges applied chart code tasks 1 Low executability poor restoration chart details generated code 2 Lack large scale diverse training data address challenges propose textbf ChartCoder dedicated chart code MLLM leverages Code LLMs language backbone enhance executability generated code Furthermore introduce textbf Chart2Code 160k large scale diverse dataset chart code generation propose textbf Snippet Thought SoT method transforms direct chart code generation data step step generation Experiments demonstrate ChartCoder 7B parameters surpasses existing open source MLLMs chart code benchmarks achieving superior chart restoration code excitability code available https github com thunlp ChartCoder
384,The Cross-linguistic Role of Animacy in Grammar Structures,"['Nina Gregorio', 'Matteo Gay', 'Sharon Goldwater', 'Edoardo Ponti']",,Cross linguistic Role Animacy Grammar Structures
385,LexGen: Domain-aware Multilingual Lexicon Generation,"['Ayush Maheshwari', 'Atul Kumar Singh', 'N J Karthika', 'Krishnakant Bhatt', 'Preethi Jyothi', 'Ganesh Ramakrishnan']","Lexicon or dictionary generation across domains has the potential for societal impact, as it can potentially enhance information accessibility for a diverse user base while preserving language identity. Prior work in the field primarily focuses on bilingual lexical induction, which deals with word alignments using mapping or corpora-based approaches. However, these approaches do not cater to domain-specific lexicon generation that consists of domain-specific terminology. This task becomes particularly important in specialized medical, engineering, and other technical domains, owing to the highly infrequent usage of the terms and scarcity of data involving domain-specific terms especially for low/mid-resource languages. In this paper, we propose a new model to generate dictionary words for $6$ Indian languages in the multi-domain setting. Our model consists of domain-specific and domain-generic layers that encode information, and these layers are invoked via a learnable routing technique. We also release a new benchmark dataset consisting of >75K translation pairs across 6 Indian languages spanning 8 diverse domains.We conduct both zero-shot and few-shot experiments across multiple domains to show the efficacy of our proposed model in generalizing to unseen domains and unseen languages. Additionally, we also perform a post-hoc human evaluation on unseen languages. The source code and dataset is present at https://github.com/Atulkmrsingh/lexgen.",LexGen Domain aware Multilingual Lexicon Generation Lexicon dictionary generation domains potential societal impact potentially enhance information accessibility diverse user base preserving language identity Prior work field primarily focuses bilingual lexical induction deals word alignments using mapping corpora based approaches approaches cater domain specific lexicon generation consists domain specific terminology task particularly important specialized medical engineering technical domains owing highly infrequent usage terms scarcity data involving domain specific terms especially low mid resource languages paper propose new model generate dictionary words 6 Indian languages multi domain setting model consists domain specific domain generic layers encode information layers invoked learnable routing technique release new benchmark dataset consisting 75K translation pairs 6 Indian languages spanning 8 diverse domains conduct zero shot shot experiments multiple domains efficacy proposed model generalizing unseen domains unseen languages Additionally perform post hoc human evaluation unseen languages source code dataset present https github com Atulkmrsingh lexgen
386,How to Train Long-Context Language Models (Effectively),"['Tianyu Gao', 'Alexander Wettig', 'Howard Yen', 'Danqi Chen']","We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development -- instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context downstream tasks, and we evaluate models after SFT as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices such as position extrapolation. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short-context data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.1-8B-Instruct on the majority of long-context tasks despite using only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.",Train Long Context Language Models Effectively study continued training supervised fine tuning SFT language model LM make effective use long context information establish reliable evaluation protocol guide model development instead perplexity simple needle haystack NIAH tests use broad set long context downstream tasks evaluate models SFT better reveals long context abilities Supported robust evaluations run thorough experiments decide data mix continued pre training instruction tuning dataset design choices position extrapolation 1 code repositories books excellent sources long data crucial combine high quality short context data 2 training sequence length evaluation length boosts long context performance 3 SFT using short instruction datasets yields strong performance long context tasks final model ProLong 8B initialized Llama 3 trained 40B tokens demonstrates state art long context performance similarly sized models length 128K ProLong outperforms Llama 3 1 8B Instruct majority long context tasks despite using 5 tokens long context training Additionally ProLong effectively process 512K tokens longest context windows publicly available LMs
387,MathFusion: Enhancing Mathematical Problem-solving of LLM through Instruction Fusion,"['Qizhi Pei', 'Lijun Wu', 'Zhuoshi Pan', 'Yu Li', 'Honglin Lin', 'Chenlin Ming', 'Xin Gao', 'Conghui He', 'Rui Yan']","Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variations-which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, \textbf{MathFusionQA}, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches. Our datasets, models, and code are publicly available at https://github.com/QizhiPei/mathfusion.",MathFusion Enhancing Mathematical Problem solving LLM Instruction Fusion Large Language Models LLMs shown impressive progress mathematical reasoning data augmentation promising enhance mathematical problem solving ability current approaches predominantly limited instance level modifications rephrasing generating syntactic variations fail capture leverage intrinsic relational structures inherent mathematical knowledge Inspired human learning processes mathematical proficiency develops systematic exposure interconnected concepts introduce MathFusion novel framework enhances mathematical reasoning cross problem instruction synthesis MathFusion implements fusion strategies 1 sequential fusion chains related problems model solution dependencies 2 parallel fusion combines analogous problems reinforce conceptual understanding 3 conditional fusion creates context aware selective problems enhance reasoning flexibility applying strategies generate new dataset textbf MathFusionQA followed fine tuning models DeepSeekMath 7B Mistral 7B Llama3 8B Experimental results demonstrate MathFusion achieves substantial improvements mathematical reasoning maintaining high data efficiency boosting performance 18 0 points accuracy diverse benchmarks requiring 45K additional synthetic instructions representing substantial improvement traditional single instruction approaches datasets models code publicly available https github com QizhiPei mathfusion
388,Mining Complex Patterns of Argumentative Reasoning in Natural Language Dialogue,"['Ramon Ruiz-Dolz', 'Zlata Kikteva', 'John Lawrence']",,Mining Complex Patterns Argumentative Reasoning Natural Language Dialogue
389,"OS Agents: A Survey on MLLM-based Agents for Computer, Phone and Browser Use","['Xueyu Hu', 'Tao Xiong', 'Biao Yi', 'Ruixuan Xiao', 'Zishu Wei', 'Yurun Chen', 'Jiasheng Ye', 'Meiling Tao', 'Xiangxin Zhou', 'Ziyu Zhao', 'Yuhuai Li', 'Shengze Xu', 'Shenzhi Wang', 'Xinchen Xu', 'Shuofei Qiao', 'Zhaokai Wang', 'Kun Kuang', 'Tieyong Zeng', 'Liang Wang', 'Jiwei Li', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou', 'Guoyin Wang', 'Keting Yin', 'Zhou Zhao', 'Hongxia Yang', 'Fan Wu', 'Shengyu Zhang', 'Fei Wu']",,OS Agents Survey MLLM based Agents Computer Phone Browser Use
390,Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning,"['Mingfei Lau', 'Qian Chen', 'Yeming Fang', 'Tingting Xu', 'Tongzhou Chen', 'Pavel Golik']","Our quality audit for three widely used public multilingual speech datasets - Mozilla Common Voice 17.0, FLEURS, and Vox Populi - shows that in some languages, these datasets suffer from significant quality issues, which may obfuscate downstream evaluation results while creating an illusion of success. We divide these quality issues into two categories: micro-level and macro-level. We find that macro-level issues are more prevalent in less institutionalized, often under-resourced languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that highlights the need for proactive language planning (e.g. orthography prescriptions, dialect boundary definition) and enhanced data quality control in the dataset creation process. We conclude by proposing guidelines and recommendations to mitigate these issues in future dataset development, emphasizing the importance of sociolinguistic awareness and language planning principles. Furthermore, we encourage research into how this creation process itself can be leveraged as a tool for community-led language planning and revitalization.",Data Quality Issues Multilingual Speech Datasets Need Sociolinguistic Awareness Proactive Language Planning quality audit widely used public multilingual speech datasets Mozilla Common Voice 17 0 FLEURS Vox Populi shows languages datasets suffer significant quality issues obfuscate downstream evaluation results creating illusion success divide quality issues categories micro level macro level macro level issues prevalent institutionalized resourced languages provide case analysis Taiwanese Southern Min nan_tw highlights need proactive language planning e g orthography prescriptions dialect boundary definition enhanced data quality control dataset creation process conclude proposing guidelines recommendations mitigate issues future dataset development emphasizing importance sociolinguistic awareness language planning principles Furthermore encourage research creation process leveraged tool community led language planning revitalization
391,LLM as a Broken Telephone: Iterative Generation Distorts Information,"['Amr Mohamed', 'Mingmeng Geng', 'Michalis Vazirgiannis', 'Guokan Shang']","As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the ""broken telephone"" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.",LLM Broken Telephone Iterative Generation Distorts Information large language models increasingly responsible online content concerns arise impact repeatedly processing outputs Inspired broken telephone effect chained human communication study investigates LLMs similarly distort information iterative generation translation based experiments distortion accumulates time influenced language choice chain complexity degradation inevitable mitigated strategic prompting techniques findings contribute discussions long term effects AI mediated information propagation raising important questions reliability LLM generated content iterative workflows
392,VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues,"['Jianshu Zhang', 'Dongyu Yao', 'Renjie Pi', 'Paul Pu Liang', 'Yi R. Fung']","Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce \textbf{VLM2-Bench}, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across twelve VLMs, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.",VLM2 Bench Closer Look VLMs Implicitly Link Explicit Matching Visual Cues Visually linking matching cues crucial ability daily life identifying person multiple photos based cues knowing Despite extensive knowledge vision language models VLMs possess remains largely unexplored capable performing fundamental task address introduce textbf VLM2 Bench benchmark designed assess VLMs Visually Link Matching cues 9 subtasks 3 000 test cases Comprehensive evaluation VLMs analysis various language vision prompting methods leads total key findings identify critical challenges models ability link visual cues highlighting significant performance gap Based insights advocate enhancing core visual capabilities improve adaptability reduce reliance prior knowledge ii establishing clearer principles integrating language based reasoning vision centric tasks prevent unnecessary biases iii shifting vision text training paradigms fostering models ability independently structure infer relationships visual cues
393,Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation,"['Xiang Geng', 'Zhejian Lai', 'Jiajun Chen', 'Hao Yang', 'Shujian Huang']","Quality Estimation (QE) models evaluate the quality of machine translations without reference translations, serving as the reward models for the translation task. Due to the data scarcity, synthetic data generation has emerged as a promising solution. However, synthetic QE data often suffers from distribution shift, which can manifest as discrepancies between pseudo and real translations, or in pseudo labels that do not align with human preferences. To tackle this issue, we introduce DCSQE, a novel framework for alleviating distribution shift in synthetic QE data. To reduce the difference between pseudo and real translations, we employ the constrained beam search algorithm and enhance translation diversity through the use of distinct generation models. DCSQE uses references, i.e., translation supervision signals, to guide both the generation and annotation processes, enhancing the quality of token-level labels. DCSQE further identifies the shortest phrase covering consecutive error tokens, mimicking human annotation behavior, to assign the final phrase-level labels. Specially, we underscore that the translation model can not annotate translations of itself accurately. Extensive experiments demonstrate that DCSQE outperforms SOTA baselines like CometKiwi in both supervised and unsupervised settings. Further analysis offers insights into synthetic data generation that could benefit reward models for other tasks. The code is available at https://github.com/NJUNLP/njuqe.",Alleviating Distribution Shift Synthetic Data Machine Translation Quality Estimation Quality Estimation QE models evaluate quality machine translations reference translations serving reward models translation task data scarcity synthetic data generation emerged promising solution synthetic QE data suffers distribution shift manifest discrepancies pseudo real translations pseudo labels align human preferences tackle issue introduce DCSQE novel framework alleviating distribution shift synthetic QE data reduce difference pseudo real translations employ constrained beam search algorithm enhance translation diversity use distinct generation models DCSQE uses references e translation supervision signals guide generation annotation processes enhancing quality token level labels DCSQE identifies shortest phrase covering consecutive error tokens mimicking human annotation behavior assign final phrase level labels Specially underscore translation model annotate translations accurately Extensive experiments demonstrate DCSQE outperforms SOTA baselines like CometKiwi supervised unsupervised settings analysis offers insights synthetic data generation benefit reward models tasks code available https github com NJUNLP njuqe
394,Combining Domain and Alignment Vectors Provides Better Knowledge-Safety Trade-offs in LLMs,"['Megh Thakkar', 'Quentin Fournier', 'Matthew Riemer', 'Pin-Yu Chen', 'Amal Zouaq', 'Payel Das', 'Sarath Chandar']",,Combining Domain Alignment Vectors Provides Better Knowledge Safety Trade offs LLMs
395,Can Uniform Meaning Representation Help GPT-4 Translate from Indigenous Languages?,['Shira Wein'],"While ChatGPT and GPT-based models are able to effectively perform many tasks without additional fine-tuning, they struggle with tasks related to extremely low-resource languages and indigenous languages. Uniform Meaning Representation (UMR), a semantic representation designed to capture the meaning of texts in many languages, is well-positioned to be leveraged in the development of low-resource language technologies. In this work, we explore the downstream utility of UMR for low-resource languages by incorporating it into GPT-4 prompts. Specifically, we examine the ability of GPT-4 to perform translation from three indigenous languages (Navajo, Ar\'apaho, and Kukama), with and without demonstrations, as well as with and without UMR annotations. Ultimately, we find that in the majority of our test cases, integrating UMR into the prompt results in a statistically significant increase in performance, which is a promising indication of future applications of the UMR formalism.",Uniform Meaning Representation Help GPT 4 Translate Indigenous Languages ChatGPT GPT based models able effectively perform tasks additional fine tuning struggle tasks related extremely low resource languages indigenous languages Uniform Meaning Representation UMR semantic representation designed capture meaning texts languages positioned leveraged development low resource language technologies work explore downstream utility UMR low resource languages incorporating GPT 4 prompts Specifically examine ability GPT 4 perform translation indigenous languages Navajo Ar apaho Kukama demonstrations UMR annotations Ultimately majority test cases integrating UMR prompt results statistically significant increase performance promising indication future applications UMR formalism
396,Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models,"['Fan Zhang', 'Shulin Tian', 'Ziqi Huang', 'Yu Qiao', 'Ziwei Liu']","Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of a model's capabilities by observing only a few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only a few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation.",Evaluation Agent Efficient Promptable Evaluation Framework Visual Generative Models Recent advancements visual generative models enabled high quality image video generation opening diverse applications evaluating models demands sampling hundreds thousands images videos making process computationally expensive especially diffusion based models inherently slow sampling existing evaluation methods rely rigid pipelines overlook specific user needs provide numerical results clear explanations contrast humans quickly form impressions model s capabilities observing samples mimic propose Evaluation Agent framework employs human like strategies efficient dynamic multi round evaluations using samples round offering detailed user tailored analyses offers key advantages 1 efficiency 2 promptable evaluation tailored diverse user needs 3 explainability single numerical scores 4 scalability various models tools Experiments Evaluation Agent reduces evaluation time 10 traditional methods delivering comparable results Evaluation Agent framework fully open sourced advance research visual generative models efficient evaluation
397,LLMs Struggle to Describe the Haystack without Human Help: A Social Science-Inspired Evaluation of Topic Models,"['Zongxia Li', 'Lorena Calvo-Bartolomé', 'Alexander Miserlis Hoyle', 'Paiheng Xu', 'Daniel Kofi Stephens', 'Juan Francisco Fung', 'Alden Dima', 'Jordan Lee Boyd-Graber']",,LLMs Struggle Haystack Human Help Social Science Inspired Evaluation Topic Models
398,ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models,"['Ziyue Wang', 'Chi Chen', 'Fuwen Luo', 'Yurui Dong', 'Yuanchi Zhang', 'Yuzhuang Xu', 'Xiaolong Wang', 'Peng Li', 'Yang Liu']","Active perception, a crucial human capability, involves setting a goal based on the current understanding of the environment and performing actions to achieve that goal. Despite significant efforts in evaluating Multimodal Large Language Models (MLLMs), active perception has been largely overlooked. To address this gap, we propose a novel benchmark named ActiView to evaluate active perception in MLLMs. We focus on a specialized form of Visual Question Answering (VQA) that eases and quantifies the evaluation yet challenging for existing MLLMs. Meanwhile, intermediate reasoning behaviors of models are also discussed. Given an image, we restrict the perceptual field of a model, requiring it to actively zoom or shift its perceptual field based on reasoning to answer the question successfully. We conduct extensive evaluation over 30 models, including proprietary and open-source models, and observe that restricted perceptual fields play a significant role in enabling active perception. Results reveal a significant gap in the active perception capability of MLLMs, indicating that this area deserves more attention. We hope that ActiView could help develop methods for MLLMs to understand multimodal inputs in more natural and holistic ways.",ActiView Evaluating Active Perception Ability Multimodal Large Language Models Active perception crucial human capability involves setting goal based current understanding environment performing actions achieve goal Despite significant efforts evaluating Multimodal Large Language Models MLLMs active perception largely overlooked address gap propose novel benchmark named ActiView evaluate active perception MLLMs focus specialized form Visual Question Answering VQA eases quantifies evaluation challenging existing MLLMs intermediate reasoning behaviors models discussed Given image restrict perceptual field model requiring actively zoom shift perceptual field based reasoning answer question successfully conduct extensive evaluation 30 models including proprietary open source models observe restricted perceptual fields play significant role enabling active perception Results reveal significant gap active perception capability MLLMs indicating area deserves attention hope ActiView help develop methods MLLMs understand multimodal inputs natural holistic ways
399,Enough Coin Flips Can Make LLMs Act Bayesian,"['Ritwik Gupta', 'Rodolfo Corona', 'Jiaxin Ge', 'Eric Wang', 'Dan Klein', 'Trevor Darrell', 'David M. Chan']","Large language models (LLMs) exhibit the ability to generalize given few-shot examples in their input prompt, an emergent capability known as in-context learning (ICL). We investigate whether LLMs use ICL to perform structured reasoning in ways that are consistent with a Bayesian framework or rely on pattern matching. Using a controlled setting of biased coin flips, we find that: (1) LLMs often possess biased priors, causing initial divergence in zero-shot settings, (2) in-context evidence outweighs explicit bias instructions, (3) LLMs broadly follow Bayesian posterior updates, with deviations primarily due to miscalibrated priors rather than flawed updates, and (4) attention magnitude has negligible effect on Bayesian inference. With sufficient demonstrations of biased coin flips via ICL, LLMs update their priors in a Bayesian manner.",Coin Flips Make LLMs Act Bayesian Large language models LLMs exhibit ability generalize given shot examples input prompt emergent capability known context learning ICL investigate LLMs use ICL perform structured reasoning ways consistent Bayesian framework rely pattern matching Using controlled setting biased coin flips 1 LLMs possess biased priors causing initial divergence zero shot settings 2 context evidence outweighs explicit bias instructions 3 LLMs broadly follow Bayesian posterior updates deviations primarily miscalibrated priors flawed updates 4 attention magnitude negligible effect Bayesian inference sufficient demonstrations biased coin flips ICL LLMs update priors Bayesian manner
400,Beyond Outcomes: Transparent Assessment of LLM Reasoning in Games,"['Wenye Lin', 'Jonathan Roberts', 'Yunhan Yang', 'Samuel Albanie', 'Zongqing Lu', 'Kai Han']",,Outcomes Transparent Assessment LLM Reasoning Games
401,A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens,"['Zhijie Nie', 'Richong Zhang', 'Zhanyu Wu']","Text embeddings from large language models (LLMs) have achieved excellent results in tasks such as information retrieval, semantic textual similarity, etc. In this work, we show an interesting finding: when feeding a text into the LLM-based embedder, the obtained text embedding will be able to be aligned with the key tokens in the input text. We first fully analyze this phenomenon on eight LLM-based embedders and show that this phenomenon is universal and is not affected by model architecture, training strategy, and embedding method. With a deeper analysis, we find that the main change in embedding space between these embedders and their LLM backbones is in the first principal component. By adjusting the first principal component, we can align text embedding with the key tokens. Finally, we give several examples to demonstrate the vast application potential of this finding: (1) we propose a simple and practical sparse retrieval method based on the aligned tokens, which can achieve 80% of the dense retrieval effect of the same model while reducing the computation significantly; (2) we show that our findings provide a novel perspective to help understand novel technologies (e.g., instruction-following embedding) and fuzzy concepts (e.g., semantic relatedness vs. similarity) in this field.",Text Worth Tokens Text Embedding LLMs Secretly Aligns Key Tokens Text embeddings large language models LLMs achieved excellent results tasks information retrieval semantic textual similarity work interesting finding feeding text LLM based embedder obtained text embedding able aligned key tokens input text fully analyze phenomenon LLM based embedders phenomenon universal affected model architecture training strategy embedding method deeper analysis main change embedding space embedders LLM backbones principal component adjusting principal component align text embedding key tokens Finally examples demonstrate vast application potential finding 1 propose simple practical sparse retrieval method based aligned tokens achieve 80 dense retrieval effect model reducing computation significantly 2 findings provide novel perspective help understand novel technologies e g instruction following embedding fuzzy concepts e g semantic relatedness vs similarity field
402,Commonsense Reasoning in Arab Culture,"['Abdelrahman Sadallah', 'Junior Cedric Tonga', 'Khalid Almubarak', 'Saeed Almheiri', 'Farah Atif', 'Chatrine Qwaider', 'Karima Kadaoui', 'Sara Shatnawi', 'Yaser Alesh', 'Fajri Koto']","Despite progress in Arabic large language models, such as Jais and AceGPT, their evaluation on commonsense reasoning has largely relied on machine-translated datasets, which lack cultural depth and may introduce Anglocentric biases. Commonsense reasoning is shaped by geographical and cultural contexts, and existing English datasets fail to capture the diversity of the Arab world. To address this, we introduce \datasetname, a commonsense reasoning dataset in Modern Standard Arabic (MSA), covering cultures of 13 countries across the Gulf, Levant, North Africa, and the Nile Valley. The dataset was built from scratch by engaging native speakers to write and validate culturally relevant questions for their respective countries. \datasetname spans 12 daily life domains with 54 fine-grained subtopics, reflecting various aspects of social norms, traditions, and everyday experiences. Zero-shot evaluations show that open-weight language models with up to 32B parameters struggle to comprehend diverse Arab cultures, with performance varying across regions. These findings highlight the need for more culturally aware models and datasets tailored to the Arabic-speaking world.",Commonsense Reasoning Arab Culture Despite progress Arabic large language models Jais AceGPT evaluation commonsense reasoning largely relied machine translated datasets lack cultural depth introduce Anglocentric biases Commonsense reasoning shaped geographical cultural contexts existing English datasets fail capture diversity Arab world address introduce datasetname commonsense reasoning dataset Modern Standard Arabic MSA covering cultures 13 countries Gulf Levant North Africa Nile Valley dataset built scratch engaging native speakers write validate culturally relevant questions respective countries datasetname spans 12 daily life domains 54 fine grained subtopics reflecting various aspects social norms traditions everyday experiences Zero shot evaluations open weight language models 32B parameters struggle comprehend diverse Arab cultures performance varying regions findings highlight need culturally aware models datasets tailored Arabic speaking world
403,AXIS: Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents,"['Junting Lu', 'Zhiyang Zhang', 'Fangkai Yang', 'Jue Zhang', 'Lu Wang', 'Chao Du', 'Qingwei Lin', 'Saravan Rajmohan', 'Dongmei Zhang', 'Qi Zhang']","Multimodal large language models (MLLMs) have enabled LLM-based agents to directly interact with application user interfaces (UIs), enhancing agents' performance in complex tasks. However, these agents often suffer from high latency and low reliability due to the extensive sequential UI interactions. To address this issue, we propose AXIS, a novel LLM-based agents framework that prioritize actions through application programming interfaces (APIs) over UI actions. This framework also facilitates the creation and expansion of APIs through automated exploration of applications. Our experiments on Microsoft Word demonstrate that AXIS reduces task completion time by 65%-70% and cognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compared to humans. Our work contributes to a new human-agent-computer interaction (HACI) framework and explores a fresh UI design principle for application providers to turn applications into agents in the era of LLMs, paving the way towards an agent-centric operating system (Agent OS).",AXIS Efficient Human Agent Computer Interaction API LLM Based Agents Multimodal large language models MLLMs enabled LLM based agents directly interact application user interfaces UIs enhancing agents performance complex tasks agents suffer high latency low reliability extensive sequential UI interactions address issue propose AXIS novel LLM based agents framework prioritize actions application programming interfaces APIs UI actions framework facilitates creation expansion APIs automated exploration applications experiments Microsoft Word demonstrate AXIS reduces task completion time 65 70 cognitive workload 38 53 maintaining accuracy 97 98 compared humans work contributes new human agent computer interaction HACI framework explores fresh UI design principle application providers turn applications agents era LLMs paving way agent centric operating Agent OS
404,Translation and Fusion Improves Cross-lingual Information Extraction,"['Yang Chen', 'Vedaant Shah', 'Alan Ritter']",,Translation Fusion Improves Cross lingual Information Extraction
405,Conditional Dichotomy Quantification via Geometric Embedding,"['Shaobo Cui', 'Wenqing Liu', 'Yiyang Feng', 'Jiawei Zhou', 'Boi Faltings']",,Conditional Dichotomy Quantification Geometric Embedding
406,Aligning Large Language Models with Implicit Preferences from User-Generated Content,"['Zhaoxuan Tan', 'Zheng Li', 'Tianyi Liu', 'Haodong Wang', 'Hyokun Yun', 'Ming Zeng', 'Pei Chen', 'Zhihan Zhang', 'Yifan Gao', 'Ruijie Wang', 'Priyanka Nigam', 'Bing Yin', 'Meng Jiang']","Learning from preference feedback is essential for aligning large language models (LLMs) with human values and improving the quality of generated responses. However, existing preference learning methods rely heavily on curated data from humans or advanced LLMs, which is costly and difficult to scale. In this work, we present PUGC, a novel framework that leverages implicit human Preferences in unlabeled User-Generated Content (UGC) to generate preference data. Although UGC is not explicitly created to guide LLMs in generating human-preferred responses, it often reflects valuable insights and implicit preferences from its creators that has the potential to address readers' questions. PUGC transforms UGC into user queries and generates responses from the policy model. The UGC is then leveraged as a reference text for response scoring, aligning the model with these implicit preferences. This approach improves the quality of preference data while enabling scalable, domain-specific alignment. Experimental results on Alpaca Eval 2 show that models trained with DPO and PUGC achieve a 9.37% performance improvement over traditional methods, setting a 35.93% state-of-the-art length-controlled win rate using Mistral-7B-Instruct. Further studies highlight gains in reward quality, domain-specific alignment effectiveness, robustness against UGC quality, and theory of mind capabilities. Our code and dataset are available at https://zhaoxuan.info/PUGC.github.io/",Aligning Large Language Models Implicit Preferences User Generated Content Learning preference feedback essential aligning large language models LLMs human values improving quality generated responses existing preference learning methods rely heavily curated data humans advanced LLMs costly difficult scale work present PUGC novel framework leverages implicit human Preferences unlabeled User Generated Content UGC generate preference data UGC explicitly created guide LLMs generating human preferred responses reflects valuable insights implicit preferences creators potential address readers questions PUGC transforms UGC user queries generates responses policy model UGC leveraged reference text response scoring aligning model implicit preferences approach improves quality preference data enabling scalable domain specific alignment Experimental results Alpaca Eval 2 models trained DPO PUGC achieve 9 37 performance improvement traditional methods setting 35 93 state art length controlled win rate using Mistral 7B Instruct studies highlight gains reward quality domain specific alignment effectiveness robustness UGC quality theory mind capabilities code dataset available https zhaoxuan info PUGC github io
407,VQAGuider: Guiding Multimodal Large Language Models to Answer Complex Video Questions,"['Yuyan Chen', 'Jiyuan Jia', 'Jiaxin Lu', 'Siyue Li', 'Yu Guan', 'Ming Yang', 'Qingpei Guo']",,VQAGuider Guiding Multimodal Large Language Models Answer Complex Video Questions
408,Large Language Models are Good Relational Learners,"['Fang Wu', 'Vijay Prakash Dwivedi', 'Jure Leskovec']","Large language models (LLMs) have demonstrated remarkable capabilities across various domains, yet their application to relational deep learning (RDL) remains underexplored. Existing approaches adapt LLMs by traversing relational links between entities in a database and converting the structured data into flat text documents. Still, this text-based serialization disregards critical relational structures, introduces redundancy, and often exceeds standard LLM context lengths. We introduce Rel-LLM, a novel architecture that utilizes a graph neural network (GNN)- based encoder to generate structured relational prompts for LLMs within a retrieval-augmented generation (RAG) framework. Unlike traditional text-based serialization approaches, our method preserves the inherent relational structure of databases while enabling LLMs to effectively process and reason over complex entity relationships. Specifically, the GNN encoder extracts a local subgraph around an entity to build feature representations that contain relevant entity relationships and temporal dependencies. These representations are transformed into structured prompts using a denormalization process, effectively allowing the LLM to reason over relational structures. Through extensive experiments, we demonstrate that Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and efficient approach to integrating LLMs with structured data sources. Code is available at https://github.com/smiles724/Rel-LLM.",Large Language Models Good Relational Learners Large language models LLMs demonstrated remarkable capabilities various domains application relational deep learning RDL remains underexplored Existing approaches adapt LLMs traversing relational links entities database converting structured data flat text documents text based serialization disregards critical relational structures introduces redundancy exceeds standard LLM context lengths introduce Rel LLM novel architecture utilizes graph neural network GNN based encoder generate structured relational prompts LLMs retrieval augmented generation RAG framework Unlike traditional text based serialization approaches method preserves inherent relational structure databases enabling LLMs effectively process reason complex entity relationships Specifically GNN encoder extracts local subgraph entity build feature representations contain relevant entity relationships temporal dependencies representations transformed structured prompts using denormalization process effectively allowing LLM reason relational structures extensive experiments demonstrate Rel LLM outperforms existing methods key RDL tasks offering scalable efficient approach integrating LLMs structured data sources Code available https github com smiles724 Rel LLM
409,SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data,"['Michael Ogezi', 'Freda Shi']","Vision-language models (VLMs) work well in tasks ranging from image captioning to visual question answering (VQA), yet they struggle with spatial reasoning, a key skill for understanding our physical world that humans excel at. We find that spatial relations are generally rare in widely used VL datasets, with only a few being well represented, while most form a long tail of underrepresented relations. This gap leaves VLMs ill-equipped to handle diverse spatial relationships. To bridge it, we construct a synthetic VQA dataset focused on spatial reasoning generated from hyper-detailed image descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset consists of 455k samples containing 3.4 million QA pairs. Trained on this dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements on spatial reasoning benchmarks, achieving up to a 49% performance gain on the What's Up benchmark, while maintaining strong results on general tasks. Our work narrows the gap between human and VLM spatial reasoning and makes VLMs more capable in real-world tasks such as robotics and navigation.",SpaRE Enhancing Spatial Reasoning Vision Language Models Synthetic Data Vision language models VLMs work tasks ranging image captioning visual question answering VQA struggle spatial reasoning key skill understanding physical world humans excel spatial relations generally rare widely used VL datasets represented form long tail underrepresented relations gap leaves VLMs ill equipped handle diverse spatial relationships bridge construct synthetic VQA dataset focused spatial reasoning generated hyper detailed image descriptions Localized Narratives DOCCI PixMo Cap dataset consists 455k samples containing 3 4 million QA pairs Trained dataset Spatial Reasoning Enhanced SpaRE VLMs strong improvements spatial reasoning benchmarks achieving 49 performance gain s benchmark maintaining strong results general tasks work narrows gap human VLM spatial reasoning makes VLMs capable real world tasks robotics navigation
410,Distilling an End-to-End Voice Assistant Without Instruction Training Data,"['William Barr Held', 'Yanzhe Zhang', 'Weiyan Shi', 'Minzhi Li', 'Michael J Ryan', 'Diyi Yang']","Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT) have led to models ``forgetting"" capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, we show that DiVA better meets user preferences, achieving a 72\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using $>$100x less training compute.",Distilling End End Voice Assistant Instruction Training Data Voice assistants Siri Google Assistant typically model audio text separately resulting lost speech information increased complexity Recent efforts address end end Speech Large Language Models LLMs trained supervised finetuning SFT led models forgetting capabilities text LLMs work proposes alternative paradigm training Speech LLMs instruction data using response text LLM transcripts self supervision Importantly process performed annotated responses Distilled Voice Assistant DiVA generalizes Spoken Question Answering Classification Translation Furthermore DiVA better meets user preferences achieving 72 win rate compared state art models like Qwen 2 Audio despite using 100x training compute
411,CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games,"['Shuhang Xu', 'Fangwei Zhong']","Metaphors are a crucial way for humans to express complex or subtle ideas by comparing one concept to another, often from a different domain. However, many large language models (LLMs) struggle to interpret and apply metaphors in multi-agent language games, hindering their ability to engage in covert communication and semantic evasion, which are crucial for strategic communication. To address this challenge, we introduce CoMet, a framework that enables LLM-based agents to engage in metaphor processing. CoMet combines a hypothesis-based metaphor reasoner with a metaphor generator that improves through self-reflection and knowledge integration. This enhances the agents' ability to interpret and apply metaphors, improving the strategic and nuanced quality of their interactions. We evaluate CoMet on two multi-agent language games - Undercover and Adversarial Taboo - which emphasize Covert Communication and Semantic Evasion. Experimental results demonstrate that CoMet significantly enhances the agents' ability to communicate strategically using metaphors.",CoMet Metaphor Driven Covert Communication Multi Agent Language Games Metaphors crucial way humans express complex subtle ideas comparing concept different domain large language models LLMs struggle interpret apply metaphors multi agent language games hindering ability engage covert communication semantic evasion crucial strategic communication address challenge introduce CoMet framework enables LLM based agents engage metaphor processing CoMet combines hypothesis based metaphor reasoner metaphor generator improves self reflection knowledge integration enhances agents ability interpret apply metaphors improving strategic nuanced quality interactions evaluate CoMet multi agent language games Undercover Adversarial Taboo emphasize Covert Communication Semantic Evasion Experimental results demonstrate CoMet significantly enhances agents ability communicate strategically using metaphors
412,CER: Confidence Enhanced Reasoning in LLMs,"['Ali Razghandi', 'Seyed Mohammad Hadi Hosseini', 'Mahdieh Soleymani Baghshah']","Ensuring the reliability of Large Language Models (LLMs) in complex reasoning tasks remains a formidable challenge, particularly in scenarios that demand precise mathematical calculations and knowledge-intensive open-domain generation. In this work, we introduce an uncertainty-aware framework designed to enhance the accuracy of LLM responses by systematically incorporating model confidence at critical decision points. We propose an approach that encourages multi-step reasoning in LLMs and quantify the confidence of intermediate answers such as numerical results in mathematical reasoning and proper nouns in open-domain generation. Then, the overall confidence of each reasoning chain is evaluated based on confidence of these critical intermediate steps. Finally, we aggregate the answer of generated response paths in a way that reflects the reliability of each generated content (as opposed to self-consistency in which each generated chain contributes equally to majority voting). We conducted extensive experiments in five datasets, three mathematical datasets and two open-domain datasets, using four LLMs. The results consistently validate the effectiveness of our novel confidence aggregation method, leading to an accuracy improvement of up to 7.4% and 5.8% over baseline approaches in math and open-domain generation tasks, respectively. Code is publicly available at https://github.com/ Aquasar11/CER.",CER Confidence Enhanced Reasoning LLMs Ensuring reliability Large Language Models LLMs complex reasoning tasks remains formidable challenge particularly scenarios demand precise mathematical calculations knowledge intensive open domain generation work introduce uncertainty aware framework designed enhance accuracy LLM responses systematically incorporating model confidence critical decision points propose approach encourages multi step reasoning LLMs quantify confidence intermediate answers numerical results mathematical reasoning proper nouns open domain generation overall confidence reasoning chain evaluated based confidence critical intermediate steps Finally aggregate answer generated response paths way reflects reliability generated content opposed self consistency generated chain contributes equally majority voting conducted extensive experiments datasets mathematical datasets open domain datasets using LLMs results consistently validate effectiveness novel confidence aggregation method leading accuracy improvement 7 4 5 8 baseline approaches math open domain generation tasks respectively Code publicly available https github com Aquasar11 CER
413,Watermarking Large Language Models: An Unbiased and Low-risk Method,"['Minjia Mao', 'Dongjun Wei', 'Zeyu Chen', 'Xiao Fang', 'Michael Chau']",,Watermarking Large Language Models Unbiased Low risk Method
414,On Synthetic Data Strategies for Domain-Specific Generative Retrieval,"['Haoyang Wen', 'Jiang Guo', 'Yi Zhang', 'Jiarong Jiang', 'Zhiguo Wang']","This paper investigates synthetic data generation strategies in developing generative retrieval models for domain-specific corpora, thereby addressing the scalability challenges inherent in manually annotating in-domain queries. We study the data strategies for a two-stage training framework: in the first stage, which focuses on learning to decode document identifiers from queries, we investigate LLM-generated queries across multiple granularity (e.g. chunks, sentences) and domain-relevant search constraints that can better capture nuanced relevancy signals. In the second stage, which aims to refine document ranking through preference learning, we explore the strategies for mining hard negatives based on the initial model's predictions. Experiments on public datasets over diverse domains demonstrate the effectiveness of our synthetic data generation and hard negative sampling approach.",Synthetic Data Strategies Domain Specific Generative Retrieval paper investigates synthetic data generation strategies developing generative retrieval models domain specific corpora addressing scalability challenges inherent manually annotating domain queries study data strategies stage training framework stage focuses learning decode document identifiers queries investigate LLM generated queries multiple granularity e g chunks sentences domain relevant search constraints better capture nuanced relevancy signals second stage aims refine document ranking preference learning explore strategies mining hard negatives based initial model s predictions Experiments public datasets diverse domains demonstrate effectiveness synthetic data generation hard negative sampling approach
415,LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates,"['Ying Shen', 'Lifu Huang']","Recent findings reveal that much of the knowledge in a Transformer-based Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where each FNN layer can be interpreted as the summation of sub-updates, each corresponding to a weighted column vector from the FFN's value parameter matrix that often encodes human-interpretable concepts. In light of this, we hypothesize that model performance and behaviors can be further enhanced and controlled by modulating the contributions of these sub-updates based on their relevance to the input or target output style, and propose LLMBRACES, a novel and efficient method that computes relevance scores associated with value vectors in FFN layers and leverages these scores to dynamically adjust the contribution of sub-updates. By optimizing sub-update contributions, LLMBRACES refines the prediction process, leading to more accurate and reliable outputs, much like a 'brace' providing support and stability. Moreover, LLMBRACES can be extended to support conditional control over generation characteristics, such as sentiment, thereby offering fine-grained steering of LLM outputs. Extensive experiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and Llama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both fine-tuning and zero-shot settings while requiring significantly fewer tunable parameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in sentiment-controlled generation and toxicity reduction, highlighting its potential for flexible, controlled text generation across applications.",LLM Braces Straightening LLM Predictions Relevant Sub Updates Recent findings reveal knowledge Transformer based Large Language Model LLM encoded feed forward FFN layers FNN layer interpreted summation sub updates corresponding weighted column vector FFN s value parameter matrix encodes human interpretable concepts light hypothesize model performance behaviors enhanced controlled modulating contributions sub updates based relevance input target output style propose LLMBRACES novel efficient method computes relevance scores associated value vectors FFN layers leverages scores dynamically adjust contribution sub updates optimizing sub update contributions LLMBRACES refines prediction process leading accurate reliable outputs like brace providing support stability LLMBRACES extended support conditional control generation characteristics sentiment offering fine grained steering LLM outputs Extensive experiments various LLMs including Qwen2 5 1 5B Llama2 7B Llama3 8B demonstrate LLMBRACES outperforms baseline approaches fine tuning zero shot settings requiring significantly fewer tunable parameters 75 fewer compared LoRA Furthermore LLMBRACES excels sentiment controlled generation toxicity reduction highlighting potential flexible controlled text generation applications
416,CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions,"['Tamer Alkhouli', 'Katerina Margatina', 'James Gung', 'Raphael Shu', 'Claudia Zaghi', 'MONICA SUNKARA', 'Yi Zhang']","We introduce Conversational Function-Calling Evaluation Through Turn-Level Interactions (CONFETTI), a conversational benchmark1 designed to evaluate the function-calling capabilities and response quality of large language models (LLMs). Current benchmarks lack comprehensive assessment of LLMs in complex conversational scenarios. CONFETTI addresses this gap through 109 human-simulated conversations, comprising 313 user turns and covering 86 APIs. These conversations explicitly target various conversational complexities, such as follow-ups, goal correction and switching, ambiguous and implicit goals. We perform off-policy turn-level evaluation using this benchmark targeting function-calling. Our benchmark also incorporates dialog act annotations to assess agent responses. We evaluate a series of state-of-the-art LLMs and analyze their performance with respect to the number of available APIs, conversation lengths, and chained function calling. Our results reveal that while some models are able to handle long conversations, and leverage more than 20+ APIs successfully, other models struggle with longer context or when increasing the number of APIs. We also report that the performance on chained function-calls is severely limited across the models. Overall, the top performing models on CONFETTI are Nova Pro (40.01%), Claude Sonnet v3.5 (35.46%) and Llama 3.1 405B (33.19%) followed by command-r-plus (31.18%) and Mistral-Large-2407 (30.07%).",CONFETTI Conversational Function Calling Evaluation Turn Level Interactions introduce Conversational Function Calling Evaluation Turn Level Interactions CONFETTI conversational benchmark1 designed evaluate function calling capabilities response quality large language models LLMs Current benchmarks lack comprehensive assessment LLMs complex conversational scenarios CONFETTI addresses gap 109 human simulated conversations comprising 313 user turns covering 86 APIs conversations explicitly target various conversational complexities follow ups goal correction switching ambiguous implicit goals perform policy turn level evaluation using benchmark targeting function calling benchmark incorporates dialog act annotations assess agent responses evaluate series state art LLMs analyze performance respect number available APIs conversation lengths chained function calling results reveal models able handle long conversations leverage 20 APIs successfully models struggle longer context increasing number APIs report performance chained function calls severely limited models Overall performing models CONFETTI Nova Pro 40 01 Claude Sonnet v3 5 35 46 Llama 3 1 405B 33 19 followed command r plus 31 18 Mistral Large 2407 30 07
417,Evaluating Theory of (an uncertain) Mind: Predicting the Uncertain Beliefs of Others from Conversational Cues,"['Anthony Sicilia', 'Malihe Alikhani']",,Evaluating Theory uncertain Mind Predicting Uncertain Beliefs Conversational Cues
418,Uncertainty in Causality: A New Frontier,"['Shaobo Cui', 'Luca Mouchel', 'Boi Faltings']",,Uncertainty Causality New Frontier
419,SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs,"['Michael J Ryan', 'Omar Shaikh', 'Aditri Bhagirath', 'Daniel Frees', 'William Barr Held', 'Diyi Yang']","Recent calls for pluralistic alignment of Large Language Models (LLMs) encourage adapting models to diverse user preferences. However, most prior work on personalized reward models heavily rely on additional identity information, such as demographic details or a predefined set of preference categories. To this end, we introduce SynthesizeMe, an approach to inducing synthetic user personas from user interactions for personalized reward modeling. SynthesizeMe first generates and verifies reasoning to explain user preferences, then induces synthetic user personas from that reasoning, and finally filters to informative prior user interactions in order to build personalized prompts for a particular user. We show that using SynthesizeMe induced prompts improves personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining SynthesizeMe derived prompts with a reward model achieves top performance on PersonalRewardBench: a new curation of user-stratified interactions with chatbots collected from 854 users of Chatbot Arena and PRISM.",SynthesizeMe Inducing Persona Guided Prompts Personalized Reward Models LLMs Recent calls pluralistic alignment Large Language Models LLMs encourage adapting models diverse user preferences prior work personalized reward models heavily rely additional identity information demographic details predefined set preference categories end introduce SynthesizeMe approach inducing synthetic user personas user interactions personalized reward modeling SynthesizeMe generates verifies reasoning explain user preferences induces synthetic user personas reasoning finally filters informative prior user interactions order build personalized prompts particular user using SynthesizeMe induced prompts improves personalized LLM judge accuracy 4 4 Chatbot Arena Combining SynthesizeMe derived prompts reward model achieves performance PersonalRewardBench new curation user stratified interactions chatbots collected 854 users Chatbot Arena PRISM
420,When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models,"['Julia Mendelsohn', 'Ceren Budak']","Metaphor, discussing one concept in terms of another, is abundant in politics and can shape how people understand important issues. We develop a computational approach to measure metaphorical language, focusing on immigration discourse on social media. Grounded in qualitative social science research, we identify seven concepts evoked in immigration discourse (e.g. ""water"" or ""vermin""). We propose and evaluate a novel technique that leverages both word-level and document-level signals to measure metaphor with respect to these concepts. We then study the relationship between metaphor, political ideology, and user engagement in 400K US tweets about immigration. While conservatives tend to use dehumanizing metaphors more than liberals, this effect varies widely across concepts. Moreover, creature-related metaphor is associated with more retweets, especially for liberal authors. Our work highlights the potential for computational methods to complement qualitative approaches in understanding subtle and implicit language in political discourse.",People Floods Analyzing Dehumanizing Metaphors Immigration Discourse Large Language Models Metaphor discussing concept terms abundant politics shape people understand important issues develop computational approach measure metaphorical language focusing immigration discourse social media Grounded qualitative social science research identify seven concepts evoked immigration discourse e g water vermin propose evaluate novel technique leverages word level document level signals measure metaphor respect concepts study relationship metaphor political ideology user engagement 400K tweets immigration conservatives tend use dehumanizing metaphors liberals effect varies widely concepts creature related metaphor associated retweets especially liberal authors work highlights potential computational methods complement qualitative approaches understanding subtle implicit language political discourse
421,AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection,"['Weidi Luo', 'Shenghong Dai', 'Xiaogeng Liu', 'Suman Banerjee', 'Huan Sun', 'Muhao Chen', 'Chaowei Xiao']","The rapid advancements in Large Language Models (LLMs) have enabled their deployment as autonomous agents for handling complex tasks in dynamic environments. These LLMs demonstrate strong problem-solving capabilities and adaptability to multifaceted scenarios. However, their use as agents also introduces significant risks, including task-specific risks, which are identified by the agent administrator based on the specific task requirements and constraints, and systemic risks, which stem from vulnerabilities in their design or interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security risks. Existing defense agencies fail to adaptively and effectively mitigate these risks. In this paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent safety, which features adaptive safety check generation, effective safety check optimization, and tool compatibility and flexibility. Extensive experiments demonstrate that AGrail not only achieves strong performance against task-specific and system risks but also exhibits transferability across different LLM agents' tasks.",AGrail Lifelong Agent Guardrail Effective Adaptive Safety Detection rapid advancements Large Language Models LLMs enabled deployment autonomous agents handling complex tasks dynamic environments LLMs demonstrate strong problem solving capabilities adaptability multifaceted scenarios use agents introduces significant risks including task specific risks identified agent administrator based specific task requirements constraints systemic risks stem vulnerabilities design interactions potentially compromising confidentiality integrity availability CIA information triggering security risks Existing defense agencies fail adaptively effectively mitigate risks paper propose AGrail lifelong agent guardrail enhance LLM agent safety features adaptive safety check generation effective safety check optimization tool compatibility flexibility Extensive experiments demonstrate AGrail achieves strong performance task specific risks exhibits transferability different LLM agents tasks
422,Improving Model Factuality with Fine-grained Critique-based Evaluator,"['Yiqing Xie', 'Wenxuan Zhou', 'Pradyot Prakash', 'Di Jin', 'Yuning Mao', 'Quintin Fettes', 'Arya Talebzadeh', 'Sinong Wang', 'Han Fang', 'Carolyn Rose', 'Daniel Fried', 'Hejia Zhang']","Factuality evaluation aims to detect factual errors produced by language models (LMs) and hence guide the development of more factual models. Towards this goal, we train a factuality evaluator, FenCE, that provides LM generators with claim-level factuality feedback. We conduct data augmentation on a combination of public judgment datasets to train FenCE to (1) generate textual critiques along with scores and (2) make claim-level judgment based on diverse source documents obtained by various tools. We then present a framework that leverages FenCE to improve the factuality of LM generators by constructing training data. Specifically, we generate a set of candidate responses, leverage FenCE to revise and score each response without introducing lesser-known facts, and train the generator by preferring highly scored revised responses. Experiments show that our data augmentation methods improve the evaluator's accuracy by 2.9% on LLM-AggreFact. With FenCE, we improve Llama2-7B-chat and Llama3-8B-chat's factuality rate by 16.86% and 14.45% on FActScore, outperforming state-of-the-art factuality finetuning methods by 8.83% and 6.96%.",Improving Model Factuality Fine grained Critique based Evaluator Factuality evaluation aims detect factual errors produced language models LMs guide development factual models goal train factuality evaluator FenCE provides LM generators claim level factuality feedback conduct data augmentation combination public judgment datasets train FenCE 1 generate textual critiques scores 2 make claim level judgment based diverse source documents obtained various tools present framework leverages FenCE improve factuality LM generators constructing training data Specifically generate set candidate responses leverage FenCE revise score response introducing lesser known facts train generator preferring highly scored revised responses Experiments data augmentation methods improve evaluator s accuracy 2 9 LLM AggreFact FenCE improve Llama2 7B chat Llama3 8B chat s factuality rate 16 86 14 45 FActScore outperforming state art factuality finetuning methods 8 83 6 96
423,Building a Long Text Privacy Policy Corpus with Multi-Class Labels,"['David Stein', 'Florencia Marotta-Wurgler']",,Building Long Text Privacy Policy Corpus Multi Class Labels
424,x-SAL: Leading Symbolic Reasoning across Languages via Cross-lingual Symbolic-Aided Language Model,"['Leonardo Ranaldi', 'Giulia Pucci']",,x SAL Leading Symbolic Reasoning Languages Cross lingual Symbolic Aided Language Model
425,When the LM misunderstood the human chuckled: Analyzing garden path effects in humans and language models,"['Samuel Joseph Amouyal', 'Aya Meltzer-Asscher', 'Jonathan Berant']","Modern Large Language Models (LLMs) have shown human-like abilities in many language tasks, sparking interest in comparing LLMs' and humans' language processing. In this paper, we conduct a detailed comparison of the two on a sentence comprehension task using garden-path constructions, which are notoriously challenging for humans. Based on psycholinguistic research, we formulate hypotheses on why garden-path sentences are hard, and test these hypotheses on human participants and a large suite of LLMs using comprehension questions. Our findings reveal that both LLMs and humans struggle with specific syntactic complexities, with some models showing high correlation with human comprehension. To complement our findings, we test LLM comprehension of garden-path constructions with paraphrasing and text-to-image generation tasks, and find that the results mirror the sentence comprehension question results, further validating our findings on LLM understanding of these constructions.",LM misunderstood human chuckled Analyzing garden path effects humans language models Modern Large Language Models LLMs shown human like abilities language tasks sparking comparing LLMs humans language processing paper conduct detailed comparison sentence comprehension task using garden path constructions notoriously challenging humans Based psycholinguistic research formulate hypotheses garden path sentences hard test hypotheses human participants large suite LLMs using comprehension questions findings reveal LLMs humans struggle specific syntactic complexities models showing high correlation human comprehension complement findings test LLM comprehension garden path constructions paraphrasing text image generation tasks results mirror sentence comprehension question results validating findings LLM understanding constructions
426,Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models,"['Zixiang Xu', 'Yanbo Wang', 'Yue Huang', 'Xiuying Chen', 'Jieyu Zhao', 'Meng Jiang', 'Xiangliang Zhang']","Large Language Models (LLMs) have achieved remarkable success in Natural Language Processing (NLP), yet their cross-lingual performance consistency remains a significant challenge. This paper introduces a novel methodology for efficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach leverages beam search and LLM-based simulation to generate bilingual question pairs that expose performance discrepancies between English and target languages. We construct a new dataset of over 6,000 bilingual pairs across 16 languages using this methodology, demonstrating its effectiveness in revealing weaknesses even in state-of-the-art models. The extensive experiments demonstrate that our method precisely and cost-effectively pinpoints cross-lingual weaknesses, consistently revealing over 50\% accuracy drops in target languages across a wide range of models. Moreover, further experiments investigate the relationship between linguistic similarity and cross-lingual weaknesses, revealing that linguistically related languages share similar performance patterns and benefit from targeted post-training. Code is available at https://github.com/xzx34/Cross-Lingual-Pitfalls.",Cross Lingual Pitfalls Automatic Probing Cross Lingual Weakness Multilingual Large Language Models Large Language Models LLMs achieved remarkable success Natural Language Processing NLP cross lingual performance consistency remains significant challenge paper introduces novel methodology efficiently identifying inherent cross lingual weaknesses LLMs approach leverages beam search LLM based simulation generate bilingual question pairs expose performance discrepancies English target languages construct new dataset 6 000 bilingual pairs 16 languages using methodology demonstrating effectiveness revealing weaknesses state art models extensive experiments demonstrate method precisely cost effectively pinpoints cross lingual weaknesses consistently revealing 50 accuracy drops target languages wide range models experiments investigate relationship linguistic similarity cross lingual weaknesses revealing linguistically related languages share similar performance patterns benefit targeted post training Code available https github com xzx34 Cross Lingual Pitfalls
427,VLSBench: Unveiling Visual Leakage in Multimodal Safety,"['Xuhao Hu', 'Dongrui Liu', 'Hao Li', 'Xuanjing Huang', 'Jing Shao']","Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counterintuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs aligned with image text pairs. To explain such a phenomenon, we discover a Visual Safety Information Leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky content in the image has been revealed in the textual query. Thus, MLLMs can easily refuse these sensitive image-text pairs according to textual queries only, leading to unreliable cross-modality safety evaluation of MLLMs. We also conduct a further comparison experiment between textual alignment and multimodal alignment to highlight this drawback. To this end, we construct multimodal Visual Leakless Safety Bench (VLSBench) with 2.2k image-text pairs through an automated data pipeline. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, e.g., LLaVA, Qwen2-VL and GPT-4o. Besides, we empirically compare textual and multimodal alignment methods on VLSBench and find that textual alignment is effective enough for multimodal safety scenarios with VSIL, while multimodal alignment is preferable for safety scenarios without VSIL. Code and data are released under https://github.com/AI45Lab/VLSBench",VLSBench Unveiling Visual Leakage Multimodal Safety Safety concerns Multimodal large language models MLLMs gradually important problem various applications Surprisingly previous works indicate counterintuitive phenomenon using textual unlearning align MLLMs achieves comparable safety performances MLLMs aligned image text pairs explain phenomenon discover Visual Safety Information Leakage VSIL problem existing multimodal safety benchmarks e potentially risky content image revealed textual query MLLMs easily refuse sensitive image text pairs according textual queries leading unreliable cross modality safety evaluation MLLMs conduct comparison experiment textual alignment multimodal alignment highlight drawback end construct multimodal Visual Leakless Safety Bench VLSBench 2 2k image text pairs automated data pipeline Experimental results indicate VLSBench poses significant challenge open source close source MLLMs e g LLaVA Qwen2 VL GPT 4o empirically compare textual multimodal alignment methods VLSBench textual alignment effective multimodal safety scenarios VSIL multimodal alignment preferable safety scenarios VSIL Code data released https github com AI45Lab VLSBench
428,Browsing Lost Unformed Recollections: A Benchmark for Tip-of-the-Tongue Search and Reasoning,"['Sky CH-Wang', 'Darshan Girish Deshpande', 'Smaranda Muresan', 'Anand Kannappan', 'Rebecca Qian']","We introduce Browsing Lost Unformed Recollections, a tip-of-the-tongue known-item search and reasoning benchmark for general AI assistants. BLUR introduces a set of 573 real-world validated questions that demand searching and reasoning across multi-modal and multilingual inputs, as well as proficient tool use, in order to excel on. Humans easily ace these questions (scoring on average 98%), while the best-performing system scores around 56%. To facilitate progress toward addressing this challenging and aspirational use case for general AI assistants, we release 350 questions through a public leaderboard, retain the answers to 250 of them, and have the rest as a private test set.",Browsing Lost Unformed Recollections Benchmark Tip Tongue Search Reasoning introduce Browsing Lost Unformed Recollections tip tongue known item search reasoning benchmark general AI assistants BLUR introduces set 573 real world validated questions demand searching reasoning multi modal multilingual inputs proficient tool use order excel Humans easily ace questions scoring average 98 best performing scores 56 facilitate progress addressing challenging aspirational use case general AI assistants release 350 questions public leaderboard retain answers 250 rest private test set
429,"Subword models struggle with word learning, but surprisal hides it","['Bastian Bunzeck', 'Sina Zarrieß']","We study word learning in subword and character language models with the psycholinguistic lexical decision task. While subword LMs struggle to discern words and non-words with high accuracy, character LMs solve this task easily and consistently. Only when supplied with further contexts do subword LMs perform similarly to character models. Additionally, when looking at word-level and syntactic learning trajectories, we find that both processes are separable in character LMs. Word learning happens before syntactic learning, whereas both occur simultaneously in subword LMs. This raises questions about the adequacy of subword LMs for modeling language acquisition and positions character LMs as a viable alternative to study processes below the syntactic level.",Subword models struggle word learning surprisal hides study word learning subword character language models psycholinguistic lexical decision task subword LMs struggle discern words non words high accuracy character LMs solve task easily consistently supplied contexts subword LMs perform similarly character models Additionally looking word level syntactic learning trajectories processes separable character LMs Word learning happens syntactic learning occur simultaneously subword LMs raises questions adequacy subword LMs modeling language acquisition positions character LMs viable alternative study processes syntactic level
430,Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation,"['Jonibek Mansurov', 'Akhmed Sakip', 'Alham Fikri Aji']","In this paper, we show that knowledge distillation can be subverted to manipulate language model benchmark scores, revealing a critical vulnerability in current evaluation practices. We introduce ""Data Laundering,"" a process that enables the covert transfer of benchmark-specific knowledge through seemingly legitimate intermediate training steps. Through extensive experiments with a 2-layer BERT student model, we show how this approach can achieve substantial improvements in benchmark accuracy (up to 75\% on GPQA) without developing genuine reasoning capabilities. Notably, this method can be exploited intentionally or even unintentionally, as researchers may inadvertently adopt this method and inflate scores without realising the implications. While our findings demonstrate the effectiveness of this technique, we present them as a cautionary tale highlighting the urgent need for more robust evaluation methods in AI. This work aims to contribute to the ongoing discussion about evaluation integrity in AI development and the need for benchmarks that more accurately reflect true model capabilities. The code is available at https://github.com/mbzuai-nlp/data_laundering.",Data Laundering Artificially Boosting Benchmark Results Knowledge Distillation paper knowledge distillation subverted manipulate language model benchmark scores revealing critical vulnerability current evaluation practices introduce Data Laundering process enables covert transfer benchmark specific knowledge seemingly legitimate intermediate training steps extensive experiments 2 layer BERT student model approach achieve substantial improvements benchmark accuracy 75 GPQA developing genuine reasoning capabilities Notably method exploited intentionally unintentionally researchers inadvertently adopt method inflate scores realising implications findings demonstrate effectiveness technique present cautionary tale highlighting urgent need robust evaluation methods AI work aims contribute ongoing discussion evaluation integrity AI development need benchmarks accurately reflect true model capabilities code available https github com mbzuai nlp data_laundering
431,Conspiracy Theories and Where to Find Them on TikTok,"['Francesco Corso', 'Francesco Pierri', 'Gianmarco De Francisci Morales']",,Conspiracy Theories TikTok
432,Growing Through Experience: Scaling Episodic Grounding in Language Models,"['Chunhui Zhang', 'Sirui Wang', 'Zhongyu Ouyang', 'Xiangchi Yuan', 'Soroush Vosoughi']","Language models (LMs) require robust episodic grounding-the capacity to learn from and apply past experiences-to excel at physical planning tasks. Current episodic grounding approaches struggle with scalability and integration, limiting their effectiveness, especially for medium-sized LMs (7B parameters). While larger LMs (70-405B parameters) possess superior hierarchical representations and extensive pre-trained knowledge, they encounter a fundamental scale paradox: despite their advanced abstraction capabilities, they lack efficient mechanisms to leverage experience streams. We propose a scalable weak-to-strong episodic learning framework that effectively transfers episodic behaviors from smaller to larger LMs. This framework integrates Monte Carlo tree search for structured experience collection with a novel distillation method, preserving the inherent LM capabilities while embedding episodic memory. Experiments demonstrate our method surpasses state-of-the-art proprietary LMs by 3.45% across diverse planning and question-answering tasks. Layer-wise probing further indicates significant improvements in task alignment, especially within deeper LM layers, highlighting stable generalization even for previously unseen scenarios with increased planning complexity-conditions where baseline methods degrade markedly.",Growing Experience Scaling Episodic Grounding Language Models Language models LMs require robust episodic grounding capacity learn apply past experiences excel physical planning tasks Current episodic grounding approaches struggle scalability integration limiting effectiveness especially medium sized LMs 7B parameters larger LMs 70 405B parameters possess superior hierarchical representations extensive pre trained knowledge encounter fundamental scale paradox despite advanced abstraction capabilities lack efficient mechanisms leverage experience streams propose scalable weak strong episodic learning framework effectively transfers episodic behaviors smaller larger LMs framework integrates Monte Carlo tree search structured experience collection novel distillation method preserving inherent LM capabilities embedding episodic memory Experiments demonstrate method surpasses state art proprietary LMs 3 45 diverse planning question answering tasks Layer wise probing indicates significant improvements task alignment especially deeper LM layers highlighting stable generalization previously unseen scenarios increased planning complexity conditions baseline methods degrade markedly
433,LLM as Entity Disambiguator for Biomedical Entity-Linking,"['Christophe Ye', 'Cassie S. Mitchell']",,LLM Entity Disambiguator Biomedical Entity Linking
434,Exploiting the Shadows: Unveiling Privacy Leaks through Lower-Ranked Tokens in Large Language Models,"['Yuan Zhou', 'ZHUO ZHANG', 'Xiangyu Zhang']",,Exploiting Shadows Unveiling Privacy Leaks Lower Ranked Tokens Large Language Models
435,Towards Geo-Culturally Grounded LLM Generations,"['Piyawat Lertvittayakumjorn', 'David Kinney', 'Vinodkumar Prabhakaran', 'Donald Martin Jr.', 'Sunipa Dev']","Generative large language models (LLMs) have demonstrated gaps in diverse cultural awareness across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on LLMs' ability to display familiarity with various national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on multiple cultural awareness benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., cultural norms, artifacts, and institutions), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models and fails to improve evaluators' judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional cultural knowledge and open-ended cultural fluency when it comes to evaluating LLMs' cultural awareness.",Geo Culturally Grounded LLM Generations Generative large language models LLMs demonstrated gaps diverse cultural awareness globe investigate effect retrieval augmented generation search grounding techniques LLMs ability display familiarity various national cultures Specifically compare performance standard LLMs LLMs augmented retrievals bespoke knowledge base e KB grounding LLMs augmented retrievals web search e search grounding multiple cultural awareness benchmarks search grounding significantly improves LLM performance multiple choice benchmarks test propositional knowledge e g cultural norms artifacts institutions KB grounding s effectiveness limited inadequate knowledge base coverage suboptimal retriever search grounding increases risk stereotypical judgments language models fails improve evaluators judgments cultural familiarity human evaluation adequate statistical power results highlight distinction propositional cultural knowledge open ended cultural fluency comes evaluating LLMs cultural awareness
436,Attacking Vision-Language Computer Agents via Pop-ups,"['Yanzhe Zhang', 'Tao Yu', 'Diyi Yang']","Autonomous agents powered by large vision and language models (VLM) have demonstrated significant potential in completing daily computer tasks, such as browsing the web to book travel and operating desktop software, which requires agents to understand these interfaces. Despite such visual inputs becoming more integrated into agentic applications, what types of risks and attacks exist around them still remain unclear. In this work, we demonstrate that VLM agents can be easily attacked by a set of carefully designed adversarial pop-ups, which human users would typically recognize and ignore. This distraction leads agents to click these pop-ups instead of performing their tasks as usual. Integrating these pop-ups into existing agent testing environments like OSWorld and VisualWebArena leads to an attack success rate (the frequency of the agent clicking the pop-ups) of 86% on average and decreases the task success rate by 47%. Basic defense techniques, such as asking the agent to ignore pop-ups or including an advertisement notice, are ineffective against the attack.",Attacking Vision Language Computer Agents Pop ups Autonomous agents powered large vision language models VLM demonstrated significant potential completing daily computer tasks browsing web book travel operating desktop software requires agents understand interfaces Despite visual inputs integrated agentic applications types risks attacks exist remain unclear work demonstrate VLM agents easily attacked set carefully designed adversarial pop ups human users typically recognize ignore distraction leads agents click pop ups instead performing tasks usual Integrating pop ups existing agent testing environments like OSWorld VisualWebArena leads attack success rate frequency agent clicking pop ups 86 average decreases task success rate 47 Basic defense techniques asking agent ignore pop ups including advertisement notice ineffective attack
437,Explicit and Implicit Data Augmentation for Social Event Detection,"['Congbo Ma', 'Yuxia Wang', 'Jia Wu', 'Jian Yang', 'Jing Du', 'Zitai Qiu', 'Qing Li', 'Hu Wang', 'Preslav Nakov']",,Explicit Implicit Data Augmentation Social Event Detection
438,In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents,"['Zhen Tan', 'Jun Yan', 'I-Hung Hsu', 'Rujun Han', 'Zifeng Wang', 'Long Le', 'Yiwen Song', 'Yanfei Chen', 'Hamid Palangi', 'George Lee', 'Anand Rajan Iyer', 'Tianlong Chen', 'huan liu', 'Chen-Yu Lee', 'Tomas Pfister']","Large Language Models (LLMs) have made significant progress in open-ended dialogue, yet their inability to retain and retrieve relevant information from long-term interactions limits their effectiveness in applications requiring sustained personalization. External memory mechanisms have been proposed to address this limitation, enabling LLMs to maintain conversational continuity. However, existing approaches struggle with two key challenges. First, rigid memory granularity fails to capture the natural semantic structure of conversations, leading to fragmented and incomplete representations. Second, fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user interaction patterns. In this work, we propose Reflective Memory Management (RMM), a novel mechanism for long-term dialogue agents, integrating forward- and backward-looking reflections: (1) Prospective Reflection, which dynamically summarizes interactions across granularities-utterances, turns, and sessions-into a personalized memory bank for effective future retrieval, and (2) Retrospective Reflection, which iteratively refines the retrieval in an online reinforcement learning (RL) manner based on LLMs' cited evidence. Experiments show that RMM demonstrates consistent improvement across various metrics and benchmarks. For example, RMM shows more than 10% accuracy improvement over the baseline without memory management on the LongMemEval dataset.",Prospect Retrospect Reflective Memory Management Long term Personalized Dialogue Agents Large Language Models LLMs significant progress open ended dialogue inability retain retrieve relevant information long term interactions limits effectiveness applications requiring sustained personalization External memory mechanisms proposed address limitation enabling LLMs maintain conversational continuity existing approaches struggle key challenges rigid memory granularity fails capture natural semantic structure conversations leading fragmented incomplete representations Second fixed retrieval mechanisms adapt diverse dialogue contexts user interaction patterns work propose Reflective Memory Management RMM novel mechanism long term dialogue agents integrating forward backward looking reflections 1 Prospective Reflection dynamically summarizes interactions granularities utterances turns sessions personalized memory bank effective future retrieval 2 Retrospective Reflection iteratively refines retrieval online reinforcement learning RL manner based LLMs cited evidence Experiments RMM demonstrates consistent improvement various metrics benchmarks example RMM shows 10 accuracy improvement baseline memory management LongMemEval dataset
439,Revisiting Classical Chinese Event Extraction with Ancient Literature Information,"['Xiaoyi Bao', 'Zhongqing Wang', 'Jinghang Gu', 'Chu-Ren Huang']",,Revisiting Classical Chinese Event Extraction Ancient Literature Information
440,Unanswerability Evaluation for Retrieval Augmented Generation,"['XIANGYU PENG', 'Prafulla Kumar Choubey', 'Caiming Xiong', 'Chien-Sheng Wu']","Existing evaluation frameworks for retrieval-augmented generation (RAG) systems focus on answerable queries, but they overlook the importance of appropriately rejecting unanswerable requests. In this paper, we introduce UAEval4RAG, a framework designed to evaluate whether RAG systems can handle unanswerable queries effectively. We define a taxonomy with six unanswerable categories, and UAEval4RAG automatically synthesizes diverse and challenging queries for any given knowledge base with unanswered ratio and acceptable ratio metrics. We conduct experiments with various RAG components, including retrieval models, rewriting methods, rerankers, language models, and prompting strategies, and reveal hidden trade-offs in performance of RAG systems. Our findings highlight the critical role of component selection and prompt design in optimizing RAG systems to balance the accuracy of answerable queries with high rejection rates of unanswerable ones. UAEval4RAG provides valuable insights and tools for developing more robust and reliable RAG systems.",Unanswerability Evaluation Retrieval Augmented Generation Existing evaluation frameworks retrieval augmented generation RAG systems focus answerable queries overlook importance appropriately rejecting unanswerable requests paper introduce UAEval4RAG framework designed evaluate RAG systems handle unanswerable queries effectively define taxonomy unanswerable categories UAEval4RAG automatically synthesizes diverse challenging queries given knowledge base unanswered ratio acceptable ratio metrics conduct experiments various RAG components including retrieval models rewriting methods rerankers language models prompting strategies reveal hidden trade offs performance RAG systems findings highlight critical role component selection prompt design optimizing RAG systems balance accuracy answerable queries high rejection rates unanswerable ones UAEval4RAG provides valuable insights tools developing robust reliable RAG systems
441,SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention,"['Chengshuai Zhao', 'Zhen Tan', 'Chau-Wai Wong', 'Xinyan Zhao', 'Tianlong Chen', 'huan liu']","Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\underline{\textbf{S}}$imulates $\underline{\textbf{C}}$ontent $\underline{\textbf{A}}$nalysis via $\underline{\textbf{L}}$arge language model (LLM) ag$\underline{\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.",SCALE Collaborative Content Analysis Social Science Large Language Model Agents Human Intervention Content analysis breaks complex unstructured texts theory informed numerical categories Particularly social science process usually relies multiple rounds manual annotation domain expert discussion rule based refinement paper introduce SCALE novel multi agent framework effectively underline textbf S imulates underline textbf C ontent underline textbf nalysis underline textbf L arge language model LLM ag underline textbf E nts SCALE imitates key phases content analysis including text coding collaborative discussion dynamic codebook evolution capturing reflective depth adaptive discussions human researchers Furthermore integrating diverse modes human intervention SCALE augmented expert input enhance performance Extensive evaluations real world datasets demonstrate SCALE achieves human approximated performance various complex content analysis tasks offering innovative potential future social science research
442,Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning,"['Erxin Yu', 'Jing Li', 'Ming Liao', 'Qi Zhu', 'Boyang XUE', 'Minghui Xu', 'Baojun Wang', 'Lanqing HONG', 'Fei Mi', 'Lifeng Shang']","Although large language models demonstrate strong performance across various domains, they still struggle with numerous bad cases in mathematical reasoning. Previous approaches to learning from errors synthesize training data by solely extrapolating from isolated bad cases, thereby failing to generalize the extensive patterns inherent within these cases. This paper presents Self-Error-Instruct (SEI), a framework that addresses these model weaknesses and synthesizes more generalized targeted training data. Specifically, we explore a target model on two mathematical datasets, GSM8K and MATH, to pinpoint bad cases. Then, we generate error keyphrases for these cases based on the instructor model's (GPT-4o) analysis and identify error types by clustering these keyphrases. Next, we sample a few bad cases during each generation for each identified error type and input them into the instructor model, which synthesizes additional training data using a self-instruct approach. This new data is refined through a one-shot learning process to ensure that only the most effective examples are kept. Finally, we use these curated data to fine-tune the target model, iteratively repeating the process to enhance performance. We apply our framework to various models and observe improvements in their reasoning abilities across both in-domain and out-of-domain mathematics datasets. These results demonstrate the effectiveness of self-error instruction in improving LLMs' mathematical reasoning through error generalization.",Self Error Instruct Generalizing Errors LLMs Mathematical Reasoning large language models demonstrate strong performance various domains struggle numerous bad cases mathematical reasoning Previous approaches learning errors synthesize training data solely extrapolating isolated bad cases failing generalize extensive patterns inherent cases paper presents Self Error Instruct SEI framework addresses model weaknesses synthesizes generalized targeted training data Specifically explore target model mathematical datasets GSM8K MATH pinpoint bad cases generate error keyphrases cases based instructor model s GPT 4o analysis identify error types clustering keyphrases sample bad cases generation identified error type input instructor model synthesizes additional training data using self instruct approach new data refined shot learning process ensure effective examples kept Finally use curated data fine tune target model iteratively repeating process enhance performance apply framework various models observe improvements reasoning abilities domain domain mathematics datasets results demonstrate effectiveness self error instruction improving LLMs mathematical reasoning error generalization
443,RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework,"['Kunlun Zhu', 'Yifan Luo', 'Dingling Xu', 'Yukun Yan', 'Zhenghao Liu', 'Shi Yu', 'Ruobing Wang', 'Shuo Wang', 'Yishan Li', 'Nan Zhang', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun']","Retrieval-Augmented Generation (RAG) is a powerful approach that enables large language models (LLMs) to incorporate external knowledge. However, evaluating the effectiveness of RAG systems in specialized scenarios remains challenging due to the high costs of data construction and the lack of suitable evaluation metrics. This paper introduces RAGEval, a framework designed to assess RAG systems across diverse scenarios by generating high-quality documents, questions, answers, and references through a schema-based pipeline. With a focus on factual accuracy, we propose three novel metrics: Completeness, Hallucination, and Irrelevance to evaluate LLM generated responses rigorously. Experimental results show that RAGEval outperforms zero-shot and one-shot methods in terms of clarity, safety, conformity, and richness of generated samples. Furthermore, the use of LLMs for scoring the proposed metrics demonstrates a high level of consistency with human evaluations. RAGEval establishes a new paradigm for evaluating RAG systems in real-world applications. The code and dataset are released at https://github.com/OpenBMB/RAGEval.",RAGEval Scenario Specific RAG Evaluation Dataset Generation Framework Retrieval Augmented Generation RAG powerful approach enables large language models LLMs incorporate external knowledge evaluating effectiveness RAG systems specialized scenarios remains challenging high costs data construction lack suitable evaluation metrics paper introduces RAGEval framework designed assess RAG systems diverse scenarios generating high quality documents questions answers references schema based pipeline focus factual accuracy propose novel metrics Completeness Hallucination Irrelevance evaluate LLM generated responses rigorously Experimental results RAGEval outperforms zero shot shot methods terms clarity safety conformity richness generated samples Furthermore use LLMs scoring proposed metrics demonstrates high level consistency human evaluations RAGEval establishes new paradigm evaluating RAG systems real world applications code dataset released https github com OpenBMB RAGEval
444,A Survey on Patent Analysis: From NLP to Multimodal AI,"['Homaira Huda Shomee', 'Zhu Wang', 'Sathya N. Ravi', 'Sourav Medya']","Recent advances in Pretrained Language Models (PLMs) and Large Language Models (LLMs) have demonstrated transformative capabilities across diverse domains. The field of patent analysis and innovation is not an exception, where natural language processing (NLP) techniques presents opportunities to streamline and enhance important tasks -- such as patent classification and patent retrieval -- in the patent cycle. This not only accelerates the efficiency of patent researchers and applicants, but also opens new avenues for technological innovation and discovery. Our survey provides a comprehensive summary of recent NLP-based methods -- including multimodal ones -- in patent analysis. We also introduce a novel taxonomy for categorization based on tasks in the patent life cycle, as well as the specifics of the methods. This interdisciplinary survey aims to serve as a comprehensive resource for researchers and practitioners who work at the intersection of NLP, Multimodal AI, and patent analysis, as well as patent offices to build efficient patent systems.",Survey Patent Analysis NLP Multimodal AI Recent advances Pretrained Language Models PLMs Large Language Models LLMs demonstrated transformative capabilities diverse domains field patent analysis innovation exception natural language processing NLP techniques presents opportunities streamline enhance important tasks patent classification patent retrieval patent cycle accelerates efficiency patent researchers applicants opens new avenues technological innovation discovery survey provides comprehensive summary recent NLP based methods including multimodal ones patent analysis introduce novel taxonomy categorization based tasks patent life cycle specifics methods interdisciplinary survey aims serve comprehensive resource researchers practitioners work intersection NLP Multimodal AI patent analysis patent offices build efficient patent systems
445,SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification,"['Chengye Wang', 'Yifei Shen', 'Zexi Kuang', 'Arman Cohan', 'Yilun Zhao']","We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.",SciVer Evaluating Foundation Models Multimodal Scientific Claim Verification introduce SciVer benchmark specifically designed evaluate ability foundation models verify claims multimodal scientific context SciVer consists 3 000 expert annotated examples 1 113 scientific papers covering subsets representing common reasoning type multimodal scientific claim verification enable fine grained evaluation example includes expert annotated supporting evidence assess performance 21 state art multimodal foundation models including o4 mini Gemini 2 5 Flash Llama 3 2 Vision Qwen2 5 VL experiment reveals substantial performance gap models human experts SciVer depth analysis retrieval augmented generation RAG human conducted error evaluations identify critical limitations current open source models offering key insights advance models comprehension reasoning multimodal scientific literature tasks
446,MultiAgentBench : Evaluating the Collaboration and Competition of LLM agents,"['Kunlun Zhu', 'Hongyi Du', 'Zhaochen Hong', 'Xiaocheng Yang', 'Shuyi Guo', 'Zhe Wang', 'Zhenhailong Wang', 'Cheng Qian', 'Xiangru Tang', 'Heng Ji', 'Jiaxuan You']",,MultiAgentBench Evaluating Collaboration Competition LLM agents
447,Sinhala Encoder-only Language Models and Evaluation,"['Tharindu Ranasinghe', 'Hansi Hettiarachchi', 'Nadeesha Chathurangi Naradde Vidana Pathirana', 'Damith Premasiri', 'Lasitha Uyangodage', 'Isuri Nanomi Arachchige', 'Alistair Plum', 'Paul Rayson', 'Ruslan Mitkov']",,Sinhala Encoder Language Models Evaluation
448,LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing,"['Zhengxiang Wang', 'Veronika Makarova', 'Zhi Li', 'Jordan Kodner', 'Owen Rambow']","The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus and code for reproducibility.",LLMs Perform Multi Dimensional Analytic Writing Assessments Case Study L2 Graduate Level Academic English Writing paper explores performance LLMs context multi dimensional analytic writing assessments e ability provide scores comments based multiple assessment criteria Using corpus literature reviews written L2 graduate students assessed human experts 9 analytic criteria prompt popular LLMs perform task various conditions evaluate quality feedback comments apply novel feedback comment quality evaluation framework framework interpretable cost efficient scalable reproducible compared existing methods rely manual judgments LLMs generate reasonably good generally reliable multi dimensional analytic assessments release corpus code reproducibility
449,SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?,"['Haomin Zhuang', 'Yihua Zhang', 'Kehan Guo', 'Jinghan Jia', 'Gaowen Liu', 'Sijia Liu', 'Xiangliang Zhang']","Recent advancements in LLMs unlearning have shown remarkable success in removing unwanted data-model influences while preserving the model's utility for legitimate knowledge. Despite these strides, sparse Mixture-of-Experts (MoE) LLMs--a key subset of the LLM family--have remained unexplored in the context of unlearning. As MoE LLMs are celebrated for their exceptional performance, we ask:How can unlearning be performed effectively and efficiently on MoE LLMs? Our pilot study shows that the dynamic routing nature of MoE LLMs introduces unique challenges, leading to excessive forgetting, uncontrolled knowledge erasure and substantial utility drops when existing unlearning methods are applied. To address this, we propose a novel Selected-Expert Unlearning Framework (SEUF). Through expert attribution, unlearning is concentrated on the most actively engaged experts for the specified knowledge. Concurrently, an anchor loss is applied to the router to stabilize the active state of this targeted expert, ensuring focused and controlled unlearning. SEUF is compatible with various standard unlearning algorithms. Extensive experiments demonstrate that SEUF enhances both forget quality up to 5% and model utility by 35% on MoE LLMs across various benchmarks and LLM architectures (compared to standard unlearning algorithms), while only unlearning 0.06% of the model parameters.",SEUF Unlearning Expert Mixture Experts LLMs Recent advancements LLMs unlearning shown remarkable success removing unwanted data model influences preserving model s utility legitimate knowledge Despite strides sparse Mixture Experts MoE LLMs key subset LLM family remained unexplored context unlearning MoE LLMs celebrated exceptional performance ask unlearning performed effectively efficiently MoE LLMs pilot study shows dynamic routing nature MoE LLMs introduces unique challenges leading excessive forgetting uncontrolled knowledge erasure substantial utility drops existing unlearning methods applied address propose novel Selected Expert Unlearning Framework SEUF expert attribution unlearning concentrated actively engaged experts specified knowledge Concurrently anchor loss applied router stabilize active state targeted expert ensuring focused controlled unlearning SEUF compatible various standard unlearning algorithms Extensive experiments demonstrate SEUF enhances forget quality 5 model utility 35 MoE LLMs various benchmarks LLM architectures compared standard unlearning algorithms unlearning 0 06 model parameters
450,"Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges","['Bolei Ma', 'Yuting Li', 'Wei Zhou', 'Ziwei Gong', 'Yang Janet Liu', 'Katja Jasinskaja', 'Annemarie Friedrich', 'Julia Hirschberg', 'Frauke Kreuter', 'Barbara Plank']","Understanding pragmatics-the use of language in context-is crucial for developing NLP systems capable of interpreting nuanced language use. Despite recent advances in language technologies, including large language models, evaluating their ability to handle pragmatic phenomena such as implicatures and references remains challenging. To advance pragmatic abilities in models, it is essential to understand current evaluation trends and identify existing limitations. In this survey, we provide a comprehensive review of resources designed for evaluating pragmatic capabilities in NLP, categorizing datasets by the pragmatic phenomena they address. We analyze task designs, data collection methods, evaluation approaches, and their relevance to real-world applications. By examining these resources in the context of modern language models, we highlight emerging trends, challenges, and gaps in existing benchmarks. Our survey aims to clarify the landscape of pragmatic evaluation and guide the development of more comprehensive and targeted benchmarks, ultimately contributing to more nuanced and context-aware NLP models.",Pragmatics Era Large Language Models Survey Datasets Evaluation Opportunities Challenges Understanding pragmatics use language context crucial developing NLP systems capable interpreting nuanced language use Despite recent advances language technologies including large language models evaluating ability handle pragmatic phenomena implicatures references remains challenging advance pragmatic abilities models essential understand current evaluation trends identify existing limitations survey provide comprehensive review resources designed evaluating pragmatic capabilities NLP categorizing datasets pragmatic phenomena address analyze task designs data collection methods evaluation approaches relevance real world applications examining resources context modern language models highlight emerging trends challenges gaps existing benchmarks survey aims clarify landscape pragmatic evaluation guide development comprehensive targeted benchmarks ultimately contributing nuanced context aware NLP models
451,LocAgent: Agentic Code Localization with Graph-Based Indexing,"['Zhaoling Chen', 'Xiangru Tang', 'Gangda Deng', 'Fang Wu', 'Jialong Wu', 'Zhiwei Jiang', 'Viktor Prasanna', 'Arman Cohan', 'Xingyao Wang']",,LocAgent Agentic Code Localization Graph Based Indexing
452,COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation,"['Raghvendra Kumar', 'Mohammed Salman S A', 'Aryan Sahu', 'Tridib Nandi', 'Pragathi Y P', 'Sriparna Saha', 'Jose G Moreno']","Despite progress in comment-aware multimodal and multilingual summarization for English and Chinese, research in Indian languages remains limited. This study addresses this gap by introducing COSMMIC, a pioneering comment-sensitive multimodal, multilingual dataset featuring nine major Indian languages. COSMMIC comprises 4,959 article-image pairs and 24,484 reader comments, with ground-truth summaries available in all included languages. Our approach enhances summaries by integrating reader insights and feedback. We explore summarization and headline generation across four configurations: (1) using article text alone, (2) incorporating user comments, (3) utilizing images, and (4) combining text, comments, and images. To assess the dataset's effectiveness, we employ state-of-the-art language models such as LLama3 and GPT-4. We conduct a comprehensive study to evaluate different component combinations, including identifying supportive comments, filtering out noise using a dedicated comment classifier using IndicBERT, and extracting valuable insights from images with a multilingual CLIP-based classifier. This helps determine the most effective configurations for natural language generation (NLG) tasks. Unlike many existing datasets that are either text-only or lack user comments in multimodal settings, COSMMIC uniquely integrates text, images, and user feedback. This holistic approach bridges gaps in Indian language resources, advancing NLP research and fostering inclusivity.",COSMMIC Comment Sensitive Multimodal Multilingual Indian Corpus Summarization Headline Generation Despite progress comment aware multimodal multilingual summarization English Chinese research Indian languages remains limited study addresses gap introducing COSMMIC pioneering comment sensitive multimodal multilingual dataset featuring major Indian languages COSMMIC comprises 4 959 article image pairs 24 484 reader comments ground truth summaries available included languages approach enhances summaries integrating reader insights feedback explore summarization headline generation configurations 1 using article text 2 incorporating user comments 3 utilizing images 4 combining text comments images assess dataset s effectiveness employ state art language models LLama3 GPT 4 conduct comprehensive study evaluate different component combinations including identifying supportive comments filtering noise using dedicated comment classifier using IndicBERT extracting valuable insights images multilingual CLIP based classifier helps determine effective configurations natural language generation NLG tasks Unlike existing datasets text lack user comments multimodal settings COSMMIC uniquely integrates text images user feedback holistic approach bridges gaps Indian language resources advancing NLP research fostering inclusivity
453,Mind the Gap: Static and Interactive Evaluations of Large Audio Models,"['Minzhi Li', 'William Barr Held', 'Michael J Ryan', 'Kunat Pipatanakul', 'Potsawee Manakul', 'Hao Zhu', 'Diyi Yang']",,Mind Gap Static Interactive Evaluations Large Audio Models
454,Understanding In-context Machine Translation for Low-Resource Languages: A Case Study on Manchu,"['Renhao Pei', 'Yihong Liu', 'Peiqin Lin', 'François Yvon', 'Hinrich Schuetze']",,Understanding context Machine Translation Low Resource Languages Case Study Manchu
455,"CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs","['Jizhan Fang', 'Tianhe Lu', 'Yunzhi Yao', 'Ziyan Jiang', 'Xin Xu', 'Huajun Chen', 'Ningyu Zhang']","Chinese, as a linguistic system rich in depth and complexity, is characterized by distinctive elements such as ancient poetry, proverbs, idioms, and other cultural constructs. However, current Large Language Models (LLMs) face limitations in these specialized domains, highlighting the need for the development of comprehensive datasets that can assess, continuously update, and progressively improve these culturally-grounded linguistic competencies through targeted training optimizations. To address this gap, we introduce CKnowEdit, the first-ever Chinese knowledge editing dataset designed to correct linguistic, factual, and logical errors in LLMs. We collect seven types of knowledge from a wide range of sources, including classical texts, idioms, and content from Baidu Tieba Ruozhiba, taking into account the unique polyphony, antithesis, and logical structures inherent in the Chinese language. By analyzing this dataset, we highlight the challenges current LLMs face in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge editing techniques reveals opportunities to advance the correction of Chinese knowledge. Code and dataset are available at https://github.com/zjunlp/EasyEdit.",CKnowEdit New Chinese Knowledge Editing Dataset Linguistics Facts Logic Error Correction LLMs Chinese linguistic rich depth complexity characterized distinctive elements ancient poetry proverbs idioms cultural constructs current Large Language Models LLMs face limitations specialized domains highlighting need development comprehensive datasets assess continuously update progressively improve culturally grounded linguistic competencies targeted training optimizations address gap introduce CKnowEdit Chinese knowledge editing dataset designed correct linguistic factual logical errors LLMs collect seven types knowledge wide range sources including classical texts idioms content Baidu Tieba Ruozhiba taking account unique polyphony antithesis logical structures inherent Chinese language analyzing dataset highlight challenges current LLMs face mastering Chinese Furthermore evaluation state art knowledge editing techniques reveals opportunities advance correction Chinese knowledge Code dataset available https github com zjunlp EasyEdit
456,TripleFact: Defending Data Contamination in the Evaluation of LLM-driven Fake News Detection,"['Cheng Xu', 'Nan Yan']",,TripleFact Defending Data Contamination Evaluation LLM driven Fake News Detection
457,MUSTS: MUltilingual Semantic Textual Similarity Benchmark,"['Tharindu Ranasinghe', 'Hansi Hettiarachchi', 'Constantin Orasan', 'Ruslan Mitkov']",,MUSTS MUltilingual Semantic Textual Similarity Benchmark
458,Meaning Beyond Truth Conditions: Evaluating Discourse Level Understanding via Anaphora Accessibility,"['Xiaomeng Zhu', 'Zhenghao Zhou', 'Simon Charlow', 'Robert Frank']","We present a hierarchy of natural language understanding abilities and argue for the importance of moving beyond assessments of understanding at the lexical and sentence levels to the discourse level. We propose the task of anaphora accessibility as a diagnostic for assessing discourse understanding, and to this end, present an evaluation dataset inspired by theoretical research in dynamic semantics. We evaluate human and LLM performance on our dataset and find that LLMs and humans align on some tasks and diverge on others. Such divergence can be explained by LLMs' reliance on specific lexical items during language comprehension, in contrast to human sensitivity to structural abstractions.",Meaning Truth Conditions Evaluating Discourse Level Understanding Anaphora Accessibility present hierarchy natural language understanding abilities argue importance moving assessments understanding lexical sentence levels discourse level propose task anaphora accessibility diagnostic assessing discourse understanding end present evaluation dataset inspired theoretical research dynamic semantics evaluate human LLM performance dataset LLMs humans align tasks diverge divergence explained LLMs reliance specific lexical items language comprehension contrast human sensitivity structural abstractions
459,Benchmarking Systematic Relational Reasoning with Large Language and Reasoning Models,"['Irtaza Khalid', 'Amir Masoud Nourollah', 'Steven Schockaert']",,Benchmarking Systematic Relational Reasoning Large Language Reasoning Models
460,Warmup Generations: A Task-Agnostic Approach for Guiding Sequence-to-Sequence Learning with Unsupervised Initial State Generation,"['Senyu Li', 'Zipeng Sun', 'Jiayi Wang', 'Xue Liu', 'Pontus Stenetorp', 'Siva Reddy', 'David Ifeoluwa Adelani']","Traditional supervised fine-tuning (SFT) strategies for sequence-to-sequence tasks often train models to directly generate the target output. Recent work has shown that guiding models with intermediate steps, such as keywords, outlines, or reasoning chains, can significantly improve performance, coherence, and interpretability. However, these methods often depend on predefined intermediate formats and annotated data, limiting their scalability and generalizability. In this work, we introduce a task-agnostic framework that enables models to generate intermediate ""warmup"" sequences. These warmup sequences, serving as an initial state for subsequent generation, are optimized to enhance the probability of generating the target sequence without relying on external supervision or human-designed structures. Drawing inspiration from reinforcement learning principles, our method iteratively refines these intermediate steps to maximize their contribution to the final output, similar to reward-driven optimization in reinforcement learning with human feedback. Experimental results across tasks such as translation, summarization, and multi-choice question answering for logical reasoning show that our approach outperforms traditional SFT methods, and offers a scalable and flexible solution for sequence-to-sequence tasks.",Warmup Generations Task Agnostic Approach Guiding Sequence Sequence Learning Unsupervised Initial State Generation Traditional supervised fine tuning SFT strategies sequence sequence tasks train models directly generate target output Recent work shown guiding models intermediate steps keywords outlines reasoning chains significantly improve performance coherence interpretability methods depend predefined intermediate formats annotated data limiting scalability generalizability work introduce task agnostic framework enables models generate intermediate warmup sequences warmup sequences serving initial state subsequent generation optimized enhance probability generating target sequence relying external supervision human designed structures Drawing inspiration reinforcement learning principles method iteratively refines intermediate steps maximize contribution final output similar reward driven optimization reinforcement learning human feedback Experimental results tasks translation summarization multi choice question answering logical reasoning approach outperforms traditional SFT methods offers scalable flexible solution sequence sequence tasks
461,Building Better: Avoiding Pitfalls in Developing Language Resources when Data is Scarce,"['Nedjma Ousidhoum', 'Meriem Beloucif', 'Saif M. Mohammad']","Language is a form of symbolic capital that affects people's lives in many ways (Bourdieu1977,1991). As a powerful means of communication, it reflects identities, cultures, traditions, and societies more broadly. Therefore, data in a given language should be regarded as more than just a collection of tokens. Rigorous data collection and labeling practices are essential for developing more human-centered and socially aware technologies. Although there has been growing interest in under-resourced languages within the NLP community, work in this area faces unique challenges, such as data scarcity and limited access to qualified annotators. In this paper, we collect feedback from individuals directly involved in and impacted by NLP artefacts for medium- and low-resource languages. We conduct both quantitative and qualitative analyses of their responses and highlight key issues related to: (1) data quality, including linguistic and cultural appropriateness; and (2) the ethics of common annotation practices, such as the misuse of participatory research. Based on these findings, we make several recommendations for creating high-quality language artefacts that reflect the cultural milieu of their speakers, while also respecting the dignity and labor of data workers.",Building Better Avoiding Pitfalls Developing Language Resources Data Scarce Language form symbolic capital affects people s lives ways Bourdieu1977 1991 powerful means communication reflects identities cultures traditions societies broadly data given language regarded just collection tokens Rigorous data collection labeling practices essential developing human centered socially aware technologies growing resourced languages NLP community work area faces unique challenges data scarcity limited access qualified annotators paper collect feedback individuals directly involved impacted NLP artefacts medium low resource languages conduct quantitative qualitative analyses responses highlight key issues related 1 data quality including linguistic cultural appropriateness 2 ethics common annotation practices misuse participatory research Based findings make recommendations creating high quality language artefacts reflect cultural milieu speakers respecting dignity labor data workers
462,Can Large Language Models Accurately Generate Answer Keys for Health-related Questions?,"['Davis Bartels', 'Deepak Gupta', 'Dina Demner-Fushman']",,Large Language Models Accurately Generate Answer Keys Health related Questions
463,Literary Evidence Retrieval via Long-Context Language Models,"['Katherine Thai', 'Mohit Iyyer']","How well do modern long-context language models understand literary fiction? We explore this question via the task of literary evidence retrieval, repurposing the RELiC dataset of That et al. (2022) to construct a benchmark where the entire text of a primary source (e.g., The Great Gatsby) is provided to an LLM alongside literary criticism with a missing quotation from that work. This setting, in which the model must generate the missing quotation, mirrors the human process of literary analysis by requiring models to perform both global narrative reasoning and close textual examination. We curate a high-quality subset of 292 examples through extensive filtering and human verification. Our experiments show that recent reasoning models, such as Gemini Pro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In contrast, the best open-weight model achieves only 29.1% accuracy, highlighting a wide gap in interpretive reasoning between open and closed-weight models. Despite their speed and apparent accuracy, even the strongest models struggle with nuanced literary signals and overgeneration, signaling open challenges for applying LLMs to literary analysis. We release our dataset and evaluation code to encourage future work in this direction.",Literary Evidence Retrieval Long Context Language Models modern long context language models understand literary fiction explore question task literary evidence retrieval repurposing RELiC dataset et al 2022 construct benchmark entire text primary source e g Great Gatsby provided LLM alongside literary criticism missing quotation work setting model generate missing quotation mirrors human process literary analysis requiring models perform global narrative reasoning close textual examination curate high quality subset 292 examples extensive filtering human verification experiments recent reasoning models Gemini Pro 2 5 exceed human expert performance 62 5 vs 50 accuracy contrast best open weight model achieves 29 1 accuracy highlighting wide gap interpretive reasoning open closed weight models Despite speed apparent accuracy strongest models struggle nuanced literary signals overgeneration signaling open challenges applying LLMs literary analysis release dataset evaluation code encourage future work direction
464,BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages,"['Shamsuddeen Hassan Muhammad', 'Nedjma Ousidhoum', 'Idris Abdulmumin', 'Jan Philip Wahle', 'Terry Ruas', 'Meriem Beloucif', 'Christine de Kock', 'Nirmal Surange', 'Daniela Teodorescu', 'Ibrahim Said Ahmad', 'David Ifeoluwa Adelani', 'Alham Fikri Aji', 'Felermino D. M. A. Ali', 'Ilseyar Alimova', 'Vladimir Araujo', 'Nikolay Babakov', 'Naomi Baes', 'Ana-Maria Bucur', 'Andiswa Bukula', 'Guanqun Cao', 'Rodrigo Tufiño', 'Rendi Chevi', 'Chiamaka Ijeoma Chukwuneke', 'Alexandra Ciobotaru', 'Daryna Dementieva', 'Murja Sani Gadanya', 'Robert Geislinger', 'Bela Gipp', 'Oumaima Hourrane', 'Oana Ignat', 'Falalu Ibrahim Lawan', 'Rooweither Mabuya', 'Rahmad Mahendra', 'Vukosi Marivate', 'Alexander Panchenko', 'Andrew Piper', 'Charles Henrique Porto Ferreira', 'Vitaly Protasov', 'Samuel Rutunda', 'Manish Shrivastava', 'Aura Cristina Udrea', 'Lilian Diana Awuor Wanzare', 'Sophie Wu', 'Florian Valentin Wunderlich', 'Hanif Muhammad Zhafran', 'Tianhui Zhang', 'Yi Zhou', 'Saif M. Mohammad']","People worldwide use language in subtle and complex ways to express emotions. Although emotion recognition--an umbrella term for several NLP tasks--impacts various applications within NLP and beyond, most work in this area has focused on high-resource languages. This has led to significant disparities in research efforts and proposed solutions, particularly for under-resourced languages, which often lack high-quality annotated datasets. In this paper, we present BRIGHTER--a collection of multi-labeled, emotion-annotated datasets in 28 different languages and across several domains. BRIGHTER primarily covers low-resource languages from Africa, Asia, Eastern Europe, and Latin America, with instances labeled by fluent speakers. We highlight the challenges related to the data collection and annotation processes, and then report experimental results for monolingual and crosslingual multi-label emotion identification, as well as emotion intensity recognition. We analyse the variability in performance across languages and text domains, both with and without the use of LLMs, and show that the BRIGHTER datasets represent a meaningful step towards addressing the gap in text-based emotion recognition.",BRIGHTER BRIdging Gap Human Annotated Textual Emotion Recognition Datasets 28 Languages People worldwide use language subtle complex ways express emotions emotion recognition umbrella term NLP tasks impacts various applications NLP work area focused high resource languages led significant disparities research efforts proposed solutions particularly resourced languages lack high quality annotated datasets paper present BRIGHTER collection multi labeled emotion annotated datasets 28 different languages domains BRIGHTER primarily covers low resource languages Africa Asia Eastern Europe Latin America instances labeled fluent speakers highlight challenges related data collection annotation processes report experimental results monolingual crosslingual multi label emotion identification emotion intensity recognition analyse variability performance languages text domains use LLMs BRIGHTER datasets represent meaningful step addressing gap text based emotion recognition
465,SkillVerse : Assessing and Enhancing LLMs with Tree Evaluation,"['Yufei Tian', 'Jiao Sun', 'Nanyun Peng', 'Zizhao Zhang']","As language models evolve to tackle complex, multifaceted tasks, their evaluation must adapt to capture this intricacy. A granular, skill-specific understanding of model capabilities can empower researchers to make informed model development plans. In this paper, we introduce SkillVerse, an unsupervised tree-structured diagnosis framework for understanding model proficiency in specific abilities. With LLM as a judge, SkillVerse first critiques the model responses, and then organizes them into a hierarchical structure termed dendrogram. Given proficiency at arbitrary levels of granularity, SkillVerse is flexible to produce insights of behaviors of modern large models. We also demonstrate its efficacy in two downstream tasks: 1) improving model in-context learning by 25% using a tree-search algorithm to select more informative few-shot demonstrations, and 2) accurately predicting new model weaknesses with a 55% success rate, 22% higher than without SkillVerse.",SkillVerse Assessing Enhancing LLMs Tree Evaluation language models evolve tackle complex multifaceted tasks evaluation adapt capture intricacy granular skill specific understanding model capabilities empower researchers make informed model development plans paper introduce SkillVerse unsupervised tree structured diagnosis framework understanding model proficiency specific abilities LLM judge SkillVerse critiques model responses organizes hierarchical structure termed dendrogram Given proficiency arbitrary levels granularity SkillVerse flexible produce insights behaviors modern large models demonstrate efficacy downstream tasks 1 improving model context learning 25 using tree search algorithm select informative shot demonstrations 2 accurately predicting new model weaknesses 55 success rate 22 higher SkillVerse
466,CypherBench: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the LLM Era,"['Yanlin Feng', 'Simone Papicchio', 'Sajjadur Rahman']","Retrieval from graph data is crucial for augmenting large language models (LLM) with both open-domain knowledge and private enterprise data, and it is also a key component in the recent GraphRAG system (edge et al., 2024). Despite decades of research on knowledge graphs and knowledge base question answering, leading LLM frameworks (e.g. Langchain and LlamaIndex) have only minimal support for retrieval from modern encyclopedic knowledge graphs like Wikidata. In this paper, we analyze the root cause and suggest that modern RDF knowledge graphs (e.g. Wikidata, Freebase) are less efficient for LLMs due to overly large schemas that far exceed the typical LLM context window, use of resource identifiers, overlapping relation types and lack of normalization. As a solution, we propose property graph views on top of the underlying RDF graph that can be efficiently queried by LLMs using Cypher. We instantiated this idea on Wikidata and introduced CypherBench, the first benchmark with 11 large-scale, multi-domain property graphs with 7.8 million entities and over 10,000 questions. To achieve this, we tackled several key challenges, including developing an RDF-to-property graph conversion engine, creating a systematic pipeline for text-to-Cypher task generation, and designing new evaluation metrics.",CypherBench Precise Retrieval scale Modern Knowledge Graphs LLM Era Retrieval graph data crucial augmenting large language models LLM open domain knowledge private enterprise data key component recent GraphRAG edge et al 2024 Despite decades research knowledge graphs knowledge base question answering leading LLM frameworks e g Langchain LlamaIndex minimal support retrieval modern encyclopedic knowledge graphs like Wikidata paper analyze root cause suggest modern RDF knowledge graphs e g Wikidata Freebase efficient LLMs overly large schemas far exceed typical LLM context window use resource identifiers overlapping relation types lack normalization solution propose property graph views underlying RDF graph efficiently queried LLMs using Cypher instantiated idea Wikidata introduced CypherBench benchmark 11 large scale multi domain property graphs 7 8 million entities 10 000 questions achieve tackled key challenges including developing RDF property graph conversion engine creating systematic pipeline text Cypher task generation designing new evaluation metrics
467,Empathy Prediction from Diverse Perspectives,"['Francine Chen', 'Scott Carter', 'Tatiana Lau', 'Nayeli Suseth Bravo', 'Sumanta Bhattacharyya', 'Kate Sieck', 'Charlene C. Wu']",,Empathy Prediction Diverse Perspectives
468,Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice,"['Federico Ravenda', 'Seyed Ali Bahrainian', 'Andrea Raballo', 'Antonietta Mira', 'Noriko Kando']","In psychological practices, standardized questionnaires serve as essential tools for assessing mental health through structured, clinically-validated questions (i.e., items). While social media platforms offer rich data for mental health screening, computational approaches often bypass these established clinical assessment tools in favor of black-box classification. We propose a novel questionnaire-guided screening framework that bridges psychological practice and computational methods through adaptive Retrieval-Augmented Generation (\textit{aRAG}). Our approach links unstructured social media content and standardized clinical assessments by retrieving relevant posts for each questionnaire item and using Large Language Models (LLMs) to complete validated psychological instruments. Our findings demonstrate two key advantages of questionnaire-guided screening: First, when completing the Beck Depression Inventory-II (BDI-II), our approach matches or outperforms state-of-the-art performance on Reddit-based benchmarks without requiring training data. Second, we show that guiding LLMs through standardized questionnaires can yield superior results compared to directly prompting them for depression screening, while also providing a more interpretable assessment by linking model outputs to clinically validated diagnostic criteria. Additionally, we show, as a proof-of-concept, how our questionnaire-based methodology can be extended to other mental conditions' screening, highlighting the promising role of LLMs as psychological assessors.",LLMs effective psychological assessors Leveraging adaptive RAG interpretable mental health screening psychometric practice psychological practices standardized questionnaires serve essential tools assessing mental health structured clinically validated questions e items social media platforms offer rich data mental health screening computational approaches bypass established clinical assessment tools favor black box classification propose novel questionnaire guided screening framework bridges psychological practice computational methods adaptive Retrieval Augmented Generation textit aRAG approach links unstructured social media content standardized clinical assessments retrieving relevant posts questionnaire item using Large Language Models LLMs complete validated psychological instruments findings demonstrate key advantages questionnaire guided screening completing Beck Depression Inventory II BDI II approach matches outperforms state art performance Reddit based benchmarks requiring training data Second guiding LLMs standardized questionnaires yield superior results compared directly prompting depression screening providing interpretable assessment linking model outputs clinically validated diagnostic criteria Additionally proof concept questionnaire based methodology extended mental conditions screening highlighting promising role LLMs psychological assessors
469,"INTERACT: Enabling Interactive, Question-Driven Learning in Large Language Models","['Aum Kendapadi', 'Kerem Zaman', 'Rakesh R Menon', 'Shashank Srivastava']","Large language models (LLMs) excel at answering questions but remain passive learners-absorbing static data without the ability to question and refine knowledge. This paper explores how LLMs can transition to interactive, question-driven learning through student-teacher dialogues. We introduce INTERACT (INTERactive learning for Adaptive Concept Transfer), a framework in which a ""student"" LLM engages a ""teacher"" LLM through iterative inquiries to acquire knowledge across 1,347 contexts, including song lyrics, news articles, movie plots, academic papers, and images. Our experiments show that across a wide range of scenarios and LLM architectures, interactive learning consistently enhances performance, achieving up to a 25% improvement, with 'cold-start' student models matching static learning baselines in as few as five dialogue turns. Interactive setups can also mitigate the disadvantages of weaker teachers, showcasing the robustness of question-driven learning.",INTERACT Enabling Interactive Question Driven Learning Large Language Models Large language models LLMs excel answering questions remain passive learners absorbing static data ability question refine knowledge paper explores LLMs transition interactive question driven learning student teacher dialogues introduce INTERACT INTERactive learning Adaptive Concept Transfer framework student LLM engages teacher LLM iterative inquiries acquire knowledge 1 347 contexts including song lyrics news articles movie plots academic papers images experiments wide range scenarios LLM architectures interactive learning consistently enhances performance achieving 25 improvement cold start student models matching static learning baselines dialogue turns Interactive setups mitigate disadvantages weaker teachers showcasing robustness question driven learning
470,Circuit Stability Characterizes Language Model Generalization,['Alan Sun'],"Extensively evaluating the capabilities of (large) language models is difficult. Rapid development of state-of-the-art models induce benchmark saturation, while creating more challenging datasets is labor-intensive. Inspired by the recent developments in mechanistic interpretability, we introduce circuit stability as a new way to assess model performance. Circuit stability refers to a model's ability to apply a consistent reasoning process-its circuit-across various inputs. We mathematically formalize circuit stability and circuit equivalence. Then, through three case studies, we empirically show that circuit stability and the lack thereof can characterize and predict different aspects of generalization. Our proposed methods offer a step towards rigorously relating the generality of models to their interpretability.",Circuit Stability Characterizes Language Model Generalization Extensively evaluating capabilities large language models difficult Rapid development state art models induce benchmark saturation creating challenging datasets labor intensive Inspired recent developments mechanistic interpretability introduce circuit stability new way assess model performance Circuit stability refers model s ability apply consistent reasoning process circuit various inputs mathematically formalize circuit stability circuit equivalence case studies empirically circuit stability lack thereof characterize predict different aspects generalization proposed methods offer step rigorously relating generality models interpretability
471,Comparing LLM-generated and human-authored news text using formal syntactic theory,"['Olga Zamaraeva', 'Dan Flickinger', 'Francis Bond', 'Carlos Gómez-Rodríguez']","This study provides the first comprehensive comparison of New York Times-style text generated by six large language models against real, human-authored NYT writing. The comparison is based on a formal syntactic theory. We use Head-driven Phrase Structure Grammar (HPSG) to analyze the grammatical structure of the texts. We then investigate and illustrate the differences in the distributions of HPSG grammar types, revealing systematic distinctions between human and LLM-generated writing. These findings contribute to a deeper understanding of the syntactic behavior of LLMs as well as humans, within the NYT genre.",Comparing LLM generated human authored news text using formal syntactic theory study provides comprehensive comparison New York Times style text generated large language models real human authored NYT writing comparison based formal syntactic theory use Head driven Phrase Structure Grammar HPSG analyze grammatical structure texts investigate illustrate differences distributions HPSG grammar types revealing systematic distinctions human LLM generated writing findings contribute deeper understanding syntactic behavior LLMs humans NYT genre
472,Improving Preference Extraction In LLMs By Identifying Latent Knowledge Through Classifying Probes,"['Sharan Maiya', 'Yinhong Liu', 'Ramit Debnath', 'Anna Korhonen']","Large Language Models (LLMs) are often used as automated judges to evaluate text, but their effectiveness can be hindered by various unintentional biases. We propose using linear classifying probes, trained by leveraging differences between contrasting pairs of prompts, to directly access LLMs' latent knowledge and extract more accurate preferences. Through extensive experiments using models of varying size from four different families and six diverse datasets assessing text quality evaluation and common sense reasoning, we demonstrate that both supervised and unsupervised probing approaches consistently outperform traditional generation-based judgement while maintaining similar computational costs. These probes generalise under domain shifts and can even outperform finetuned evaluators with the same training data size. Our results suggest linear probing offers an accurate, robust and computationally efficient approach for LLM-as-judge tasks while providing interpretable insights into how models encode judgement-relevant knowledge. Our data and code will be openly released in the future.",Improving Preference Extraction LLMs Identifying Latent Knowledge Classifying Probes Large Language Models LLMs used automated judges evaluate text effectiveness hindered various unintentional biases propose using linear classifying probes trained leveraging differences contrasting pairs prompts directly access LLMs latent knowledge extract accurate preferences extensive experiments using models varying size different families diverse datasets assessing text quality evaluation common sense reasoning demonstrate supervised unsupervised probing approaches consistently outperform traditional generation based judgement maintaining similar computational costs probes generalise domain shifts outperform finetuned evaluators training data size results suggest linear probing offers accurate robust computationally efficient approach LLM judge tasks providing interpretable insights models encode judgement relevant knowledge data code openly released future
473,"White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in LLMs","['Yixin Wan', 'Kai-Wei Chang']","Social biases can manifest in language agency. However, very limited research has investigated such biases in Large Language Model (LLM)-generated content. In addition, previous works often rely on string-matching techniques to identify agentic and communal words within texts, falling short of accurately classifying language agency. We introduce the Language Agency Bias Evaluation (LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing agency levels attributed to different demographic groups in model generations. LABE tests for gender, racial, and intersectional language agency biases in LLMs on 3 text generation tasks: biographies, professor reviews, and reference letters. Using LABE, we unveil language agency social biases in 3 recent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations tend to demonstrate greater gender bias than human-written texts; (2) Models demonstrate remarkably higher levels of intersectional bias than the other bias aspects. (3) Prompt-based mitigation is unstable and frequently leads to bias exacerbation. Based on our observations, we propose Mitigation via Selective Rewrite (MSR), a novel bias mitigation strategy that leverages an agency classifier to identify and selectively revise parts of generated texts that demonstrate communal traits. Empirical results prove MSR to be more effective and reliable than prompt-based mitigation method, showing a promising research direction.",White Men Lead Black Women Help Benchmarking Mitigating Language Agency Social Biases LLMs Social biases manifest language agency limited research investigated biases Large Language Model LLM generated content addition previous works rely string matching techniques identify agentic communal words texts falling short accurately classifying language agency introduce Language Agency Bias Evaluation LABE benchmark comprehensively evaluates biases LLMs analyzing agency levels attributed different demographic groups model generations LABE tests gender racial intersectional language agency biases LLMs 3 text generation tasks biographies professor reviews reference letters Using LABE unveil language agency social biases 3 recent LLMs ChatGPT Llama3 Mistral observe 1 LLM generations tend demonstrate greater gender bias human written texts 2 Models demonstrate remarkably higher levels intersectional bias bias aspects 3 Prompt based mitigation unstable frequently leads bias exacerbation Based observations propose Mitigation Selective Rewrite MSR novel bias mitigation strategy leverages agency classifier identify selectively revise parts generated texts demonstrate communal traits Empirical results prove MSR effective reliable prompt based mitigation method showing promising research direction
474,AIMSCheck: Leveraging LLMs for AI-Assisted Review of Modern Slavery Statements Across Jurisdictions,"['Adriana Eufrosina Bora', 'Akshatha Arodi', 'Duoyi Zhang', 'Jordan Bannister', 'Mirko Bronzi', 'Arsene Fansi Tchango', 'Md Abul Bashar', 'Richi Nayak', 'Kerrie Mengersen']","Modern Slavery Acts mandate that corporations disclose their efforts to combat modern slavery, aiming to enhance transparency and strengthen practices for its eradication. However, verifying these statements remains challenging due to their complex, diversified language and the sheer number of statements that must be reviewed. The development of NLP tools to assist in this task is also difficult due to a scarcity of annotated data. Furthermore, as modern slavery transparency legislation has been introduced in several countries, the generalizability of such tools across legal jurisdictions must be studied. To address these challenges, we work with domain experts to make two key contributions. First, we present AIMS.uk and AIMS.ca, newly annotated datasets from the UK and Canada to enable cross-jurisdictional evaluation. Second, we introduce AIMSCheck, an end-to-end framework for compliance validation. AIMSCheck decomposes the compliance assessment task into three levels, enhancing interpretability and practical applicability. Our experiments show that models trained on an Australian dataset generalize well across UK and Canadian jurisdictions, demonstrating the potential for broader application in compliance monitoring. We release the benchmark datasets and AIMSCheck to the public to advance AI-adoption in compliance assessment and drive further research in this field.",AIMSCheck Leveraging LLMs AI Assisted Review Modern Slavery Statements Jurisdictions Modern Slavery Acts mandate corporations disclose efforts combat modern slavery aiming enhance transparency strengthen practices eradication verifying statements remains challenging complex diversified language sheer number statements reviewed development NLP tools assist task difficult scarcity annotated data Furthermore modern slavery transparency legislation introduced countries generalizability tools legal jurisdictions studied address challenges work domain experts make key contributions present AIMS uk AIMS ca newly annotated datasets UK Canada enable cross jurisdictional evaluation Second introduce AIMSCheck end end framework compliance validation AIMSCheck decomposes compliance assessment task levels enhancing interpretability practical applicability experiments models trained Australian dataset generalize UK Canadian jurisdictions demonstrating potential broader application compliance monitoring release benchmark datasets AIMSCheck public advance AI adoption compliance assessment drive research field
475,"Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence","['Mohsen Fayyaz', 'Ali Modarressi', 'Hinrich Schuetze', 'Nanyun Peng']","Dense retrieval models are commonly used in Information Retrieval (IR) applications, such as Retrieval-Augmented Generation (RAG). Since they often serve as the first step in these systems, their robustness is critical to avoid downstream failures. In this work, we repurpose a relation extraction dataset (e.g., Re-DocRED) to design controlled experiments that quantify the impact of heuristic biases, such as a preference for shorter documents, on retrievers like Dragon+ and Contriever. We uncover major vulnerabilities, showing retrievers favor shorter documents, early positions, repeated entities, and literal matches, all while ignoring the answer's presence! Notably, when multiple biases combine, models exhibit catastrophic performance degradation, selecting the answer-containing document in less than 10% of cases over a synthetic biased document without the answer. Furthermore, we show that these biases have direct consequences for downstream applications like RAG, where retrieval-preferred documents can mislead LLMs, resulting in a 34% performance drop than providing no documents at all. https://huggingface.co/datasets/mohsenfayyaz/ColDeR",Collapse Dense Retrievers Short Early Literal Biases Outranking Factual Evidence Dense retrieval models commonly used Information Retrieval IR applications Retrieval Augmented Generation RAG serve step systems robustness critical avoid downstream failures work repurpose relation extraction dataset e g DocRED design controlled experiments quantify impact heuristic biases preference shorter documents retrievers like Dragon Contriever uncover major vulnerabilities showing retrievers favor shorter documents early positions repeated entities literal matches ignoring answer s presence Notably multiple biases combine models exhibit catastrophic performance degradation selecting answer containing document 10 cases synthetic biased document answer Furthermore biases direct consequences downstream applications like RAG retrieval preferred documents mislead LLMs resulting 34 performance drop providing documents https huggingface datasets mohsenfayyaz ColDeR
476,SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence,"['Zhining Liu', 'Rana Ali Amjad', 'Ravinarayana Adkathimar', 'Tianxin Wei', 'Hanghang Tong']","Providing Language Models (LMs) with relevant evidence in the context (either via retrieval or user-provided) can significantly improve their ability to provide better-grounded responses. However, recent studies have found that LMs often struggle to fully comprehend and utilize key evidence from the context, especially when it contains noise and irrelevant information, an issue common in real-world scenarios. To address this, we propose SelfElicit, an inference-time approach that helps LMs focus on key contextual evidence through self-guided explicit highlighting. By leveraging the inherent evidence-finding capabilities of LMs using the attention scores of deeper layers, our method automatically identifies and emphasizes key evidence within the input context, facilitating more accurate and grounded responses without additional training or iterative prompting. We demonstrate that SelfElicit brings consistent and significant improvement on multiple evidence-based QA tasks for various LM families while maintaining computational efficiency. Our code and documentation are available at https://github.com/ZhiningLiu1998/SelfElicit.",SelfElicit Language Model Secretly Knows Relevant Evidence Providing Language Models LMs relevant evidence context retrieval user provided significantly improve ability provide better grounded responses recent studies LMs struggle fully comprehend utilize key evidence context especially contains noise irrelevant information issue common real world scenarios address propose SelfElicit inference time approach helps LMs focus key contextual evidence self guided explicit highlighting leveraging inherent evidence finding capabilities LMs using attention scores deeper layers method automatically identifies emphasizes key evidence input context facilitating accurate grounded responses additional training iterative prompting demonstrate SelfElicit brings consistent significant improvement multiple evidence based QA tasks various LM families maintaining computational efficiency code documentation available https github com ZhiningLiu1998 SelfElicit
477,The Male CEO and the Female Assistant: Evaluation and Mitigation of Gender Biases in Text-To-Image Generation of Dual Subjects,"['Yixin Wan', 'Kai-Wei Chang']","Recent large-scale T2I models like DALLE-3 have made progress in reducing gender stereotypes when generating single-person images. However, significant biases remain when generating images with more than one person. To systematically evaluate this, we propose the Paired Stereotype Test (PST) framework, which queries T2I models to depict two individuals assigned with male-stereotyped and female-stereotyped social identities, respectively (e.g. ""a CEO"" and ""an Assistant""). This contrastive setting often triggers T2I models to generate gender-stereotyped images. Using PST, we evaluate two aspects of gender biases -- the well-known bias in gendered occupation and a novel aspect: bias in organizational power. Experiments show that over 74\% images generated by DALLE-3 display gender-occupational biases. Additionally, compared to single-person settings, DALLE-3 is more likely to perpetuate male-associated stereotypes under PST. We further propose FairCritic, a novel and interpretable framework that leverages an LLM-based critic model to i) detect bias in generated images, and ii) adaptively provide feedback to T2I models for improving fairness. FairCritic achieves near-perfect fairness on PST, overcoming the limitations of previous prompt-based intervention approaches.",Male CEO Female Assistant Evaluation Mitigation Gender Biases Text Image Generation Dual Subjects Recent large scale T2I models like DALLE 3 progress reducing gender stereotypes generating single person images significant biases remain generating images person systematically evaluate propose Paired Stereotype Test PST framework queries T2I models depict individuals assigned male stereotyped female stereotyped social identities respectively e g CEO Assistant contrastive setting triggers T2I models generate gender stereotyped images Using PST evaluate aspects gender biases known bias gendered occupation novel aspect bias organizational power Experiments 74 images generated DALLE 3 display gender occupational biases Additionally compared single person settings DALLE 3 likely perpetuate male associated stereotypes PST propose FairCritic novel interpretable framework leverages LLM based critic model detect bias generated images ii adaptively provide feedback T2I models improving fairness FairCritic achieves near perfect fairness PST overcoming limitations previous prompt based intervention approaches
478,A Little Human Data Goes A Long Way,"['Dhananjay Ashok', 'Jonathan May']","Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points. We show that matching the performance gain of just a little additional human data (only 200 points) requires an order of magnitude more synthetic data and estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.",Little Human Data Goes Long Way Faced expensive human annotation process creators NLP systems increasingly turn synthetic data generation method shows promise extent synthetic data replace human annotation poorly understood investigate use synthetic data Fact Verification FV Question Answering QA studying effects incrementally replacing human generated data synthetic points diverse datasets Strikingly replacing 90 training data marginally decreases performance replacing final 10 leads severe declines models trained purely synthetic data reliably improved including 125 human generated data points matching performance gain just little additional human data 200 points requires order magnitude synthetic data estimate price ratios human annotation cost effective solution results suggest human annotation scale infeasible great value having small proportion dataset human generated
479,Mitigating Shortcut Learning with InterpoLated Learning,"['Michalis Korakakis', 'Andreas Vlachos', 'Adrian Weller']","Empirical risk minimization (ERM) incentivizes models to exploit shortcuts, i.e., spurious correlations between input attributes and labels that are prevalent in the majority of the training data but unrelated to the task at hand. This reliance hinders generalization on minority examples, where such correlations do not hold. Existing shortcut mitigation approaches are model-specific, difficult to tune, computationally expensive, and fail to improve learned representations. To address these issues, we propose InterpoLated Learning (InterpoLL) which interpolates the representations of majority examples to include features from intra-class minority examples with shortcut-mitigating patterns. This weakens shortcut influence, enabling models to acquire features predictive across both minority and majority examples. Experimental results on multiple natural language understanding tasks demonstrate that InterpoLL improves minority generalization over both ERM and state-of-the-art shortcut mitigation methods, without compromising accuracy on majority examples. Notably, these gains persist across encoder, encoder-decoder, and decoder-only architectures, demonstrating the method's broad applicability.",Mitigating Shortcut Learning InterpoLated Learning Empirical risk minimization ERM incentivizes models exploit shortcuts e spurious correlations input attributes labels prevalent majority training data unrelated task hand reliance hinders generalization minority examples correlations hold Existing shortcut mitigation approaches model specific difficult tune computationally expensive fail improve learned representations address issues propose InterpoLated Learning InterpoLL interpolates representations majority examples include features intra class minority examples shortcut mitigating patterns weakens shortcut influence enabling models acquire features predictive minority majority examples Experimental results multiple natural language understanding tasks demonstrate InterpoLL improves minority generalization ERM state art shortcut mitigation methods compromising accuracy majority examples Notably gains persist encoder encoder decoder decoder architectures demonstrating method s broad applicability
480,Toward Automatic Discovery of a Canine Phonetic Alphabet,"['Theron S. Wang', 'Xingyuan Li', 'Hridayesh Lekhak', 'Tuan Minh Dang', 'Mengyue Wu', 'Kenny Q. Zhu']",,Automatic Discovery Canine Phonetic Alphabet
481,DavIR: Data Selection via Implicit Reward for Large Language Models,"['Haotian Zhou', 'Tingkai Liu', 'Qianli Ma', 'Yufeng Zhang', 'Jianbo Yuan', 'Pengfei Liu', 'Yang You', 'Hongxia Yang']","We introduce DavIR, a model-based data selection method for post-training Large Language Models. DavIR generalizes Reducible Holdout Loss to core-set selection problem of causal language modeling, and quantifies the learnability of a given datum with respect to a pre-trained LLM based on relative reduction in loss during fine-tuning, a metric we show to be closely related to the implicit reward model described in Direct Preference Optimization (DPO). We show that 6% of Alpaca dataset selected with DavIR can steer both the LLaMA and Gemma model family to produce superior performance compared to the same models trained on the full 52K dataset. We also show that Alpaca dataset compressed with DavIR can be combined with GSM8K dataset to effectively balance open-domain freeform QA and mathematical reasoning capabilities. Finally, we apply the DavIR objective to DPO and develop a normalized DavIR-DPO objective which improves alignment performance of Zephyr-7B-SFT model by 8% (relative) on AlpacaEval, compared against training on vanilla DPO objective.",DavIR Data Selection Implicit Reward Large Language Models introduce DavIR model based data selection method post training Large Language Models DavIR generalizes Reducible Holdout Loss core set selection problem causal language modeling quantifies learnability given datum respect pre trained LLM based relative reduction loss fine tuning metric closely related implicit reward model described Direct Preference Optimization DPO 6 Alpaca dataset selected DavIR steer LLaMA Gemma model family produce superior performance compared models trained 52K dataset Alpaca dataset compressed DavIR combined GSM8K dataset effectively balance open domain freeform QA mathematical reasoning capabilities Finally apply DavIR objective DPO develop normalized DavIR DPO objective improves alignment performance Zephyr 7B SFT model 8 relative AlpacaEval compared training vanilla DPO objective
482,Byte Latent Transformer: Patches Scale Better Than Tokens,"['Artidoro Pagnoni', 'Ramakanth Pasunuru', 'Pedro Rodriguez', 'John Nguyen', 'Benjamin Muller', 'Margaret Li', 'Chunting Zhou', 'LILI YU', 'Jason E Weston', 'Luke Zettlemoyer', 'Gargi Ghosh', 'Mike Lewis', 'Ari Holtzman', 'Srini Iyer']","We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.",Byte Latent Transformer Patches Scale Better Tokens introduce Byte Latent Transformer BLT new byte level LLM architecture time matches tokenization based LLM performance scale significant improvements inference efficiency robustness BLT encodes bytes dynamically sized patches serve primary units computation Patches segmented based entropy byte allocating compute model capacity increased data complexity demands present FLOP controlled scaling study byte level models 8B parameters 4T training bytes results demonstrate feasibility scaling models trained raw bytes fixed vocabulary training inference efficiency improve dynamically selecting long patches data predictable qualitative improvements reasoning long tail generalization Overall fixed inference costs BLT shows significantly better scaling tokenization based models simultaneously growing patch model size
483,DiffuseDef: Improved Robustness to Adversarial Attacks via Iterative Denoising,"['Zhenhao Li', 'Huichi Zhou', 'Marek Rei', 'Lucia Specia']","Pretrained language models have significantly advanced performance across various natural language processing tasks. However, adversarial attacks continue to pose a critical challenge to systems built using these models, as they can be exploited with carefully crafted adversarial texts. Inspired by the ability of diffusion models to predict and reduce noise in computer vision, we propose a novel and flexible adversarial defense method for language classification tasks, DiffuseDef, which incorporates a diffusion layer as a denoiser between the encoder and the classifier. The diffusion layer is trained on top of the existing classifier, ensuring seamless integration with any model in a plug-and-play manner. During inference, the adversarial hidden state is first combined with sampled noise, then denoised iteratively and finally ensembled to produce a robust text representation. By integrating adversarial training, denoising, and ensembling techniques, we show that DiffuseDef improves over existing adversarial defense methods and achieves state-of-the-art performance against common black-box and white-box adversarial attacks.",DiffuseDef Improved Robustness Adversarial Attacks Iterative Denoising Pretrained language models significantly advanced performance various natural language processing tasks adversarial attacks continue pose critical challenge systems built using models exploited carefully crafted adversarial texts Inspired ability diffusion models predict reduce noise computer vision propose novel flexible adversarial defense method language classification tasks DiffuseDef incorporates diffusion layer denoiser encoder classifier diffusion layer trained existing classifier ensuring seamless integration model plug play manner inference adversarial hidden state combined sampled noise denoised iteratively finally ensembled produce robust text representation integrating adversarial training denoising ensembling techniques DiffuseDef improves existing adversarial defense methods achieves state art performance common black box white box adversarial attacks
484,Identifying Cellular Niches in Spatial Transcriptomics: An Investigation into the Capabilities of Large Language Models,"['Huanhuan Wei', 'Xiao Luo', 'Hongyi Yu', 'Jinping Liang', 'Luning Yang', 'Lixing Lin', 'Alexandra Popa', 'Xiting Yan']",,Identifying Cellular Niches Spatial Transcriptomics Investigation Capabilities Large Language Models
485,Culture Matters in Toxic Language Detection in Persian,"['Zahra Bokaei', 'Walid Magdy', 'Bonnie Webber']","Toxic language detection is crucial for creating safer online environments and limiting the spread of harmful content. While toxic language detection has been under-explored in Persian, the current work compares different methods for this task, including fine-tuning, data enrichment, zero-shot and few-shot learning, and cross-lingual transfer learning. What is especially compelling is the impact of cultural context on transfer learning for this task: We show that the language of a country with cultural similarities to Persian yields better results in transfer learning. Conversely, the improvement is lower when the language comes from a culturally distinct country. Warning: This paper contains examples of toxic language that may disturb some readers. These examples are included for the purpose of research on toxic detection.",Culture Matters Toxic Language Detection Persian Toxic language detection crucial creating safer online environments limiting spread harmful content toxic language detection explored Persian current work compares different methods task including fine tuning data enrichment zero shot shot learning cross lingual transfer learning especially compelling impact cultural context transfer learning task language country cultural similarities Persian yields better results transfer learning Conversely improvement lower language comes culturally distinct country Warning paper contains examples toxic language disturb readers examples included purpose research toxic detection
486,Bitnet.cpp: Efficient Edge Inference for Ternary LLMs,"['Jinheng Wang', 'Hansong Zhou', 'Ting Song', 'Shijie Cao', 'Yan Xia', 'Ting Cao', 'Jianyu Wei', 'Shuming Ma', 'Hongyu Wang', 'Furu Wei']",,Bitnet cpp Efficient Edge Inference Ternary LLMs
487,Instance-Selection-Inspired Undersampling Strategies for Bias Reduction in Small and Large Language Models for Binary Text Classification,"['Guilherme Fonseca', 'Washington Cunha', 'Gabriel Prenassi', 'Marcos André Gonçalves', 'Leonardo Chaves Dutra da Rocha']",,Instance Selection Inspired Undersampling Strategies Bias Reduction Small Large Language Models Binary Text Classification
488,Forward Knows Efficient Backward Path: Saliency-Guided Memory-Efficient Fine-tuning of Large Language Models,"['Yeachan Kim', 'SangKeun Lee']",,Forward Knows Efficient Backward Path Saliency Guided Memory Efficient Fine tuning Large Language Models
489,Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning,"['Aofei Chang', 'Le Huang', 'Alex James Boyd', 'Parminder Bhatia', 'Taha Kass-Hout', 'Cao Xiao', 'Fenglong Ma']","Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal attention distribution on visual inputs, leading to hallucinated or inaccurate outputs. Existing mitigation methods primarily rely on inference-time interventions, which are limited in attention adaptation or require additional supervision. To address this, we propose A$^3$Tune, a novel fine-tuning framework for Automatic Attention Alignment Tuning. A$^3$Tune leverages zero-shot weak labels from SAM, refines them into prompt-aware labels using BioMedCLIP, and then selectively modifies visually-critical attention heads to improve alignment while minimizing interference. Additionally, we introduce a A$^3$MoE module, enabling adaptive parameter selection for attention tuning across diverse prompts and images. Extensive experiments on medical VQA and report generation benchmarks show that A$^3$Tune outperforms state-of-the-art baselines, achieving enhanced attention distributions and performance in Med-LVLMs.",Focus Matters Enhancing Medical Vision Language Models Automatic Attention Alignment Tuning Medical Large Vision Language Models Med LVLMs exhibit suboptimal attention distribution visual inputs leading hallucinated inaccurate outputs Existing mitigation methods primarily rely inference time interventions limited attention adaptation require additional supervision address propose 3 Tune novel fine tuning framework Automatic Attention Alignment Tuning 3 Tune leverages zero shot weak labels SAM refines prompt aware labels using BioMedCLIP selectively modifies visually critical attention heads improve alignment minimizing interference Additionally introduce 3 MoE module enabling adaptive parameter selection attention tuning diverse prompts images Extensive experiments medical VQA report generation benchmarks 3 Tune outperforms state art baselines achieving enhanced attention distributions performance Med LVLMs
490,LLMs + Persona-Plug = Personalized LLMs,"['Jiongnan Liu', 'Yutao Zhu', 'Shuting Wang', 'Xiaochi Wei', 'Erxue Min', 'Yu Lu', 'Shuaiqiang Wang', 'Dawei Yin', 'Zhicheng Dou']","Personalization plays a critical role in numerous language tasks and applications, since users with the same requirements may prefer diverse outputs based on their individual interests. This has led to the development of various personalized approaches aimed at adapting large language models (LLMs) to generate customized outputs aligned with user preferences. Some of them involve fine-tuning a unique personalized LLM for each user, which is too expensive for widespread application. Alternative approaches introduce personalization information in a plug-and-play manner by retrieving the user's relevant historical texts as demonstrations. However, this retrieval-based strategy may break the continuity of the user history and fail to capture the user's overall styles and patterns, hence leading to sub-optimal performance. To address these challenges, we propose a novel personalized LLM model, \ours{}. It constructs a user-specific embedding for each individual by modeling all her historical contexts through a lightweight plug-in user embedder module. By attaching this embedding to the task input, LLMs can better understand and capture user habits and preferences, thereby producing more personalized outputs without tuning their own parameters. Extensive experiments on various tasks in the language model personalization (LaMP) benchmark demonstrate that the proposed model significantly outperforms existing personalized LLM approaches.",LLMs Persona Plug Personalized LLMs Personalization plays critical role numerous language tasks applications users requirements prefer diverse outputs based individual interests led development various personalized approaches aimed adapting large language models LLMs generate customized outputs aligned user preferences involve fine tuning unique personalized LLM user expensive widespread application Alternative approaches introduce personalization information plug play manner retrieving user s relevant historical texts demonstrations retrieval based strategy break continuity user history fail capture user s overall styles patterns leading sub optimal performance address challenges propose novel personalized LLM model constructs user specific embedding individual modeling historical contexts lightweight plug user embedder module attaching embedding task input LLMs better understand capture user habits preferences producing personalized outputs tuning parameters Extensive experiments various tasks language model personalization LaMP benchmark demonstrate proposed model significantly outperforms existing personalized LLM approaches
491,Developmentally-plausible Working Memory Shapes a Critical Period for Language Acquisition,"['Masato Mita', 'Ryo Yoshida', 'Yohei Oseki']","Large language models possess general linguistic abilities but acquire language less efficiently than humans. This study proposes a method for integrating the developmental characteristics of working memory during the critical period, a stage when human language acquisition is particularly efficient, into the training process of language models. The proposed method introduces a mechanism that initially constrains working memory during the early stages of training and gradually relaxes this constraint in an exponential manner as learning progresses. Targeted syntactic evaluation shows that the proposed method outperforms conventional methods without memory constraints or with static memory constraints. These findings not only provide new directions for designing data-efficient language models but also offer indirect evidence supporting the role of the developmental characteristics of working memory as the underlying mechanism of the critical period in language acquisition.",Developmentally plausible Working Memory Shapes Critical Period Language Acquisition Large language models possess general linguistic abilities acquire language efficiently humans study proposes method integrating developmental characteristics working memory critical period stage human language acquisition particularly efficient training process language models proposed method introduces mechanism initially constrains working memory early stages training gradually relaxes constraint exponential manner learning progresses Targeted syntactic evaluation shows proposed method outperforms conventional methods memory constraints static memory constraints findings provide new directions designing data efficient language models offer indirect evidence supporting role developmental characteristics working memory underlying mechanism critical period language acquisition
492,IRIS: An Iterative and Integrated Framework for Verifiable Causal Discovery in the Absence of Tabular Data,"['Tao Feng', 'Lizhen Qu', 'Niket Tandon', 'Gholamreza Haffari']",,IRIS Iterative Integrated Framework Verifiable Causal Discovery Absence Tabular Data
493,INJONGO: A Multicultural Intent Detection and Slot-filling Dataset for 16 African Languages,"['Hao Yu', 'Jesujoba Oluwadara Alabi', 'Andiswa Bukula', 'Jian Yun Zhuang', 'En-Shiun Annie Lee', 'Tadesse Kebede Guge', 'Israel Abebe Azime', 'Happy Buzaaba', 'Blessing Kudzaishe Sibanda', 'Godson Koffi KALIPE', 'Jonathan Mukiibi', 'Salomon KABONGO KABENAMUALU', 'Mmasibidi Setaka', 'Lolwethu Ndolela', 'Nkiruka Odu', 'Rooweither Mabuya', 'Shamsuddeen Hassan Muhammad', 'Salomey Osei', 'Sokhar Samb', 'Dietrich Klakow', 'David Ifeoluwa Adelani']","Slot-filling and intent detection are well-established tasks in Conversational AI. However, current large-scale benchmarks for these tasks often exclude evaluations of low-resource languages and rely on translations from English benchmarks, thereby predominantly reflecting Western-centric concepts. In this paper, we introduce Injongo -- a multicultural, open-source benchmark dataset for 16 African languages with utterances generated by native speakers across diverse domains, including banking, travel, home, and dining. Through extensive experiments, we benchmark the fine-tuning multilingual transformer models and the prompting large language models (LLMs), and show the advantage of leveraging African-cultural utterances over Western-centric utterances for improving cross-lingual transfer from the English language. Experimental results reveal that current LLMs struggle with the slot-filling task, with GPT-4o achieving an average performance of 26 F1-score. In contrast, intent detection performance is notably better, with an average accuracy of 70.6%, though it still falls behind the fine-tuning baselines. Compared to the English language, GPT-4o and fine-tuning baselines perform similarly on intent detection, achieving an accuracy of approximately 81%. Our findings suggest that the performance of LLMs is still behind for many low-resource African languages, and more work is needed to further improve their downstream performance.",INJONGO Multicultural Intent Detection Slot filling Dataset 16 African Languages Slot filling intent detection established tasks Conversational AI current large scale benchmarks tasks exclude evaluations low resource languages rely translations English benchmarks predominantly reflecting Western centric concepts paper introduce Injongo multicultural open source benchmark dataset 16 African languages utterances generated native speakers diverse domains including banking travel home dining extensive experiments benchmark fine tuning multilingual transformer models prompting large language models LLMs advantage leveraging African cultural utterances Western centric utterances improving cross lingual transfer English language Experimental results reveal current LLMs struggle slot filling task GPT 4o achieving average performance 26 F1 score contrast intent detection performance notably better average accuracy 70 6 falls fine tuning baselines Compared English language GPT 4o fine tuning baselines perform similarly intent detection achieving accuracy approximately 81 findings suggest performance LLMs low resource African languages work needed improve downstream performance
494,Boosting Long-Context Information Seeking via Query-Guided Activation Refilling,"['Hongjin Qian', 'Zheng Liu', 'Peitian Zhang', 'Zhicheng Dou', 'Defu Lian']",,Boosting Long Context Information Seeking Query Guided Activation Refilling
495,Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration,"['Tianyi Bai', 'Ling Yang', 'Zhen Hao Wong', 'Fupeng Sun', 'Xinlin Zhuang', 'Jiahui Peng', 'Chi Zhang', 'Lijun Wu', 'Qiu Jiantao', 'Wentao Zhang', 'Binhang Yuan', 'Conghui He']","Efficient data selection is crucial to accelerate the pretraining of language model (LMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LM pretraining. To tackle this problem, we propose a multi-actor collaborative data selection mechanism: each data selection method independently prioritizes data based on its criterion and updates its prioritization rules using the current state of the model, functioning as an independent actor for data selection; and a console is designed to adjust the impacts of different actors at various stages and dynamically integrate information from all actors throughout the LM pretraining process. We conduct extensive empirical studies to evaluate our multi-actor framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LM pretraining, and achieves an average relative performance gain up to $10.5\%$ across multiple language model benchmarks compared to the state-of-the-art methods.",Efficient Pretraining Data Selection Language Models Multi Actor Collaboration Efficient data selection crucial accelerate pretraining language model LMs various methods proposed enhance data efficiency limited research addressed inherent conflicts approaches achieve optimal data selection LM pretraining tackle problem propose multi actor collaborative data selection mechanism data selection method independently prioritizes data based criterion updates prioritization rules using current state model functioning independent actor data selection console designed adjust impacts different actors various stages dynamically integrate information actors LM pretraining process conduct extensive empirical studies evaluate multi actor framework experimental results demonstrate approach significantly improves data efficiency accelerates convergence LM pretraining achieves average relative performance gain 10 5 multiple language model benchmarks compared state art methods
496,AdaDHP: Fine-Grained Fine-Tuning via Dual Hadamard Product and Adaptive Parameter Selection,"['Han Liu', 'Changya Li', 'Xiaotong Zhang', 'Feng Zhang', 'Fenglong Ma', 'Wei Wang', 'Hong Yu']",,AdaDHP Fine Grained Fine Tuning Dual Hadamard Product Adaptive Parameter Selection
497,KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph,"['Jinhao Jiang', 'Kun Zhou', 'Xin Zhao', 'Yang Song', 'Chen Zhu', 'Hengshu Zhu', 'Ji-Rong Wen']",,KG Agent Efficient Autonomous Agent Framework Complex Reasoning Knowledge Graph
498,Curriculum Debiasing: Toward Robust Parameter-Efficient Fine-Tuning Against Dataset Biases,"['Mingyu Lee', 'Yeachan Kim', 'Wing-Lam Mok', 'SangKeun Lee']",,Curriculum Debiasing Robust Parameter Efficient Fine Tuning Dataset Biases
499,Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings,"['Austin Xu', 'Srijan Bansal', 'Yifei Ming', 'Semih Yavuz', 'Shafiq Joty']","The large language model (LLM)-as-judge paradigm has been used to meet the demand for a cheap, reliable, and fast evaluation of model outputs during AI system development and post-deployment monitoring. While judge models -- LLMs finetuned to specialize in assessing and critiquing model outputs -- have been touted as general purpose evaluators, they are typically evaluated only on non-contextual scenarios, such as instruction following. The omission of contextual settings -- those where external information is used as context to generate an output -- is surprising given the increasing prevalence of retrieval-augmented generation (RAG) and summarization use cases. Contextual assessment is uniquely challenging, as evaluation often depends on practitioner priorities, leading to conditional evaluation criteria (e.g., comparing responses based on factuality and then considering completeness if they are equally factual). To address the gap, we propose ContextualJudgeBench, a judge benchmark with 2,000 challenging response pairs across eight splits inspired by real-world contextual evaluation scenarios. We build our benchmark with a multi-pronged data construction pipeline that leverages both existing human annotations and model-based perturbations. Our comprehensive study across 11 judge models and 9 general purpose models, reveals that the contextual information and its assessment criteria present a significant challenge to even state-of-the-art models. For example, OpenAI's o1, the best-performing model, barely reaches 55% consistent accuracy.",Does Context Matter ContextualJudgeBench Evaluating LLM based Judges Contextual Settings large language model LLM judge paradigm used meet demand cheap reliable fast evaluation model outputs AI development post deployment monitoring judge models LLMs finetuned specialize assessing critiquing model outputs touted general purpose evaluators typically evaluated non contextual scenarios instruction following omission contextual settings external information used context generate output surprising given increasing prevalence retrieval augmented generation RAG summarization use cases Contextual assessment uniquely challenging evaluation depends practitioner priorities leading conditional evaluation criteria e g comparing responses based factuality considering completeness equally factual address gap propose ContextualJudgeBench judge benchmark 2 000 challenging response pairs splits inspired real world contextual evaluation scenarios build benchmark multi pronged data construction pipeline leverages existing human annotations model based perturbations comprehensive study 11 judge models 9 general purpose models reveals contextual information assessment criteria present significant challenge state art models example OpenAI s o1 best performing model barely reaches 55 consistent accuracy
500,On the Reliability of Large Language Models for Causal Discovery,"['Tao Feng', 'Lizhen Qu', 'Niket Tandon', 'Zhuang Li', 'Xiaoxi Kang', 'Gholamreza Haffari']",,Reliability Large Language Models Causal Discovery
501,Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts,"['Jingxuan Li', 'Yuning Yang', 'Shengqi Yang', 'Linfan Zhang', 'Ying Nian Wu']","The recent progress in Vision-Language Models (VLMs) has broadened the scope of multimodal applications. However, evaluations often remain limited to functional tasks, neglecting abstract dimensions such as personality traits and human values. To address this gap, we introduce Value-Spectrum, a novel Visual Question Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz's value dimensions that capture core human values guiding people's preferences and actions. We design a VLM agent pipeline to simulate video browsing and construct a vector database comprising over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels. These videos span multiple months and cover diverse topics, including family, health, hobbies, society, technology, etc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs handle value-oriented content. Beyond identifying VLMs' intrinsic preferences, we also explore the ability of VLM agents to adopt specific personas when explicitly prompted, revealing insights into the adaptability of the model in role-playing scenarios. These findings highlight the potential of Value-Spectrum as a comprehensive evaluation set for tracking VLM preferences in value-based tasks and abilities to simulate diverse personas. The complete code and data are available at: https://github.com/Jeremyyny/Value-Spectrum.",Value Spectrum Quantifying Preferences Vision Language Models Value Decomposition Social Media Contexts recent progress Vision Language Models VLMs broadened scope multimodal applications evaluations remain limited functional tasks neglecting abstract dimensions personality traits human values address gap introduce Value Spectrum novel Visual Question Answering VQA benchmark aimed assessing VLMs based Schwartz s value dimensions capture core human values guiding people s preferences actions design VLM agent pipeline simulate video browsing construct vector database comprising 50 000 short videos TikTok YouTube Shorts Instagram Reels videos span multiple months cover diverse topics including family health hobbies society technology Benchmarking Value Spectrum highlights notable variations VLMs handle value oriented content identifying VLMs intrinsic preferences explore ability VLM agents adopt specific personas explicitly prompted revealing insights adaptability model role playing scenarios findings highlight potential Value Spectrum comprehensive evaluation set tracking VLM preferences value based tasks abilities simulate diverse personas complete code data available https github com Jeremyyny Value Spectrum
502,TeRDy: Temporal Relation Dynamics through Frequency Decomposition for Temporal Knowledge Graph Completion,"['Ziyang Liu', 'Chaokun Wang']",,TeRDy Temporal Relation Dynamics Frequency Decomposition Temporal Knowledge Graph Completion
503,Incorporating Domain Knowledge into Materials Tokenization,"['Yerim Oh', 'Jun-Hyung Park', 'Junho Kim', 'SungHo Kim', 'SangKeun Lee']","While language models are increasingly utilized in materials science, typical models rely on frequency-centric tokenization methods originally developed for natural language processing. However, these methods frequently produce excessive fragmentation and semantic loss, failing to maintain the structural and semantic integrity of material concepts. To address this issue, we propose MATTER, a novel tokenization approach that integrates material knowledge into tokenization. Based on MatDetector trained on our materials knowledge base and a re-ranking method prioritizing material concepts in token merging, MATTER maintains the structural integrity of identified material concepts and prevents fragmentation during tokenization, ensuring their semantic meaning remains intact. The experimental results demonstrate that MATTER outperforms existing tokenization methods, achieving an average performance gain of $4\%$ and $2\%$ in the generation and classification tasks, respectively. These results underscore the importance of domain knowledge for tokenization strategies in scientific text processing. Our code is available at https://github.com/yerimoh/MATTER",Incorporating Domain Knowledge Materials Tokenization language models increasingly utilized materials science typical models rely frequency centric tokenization methods originally developed natural language processing methods frequently produce excessive fragmentation semantic loss failing maintain structural semantic integrity material concepts address issue propose MATTER novel tokenization approach integrates material knowledge tokenization Based MatDetector trained materials knowledge base ranking method prioritizing material concepts token merging MATTER maintains structural integrity identified material concepts prevents fragmentation tokenization ensuring semantic meaning remains intact experimental results demonstrate MATTER outperforms existing tokenization methods achieving average performance gain 4 2 generation classification tasks respectively results underscore importance domain knowledge tokenization strategies scientific text processing code available https github com yerimoh MATTER
504,PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization,"['Yidan Wang', 'Yanan Cao', 'Yubing Ren', 'Fang Fang', 'Zheng Lin', 'Binxing Fang']","Large Language Models (LLMs) excel in various domains but pose inherent privacy risks. Existing methods to evaluate privacy leakage in LLMs often use memorized prefixes or simple instructions to extract data, both of which well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM safety mechanisms to generate harmful content, but their role in privacy scenarios remains underexplored. In this paper, we examine the effectiveness of jailbreak attacks in extracting sensitive information, bridging privacy leakage and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework targeting Personally Identifiable Information (PII) and addressing the limitations of current jailbreak methods. Specifically, PIG identifies PII entities and their types in privacy queries, uses in-context learning to build a privacy context, and iteratively updates it with three gradient-based strategies to elicit target PII. We evaluate PIG and existing jailbreak methods using two privacy-related datasets. Experiments on four white-box and two black-box LLMs show that PIG outperforms baseline methods and achieves state-of-the-art (SoTA) results. The results underscore significant privacy risks in LLMs, emphasizing the need for stronger safeguards. Our code is availble at https://github.com/redwyd/PrivacyJailbreak.",PIG Privacy Jailbreak Attack LLMs Gradient based Iterative Context Optimization Large Language Models LLMs excel various domains pose inherent privacy risks Existing methods evaluate privacy leakage LLMs use memorized prefixes simple instructions extract data alignment models easily block Jailbreak attacks bypass LLM safety mechanisms generate harmful content role privacy scenarios remains underexplored paper examine effectiveness jailbreak attacks extracting sensitive information bridging privacy leakage jailbreak attacks LLMs propose PIG novel framework targeting Personally Identifiable Information PII addressing limitations current jailbreak methods Specifically PIG identifies PII entities types privacy queries uses context learning build privacy context iteratively updates gradient based strategies elicit target PII evaluate PIG existing jailbreak methods using privacy related datasets Experiments white box black box LLMs PIG outperforms baseline methods achieves state art SoTA results results underscore significant privacy risks LLMs emphasizing need stronger safeguards code availble https github com redwyd PrivacyJailbreak
505,Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks,"['Rana Shahroz', 'Zhen Tan', 'Sukwon Yun', 'Charles Fleming', 'Tianlong Chen']",,Agents Siege Breaking Pragmatic Multi Agent LLM Systems Optimized Prompt Attacks
506,Semantic-Eval : A Semantic Comprehension Evaluation Framework for Large Language Models Generation without Training,"['Shusheng Li', 'Jiale Li', 'Yifei Qu', 'Xinwei Shi', 'Yanliang Guo', 'Ziyi He', 'Yubo Wang', 'Wenjun Tan']",,Semantic Eval Semantic Comprehension Evaluation Framework Large Language Models Generation Training
507,Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases,"['Michael Y. Hu', 'Jackson Petty', 'Chuan Shi', 'William Merrill', 'Tal Linzen']","Pretraining language models on formal language can improve their acquisition of natural language. Which features of the formal language impart an inductive bias that leads to effective transfer? Drawing on insights from linguistics and complexity theory, we hypothesize that effective transfer occurs when two conditions are met: the formal language should capture the dependency structures present in natural language, and it should remain within the computational limitations of the model architecture. We experiment with pre-pretraining (training on formal language before natural languages) on transformers and find that formal languages capturing hierarchical dependencies indeed enable language models to achieve lower loss on natural language and better linguistic generalization compared to other formal languages. We also find modest support for the hypothesis that the formal language should fall within the computational limitations of the architecture. Strikingly, pre-pretraining reduces loss more efficiently than training on a matched amount of natural language. For a 1B-parameter language model trained on roughly 1.6B tokens of natural language, pre-pretraining achieves the same loss and better linguistic generalization with a 33% smaller token budget. Finally, we also give mechanistic evidence of transfer from formal to natural language: attention heads acquired during pre-pretraining remain crucial for the model's performance on syntactic evaluations.",Circuits Chomsky Pre pretraining Formal Languages Imparts Linguistic Biases Pretraining language models formal language improve acquisition natural language features formal language impart inductive bias leads effective transfer Drawing insights linguistics complexity theory hypothesize effective transfer occurs conditions met formal language capture dependency structures present natural language remain computational limitations model architecture experiment pre pretraining training formal language natural languages transformers formal languages capturing hierarchical dependencies enable language models achieve lower loss natural language better linguistic generalization compared formal languages modest support hypothesis formal language fall computational limitations architecture Strikingly pre pretraining reduces loss efficiently training matched natural language 1B parameter language model trained roughly 1 6B tokens natural language pre pretraining achieves loss better linguistic generalization 33 smaller token budget Finally mechanistic evidence transfer formal natural language attention heads acquired pre pretraining remain crucial model s performance syntactic evaluations
508,"When to Speak, When to Abstain: Contrastive Decoding with Abstention","['Hyuhng Joon Kim', 'Youna Kim', 'Sang-goo Lee', 'Taeuk Kim']","Large Language Models (LLMs) demonstrate exceptional performance across diverse tasks by leveraging pre-trained (i.e., parametric) and external (i.e., contextual) knowledge. While substantial efforts have been made to enhance the utilization of both forms of knowledge, situations in which models lack relevant information remain underexplored. To investigate this challenge, we first present a controlled testbed featuring four distinct knowledge access scenarios, including the aforementioned edge case, revealing that conventional LLM usage exhibits insufficient robustness in handling all instances. Addressing this limitation, we propose Contrastive Decoding with Abstention (CDA), a novel training-free decoding method that allows LLMs to generate responses when relevant knowledge is available and to abstain otherwise. CDA estimates the relevance of both knowledge sources for a given input, adaptively deciding which type of information to prioritize and which to exclude. Through extensive experiments, we demonstrate that CDA can effectively perform accurate generation and abstention simultaneously, enhancing reliability and preserving user trust.",Speak Abstain Contrastive Decoding Abstention Large Language Models LLMs demonstrate exceptional performance diverse tasks leveraging pre trained e parametric external e contextual knowledge substantial efforts enhance utilization forms knowledge situations models lack relevant information remain underexplored investigate challenge present controlled testbed featuring distinct knowledge access scenarios including aforementioned edge case revealing conventional LLM usage exhibits insufficient robustness handling instances Addressing limitation propose Contrastive Decoding Abstention CDA novel training free decoding method allows LLMs generate responses relevant knowledge available abstain CDA estimates relevance knowledge sources given input adaptively deciding type information prioritize exclude extensive experiments demonstrate CDA effectively perform accurate generation abstention simultaneously enhancing reliability preserving user trust
509,On the Risk of Evidence Pollution for Malicious Social Text Detection in the Era of LLMs,"['Herun Wan', 'Minnan Luo', 'Zhixiong Su', 'Guang Dai', 'Xiang Zhao']","Evidence-enhanced detectors present remarkable abilities in identifying malicious social text. However, the rise of large language models (LLMs) brings potential risks of evidence pollution to confuse detectors. This paper explores potential manipulation scenarios including basic pollution, and rephrasing or generating evidence by LLMs. To mitigate the negative impact, we propose three defense strategies from the data and model sides, including machine-generated text detection, a mixture of experts, and parameter updating. Extensive experiments on four malicious social text detection tasks with ten datasets illustrate that evidence pollution significantly compromises detectors, where the generating strategy causes up to a 14.4% performance drop. Meanwhile, the defense strategies could mitigate evidence pollution, but they faced limitations for practical employment. Further analysis illustrates that polluted evidence (i) is of high quality, evaluated by metrics and humans; (ii) would compromise the model calibration, increasing expected calibration error up to 21.6%; and (iii) could be integrated to amplify the negative impact, especially for encoder-based LMs, where the accuracy drops by 21.8%.",Risk Evidence Pollution Malicious Social Text Detection Era LLMs Evidence enhanced detectors present remarkable abilities identifying malicious social text rise large language models LLMs brings potential risks evidence pollution confuse detectors paper explores potential manipulation scenarios including basic pollution rephrasing generating evidence LLMs mitigate negative impact propose defense strategies data model sides including machine generated text detection mixture experts parameter updating Extensive experiments malicious social text detection tasks datasets illustrate evidence pollution significantly compromises detectors generating strategy causes 14 4 performance drop defense strategies mitigate evidence pollution faced limitations practical employment analysis illustrates polluted evidence high quality evaluated metrics humans ii compromise model calibration increasing expected calibration error 21 6 iii integrated amplify negative impact especially encoder based LMs accuracy drops 21 8
510,Investigating and Extending Homans’ Social Exchange Theory with Large Language Model based Agents,"['Lei Wang', 'Zheqing Zhang', 'Xu Chen']",,Investigating Extending Homans Social Exchange Theory Large Language Model based Agents
511,A Drop-In Solution for On-the-Fly Adaptation of Speculative Decoding in Large Language Models,"['Jiesong Liu', 'Brian Park', 'Xipeng Shen']",,Drop Solution Fly Adaptation Speculative Decoding Large Language Models
512,"If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?","['Ryo Yoshida', 'Shinnosuke Isono', 'Kohei Kajikawa', 'Taiga Someya', 'Yushi Sugimoto', 'Yohei Oseki']","Recent work in computational psycholinguistics has revealed intriguing parallels between attention mechanisms and human memory retrieval, focusing primarily on vanilla Transformers that operate on token-level representations. However, computational psycholinguistic research has also established that syntactic structures provide compelling explanations for human sentence processing that token-level factors cannot fully account for. In this paper, we investigate whether the attention mechanism of Transformer Grammar (TG), which uniquely operates on syntactic structures as representational units, can serve as a cognitive model of human memory retrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis between models and humans. Our experiments demonstrate that TG's attention achieves superior predictive power for self-paced reading times compared to vanilla Transformer's, with further analyses revealing independent contributions from both models. These findings suggest that human sentence processing involves dual memory representations -- one based on syntactic structures and another on token sequences -- with attention serving as the general memory retrieval algorithm, while highlighting the importance of incorporating syntactic structures as representational units.",Attention Serves Cognitive Model Human Memory Retrieval Plausible Memory Representation Recent work computational psycholinguistics revealed intriguing parallels attention mechanisms human memory retrieval focusing primarily vanilla Transformers operate token level representations computational psycholinguistic research established syntactic structures provide compelling explanations human sentence processing token level factors fully account paper investigate attention mechanism Transformer Grammar TG uniquely operates syntactic structures representational units serve cognitive model human memory retrieval using Normalized Attention Entropy NAE linking hypothesis models humans experiments demonstrate TG s attention achieves superior predictive power self paced reading times compared vanilla Transformer s analyses revealing independent contributions models findings suggest human sentence processing involves dual memory representations based syntactic structures token sequences attention serving general memory retrieval algorithm highlighting importance incorporating syntactic structures representational units
513,Aligning VLM Assistants with Personalized Situated Cognition,"['Yongqi Li', 'Shen Zhou', 'Xiaohu Li', 'Xin Miao', 'Jintao Wen', 'Mayi Xu', 'Jianhao Chen', 'Birong Pan', 'Hankun Kang', 'Yuanyuan Zhu', 'Ming Zhong', 'Tieyun Qian']","Vision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans in managing visual tasks. However, people with diversified backgrounds have different cognition even in the same situation. Consequently, they may have personalized expectations for VLM assistants. This highlights the urgent need to align VLM assistants with personalized situated cognition for real-world assistance. To study this problem, we first simplify it by characterizing individuals based on the sociological concept of Role-Set. Then, we propose to evaluate the individuals' actions to examine whether the personalized alignment is achieved. Further, we construct a benchmark named PCogAlignBench, which includes 18k instances and 20 individuals with different Role-Sets. Finally, we present a framework called PCogAlign, which constructs a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations demonstrate the reliability of the PCogAlignBench and the effectiveness of our proposed PCogAlign. We will open-source the constructed benchmark and code at https://github.com/NLPGM/PCogAlign.",Aligning VLM Assistants Personalized Situated Cognition Vision language models VLMs aligned general human objectives harmless hallucination free valuable assistants humans managing visual tasks people diversified backgrounds different cognition situation Consequently personalized expectations VLM assistants highlights urgent need align VLM assistants personalized situated cognition real world assistance study problem simplify characterizing individuals based sociological concept Role Set propose evaluate individuals actions examine personalized alignment achieved construct benchmark named PCogAlignBench includes 18k instances 20 individuals different Role Sets Finally present framework called PCogAlign constructs cognition aware action based reward model personalized alignment Experimental results human evaluations demonstrate reliability PCogAlignBench effectiveness proposed PCogAlign open source constructed benchmark code https github com NLPGM PCogAlign
514,Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models,"['Zhisong Zhang', 'Yan Wang', 'Xinting Huang', 'Tianqing Fang', 'Hongming Zhang', 'Chenlong Deng', 'Shuaiyi Li', 'Dong Yu']","Large language models have shown remarkable performance across a wide range of language tasks, owing to their exceptional capabilities in context modeling. The most commonly used method of context modeling is full self-attention, as seen in standard decoder-only Transformers. Although powerful, this method can be inefficient for long sequences and may overlook inherent input structures. To address these problems, an alternative approach is parallel context encoding, which splits the context into sub-pieces and encodes them parallelly. Because parallel patterns are not encountered during training, naively applying parallel encoding leads to performance degradation. However, the underlying reasons and potential mitigations are unclear. In this work, we provide a detailed analysis of this issue and identify that unusually high attention entropy can be a key factor. Furthermore, we adopt two straightforward methods to reduce attention entropy by incorporating attention sinks and selective mechanisms. Experiments on various tasks reveal that these methods effectively lower irregular attention entropy and narrow performance gaps. We hope this study can illuminate ways to enhance context modeling mechanisms.",Attention Entropy Key Factor Analysis Parallel Context Encoding attention based Pre trained Language Models Large language models shown remarkable performance wide range language tasks owing exceptional capabilities context modeling commonly used method context modeling self attention seen standard decoder Transformers powerful method inefficient long sequences overlook inherent input structures address problems alternative approach parallel context encoding splits context sub pieces encodes parallelly parallel patterns encountered training naively applying parallel encoding leads performance degradation underlying reasons potential mitigations unclear work provide detailed analysis issue identify unusually high attention entropy key factor Furthermore adopt straightforward methods reduce attention entropy incorporating attention sinks selective mechanisms Experiments various tasks reveal methods effectively lower irregular attention entropy narrow performance gaps hope study illuminate ways enhance context modeling mechanisms
515,Faster Speculative Decoding via Effective Draft Decoder with Pruned Candidate Tree,"['Huanran Zheng', 'Xiaoling Wang']",,Faster Speculative Decoding Effective Draft Decoder Pruned Candidate Tree
516,Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models,"['Zhuojun Ding', 'Wei Wei', 'Chenghao Fan']","Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework's effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework.",Selecting Merging Adaptable Scalable Named Entity Recognition Large Language Models Supervised fine tuning SFT widely used align large language models LLMs information extraction tasks named entity recognition NER annotating fine grained labels training domain specific models costly Existing works typically train unified model multiple domains approaches lack adaptation scalability training data benefits target domains scaling trained models remains challenging propose SaM framework dynamically Selects Merges expert models inference time Specifically target domain select domain specific experts pre trained existing domains based domain similarity target domain ii performance sampled instances respectively experts merged create task specific models optimized target domain dynamically merging experts beneficial target domains improve generalization various domains extra training Additionally experts added removed conveniently leading great scalability Extensive experiments multiple benchmarks demonstrate framework s effectiveness outperforms unified model average 10 provide insights potential improvements practical experience extensions framework
517,Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents,"['Tao Wu', 'Jingyuan Chen', 'Wang Lin', 'Mengze Li', 'Yumeng Zhu', 'Ang Li', 'Kun Kuang', 'Fei Wu']","Large language models (LLMs) are revolutionizing education, with LLM-based agents playing a key role in simulating student behavior. A major challenge in student simulation is modeling the diverse learning patterns of students at various cognitive levels. However, current LLMs, typically trained as ``helpful assistants'', target at generating perfect responses. As a result, they struggle to simulate students with diverse cognitive abilities, as they often produce overly advanced answers, missing the natural imperfections that characterize student learning and resulting in unrealistic simulations. To address this issue, we propose a training-free framework for student simulation. We begin by constructing a cognitive prototype for each student using a knowledge graph, which captures their understanding of concepts from past learning records. This prototype is then mapped to new tasks to predict student performance. Next, we simulate student solutions based on these predictions and iteratively refine them using a beam search method to better replicate realistic mistakes. To validate our approach, we construct the \texttt{Student\_100} dataset, consisting of $100$ students working on Python programming and $5,000$ learning records. Experimental results show that our method consistently outperforms baseline models, achieving $100\%$ improvement in simulation accuracy.",Embracing Imperfection Simulating Students Diverse Cognitive Levels Using LLM based Agents Large language models LLMs revolutionizing education LLM based agents playing key role simulating student behavior major challenge student simulation modeling diverse learning patterns students various cognitive levels current LLMs typically trained helpful assistants target generating perfect responses result struggle simulate students diverse cognitive abilities produce overly advanced answers missing natural imperfections characterize student learning resulting unrealistic simulations address issue propose training free framework student simulation begin constructing cognitive prototype student using knowledge graph captures understanding concepts past learning records prototype mapped new tasks predict student performance simulate student solutions based predictions iteratively refine using beam search method better replicate realistic mistakes validate approach construct texttt Student _100 dataset consisting 100 students working Python programming 5 000 learning records Experimental results method consistently outperforms baseline models achieving 100 improvement simulation accuracy
518,CADReview: Automatically Reviewing CAD Programs with Error Detection and Correction,"['Jiali Chen', 'Xusen Hei', 'HongFei Liu', 'Yuancheng Wei', 'Zikun Deng', 'Jiayuan Xie', 'Yi Cai', 'Li Qing']","Computer-aided design (CAD) is crucial in prototyping 3D objects through geometric instructions (i.e., CAD programs). In practical design workflows, designers often engage in time-consuming reviews and refinements of these prototypes by comparing them with reference images. To bridge this gap, we introduce the CAD review task to automatically detect and correct potential errors, ensuring consistency between the constructed 3D objects and reference images. However, recent advanced multimodal large language models (MLLMs) struggle to recognize multiple geometric components and perform spatial geometric operations within the CAD program, leading to inaccurate reviews. In this paper, we propose the CAD program repairer (ReCAD) framework to effectively detect program errors and provide helpful feedback on error correction. Additionally, we create a dataset, CADReview, consisting of over 20K program-image pairs, with diverse errors for the CAD review task. Extensive experiments demonstrate that our ReCAD significantly outperforms existing MLLMs, which shows great potential in design applications.",CADReview Automatically Reviewing CAD Programs Error Detection Correction Computer aided design CAD crucial prototyping 3D objects geometric instructions e CAD programs practical design workflows designers engage time consuming reviews refinements prototypes comparing reference images bridge gap introduce CAD review task automatically detect correct potential errors ensuring consistency constructed 3D objects reference images recent advanced multimodal large language models MLLMs struggle recognize multiple geometric components perform spatial geometric operations CAD program leading inaccurate reviews paper propose CAD program repairer ReCAD framework effectively detect program errors provide helpful feedback error correction Additionally create dataset CADReview consisting 20K program image pairs diverse errors CAD review task Extensive experiments demonstrate ReCAD significantly outperforms existing MLLMs shows great potential design applications
519,Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling,"['Junyi Li', 'Hwee Tou Ng']",,Think Cite Improving Attributed Text Generation Self Guided Tree Search Progress Reward Modeling
520,The Lawyer That Never Thinks: Consistency and Fairness as Keys to Reliable AI,"['Dana R Alsagheer', 'Abdulrahman Kamal', 'Mohammad Kamal', 'Cosmo Yang Wu', 'Weidong Shi']",,Lawyer Thinks Consistency Fairness Keys Reliable AI
521,Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean,"['SungHo Kim', 'Nayeon Kim', 'Taehee Jeon', 'SangKeun Lee']","We introduce the $\underline{Ko}rean \underline{G}rammar \underline{E}valuation Bench\underline{M}ark (KoGEM)$, designed to assess the linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k multiple-choice QA pairs covering five main categories and 16 subcategories. The zero-shot evaluation of 27 LLMs of various sizes and types reveals that while LLMs perform remarkably well on straightforward tasks requiring primarily definitional knowledge, they struggle with tasks that demand the integration of real-world experiential knowledge, such as phonological rules and pronunciation. Furthermore, our in-depth analysis suggests that incorporating such experiential knowledge could enhance the linguistic competence of LLMs. With KoGEM, we not only highlight the limitations of current LLMs in linguistic competence but also uncover hidden facets of LLMs in linguistic competence, paving the way for enhancing comprehensive language understanding. Our code and dataset are available at: https://github.com/SungHo3268/KoGEM.",Polishing Facet GEM Testing Linguistic Competence LLMs Humans Korean introduce underline Ko rean underline G rammar underline E valuation Bench underline M ark KoGEM designed assess linguistic competence LLMs humans Korean KoGEM consists 1 5k multiple choice QA pairs covering main categories 16 subcategories zero shot evaluation 27 LLMs various sizes types reveals LLMs perform remarkably straightforward tasks requiring primarily definitional knowledge struggle tasks demand integration real world experiential knowledge phonological rules pronunciation Furthermore depth analysis suggests incorporating experiential knowledge enhance linguistic competence LLMs KoGEM highlight limitations current LLMs linguistic competence uncover hidden facets LLMs linguistic competence paving way enhancing comprehensive language understanding code dataset available https github com SungHo3268 KoGEM
522,SpeechFake: A Large-Scale Multilingual Speech Deepfake Dataset Incorporating Cutting-Edge Generation Methods,"['Wen Huang', 'Yanmei Gu', 'Zhiming Wang', 'Huijia Zhu', 'Yanmin Qian']",,SpeechFake Large Scale Multilingual Speech Deepfake Dataset Incorporating Cutting Edge Generation Methods
523,ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation,"['Houxing Ren', 'Mingjie Zhan', 'Zhongyuan Wu', 'Aojun Zhou', 'Junting Pan', 'Hongsheng Li']","Code generation plays a crucial role in various tasks, such as code auto-completion and mathematical reasoning. Previous work has proposed numerous methods to enhance code generation performance, including integrating feedback from the compiler. Inspired by this, we present ReflectionCoder, a novel approach that effectively leverages reflection sequences constructed by integrating compiler feedback to improve one-off code generation performance. Furthermore, we propose reflection self-distillation and dynamically masked distillation to effectively utilize these reflection sequences. Extensive experiments on three benchmarks, i.e., HumanEval (+), MBPP (+), and MultiPL-E, demonstrate that models fine-tuned with our method achieve state-of-the-art performance. Beyond the code domain, we believe this approach can benefit other domains that focus on final results and require long reasoning paths. Code and data are available at https://github.com/SenseLLM/ReflectionCoder.",ReflectionCoder Learning Reflection Sequence Enhanced Code Generation Code generation plays crucial role various tasks code auto completion mathematical reasoning Previous work proposed numerous methods enhance code generation performance including integrating feedback compiler Inspired present ReflectionCoder novel approach effectively leverages reflection sequences constructed integrating compiler feedback improve code generation performance Furthermore propose reflection self distillation dynamically masked distillation effectively utilize reflection sequences Extensive experiments benchmarks e HumanEval MBPP MultiPL E demonstrate models fine tuned method achieve state art performance code domain believe approach benefit domains focus final results require long reasoning paths Code data available https github com SenseLLM ReflectionCoder
524,InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes Under Herd Behavior,"['Huisheng Wang', 'Zhuoshi Pan', 'Hangjing Zhang', 'Mingxiao Liu', 'Hanqing Gao', 'H. Vicky Zhao']",,InvestAlign Overcoming Data Scarcity Aligning Large Language Models Investor Decision Making Processes Herd Behavior
525,Enhancing Neural Machine Translation Through Target Language Data: A $k$NN-LM Approach for Domain Adaptation,"['Abudurexiti Reheman', 'Hongyu Liu', 'Junhao Ruan', 'Abudukeyumu Abudula', 'yingfeng luo', 'Tong Xiao', 'JingBo Zhu']",,Enhancing Neural Machine Translation Target Language Data k NN LM Approach Domain Adaptation
526,Multi-level Relevance Document Identifier Learning for Generative Retrieval,"['Fuwei Zhang', 'Xiaoyu Liu', 'Xinyu Jia', 'Yingfei Zhang', 'Shuai Zhang', 'Xiang Li', 'Fuzhen Zhuang', 'Wei Lin', 'Zhao Zhang']",,Multi level Relevance Document Identifier Learning Generative Retrieval
527,EfficientQAT: Efficient Quantization-Aware Training for Large Language Models,"['Mengzhao Chen', 'Wenqi Shao', 'Peng Xu', 'Jiahao Wang', 'Peng Gao', 'Kaipeng Zhang', 'Ping Luo']","Large language models (LLMs) are crucial in modern natural language processing and artificial intelligence. However, they face challenges in managing their significant memory requirements. Although quantization-aware training (QAT) offers a solution by reducing memory consumption through low-bit representations with minimal accuracy loss, it is impractical due to substantial training resources. To address this, we propose Efficient Quantization-Aware Training (EfficientQAT), a more feasible QAT algorithm. EfficientQAT involves two consecutive phases: Block-wise training of all parameters (Block-AP) and end-to-end training of quantization parameters (E2E-QP). To the best of our knowledge, Block-AP is the first method to enable direct training of all parameters in a block-wise manner, reducing accuracy loss in low-bit scenarios by enhancing the solution space during optimization. E2E-QP then trains only the quantization parameters (step sizes) end-to-end, further improving the performance of quantized models by considering interactions among all sub-modules. Extensive experiments demonstrate that EfficientQAT outperforms previous quantization methods across a range of models, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with scales from 7B to 70B parameters at various quantization bits. For instance, EfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41 hours, with less than 3 points accuracy degradation compared to the full precision (69.48 vs. 72.41). Code is available at https://github.com/OpenGVLab/EfficientQAT.",EfficientQAT Efficient Quantization Aware Training Large Language Models Large language models LLMs crucial modern natural language processing artificial intelligence face challenges managing significant memory requirements quantization aware training QAT offers solution reducing memory consumption low bit representations minimal accuracy loss impractical substantial training resources address propose Efficient Quantization Aware Training EfficientQAT feasible QAT algorithm EfficientQAT involves consecutive phases Block wise training parameters Block AP end end training quantization parameters E2E QP best knowledge Block AP method enable direct training parameters block wise manner reducing accuracy loss low bit scenarios enhancing solution space optimization E2E QP trains quantization parameters step sizes end end improving performance quantized models considering interactions sub modules Extensive experiments demonstrate EfficientQAT outperforms previous quantization methods range models including base LLMs instruction tuned LLMs multimodal LLMs scales 7B 70B parameters various quantization bits instance EfficientQAT obtains 2 bit Llama 2 70B model single A100 80GB GPU 41 hours 3 points accuracy degradation compared precision 69 48 vs 72 41 Code available https github com OpenGVLab EfficientQAT
528,Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder,"['Siting Li', 'Pang Wei Koh', 'Simon Shaolei Du']","Recent research has shown that CLIP models struggle with visual reasoning tasks that require grounding compositionality, understanding spatial relationships, or capturing fine-grained details. One natural hypothesis is that the CLIP vision encoder does not embed essential information for these tasks. However, we find that this is not always the case: The encoder gathers query-relevant visual information, while CLIP fails to extract it. In particular, we show that another branch of Vision-Language Models (VLMs), Generative Multimodal Large Language Models (MLLMs), achieve significantly higher accuracy than CLIP in many of these tasks using the same vision encoder and weights, indicating that these Generative MLLMs perceive more -- as they extract and utilize visual information more effectively. We conduct a series of controlled experiments and reveal that their success is attributed to multiple key design choices, including patch tokens, position embeddings, and prompt-based weighting. On the other hand, enhancing the training data alone or applying a stronger text encoder does not suffice to solve the task, and additional text tokens offer little benefit. Interestingly, we find that fine-grained visual reasoning is not exclusive to generative models trained by an autoregressive loss: When converted into CLIP-like encoders by contrastive finetuning, these MLLMs still outperform CLIP under the same cosine similarity-based evaluation protocol. Our study highlights the importance of VLM architectural choices and suggests directions for improving the performance of CLIP-like contrastive VLMs.",Exploring Generative MLLMs Perceive CLIP Vision Encoder Recent research shown CLIP models struggle visual reasoning tasks require grounding compositionality understanding spatial relationships capturing fine grained details natural hypothesis CLIP vision encoder does embed essential information tasks case encoder gathers query relevant visual information CLIP fails extract particular branch Vision Language Models VLMs Generative Multimodal Large Language Models MLLMs achieve significantly higher accuracy CLIP tasks using vision encoder weights indicating Generative MLLMs perceive extract utilize visual information effectively conduct series controlled experiments reveal success attributed multiple key design choices including patch tokens position embeddings prompt based weighting hand enhancing training data applying stronger text encoder does suffice solve task additional text tokens offer little benefit Interestingly fine grained visual reasoning exclusive generative models trained autoregressive loss converted CLIP like encoders contrastive finetuning MLLMs outperform CLIP cosine similarity based evaluation protocol study highlights importance VLM architectural choices suggests directions improving performance CLIP like contrastive VLMs
529,NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization,"['Hyuntak Kim', 'Byung-Hak Kim']","Summarizing long-form narratives--such as books, movies, and TV scripts--requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipeline--without requiring fine-tuning. Our approach introduces two key innovations: (1) Dialogue-to-Description Transformation: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. (2) Hierarchical Multi-LLM Summarization: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains.",NexusSum Hierarchical LLM Agents Long Form Narrative Summarization Summarizing long form narratives books movies TV scripts requires capturing intricate plotlines character interactions thematic coherence task remains challenging existing LLMs introduce NexusSum multi agent LLM framework narrative summarization processes long form text structured sequential pipeline requiring fine tuning approach introduces key innovations 1 Dialogue Description Transformation narrative specific preprocessing method standardizes character dialogue descriptive text unified format improving coherence 2 Hierarchical Multi LLM Summarization structured summarization pipeline optimizes chunk processing controls output length accurate high quality summaries method establishes new state art narrative summarization achieving 30 0 improvement BERTScore F1 books movies TV scripts results demonstrate effectiveness multi agent LLMs handling long form content offering scalable approach structured summarization diverse storytelling domains
530,HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models,"['Xiao Wang', 'Jingyun Hua', 'Weihong Lin', 'Yuanxing Zhang', 'Fuzheng Zhang', 'Jianlong Wu', 'Di ZHANG', 'Liqiang Nie']","Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. \textbf{HAICTrain} comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, \textbf{HAICBench} includes 412 manually annotated video-caption pairs and 2,000 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.",HAIC Improving Human Action Understanding Generation Better Captions Multi modal Large Language Models Recent Multi modal Large Language Models MLLMs great progress video understanding performance videos involving human actions limited lack high quality data address introduce stage data annotation pipeline design strategies accumulate videos featuring clear human actions Internet Second videos annotated standardized caption format uses human attributes distinguish individuals chronologically details actions interactions pipeline curate datasets HAICTrain HAICBench textbf HAICTrain comprises 126K video caption pairs generated Gemini Pro verified training purposes textbf HAICBench includes 412 manually annotated video caption pairs 2 000 QA pairs comprehensive evaluation human action understanding Experimental results demonstrate training HAICTrain significantly enhances human understanding abilities 4 benchmarks improve text video generation results HAICTrain HAICBench released https huggingface datasets KuaishouHAIC HAIC
531,Uni-Retrieval: A Multi-Style Retrieval Framework for STEM’s Education,"['Yanhao Jia', 'Xinyi Wu', 'Li Hao', 'QinglinZhang', 'Yuxiao Hu', 'Shuai Zhao', 'Wenqi Fan']",,Uni Retrieval Multi Style Retrieval Framework STEM s Education
532,DenseLoRA: Dense Low-Rank Adaptation of Large Language Models,"['Lin Mu', 'Xiaoyu Wang', 'Li Ni', 'Yang Li', 'Zhize Wu', 'Peiquan Jin', 'Yiwen Zhang']","Low-rank adaptation (LoRA) has been developed as an efficient approach for adapting large language models (LLMs) by fine-tuning two low-rank matrices, thereby reducing the number of trainable parameters. However, prior research indicates that many of the weights in these matrices are redundant, leading to inefficiencies in parameter utilization. To address this limitation, we introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances parameter efficiency while achieving superior performance compared to LoRA. DenseLoRA builds upon the concept of representation fine-tuning, incorporating a single Encoder-Decoder to refine and compress hidden representations across all adaptation layers before applying adaptation. Instead of relying on two redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense low-rank matrix, improving parameter utilization and adaptation efficiency. We evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8% accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we conduct extensive experiments to systematically assess the impact of DenseLoRA's components on overall model performance. Code is available at https://github.com/mulin-ahu/DenseLoRA.",DenseLoRA Dense Low Rank Adaptation Large Language Models Low rank adaptation LoRA developed efficient approach adapting large language models LLMs fine tuning low rank matrices reducing number trainable parameters prior research indicates weights matrices redundant leading inefficiencies parameter utilization address limitation introduce Dense Low Rank Adaptation DenseLoRA novel approach enhances parameter efficiency achieving superior performance compared LoRA DenseLoRA builds concept representation fine tuning incorporating single Encoder Decoder refine compress hidden representations adaptation layers applying adaptation Instead relying redundant low rank matrices LoRA DenseLoRA adapts LLMs dense low rank matrix improving parameter utilization adaptation efficiency evaluate DenseLoRA various benchmarks showing achieves 83 8 accuracy 0 01 trainable parameters compared LoRA s 80 8 accuracy 0 70 trainable parameters LLaMA3 8B Additionally conduct extensive experiments systematically assess impact DenseLoRA s components overall model performance Code available https github com mulin ahu DenseLoRA
533,"Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis","['Jisoo Mok', 'Ik-hwan Kim', 'Sangkwon Park', 'Sungroh Yoon']","Personalized AI assistants, a hallmark of the human-like capabilities of Large Language Models (LLMs), are a challenging application that intertwines multiple problems in LLM research. Despite the growing interest in the development of personalized assistants, the lack of an open-source conversational dataset tailored for personalization remains a significant obstacle for researchers in the field. To address this research gap, we introduce HiCUPID, a new benchmark to probe and unleash the potential of LLMs to deliver personalized responses. Alongside a conversational dataset, HiCUPID provides a Llama-3.2-based automated evaluation model whose assessment closely mirrors human preferences. We release our dataset, evaluation model, and code at https://github.com/12kimih/HiCUPID.",Exploring Potential LLMs Personalized Assistants Dataset Evaluation Analysis Personalized AI assistants hallmark human like capabilities Large Language Models LLMs challenging application intertwines multiple problems LLM research Despite growing development personalized assistants lack open source conversational dataset tailored personalization remains significant obstacle researchers field address research gap introduce HiCUPID new benchmark probe unleash potential LLMs deliver personalized responses Alongside conversational dataset HiCUPID provides Llama 3 2 based automated evaluation model assessment closely mirrors human preferences release dataset evaluation model code https github com 12kimih HiCUPID
534,Cracking Factual Knowledge: A Comprehensive Analysis of Degenerate Knowledge Neurons in Large Language Models,"['Yuheng Chen', 'Pengfei Cao', 'Yubo Chen', 'Yining Wang', 'Shengping Liu', 'Kang Liu', 'Jun Zhao']","Large language models (LLMs) store extensive factual knowledge, but the underlying mechanisms remain unclear. Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). Despite the novelty and unique properties of this concept, it has not been rigorously defined or systematically studied. We first consider the connection weight patterns of MLP neurons and define DKNs from both structural and functional aspects. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, inspired by cognitive science, we explore the relationship between DKNs and the robustness, evolvability, and complexity of LLMs. Our execution of 34 experiments under 6 settings demonstrates the connection between DKNs and these three properties. The code will be available soon.",Cracking Factual Knowledge Comprehensive Analysis Degenerate Knowledge Neurons Large Language Models Large language models LLMs store extensive factual knowledge underlying mechanisms remain unclear Previous research suggests factual knowledge stored multi layer perceptron weights storage units exhibit degeneracy referred Degenerate Knowledge Neurons DKNs Despite novelty unique properties concept rigorously defined systematically studied consider connection weight patterns MLP neurons define DKNs structural functional aspects Based introduce Neurological Topology Clustering method allows formation DKNs numbers structures leading accurate DKN acquisition Furthermore inspired cognitive science explore relationship DKNs robustness evolvability complexity LLMs execution 34 experiments 6 settings demonstrates connection DKNs properties code available soon
535,Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach,"['Shenglai Zeng', 'Pengfei He', 'Kai Guo', 'Tianqi Zheng', 'Hanqing Lu', 'Yue Xing', 'Hui Liu']","Large Language Models (LLMs) enhanced with external contexts, such as through retrieval-augmented generation (RAG), often face challenges in handling imperfect evidence. They tend to over-rely on external knowledge, making them vulnerable to misleading and unhelpful contexts. To address this, we propose the concept of context-robust LLMs, which can effectively balance internal knowledge with external context, similar to human cognitive processes. Specifically, context-robust LLMs should rely on external context only when lacking internal knowledge, identify contradictions between internal and external knowledge, and disregard unhelpful contexts. To achieve this goal, we introduce Grft, a lightweight and plug-and-play gated representation fine-tuning approach. Grft consists of two key components: a gating mechanism to detect and filter problematic inputs, and low-rank representation adapters to adjust hidden representations. By training a lightweight intervention function with only 0.0004\% of model size on fewer than 200 examples, Grft can effectively adapt LLMs towards context-robust behaviors.",Context Robust LLMs Gated Representation Fine tuning Approach Large Language Models LLMs enhanced external contexts retrieval augmented generation RAG face challenges handling imperfect evidence tend rely external knowledge making vulnerable misleading unhelpful contexts address propose concept context robust LLMs effectively balance internal knowledge external context similar human cognitive processes Specifically context robust LLMs rely external context lacking internal knowledge identify contradictions internal external knowledge disregard unhelpful contexts achieve goal introduce Grft lightweight plug play gated representation fine tuning approach Grft consists key components gating mechanism detect filter problematic inputs low rank representation adapters adjust hidden representations training lightweight intervention function 0 0004 model size fewer 200 examples Grft effectively adapt LLMs context robust behaviors
536,Seeking Rational Demonstrations for Large Language Models: A Domain Generalization Approach to Unsupervised Cross-Domain Keyphrase Generation,"['Guangzhen Zhao', 'Yu Yao', 'Dechang Kong', 'Zhenjiang Dong']",,Seeking Rational Demonstrations Large Language Models Domain Generalization Approach Unsupervised Cross Domain Keyphrase Generation
537,On Support Samples of Next Word Prediction,"['Yuqian Li', 'Yupei Du', 'Yufang Liu', 'Feifei Feng', 'Mou Xiao Feng', 'Yuanbin Wu']","Language models excel in various tasks by making complex decisions, yet understanding the rationale behind these decisions remains a challenge. This paper investigates \emph{data-centric interpretability} in language models, focusing on the next-word prediction task. Using representer theorem, we identify two types of \emph{support samples}-those that either promote or deter specific predictions. Our findings reveal that being a support sample is an intrinsic property, predictable even before training begins. Additionally, while non-support samples are less influential in direct predictions, they play a critical role in preventing overfitting and shaping generalization and representation learning. Notably, the importance of non-support samples increases in deeper layers, suggesting their significant role in intermediate representation formation. These insights shed light on the interplay between data and model decisions, offering a new dimension to understanding language model behavior and interpretability.",Support Samples Word Prediction Language models excel various tasks making complex decisions understanding rationale decisions remains challenge paper investigates emph data centric interpretability language models focusing word prediction task Using representer theorem identify types emph support samples promote deter specific predictions findings reveal support sample intrinsic property predictable training begins Additionally non support samples influential direct predictions play critical role preventing overfitting shaping generalization representation learning Notably importance non support samples increases deeper layers suggesting significant role intermediate representation formation insights shed light interplay data model decisions offering new dimension understanding language model behavior interpretability
538,WebWalker: Benchmarking LLMs in Web Traversal,"['Jialong Wu', 'Wenbiao Yin', 'Yong Jiang', 'Zhenglin Wang', 'Zekun Xi', 'Runnan Fang', 'Linhai Zhang', 'Yulan He', 'Deyu Zhou', 'Pengjun Xie', 'Fei Huang']","Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website's subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios.",WebWalker Benchmarking LLMs Web Traversal Retrieval augmented generation RAG demonstrates remarkable performance tasks open domain question answering traditional search engines retrieve shallow content limiting ability LLMs handle complex multi layered information address introduce WebWalkerQA benchmark designed assess ability LLMs perform web traversal evaluates capacity LLMs traverse website s subpages extract high quality data systematically propose WebWalker multi agent framework mimics human like web navigation explore critic paradigm Extensive experimental results WebWalkerQA challenging demonstrates effectiveness RAG combined WebWalker horizontal vertical integration real world scenarios
539,From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models,"['Yidan Wang', 'Yubing Ren', 'Yanan Cao', 'Binxing Fang']","The rise of Large Language Models (LLMs) has heightened concerns about the misuse of AI-generated text, making watermarking a promising solution. Mainstream watermarking schemes for LLMs fall into two categories: logits-based and sampling-based. However, current schemes entail trade-offs among robustness, text quality, and security. To mitigate this, we integrate logits-based and sampling-based schemes, harnessing their respective strengths to achieve synergy. In this paper, we propose a versatile symbiotic watermarking framework with three strategies: serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy, optimizing the balance between detectability, robustness, text quality, and security. Furthermore, we validate our approach through comprehensive experiments on various datasets and models. Experimental results indicate that our method outperforms existing baselines and achieves state-of-the-art (SOTA) performance. We believe this framework provides novel insights into diverse watermarking paradigms. Our code is available at https://github.com/redwyd/SymMark.",Trade Synergy Versatile Symbiotic Watermarking Framework Large Language Models rise Large Language Models LLMs heightened concerns misuse AI generated text making watermarking promising solution Mainstream watermarking schemes LLMs fall categories logits based sampling based current schemes entail trade offs robustness text quality security mitigate integrate logits based sampling based schemes harnessing respective strengths achieve synergy paper propose versatile symbiotic watermarking framework strategies serial parallel hybrid hybrid framework adaptively embeds watermarks using token entropy semantic entropy optimizing balance detectability robustness text quality security Furthermore validate approach comprehensive experiments various datasets models Experimental results indicate method outperforms existing baselines achieves state art SOTA performance believe framework provides novel insights diverse watermarking paradigms code available https github com redwyd SymMark
540,AutoGUI: Scaling GUI Grounding with Automatic Functionality Annotations from LLMs,"['Hongxin Li', 'Jingfan CHEN', 'Jingran Su', 'Yuntao Chen', 'Li Qing', 'Zhaoxiang Zhang']","User interface understanding with vision-language models (VLMs) has received much attention due to its potential for enhancing software automation. However, existing datasets used to build UI-VLMs either only contain large-scale context-free element annotations or contextualized functional descriptions for elements at a small scale. In this work, we propose the \textbf{AutoGUI} pipeline for automatically annotating UI elements with detailed functionality descriptions at scale. Specifically, we leverage large language models (LLMs) to infer element functionality by comparing UI state changes before and after simulated interactions. To improve annotation quality, we propose LLM-aided rejection and verification, eliminating invalid annotations without human labor. We construct a high-quality AutoGUI-704k dataset using the proposed pipeline, featuring diverse and detailed functionality annotations that are hardly provided by previous datasets. Human evaluation shows that we achieve annotation correctness comparable to a trained human annotator. Extensive experiments show that our dataset remarkably enhances VLM's UI grounding capabilities and exhibits significant scaling effects. We also show the interesting potential use of our dataset in UI agent tasks. Please view our project at https://autogui-project.github.io/.",AutoGUI Scaling GUI Grounding Automatic Functionality Annotations LLMs User interface understanding vision language models VLMs received attention potential enhancing software automation existing datasets used build UI VLMs contain large scale context free element annotations contextualized functional descriptions elements small scale work propose textbf AutoGUI pipeline automatically annotating UI elements detailed functionality descriptions scale Specifically leverage large language models LLMs infer element functionality comparing UI state changes simulated interactions improve annotation quality propose LLM aided rejection verification eliminating invalid annotations human labor construct high quality AutoGUI 704k dataset using proposed pipeline featuring diverse detailed functionality annotations hardly provided previous datasets Human evaluation shows achieve annotation correctness comparable trained human annotator Extensive experiments dataset remarkably enhances VLM s UI grounding capabilities exhibits significant scaling effects interesting potential use dataset UI agent tasks view project https autogui project github io
541,Introducing Graph Context into Language Models through Parameter-Efficient Fine-Tuning for Lexical Relation Mining,"['Jingwen Sun', 'Zhiyi Tian', 'Yu He', 'Jingwei Sun', 'Guangzhong Sun']",,Introducing Graph Context Language Models Parameter Efficient Fine Tuning Lexical Relation Mining
542,S-RAG: A Novel Audit Framework for Detecting Unauthorized Use of Personal Data in RAG Systems,"['Zhirui Zeng', 'Jiamou Liu', 'Meng-Fen Chiang', 'Jialing He', 'Zijian Zhang']",,S RAG Novel Audit Framework Detecting Unauthorized Use Personal Data RAG Systems
543,Praetor: A Fine-Grained Generative LLM Evaluator with Instance-Level Customizable Evaluation Criteria,"['Yongqi Leng', 'Renren Jin', 'Yue chen', 'Zhuowen Han', 'Ling Shi', 'Jianxiang Peng', 'Lei Yang', 'Juesi Xiao', 'Deyi Xiong']",,Praetor Fine Grained Generative LLM Evaluator Instance Level Customizable Evaluation Criteria
544,LexKeyPlan: Planning with Keyphrases and Retrieval Augmentation for Legal Text Generation: A Case Study on European Court of Human Rights Cases,"['Santosh T.Y.S.S', 'Elvin Quero Hernandez']",,LexKeyPlan Planning Keyphrases Retrieval Augmentation Legal Text Generation Case Study European Court Human Rights Cases
545,Mitigating Gender Confounding Bias from Spoken Language in Dementia Detection via Weight Masking,"['Zhecheng Sheng', 'Xiruo Ding', 'Brian Hur', 'Changye Li', 'Trevor Cohen', 'Serguei V. S. Pakhomov']",,Mitigating Gender Confounding Bias Spoken Language Dementia Detection Weight Masking
546,MCS-Bench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in Chinese Classical Studies,"['Yang Liu', 'Jiahuan Cao', 'Hiuyi Cheng', 'Yongxin Shi', 'Kai Ding', 'Lianwen Jin']",,MCS Bench Comprehensive Benchmark Evaluating Multimodal Large Language Models Chinese Classical Studies
547,The Knowledge Microscope: Features as Better Analytical Lenses than Neurons,"['Yuheng Chen', 'Pengfei Cao', 'Kang Liu', 'Jun Zhao']","Previous studies primarily utilize MLP neurons as units of analysis for understanding the mechanisms of factual knowledge in Language Models (LMs); however, neurons suffer from polysemanticity, leading to limited knowledge expression and poor interpretability. In this paper, we first conduct preliminary experiments to validate that Sparse Autoencoders (SAE) can effectively decompose neurons into features, which serve as alternative analytical units. With this established, our core findings reveal three key advantages of features over neurons: (1) Features exhibit stronger influence on knowledge expression and superior interpretability. (2) Features demonstrate enhanced monosemanticity, showing distinct activation patterns between related and unrelated facts. (3) Features achieve better privacy protection than neurons, demonstrated through our proposed FeatureEdit method, which significantly outperforms existing neuron-based approaches in erasing privacy-sensitive information from LMs.Code and dataset will be available.",Knowledge Microscope Features Better Analytical Lenses Neurons Previous studies primarily utilize MLP neurons units analysis understanding mechanisms factual knowledge Language Models LMs neurons suffer polysemanticity leading limited knowledge expression poor interpretability paper conduct preliminary experiments validate Sparse Autoencoders SAE effectively decompose neurons features serve alternative analytical units established core findings reveal key advantages features neurons 1 Features exhibit stronger influence knowledge expression superior interpretability 2 Features demonstrate enhanced monosemanticity showing distinct activation patterns related unrelated facts 3 Features achieve better privacy protection neurons demonstrated proposed FeatureEdit method significantly outperforms existing neuron based approaches erasing privacy sensitive information LMs Code dataset available
548,From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding,"['Chiwei Zhu', 'Benfeng Xu', 'Xiaorui Wang', 'Zhendong Mao']","The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.",Real Synthetic Synthesizing Millions Diversified Complicated User Instructions Attributed Grounding pursuit diverse complex large scale instruction data crucial automatically aligning large language models LLMs methods capable generating synthetic instructions scale suffer limited grounding sources leading narrow distribution rely trivial extensions fail produce meaningful trajectories terms complexity contrast instructions benefit efficient alignment typically crafted cognitive insights grounded real world use cases paper synthesize instructions using attributed grounding involves 1 attribution process grounds selective set real instructions situated users 2 synthesis process leverages web documents generate situation meaningful instruction framework allows harvest diverse complex instructions scale utilizing vast range web documents Specifically construct dataset 1 million instructions called SynthQuestions demonstrate models trained achieve leading performance common benchmarks improvements continually scale web corpora Data models codes available https github com Ignoramus0817 SynthQuestions
549,PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance,"['Haoran Li', 'Wenbin Hu', 'Huihao JING', 'Yulin Chen', 'Qi Hu', 'Sirui Han', 'Tianshu Chu', 'Peizhao Hu', 'Yangqiu Song']","Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility. However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals' data privacy. Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs' privacy awareness and robustness from their generated outputs to their hidden representations. Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII). In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows. We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs' privacy and safety compliance. We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance.",PrivaCI Bench Evaluating Privacy Contextual Integrity Legal Compliance Recent advancements generative large language models LLMs enabled wider applicability accessibility flexibility reliability trustworthiness doubt especially concerns regarding individuals data privacy Great efforts privacy building various evaluation benchmarks study LLMs privacy awareness robustness generated outputs hidden representations Unfortunately works adopt narrow formulation privacy investigate personally identifiable information PII paper follow merit Contextual Integrity CI theory posits privacy evaluation cover transmitted attributes encompass relevant social context private information flows present PrivaCI Bench comprehensive contextual privacy evaluation benchmark targeted legal compliance cover annotated privacy safety regulations real court cases privacy policies synthetic data built official toolkit study LLMs privacy safety compliance evaluate latest LLMs including recent reasoner models QwQ 32B Deepseek R1 experimental results suggest LLMs effectively capture key CI parameters inside given context require advancements privacy compliance
550,Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View,"['Yanran Wu', 'Inez Hua', 'Yi Ding']","Large language models (LLMs) offer powerful capabilities but come with significant environmental impact, particularly in carbon emissions. Existing studies benchmark carbon emissions but lack a standardized basis for comparison across different model configurations. To address this, we introduce the concept of functional unit (FU) as a standardized basis and develop FUEL, the first FU-based framework for evaluating LLM serving's environmental impact. Through three case studies, we uncover key insights and trade-offs in reducing carbon emissions by optimizing model size, quantization strategy, and hardware choice, paving the way for more sustainable LLM serving. The code is available at https://github.com/jojacola/FUEL.",Unveiling Environmental Impacts Large Language Model Serving Functional Unit View Large language models LLMs offer powerful capabilities come significant environmental impact particularly carbon emissions Existing studies benchmark carbon emissions lack standardized basis comparison different model configurations address introduce concept functional unit FU standardized basis develop FUEL FU based framework evaluating LLM serving s environmental impact case studies uncover key insights trade offs reducing carbon emissions optimizing model size quantization strategy hardware choice paving way sustainable LLM serving code available https github com jojacola FUEL
551,ExpeTrans: LLMs Are Experiential Transfer Learners,"['Jinglong Gao', 'Xiao Ding', 'Lingxiao Zou', 'Bibo Cai', 'Bing Qin', 'Ting Liu']","Recent studies provide large language models (LLMs) with textual task-solving experiences via prompts to improve their performance. However, previous methods rely on substantial human labor or time to gather such experiences for each task, which is impractical given the growing variety of task types in user queries to LLMs. To address this issue, we design an autonomous experience transfer framework to explore whether LLMs can mimic human cognitive intelligence to autonomously transfer experience from existing source tasks to newly encountered target tasks. This not only allows the acquisition of experience without extensive costs of previous methods, but also offers a novel path for the generalization of LLMs. Experimental results on 13 datasets demonstrate that our framework effectively improves the performance of LLMs. Furthermore, we provide a detailed analysis of each module in the framework.",ExpeTrans LLMs Experiential Transfer Learners Recent studies provide large language models LLMs textual task solving experiences prompts improve performance previous methods rely substantial human labor time gather experiences task impractical given growing variety task types user queries LLMs address issue design autonomous experience transfer framework explore LLMs mimic human cognitive intelligence autonomously transfer experience existing source tasks newly encountered target tasks allows acquisition experience extensive costs previous methods offers novel path generalization LLMs Experimental results 13 datasets demonstrate framework effectively improves performance LLMs Furthermore provide detailed analysis module framework
552,Cool-Fusion: Fuse Large Language Models without Training,"['Cong Liu', 'Xiaojun Quan', 'Yan Pan', 'Weigang Wu', 'Xu Chen', 'Liang Lin']",,Cool Fusion Fuse Large Language Models Training
553,DAPE V2: Process Attention Score as Feature Map for Length Extrapolation,"['Chuanyang Zheng', 'Yihang Gao', 'Han Shi', 'Jing Xiong', 'Jiankai Sun', 'Jingyao Li', 'Minbin Huang', 'Xiaozhe Ren', 'Michael Ng', 'Xin Jiang', 'Zhenguo Li', 'Yu Li']","The attention mechanism is a fundamental component of the Transformer model, contributing to interactions among distinct tokens, in contrast to earlier feed-forward neural networks. In general, the attention scores are determined simply by the key-query products. However, this work's occasional trial (combining DAPE and NoPE) of including additional MLPs on attention scores without position encoding indicates that the classical key-query multiplication may limit the performance of Transformers. In this work, we conceptualize attention as a feature map and apply the convolution operator (for neighboring attention scores across different heads) to mimic the processing methods in computer vision. Specifically, the main contribution of this paper is identifying and interpreting the Transformer length extrapolation problem as a result of the limited expressiveness of the naive query and key dot product, and we successfully translate the length extrapolation issue into a well-understood feature map processing problem. The novel insight, which can be adapted to various attention-related models, reveals that the current Transformer architecture has the potential for further evolution. Extensive experiments demonstrate that treating attention as a feature map and applying convolution as a processing method significantly enhances Transformer performance.",DAPE V2 Process Attention Score Feature Map Length Extrapolation attention mechanism fundamental component Transformer model contributing interactions distinct tokens contrast earlier feed forward neural networks general attention scores determined simply key query products work s occasional trial combining DAPE NoPE including additional MLPs attention scores position encoding indicates classical key query multiplication limit performance Transformers work conceptualize attention feature map apply convolution operator neighboring attention scores different heads mimic processing methods computer vision Specifically main contribution paper identifying interpreting Transformer length extrapolation problem result limited expressiveness naive query key dot product successfully translate length extrapolation issue understood feature map processing problem novel insight adapted various attention related models reveals current Transformer architecture potential evolution Extensive experiments demonstrate treating attention feature map applying convolution processing method significantly enhances Transformer performance
554,MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training,"['Hui Huang', 'Jiaheng Liu', 'Yancheng He', 'Shilong Li', 'Bing Xu', 'Conghui Zhu', 'Muyun Yang', 'Tiejun Zhao']","Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.",MuSC Improving Complex Instruction Following Multi granularity Self Contrastive Training Complex instruction following elaborate constraints imperative Large Language Models LLMs existing methods constructed data complex instruction alignment rely advanced model especially GPT 4 limiting application paper propose Multi granularity Self Contrastive Training MuSC framework improve complex instruction alignment relying stronger model method conducted coarse fine granularity coarse granularity construct constraint aware preference data based instruction decomposition recombination fine granularity perform token aware preference optimization dynamic token level supervision method evaluated open sourced models experiment results method achieves significant improvement complex general instruction following benchmarks surpassing previous self alignment methods
555,LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation,"['Zican Dong', 'Junyi Li', 'Jinhao Jiang', 'Mingyu Xu', 'Xin Zhao', 'Bingning Wang', 'Weipeng Chen']","Large language models (LLMs) have gained extended context windows through scaling positional encodings and lightweight continual pre-training. However, this often leads to degraded performance on short-text tasks, while the reasons for this degradation remain insufficiently explored. In this work, we identify two primary factors contributing to this issue: distribution drift in hidden states and attention scores, and catastrophic forgetting during continual pre-training. To address these challenges, we propose Long Context Pre-training with Restoration Distillation (LongReD), a novel approach designed to mitigate short-text performance degradation through minimizing the distribution discrepancy between the extended and original models. Besides training on long texts, LongReD distills the hidden state of selected layers from the original model on short texts. Additionally, LongReD also introduces a short-to-long distillation, aligning the output distribution on short texts with that on long texts by leveraging skipped positional indices. Experiments on common text benchmarks demonstrate that LongReD effectively preserves the model's short-text performance while maintaining comparable or even better capacity to handle long texts than baselines. Our code is available at https://github.com/RUCAIBox/LongReD.",LongReD Mitigating Short Text Degradation Long Context Large Language Models Restoration Distillation Large language models LLMs gained extended context windows scaling positional encodings lightweight continual pre training leads degraded performance short text tasks reasons degradation remain insufficiently explored work identify primary factors contributing issue distribution drift hidden states attention scores catastrophic forgetting continual pre training address challenges propose Long Context Pre training Restoration Distillation LongReD novel approach designed mitigate short text performance degradation minimizing distribution discrepancy extended original models training long texts LongReD distills hidden state selected layers original model short texts Additionally LongReD introduces short long distillation aligning output distribution short texts long texts leveraging skipped positional indices Experiments common text benchmarks demonstrate LongReD effectively preserves model s short text performance maintaining comparable better capacity handle long texts baselines code available https github com RUCAIBox LongReD
556,APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs,"['Yuxiang Huang', 'Mingye Li', 'Xu Han', 'Chaojun Xiao', 'Weilin Zhao', 'Sun Ao', 'Hao Zhou', 'Jie Zhou', 'Zhiyuan Liu', 'Maosong Sun']","While long-context inference is crucial for advancing large language model (LLM) applications, its prefill speed remains a significant bottleneck. Current approaches, including sequence parallelism strategies and compute reduction through approximate attention mechanisms, still fall short of delivering optimal inference efficiency. This hinders scaling the inputs to longer sequences and processing long-context queries in a timely manner. To address this, we introduce APB, an efficient long-context inference framework that leverages multi-host approximate attention to enhance prefill speed by reducing compute and enhancing parallelism simultaneously. APB introduces a communication mechanism for essential key-value pairs within a sequence parallelism framework, enabling a faster inference speed while maintaining task performance. We implement APB by incorporating a tailored FlashAttn kernel alongside optimized distribution strategies, supporting diverse models and parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x compared with FlashAttn, RingAttn, and StarAttn, respectively, without any observable task performance degradation. We provide the implementation and experiment code of APB in https://github.com/thunlp/APB.",APB Accelerating Distributed Long Context Inference Passing Compressed Context Blocks GPUs long context inference crucial advancing large language model LLM applications prefill speed remains significant bottleneck Current approaches including sequence parallelism strategies compute reduction approximate attention mechanisms fall short delivering optimal inference efficiency hinders scaling inputs longer sequences processing long context queries timely manner address introduce APB efficient long context inference framework leverages multi host approximate attention enhance prefill speed reducing compute enhancing parallelism simultaneously APB introduces communication mechanism essential key value pairs sequence parallelism framework enabling faster inference speed maintaining task performance implement APB incorporating tailored FlashAttn kernel alongside optimized distribution strategies supporting diverse models parallelism configurations APB achieves speedups 9 2x 4 2x 1 6x compared FlashAttn RingAttn StarAttn respectively observable task performance degradation provide implementation experiment code APB https github com thunlp APB
557,PPT: A Minor Language News Recommendation Model via Cross-Lingual Preference Pattern Transfer,"['Yiyang Zhang', 'Nan Chen']",,PPT Minor Language News Recommendation Model Cross Lingual Preference Pattern Transfer
558,GainRAG: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis,"['Yi Jiang', 'Sendong Zhao', 'Jianbo Li', 'Haochun Wang', 'Bing Qin']","The Retrieval-Augmented Generation (RAG) framework introduces a retrieval module to dynamically inject retrieved information into the input context of large language models (LLMs), and has demonstrated significant success in various NLP tasks. However, the current study points out that there is a preference gap between retrievers and LLMs in the RAG framework, which limit the further improvement of system performance. Some highly relevant passages may interfere with LLM reasoning because they contain complex or contradictory information; while some indirectly related or even inaccurate content may help LLM generate more accurate answers by providing suggestive information or logical clues. To solve this, we propose GainRAG, a novel approach that aligns the retriever's and LLM's preferences by defining a new metric, ""gain"", which measure how well an input passage contributes to correct outputs. Specifically, we propose a method to estimate these gain signals and train a middleware that aligns the preferences of the retriever and the LLM using only limited data. In addition, we introduce a pseudo-passage strategy to mitigate degradation. The experimental results on 6 datasets verify the effectiveness of GainRAG.",GainRAG Preference Alignment Retrieval Augmented Generation Gain Signal Synthesis Retrieval Augmented Generation RAG framework introduces retrieval module dynamically inject retrieved information input context large language models LLMs demonstrated significant success various NLP tasks current study points preference gap retrievers LLMs RAG framework limit improvement performance highly relevant passages interfere LLM reasoning contain complex contradictory information indirectly related inaccurate content help LLM generate accurate answers providing suggestive information logical clues solve propose GainRAG novel approach aligns retriever s LLM s preferences defining new metric gain measure input passage contributes correct outputs Specifically propose method estimate gain signals train middleware aligns preferences retriever LLM using limited data addition introduce pseudo passage strategy mitigate degradation experimental results 6 datasets verify effectiveness GainRAG
559,Top-$n\sigma$: Eliminating Noise in Logit Space for Robust Token Sampling of LLM,"['Chenxia Tang', 'Jianchun Liu', 'Hongli Xu', 'Liusheng Huang']",,n sigma Eliminating Noise Logit Space Robust Token Sampling LLM
560,SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation,"['Jialong Wu', 'Zhenglin Wang', 'Linhai Zhang', 'Yilong Lai', 'Yulan He', 'Deyu Zhou']","Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods.",SCOPE Optimizing Key Value Cache Compression Long context Generation Key Value KV cache bottleneck LLMs long context generation Despite numerous efforts area optimization decoding phase generally ignored believe optimization crucial especially long output generation tasks based following observations Excessive compression prefill phase requires specific context impairs comprehension reasoning task ii Deviation heavy hitters occurs reasoning tasks long outputs SCOPE simple efficient framework separately performs KV cache optimization prefill decoding phases introduced Specifically KV cache prefill phase preserved maintain essential information novel strategy based sliding proposed select essential heavy hitters decoding phase Memory usage memory transfer optimized using adaptive discontinuous strategies Extensive experiments LongGenBench effectiveness generalization SCOPE compatibility plug prefill KV compression methods
561,Mitigating Non-Representative Prototypes and Representation Bias in Few-Shot Continual Relation Extraction,"['Thanh Duc Pham', 'Nam Le Hai', 'Linh Ngo Van', 'Nguyen Thi Ngoc Diep', 'Sang Dinh', 'Thien Huu Nguyen']",,Mitigating Non Representative Prototypes Representation Bias Shot Continual Relation Extraction
562,MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts,"['Wei Tao', 'Haocheng Lu', 'Xiaoyang Qu', 'Bin Zhang', 'Kai Lu', 'Jiguang Wan', 'Jianzong Wang']","One of the primary challenges in optimizing large language models (LLMs) for long-context inference lies in the high memory consumption of the Key-Value (KV) cache. Existing approaches, such as quantization, have demonstrated promising results in reducing memory usage. However, current quantization methods cannot take both effectiveness and efficiency into account. In this paper, we propose MoQAE, a novel mixed-precision quantization method via mixture of quantization-aware experts. First, we view different quantization bit-width configurations as experts and use the traditional mixture of experts (MoE) method to select the optimal configuration. To avoid the inefficiency caused by inputting tokens one by one into the router in the traditional MoE method, we input the tokens into the router chunk by chunk. Second, we design a lightweight router-only fine-tuning process to train MoQAE with a comprehensive loss to learn the trade-off between model accuracy and memory usage. Finally, we introduce a routing freezing (RF) and a routing sharing (RS) mechanism to further reduce the inference overhead. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms state-of-the-art KV cache quantization approaches in both efficiency and effectiveness.",MoQAE Mixed Precision Quantization Long Context LLM Inference Mixture Quantization Aware Experts primary challenges optimizing large language models LLMs long context inference lies high memory consumption Key Value KV cache Existing approaches quantization demonstrated promising results reducing memory usage current quantization methods effectiveness efficiency account paper propose MoQAE novel mixed precision quantization method mixture quantization aware experts view different quantization bit width configurations experts use traditional mixture experts MoE method select optimal configuration avoid inefficiency caused inputting tokens router traditional MoE method input tokens router chunk chunk Second design lightweight router fine tuning process train MoQAE comprehensive loss learn trade model accuracy memory usage Finally introduce routing freezing RF routing sharing RS mechanism reduce inference overhead Extensive experiments multiple benchmark datasets demonstrate method outperforms state art KV cache quantization approaches efficiency effectiveness
563,PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration,"['Ziqian Zeng', 'Jianwei Wang', 'Junyao Yang', 'ZhengdongLu', 'Haoran Li', 'Huiping Zhuang', 'Cen Chen']","The widespread usage of online Large Language Models (LLMs) inference services has raised significant privacy concerns about the potential exposure of private information in user inputs to malicious eavesdroppers. Existing privacy protection methods for LLMs suffer from either insufficient privacy protection, performance degradation, or large inference time overhead. To address these limitations, we propose PrivacyRestore, a plug-and-play method to protect the privacy of user inputs during LLM inference. The server first trains restoration vectors for each privacy span and then release to clients. Privacy span is defined as a contiguous sequence of tokens within a text that contain private information. The client then aggregate restoration vectors of all privacy spans in the input into a single meta restoration vector which is later sent to the server side along with the input without privacy spans.The private information is restored via activation steering during inference. Furthermore, we prove that PrivacyRestore inherently prevents the linear growth of the privacy budget.We create three datasets, covering medical and legal domains, to evaluate the effectiveness of privacy preserving methods. The experimental results show that PrivacyRestore effectively protects private information and maintain acceptable levels of performance and inference overhead.",PrivacyRestore Privacy Preserving Inference Large Language Models Privacy Removal Restoration widespread usage online Large Language Models LLMs inference services raised significant privacy concerns potential exposure private information user inputs malicious eavesdroppers Existing privacy protection methods LLMs suffer insufficient privacy protection performance degradation large inference time overhead address limitations propose PrivacyRestore plug play method protect privacy user inputs LLM inference server trains restoration vectors privacy span release clients Privacy span defined contiguous sequence tokens text contain private information client aggregate restoration vectors privacy spans input single meta restoration vector later sent server input privacy spans private information restored activation steering inference Furthermore prove PrivacyRestore inherently prevents linear growth privacy budget create datasets covering medical legal domains evaluate effectiveness privacy preserving methods experimental results PrivacyRestore effectively protects private information maintain acceptable levels performance inference overhead
564,Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models,"['Xinlin Zhuang', 'Jiahui Peng', 'Ren Ma', 'Yinfan Wang', 'Tianyi Bai', 'Xingjian Wei', 'Qiu Jiantao', 'Chi Zhang', 'Ying Qian', 'Conghui He']",,Meta rater Multi dimensional Data Selection Method Pre training Language Models
565,GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning,"['Qingchen Yu', 'Zifan Zheng', 'Ding Chen', 'Simin Niu', 'Bo Tang', 'Feiyu Xiong', 'Zhiyu li']","The evaluation of large language models (LLMs) has traditionally relied on static benchmarks, a paradigm that poses two major limitations: (1) predefined test sets lack adaptability to diverse application domains, and (2) standardized evaluation protocols often fail to capture fine-grained assessments of domain-specific knowledge and contextual reasoning abilities. To overcome these challenges, we propose GuessArena, an adaptive evaluation framework grounded in adversarial game-based interactions. Inspired by the interactive structure of the Guess Who I Am? game, our framework seamlessly integrates dynamic domain knowledge modeling with progressive reasoning assessment to improve evaluation fidelity. Empirical studies across five vertical domains-finance, healthcare, manufacturing, information technology, and education-demonstrate that GuessArena effectively distinguishes LLMs in terms of domain knowledge coverage and reasoning chain completeness. Compared to conventional benchmarks, our method provides substantial advantages in interpretability, scalability, and scenario adaptability.",GuessArena Guess Self Adaptive Framework Evaluating LLMs Domain Specific Knowledge Reasoning evaluation large language models LLMs traditionally relied static benchmarks paradigm poses major limitations 1 predefined test sets lack adaptability diverse application domains 2 standardized evaluation protocols fail capture fine grained assessments domain specific knowledge contextual reasoning abilities overcome challenges propose GuessArena adaptive evaluation framework grounded adversarial game based interactions Inspired interactive structure Guess game framework seamlessly integrates dynamic domain knowledge modeling progressive reasoning assessment improve evaluation fidelity Empirical studies vertical domains finance healthcare manufacturing information technology education demonstrate GuessArena effectively distinguishes LLMs terms domain knowledge coverage reasoning chain completeness Compared conventional benchmarks method provides substantial advantages interpretability scalability scenario adaptability
566,Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition,"['Kehua Feng', 'Keyan Ding', 'tan hongzhi', 'Kede Ma', 'Zhihua Wang', 'Shuangquan Guo', 'Cheng yuzhou', 'Ge Sun', 'Guozhou Zheng', 'Qiang Zhang', 'Huajun Chen']","Reliable evaluation of large language models (LLMs) is impeded by two key challenges: objective metrics often fail to reflect human perception of natural language, and exhaustive human labeling is prohibitively expensive. Here, we propose a sample-efficient human evaluation method for LLMs based on the principle of MAximum Discrepancy (MAD) Competition. Our method automatically and adaptively selects a compact set of input instructions that maximize semantic discrepancy between pairs of LLM responses. Human evaluators then perform three-alternative forced choices on these paired responses, which are aggregated into a global ranking using Elo rating. We apply our approach to compare eight widely used LLMs across four tasks: scientific knowledge understanding, mathematical reasoning, creative and functional writing, and code generation and explanation. Experimental results show that our sample-efficient evaluation method recovers ""gold-standard"" model rankings with a handful of MAD-selected instructions, reveals respective strengths and weaknesses of each LLM, and offers nuanced insights to guide future LLM development. Code is available at https://github.com/weiji-Feng/MAD-Eval .",Sample Efficient Human Evaluation Large Language Models Maximum Discrepancy Competition Reliable evaluation large language models LLMs impeded key challenges objective metrics fail reflect human perception natural language exhaustive human labeling prohibitively expensive propose sample efficient human evaluation method LLMs based principle MAximum Discrepancy MAD Competition method automatically adaptively selects compact set input instructions maximize semantic discrepancy pairs LLM responses Human evaluators perform alternative forced choices paired responses aggregated global ranking using Elo rating apply approach compare widely used LLMs tasks scientific knowledge understanding mathematical reasoning creative functional writing code generation explanation Experimental results sample efficient evaluation method recovers gold standard model rankings handful MAD selected instructions reveals respective strengths weaknesses LLM offers nuanced insights guide future LLM development Code available https github com weiji Feng MAD Eval
567,DTCRS: Dynamic Tree Construction for Recursive Summarization,['Guanran Luo'],,DTCRS Dynamic Tree Construction Recursive Summarization
568,A Generative Adaptive Replay Continual Learning Model for Temporal Knowledge Graph Reasoning,"['Zhiyu Zhang', 'Wei Chen', 'Youfang Lin', 'Huaiyu Wan']","Recent Continual Learning (CL)-based Temporal Knowledge Graph Reasoning (TKGR) methods focus on significantly reducing computational cost and mitigating catastrophic forgetting caused by fine-tuning models with new data. However, existing CL-based TKGR methods still face two key limitations: (1) They usually one-sidedly reorganize individual historical facts, while overlooking the historical context essential for accurately understanding the historical semantics of these facts; (2) They preserve historical knowledge by simply replaying historical facts, while ignoring the potential conflicts between historical and emerging facts. In this paper, we propose a Deep Generative Adaptive Replay (DGAR) method, which can generate and adaptively replay historical entity distribution representations from the whole historical context. To address the first challenge, historical context prompts as sampling units are built to preserve the whole historical context information. To overcome the second challenge, a pre-trained diffusion model is adopted to generate the historical distribution. During the generation process, the common features between the historical and current distributions are enhanced under the guidance of the TKGR model. In addition, a layer-by-layer adaptive replay mechanism is designed to effectively integrate historical and current distributions. Experimental results demonstrate that DGAR significantly outperforms baselines in reasoning and mitigating forgetting.",Generative Adaptive Replay Continual Learning Model Temporal Knowledge Graph Reasoning Recent Continual Learning CL based Temporal Knowledge Graph Reasoning TKGR methods focus significantly reducing computational cost mitigating catastrophic forgetting caused fine tuning models new data existing CL based TKGR methods face key limitations 1 usually sidedly reorganize individual historical facts overlooking historical context essential accurately understanding historical semantics facts 2 preserve historical knowledge simply replaying historical facts ignoring potential conflicts historical emerging facts paper propose Deep Generative Adaptive Replay DGAR method generate adaptively replay historical entity distribution representations historical context address challenge historical context prompts sampling units built preserve historical context information overcome second challenge pre trained diffusion model adopted generate historical distribution generation process common features historical current distributions enhanced guidance TKGR model addition layer layer adaptive replay mechanism designed effectively integrate historical current distributions Experimental results demonstrate DGAR significantly outperforms baselines reasoning mitigating forgetting
569,ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search,"['Yize Zhang', 'Tianshu Wang', 'Sirui Chen', 'Kun Wang', 'Xingyu Zeng', 'Hongyu Lin', 'Xianpei Han', 'Le Sun', 'Chaochao Lu']","Large language models (LLMs) have demonstrated impressive capabilities and are receiving increasing attention to enhance their reasoning through scaling test--time compute. However, their application in open--ended, knowledge--intensive, complex reasoning scenarios is still limited. Reasoning--oriented methods struggle to generalize to open--ended scenarios due to implicit assumptions of complete world knowledge. Meanwhile, knowledge--augmented reasoning (KAR) methods fail to address two core challenges: 1) error propagation, where errors in early steps cascade through the chain, and 2) verification bottleneck, where the explore--exploit tradeoff arises in multi--branch decision processes. To overcome these limitations, we introduce ARise, a novel framework that integrates risk assessment of intermediate reasoning states with dynamic retrieval--augmented generation (RAG) within a Monte Carlo tree search paradigm. This approach enables effective construction and optimization of reasoning plans across multiple maintained hypothesis branches. Experimental results show that ARise significantly outperforms the state--of--the--art KAR methods by up to 23.10%, and the latest RAG-equipped large reasoning models by up to 25.37%. Our project page is at https://opencausalab.github.io/ARise.",ARise Knowledge Augmented Reasoning Risk Adaptive Search Large language models LLMs demonstrated impressive capabilities receiving increasing attention enhance reasoning scaling test time compute application open ended knowledge intensive complex reasoning scenarios limited Reasoning oriented methods struggle generalize open ended scenarios implicit assumptions complete world knowledge knowledge augmented reasoning KAR methods fail address core challenges 1 error propagation errors early steps cascade chain 2 verification bottleneck explore exploit tradeoff arises multi branch decision processes overcome limitations introduce ARise novel framework integrates risk assessment intermediate reasoning states dynamic retrieval augmented generation RAG Monte Carlo tree search paradigm approach enables effective construction optimization reasoning plans multiple maintained hypothesis branches Experimental results ARise significantly outperforms state art KAR methods 23 10 latest RAG equipped large reasoning models 25 37 project page https opencausalab github io ARise
570,PKAG-DDI: Pairwise Knowledge-Augmented Language Model for Drug-Drug Interaction Event Text Generation,"['Ziyan Wang', 'Zhankun Xiong', 'Feng Huang', 'Wen Zhang']",,PKAG DDI Pairwise Knowledge Augmented Language Model Drug Drug Interaction Event Text Generation
571,Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models,"['Shuai Niu', 'Jing Ma', 'Hongzhan Lin', 'Liang Bai', 'Zhihua Wang', 'Richard Yi Da Xu', 'Yunya Song', 'Xian Yang']","Interpretation is critical for disease diagnosis, but existing models struggle to balance predictive accuracy with human-understandable rationales. While large language models (LLMs) offer strong reasoning abilities, their clinical use is limited by high computational costs and restricted multimodal reasoning ability. Small language models (SLMs) are efficient but lack advanced reasoning for integrating multimodal medical data. In addition, both LLMs and SLMs lack domain knowledge for trustworthy reasoning. Therefore, we propose ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via rationale distillation and domain knowledge injection for trustworthy multimodal rationale generation. Key innovations include a sequential rationale distillation framework that equips SLMs with LLM-comparable multimodal reasoning abilities, and a knowledge-augmented attention mechanism that jointly unifies multimodal representation from time series and textual data in the same encoding space, enabling it to be naturally interpreted by SLMs while incorporating domain knowledge for reliable rationale generation. Experiments on real-world medical datasets show that ClinRaGen achieves state-of-the-art performance in disease diagnosis and rationale generation, demonstrating the effectiveness of combining LLM-driven reasoning with knowledge augmentation for improved interpretability.",Knowledge Augmented Multimodal Clinical Rationale Generation Disease Diagnosis Small Language Models Interpretation critical disease diagnosis existing models struggle balance predictive accuracy human understandable rationales large language models LLMs offer strong reasoning abilities clinical use limited high computational costs restricted multimodal reasoning ability Small language models SLMs efficient lack advanced reasoning integrating multimodal medical data addition LLMs SLMs lack domain knowledge trustworthy reasoning propose ClinRaGen enhancing SLMs leveraging LLM derived reasoning ability rationale distillation domain knowledge injection trustworthy multimodal rationale generation Key innovations include sequential rationale distillation framework equips SLMs LLM comparable multimodal reasoning abilities knowledge augmented attention mechanism jointly unifies multimodal representation time series textual data encoding space enabling naturally interpreted SLMs incorporating domain knowledge reliable rationale generation Experiments real world medical datasets ClinRaGen achieves state art performance disease diagnosis rationale generation demonstrating effectiveness combining LLM driven reasoning knowledge augmentation improved interpretability
572,TWIST: Text-encoder Weight-editing for Inserting Secret Trojans in Text-to-Image Models,"['Xindi Li', 'Zhe Liu', 'Tong Zhang', 'Jiahao Chen', 'Qingming Li', 'Jinbao Li', 'Shouling Ji']",,TWIST Text encoder Weight editing Inserting Secret Trojans Text Image Models
573,Frictional Agent Alignment Framework: Slow Down and Don’t Break Things,"['Abhijnan Nath', 'Carine Graff', 'Andrei Bachinin', 'Nikhil Krishnaswamy']",,Frictional Agent Alignment Framework Slow Don t Break Things
574,Powerformer: Efficient and High-Accuracy Privacy-Preserving Language Model with Homomorphic Encryption,"['Dongjin Park', 'Eunsang Lee', 'Joon-Woo Lee']",,Powerformer Efficient High Accuracy Privacy Preserving Language Model Homomorphic Encryption
575,Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs,"['Weixiang Zhao', 'Yulin Hu', 'Yang Deng', 'Jiahe Guo', 'Xingyu Sui', 'Xinyang Han', 'An Zhang', 'Yanyan Zhao', 'Bing Qin', 'Tat-Seng Chua', 'Ting Liu']","Role-playing enables large language models (LLMs) to engage users in immersive and personalized interactions, but it also introduces significant safety risks. Existing role-play fine-tuning techniques improve role adaptability but may degrade safety performance, particularly for villainous characters. In this work, we conduct the first comprehensive assessment of role-play fine-tuning risks by training 95 role-specific LLMs using RoleBench. Our experiments reveal that role-play fine-tuning leads to a noticeable decline in safety performance, with safety risks varying based on character traits. To tackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a novel method designed to balance role-playing capabilities and safety. Extensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and Qwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms state-of-the-art baselines under both LoRA and full-parameter fine-tuning settings. Our findings highlight the necessity of role-adaptive safety measures and provide insights into mitigating role-specific safety risks in role-playing LLMs.",Beware Po Measuring Mitigating AI Safety Risks Role Play Fine Tuning LLMs Role playing enables large language models LLMs engage users immersive personalized interactions introduces significant safety risks Existing role play fine tuning techniques improve role adaptability degrade safety performance particularly villainous characters work conduct comprehensive assessment role play fine tuning risks training 95 role specific LLMs using RoleBench experiments reveal role play fine tuning leads noticeable decline safety performance safety risks varying based character traits tackle challenge propose Safety Aware Role Play Fine Tuning SaRFT novel method designed balance role playing capabilities safety Extensive experiments LLaMA 3 8B Instruct Gemma 2 9B Qwen2 5 7B Instruct demonstrate SaRFT consistently outperforms state art baselines LoRA parameter fine tuning settings findings highlight necessity role adaptive safety measures provide insights mitigating role specific safety risks role playing LLMs
576,Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?,"['Zihao Li', 'Lecheng Zheng', 'Bowen Jin', 'Dongqi Fu', 'Baoyu Jing', 'Yikun Ban', 'Jingrui He', 'Jiawei Han']","While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.",Graph Neural Networks Learn Language Extremely Weak Text Supervision great success achieved building vision models Contrastive Language Image Pre training CLIP internet scale image text pairs building transferable Graph Neural Networks GNNs CLIP pipeline challenging scarcity labeled data text supervision different levels downstream tasks conceptual gaps domains work address issues propose multi modal prompt learning paradigm effectively adapt pre trained GNN downstream tasks data given semantically labeled samples extremely weak text supervision new paradigm embeds graphs directly space Large Language Models LLMs learning graph prompts text prompts simultaneously demonstrate superior performance paradigm shot multi task level cross domain settings build CLIP style zero shot classification prototype generalize GNNs unseen classes extremely weak text supervision code available https github com Violet24K Morpher
577,Towards Enhanced Immersion and Agency for LLM-based Interactive Drama,"['Hongqiu Wu', 'Weiqi Wu', 'Tianyang Xu', 'Jiameng Zhang', 'hai zhao']","LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the user (i.e. the player) plays the role of a character in the story, has conversations with characters played by LLM agents, and experiences an unfolding story. This paper begins with understanding interactive drama from two aspects: Immersion, the player's feeling of being present in the story, and Agency, the player's ability to influence the story world. Both are crucial to creating an enjoyable interactive experience, while they have been underexplored in previous work. To enhance these two aspects, we first propose Playwriting-guided Generation, a novel method that helps LLMs craft dramatic stories with substantially improved structures and narrative quality. Additionally, we introduce Plot-based Reflection for LLM agents to refine their reactions to align with the player's intentions. Our evaluation relies on human judgment to assess the gains of our methods in terms of immersion and agency.",Enhanced Immersion Agency LLM based Interactive Drama LLM based Interactive Drama novel AI based dialogue scenario user e player plays role character story conversations characters played LLM agents experiences unfolding story paper begins understanding interactive drama aspects Immersion player s feeling present story Agency player s ability influence story world crucial creating enjoyable interactive experience underexplored previous work enhance aspects propose Playwriting guided Generation novel method helps LLMs craft dramatic stories substantially improved structures narrative quality Additionally introduce Plot based Reflection LLM agents refine reactions align player s intentions evaluation relies human judgment assess gains methods terms immersion agency
578,Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures,"['Shun Inadumi', 'Nobuhiro Ueda', 'Koichiro Yoshino']","Multimodal reference resolution, including phrase grounding, aims to understand the semantic relations between mentions and real-world objects. Phrase grounding between images and their captions is a well-established task. In contrast, for real-world applications, it is essential to integrate textual and multimodal reference resolution to unravel the reference relations within dialogue, especially in handling ambiguities caused by pronouns and ellipses. This paper presents a framework that unifies textual and multimodal reference resolution by mapping mention embeddings to object embeddings and selecting mentions or objects based on their similarity. Our experiments show that learning textual reference resolution, such as coreference resolution and predicate-argument structure analysis, positively affects performance in multimodal reference resolution. In particular, our model with coreference resolution performs better in pronoun phrase grounding than representative models for this task, MDETR and GLIP. Our qualitative analysis demonstrates that incorporating textual reference relations strengthens the confidence scores between mentions, including pronouns and predicates, and objects, which can reduce the ambiguities that arise in visually grounded dialogues.",Disambiguating Reference Visually Grounded Dialogues Joint Modeling Textual Multimodal Semantic Structures Multimodal reference resolution including phrase grounding aims understand semantic relations mentions real world objects Phrase grounding images captions established task contrast real world applications essential integrate textual multimodal reference resolution unravel reference relations dialogue especially handling ambiguities caused pronouns ellipses paper presents framework unifies textual multimodal reference resolution mapping mention embeddings object embeddings selecting mentions objects based similarity experiments learning textual reference resolution coreference resolution predicate argument structure analysis positively affects performance multimodal reference resolution particular model coreference resolution performs better pronoun phrase grounding representative models task MDETR GLIP qualitative analysis demonstrates incorporating textual reference relations strengthens confidence scores mentions including pronouns predicates objects reduce ambiguities arise visually grounded dialogues
579,Improving Factuality with Explicit Working Memory,"['Mingda Chen', 'Yang Li', 'Karthik Padthe', 'Rulin Shao', 'Alicia Yi Sun', 'Luke Zettlemoyer', 'Gargi Ghosh', 'Wen-tau Yih']","Large language models can generate factually inaccurate content, a problem known as hallucination. Recent works have built upon retrieved-augmented generation to improve factuality through iterative prompting but these methods are limited by the traditional RAG design. To address these challenges, we introduce EWE (Explicit Working Memory), a novel approach that enhances factuality in long-form text generation by integrating a working memory that receives real-time feedback from external resources. The memory is refreshed based on online fact-checking and retrieval feedback, allowing EWE to rectify false claims during the generation process and ensure more accurate and reliable outputs. Our experiments demonstrate that Ewe outperforms strong baselines on four fact-seeking long-form generation datasets, increasing the factuality metric, VeriScore, by 2 to 6 points absolute without sacrificing the helpfulness of the responses. Further analysis reveals that the design of rules for memory updates, configurations of memory units, and the quality of the retrieval datastore are crucial factors for influencing model performance.",Improving Factuality Explicit Working Memory Large language models generate factually inaccurate content problem known hallucination Recent works built retrieved augmented generation improve factuality iterative prompting methods limited traditional RAG design address challenges introduce EWE Explicit Working Memory novel approach enhances factuality long form text generation integrating working memory receives real time feedback external resources memory refreshed based online fact checking retrieval feedback allowing EWE rectify false claims generation process ensure accurate reliable outputs experiments demonstrate Ewe outperforms strong baselines fact seeking long form generation datasets increasing factuality metric VeriScore 2 6 points absolute sacrificing helpfulness responses analysis reveals design rules memory updates configurations memory units quality retrieval datastore crucial factors influencing model performance
580,Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models,"['Chengao Li', 'Hanyu Zhang', 'Yunkun Xu', 'Hongyan Xue', 'Xiang Ao', 'Qing He']","Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user's specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness.",Gradient Adaptive Policy Optimization Multi Objective Alignment Large Language Models Reinforcement Learning Human Feedback RLHF emerged powerful technique aligning large language models LLMs human preferences effectively aligning LLMs diverse human preferences remains significant challenge particularly conflict address issue frame human value alignment multi objective optimization problem aiming maximize set potentially conflicting objectives introduce Gradient Adaptive Policy Optimization GAPO novel fine tuning paradigm employs multiple gradient descent align LLMs diverse preference distributions GAPO adaptively rescales gradients objective determine update direction optimally balances trade offs objectives Additionally introduce P GAPO incorporates user preferences different objectives achieves Pareto solutions better align user s specific needs theoretical analysis demonstrates GAPO converges Pareto optimal solution multiple objectives Empirical results Mistral 7B GAPO outperforms current state art methods achieving superior performance helpfulness harmlessness
581,Dynamic Parallel Tree Search for Efficient LLM Reasoning,"['Yifu Ding', 'Wentao Jiang', 'Shunyu Liu', 'Yongcheng Jing', 'Jinyang Guo', 'Yingjie Wang', 'Jing Zhang', 'Zengmao Wang', 'Ziwei Liu', 'Bo Du', 'Xianglong Liu', 'Dacheng Tao']","Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by structuring problem-solving as a spanning tree. However, recent methods focus on search accuracy while overlooking computational efficiency. The challenges of accelerating the ToT lie in the frequent switching of reasoning focus, and the redundant exploration of suboptimal solutions. To alleviate this dilemma, we propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework that aims to dynamically optimize the reasoning path in inference. It includes the Parallelism Streamline in the generation phase to build up a flexible and adaptive parallelism with arbitrary paths by fine-grained cache management and alignment. Meanwhile, the Search and Transition Mechanism filters potential candidates to dynamically maintain the reasoning focus on more possible solutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with Math500 and GSM8K datasets show that DPTS significantly improves efficiency by 2-4x on average while maintaining or even surpassing existing reasoning algorithms in accuracy, making ToT-based reasoning more scalable and computationally efficient.",Dynamic Parallel Tree Search Efficient LLM Reasoning Tree Thoughts ToT enhances Large Language Model LLM reasoning structuring problem solving spanning tree recent methods focus search accuracy overlooking computational efficiency challenges accelerating ToT lie frequent switching reasoning focus redundant exploration suboptimal solutions alleviate dilemma propose Dynamic Parallel Tree Search DPTS novel parallelism framework aims dynamically optimize reasoning path inference includes Parallelism Streamline generation phase build flexible adaptive parallelism arbitrary paths fine grained cache management alignment Search Transition Mechanism filters potential candidates dynamically maintain reasoning focus possible solutions redundancy Experiments Qwen 2 5 Llama 3 Math500 GSM8K datasets DPTS significantly improves efficiency 2 4x average maintaining surpassing existing reasoning algorithms accuracy making ToT based reasoning scalable computationally efficient
582,Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation,"['Junyi Chen', 'Shihao Bai', 'Zaijun Wang', 'Siyu Wu', 'Chuheng Du', 'Hailong Yang', 'Ruihao Gong', 'Shengzhong Liu', 'Fan Wu', 'Guihai Chen']","Extensive LLM applications demand efficient structured generations, particularly for LR(1) grammars, to produce outputs in specified formats (e.g., JSON). Existing methods primarily parse LR(1) grammars into a pushdown automaton (PDA), leading to runtime execution overhead for context-dependent token processing, especially inefficient under large inference batches. To address these issues, we propose Pre$^3$ that exploits deterministic pushdown automata (DPDA) to optimize the constrained LLM decoding efficiency. First, by precomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables ahead-of-time edge analysis and thus makes parallel transition processing possible. Second, by leveraging the prefix-conditioned edges, Pre$^3$ introduces a novel approach that transforms LR(1) transition graphs into DPDA, eliminating the need for runtime path exploration and achieving edge transitions with minimal overhead. Pre$^3$ can be seamlessly integrated into standard LLM inference frameworks, reducing time per output token (TPOT) by up to 40% and increasing throughput by up to 36% in our experiments. Our code is available at https://github.com/ModelTC/lightllm.",Pre 3 Enabling Deterministic Pushdown Automata Faster Structured LLM Generation Extensive LLM applications demand efficient structured generations particularly LR 1 grammars produce outputs specified formats e g JSON Existing methods primarily parse LR 1 grammars pushdown automaton PDA leading runtime execution overhead context dependent token processing especially inefficient large inference batches address issues propose Pre 3 exploits deterministic pushdown automata DPDA optimize constrained LLM decoding efficiency precomputing prefix conditioned edges preprocessing Pre 3 enables ahead time edge analysis makes parallel transition processing possible Second leveraging prefix conditioned edges Pre 3 introduces novel approach transforms LR 1 transition graphs DPDA eliminating need runtime path exploration achieving edge transitions minimal overhead Pre 3 seamlessly integrated standard LLM inference frameworks reducing time output token TPOT 40 increasing throughput 36 experiments code available https github com ModelTC lightllm
583,SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL,"['Ge Qu', 'Jinyang Li', 'Bowen Qin', 'Xiaolong Li', 'Nan Huo', 'Chenhao Ma', 'Reynold Cheng']","Current self-correction approaches in text-to-SQL face two critical limitations: 1) Conventional self-correction methods rely on recursive self-calls of LLMs, resulting in multiplicative computational overhead, and 2) LLMs struggle to implement effective error detection and correction for declarative SQL queries, as they fail to demonstrate the underlying reasoning path. In this work, we propose SHARE, an SLM-based Hierarchical Action corREction assistant that enables LLMs to perform more precise error localization and efficient correction. SHARE orchestrates three specialized Small Language Models (SLMs) in a sequential pipeline, where it first transforms declarative SQL queries into stepwise action trajectories that reveal underlying reasoning, followed by a two-phase granular refinement. We further propose a novel hierarchical self-evolution strategy for data-efficient training. Experimental results demonstrate that SHARE effectively enhances self-correction capabilities while proving robust across various LLMs. Furthermore, our comprehensive analysis shows that SHARE maintains strong performance even in low-resource training settings, which is particularly valuable for text-to-SQL applications with data privacy constraints.",SHARE SLM based Hierarchical Action CorREction Assistant Text SQL Current self correction approaches text SQL face critical limitations 1 Conventional self correction methods rely recursive self calls LLMs resulting multiplicative computational overhead 2 LLMs struggle implement effective error detection correction declarative SQL queries fail demonstrate underlying reasoning path work propose SHARE SLM based Hierarchical Action corREction assistant enables LLMs perform precise error localization efficient correction SHARE orchestrates specialized Small Language Models SLMs sequential pipeline transforms declarative SQL queries stepwise action trajectories reveal underlying reasoning followed phase granular refinement propose novel hierarchical self evolution strategy data efficient training Experimental results demonstrate SHARE effectively enhances self correction capabilities proving robust various LLMs Furthermore comprehensive analysis shows SHARE maintains strong performance low resource training settings particularly valuable text SQL applications data privacy constraints
584,GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models,"['Tao Zhang', 'Ziqian Zeng', 'YuxiangXiao', 'Huiping Zhuang', 'Cen Chen', 'James R. Foulds', 'Shimei Pan']","Large Language Models (LLMs) are prone to generating content that exhibits gender biases, raising significant ethical concerns. Alignment, the process of fine-tuning LLMs to better align with desired behaviors, is recognized as an effective approach to mitigate gender biases. Although proprietary LLMs have made significant strides in mitigating gender bias, their alignment datasets are not publicly available. The commonly used and publicly available alignment dataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of publicly available alignment datasets specifically designed to address gender bias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating a comprehensive set of gender biases in LLMs. This dataset comprises 8k single-turn dialogues, each paired with a ""chosen"" and a ""rejected"" response. Compared to the ""rejected"" responses, the ""chosen"" responses demonstrate lower levels of gender bias and higher quality. Furthermore, we categorized the gender biases in the ""rejected"" responses of GenderAlign into 4 principal categories. The experimental results show the effectiveness of GenderAlign in reducing gender bias in LLMs.",GenderAlign Alignment Dataset Mitigating Gender Bias Large Language Models Large Language Models LLMs prone generating content exhibits gender biases raising significant ethical concerns Alignment process fine tuning LLMs better align desired behaviors recognized effective approach mitigate gender biases proprietary LLMs significant strides mitigating gender bias alignment datasets publicly available commonly used publicly available alignment dataset HH RLHF exhibits gender bias extent lack publicly available alignment datasets specifically designed address gender bias developed new dataset named GenderAlign aiming mitigating comprehensive set gender biases LLMs dataset comprises 8k single turn dialogues paired chosen rejected response Compared rejected responses chosen responses demonstrate lower levels gender bias higher quality Furthermore categorized gender biases rejected responses GenderAlign 4 principal categories experimental results effectiveness GenderAlign reducing gender bias LLMs
585,Large Language and Protein Assistant for Protein-Protein Interactions Prediction,"['Peng Zhou', 'Pengsen Ma', 'Jianmin Wang', 'Xibao Cai', 'Haitao Huang', 'Wei Liu', 'Longyue Wang', 'Lai Hou Tim', 'xiangxiang Zeng']",,Large Language Protein Assistant Protein Protein Interactions Prediction
586,SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement,"['Runnan Fang', 'Xiaobin Wang', 'Yuan Liang', 'Shuofei Qiao', 'Jialong Wu', 'Zekun Xi', 'Ningyu Zhang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen']","In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.",SynWorld Virtual Scenario Synthesis Agentic Action Knowledge Refinement interaction agents environments agents expand capabilities planning executing actions LLM based agents face substantial challenges deployed novel environments required navigate unconventional action spaces empower agents autonomously explore environments optimize workflows enhance understanding actions propose SynWorld framework allows agents synthesize possible scenarios multi step action invocation action space perform Monte Carlo Tree Search MCTS exploration effectively refine action knowledge current environment experiments demonstrate SynWorld effective general approach learning action knowledge new environments Code available https github com zjunlp SynWorld
587,An Empirical Study of Many-to-Many Summarization with Large Language Models,"['Jiaan Wang', 'Fandong Meng', 'Zengkui Sun', 'Yunlong Liang', 'Yuxuan Cao', 'Jiarong Xu', 'HAOXIANG SHI', 'Jie Zhou']","Many-to-many summarization (M2MS) aims to process documents in any language and generate the corresponding summaries also in any language. Recently, large language models (LLMs) have shown strong multi-lingual abilities, giving them the potential to perform M2MS in real applications. This work presents a systematic empirical study on LLMs' M2MS ability. Specifically, we first reorganize M2MS data based on eight previous domain-specific datasets. The reorganized data contains 47.8K samples spanning five domains and six languages, which could be used to train and evaluate LLMs. Then, we benchmark 18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned traditional models (e.g., mBART) are also conducted for comparisons. Our experiments reveal that, zero-shot LLMs achieve competitive results with fine-tuned traditional models. After instruct-tuning, open-source LLMs can significantly improve their M2MS ability, and outperform zero-shot LLMs (including GPT-4) in terms of automatic evaluations. In addition, we demonstrate that this task-specific improvement does not sacrifice the LLMs' general task-solving abilities. However, as revealed by our human evaluation, LLMs still face the factuality issue, and the instruction tuning might intensify the issue. Thus, how to control factual errors becomes the key when building LLM summarizers in real applications, and is worth noting in future research.",Empirical Study Summarization Large Language Models summarization M2MS aims process documents language generate corresponding summaries language Recently large language models LLMs shown strong multi lingual abilities giving potential perform M2MS real applications work presents systematic empirical study LLMs M2MS ability Specifically reorganize M2MS data based previous domain specific datasets reorganized data contains 47 8K samples spanning domains languages used train evaluate LLMs benchmark 18 LLMs zero shot manner instruction tuning manner Fine tuned traditional models e g mBART conducted comparisons experiments reveal zero shot LLMs achieve competitive results fine tuned traditional models instruct tuning open source LLMs significantly improve M2MS ability outperform zero shot LLMs including GPT 4 terms automatic evaluations addition demonstrate task specific improvement does sacrifice LLMs general task solving abilities revealed human evaluation LLMs face factuality issue instruction tuning intensify issue control factual errors key building LLM summarizers real applications worth noting future research
588,Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models,"['Suhang Wu', 'Jialong Tang', 'Chengyi Yang', 'Pei Zhang', 'Baosong Yang', 'Junhui Li', 'Min Zhang', 'Jinsong Su']",,Locate Focus Enhancing Terminology Translation Speech Language Models
589,GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents,"['Lingxiao Diao', 'Xinyue Xu', 'Wanxuan Sun', 'Cheng Yang', 'Zhuosheng Zhang']","Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines.",GuideBench Benchmarking Domain Oriented Guideline Following LLM Agents Large language models LLMs widely deployed autonomous agents capable following user instructions making decisions real world applications Previous studies notable progress benchmarking instruction following capabilities LLMs general domains primary focus inherent commonsense knowledge Recently LLMs increasingly deployed domain oriented agents rely domain oriented guidelines conflict commonsense knowledge guidelines exhibit key characteristics consist wide range domain oriented rules subject frequent updates Despite challenges absence comprehensive benchmarks evaluating domain oriented guideline following capabilities LLMs presents significant obstacle effective assessment development paper introduce GuideBench comprehensive benchmark designed evaluate guideline following performance LLMs GuideBench evaluates LLMs critical aspects adherence diverse rules ii robustness rule updates iii alignment human preferences Experimental results range LLMs indicate substantial opportunities improving ability follow domain oriented guidelines
590,TC–RAG: Turing–Complete RAG’s Case study on Medical LLM Systems,"['Xinke Jiang', 'Yue Fang', 'Rihong Qiu', 'Haoyu Zhang', 'Yongxin Xu', 'Hao Chen', 'WentaoZhang', 'Ruizhe Zhang', 'Yuchen Fang', 'Xinyu Ma', 'Xu Chu', 'Junfeng Zhao', 'Yasha Wang']",,TC RAG Turing Complete RAG s Case study Medical LLM Systems
591,SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning,"['Zexiong Ma', 'Chao Peng', 'Pengfei Gao', 'Xiangxin Meng', 'Yanzhen Zou', 'Bing Xie']","Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.",SoRFT Issue Resolving Subtask oriented Reinforced Fine Tuning Mainstream issue resolving frameworks predominantly rely commercial models leading high costs privacy concerns Existing training approaches issue resolving struggle poor generalization fail fully leverage open source development resources propose Subtask oriented Reinforced Fine Tuning SoRFT novel training approach enhance issue resolving capability LLMs decomposes issue resolving structured subtasks file localization function localization line localization code edit generation SoRFT consists training stages 1 rejection sampled supervised fine tuning Chain Thought CoT data filtered using ground truth fine tuning LLM 2 rule based reinforcement learning leverages PPO ground truth based rewards evaluate SoRFT trained model SWE Bench Verified SWE Bench Lite achieving state art SOTA performance open source models e g resolve 21 4 issues SWE Bench Verified SoRFT Qwen 7B experimental results demonstrate SoRFT significantly enhances issue resolving performance improves model generalization provides cost efficient alternative commercial models
592,MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models,"['Zhongzhan Huang', 'Guoming Ling', 'Shanshan Zhong', 'Hefeng Wu', 'Liang Lin']","Long Context Understanding (LCU) is a critical area for exploration in current large language models (LLMs). However, due to the inherently lengthy nature of long-text data, existing LCU benchmarks for LLMs often result in prohibitively high evaluation costs, like testing time and inference expenses. Through extensive experimentation, we discover that existing LCU benchmarks exhibit significant redundancy, which means the inefficiency in evaluation. In this paper, we propose a concise data compression method tailored for long-text data with sparse information characteristics. By pruning the well-known LCU benchmark LongBench, we create MiniLongBench. This benchmark includes only 237 test samples across six major task categories and 21 distinct tasks. Through empirical analysis of over 60 LLMs, MiniLongBench achieves an average evaluation cost reduced to only 4.5% of the original while maintaining an average rank correlation coefficient of 0.97 with LongBench results. Therefore, our MiniLongBench, as a low-cost benchmark, holds great potential to substantially drive future research into the LCU capabilities of LLMs. See https://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.",MiniLongBench Low cost Long Context Understanding Benchmark Large Language Models Long Context Understanding LCU critical area exploration current large language models LLMs inherently lengthy nature long text data existing LCU benchmarks LLMs result prohibitively high evaluation costs like testing time inference expenses extensive experimentation discover existing LCU benchmarks exhibit significant redundancy means inefficiency evaluation paper propose concise data compression method tailored long text data sparse information characteristics pruning known LCU benchmark LongBench create MiniLongBench benchmark includes 237 test samples major task categories 21 distinct tasks empirical analysis 60 LLMs MiniLongBench achieves average evaluation cost reduced 4 5 original maintaining average rank correlation coefficient 0 97 LongBench results MiniLongBench low cost benchmark holds great potential substantially drive future research LCU capabilities LLMs https github com MilkThink Lab MiniLongBench code data tutorial
593,Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG,"['Xin Sun', 'Jianan Xie', 'Zhongqi Chen', 'Qiang Liu', 'Shu Wu', 'Yuehe Chen', 'Bowen Song', 'Zilei Wang', 'Weiqiang Wang', 'Liang Wang']","Large language models (LLMs) augmented with retrieval systems have significantly advanced natural language processing tasks by integrating external knowledge sources, enabling more accurate and contextually rich responses. To improve the robustness of such systems against noisy retrievals, Retrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method. However, RAFT conditions models to generate answers even in the absence of reliable knowledge. This behavior undermines their reliability in high-stakes domains, where acknowledging uncertainty is critical. To address this issue, we propose Divide-Then-Align (DTA), a post-training approach designed to endow RAG systems with the ability to respond with ""I don't know"" when the query is out of the knowledge boundary of both the retrieved passages and the model's internal knowledge. DTA divides data samples into four knowledge quadrants and constructs tailored preference data for each quadrant, resulting in a curated dataset for Direct Preference Optimization (DPO). Experimental results on three benchmark datasets demonstrate that DTA effectively balances accuracy with appropriate abstention, enhancing the reliability and trustworthiness of retrieval-augmented systems.",Divide Align Honest Alignment based Knowledge Boundary RAG Large language models LLMs augmented retrieval systems significantly advanced natural language processing tasks integrating external knowledge sources enabling accurate contextually rich responses improve robustness systems noisy retrievals Retrieval Augmented Fine Tuning RAFT emerged widely adopted method RAFT conditions models generate answers absence reliable knowledge behavior undermines reliability high stakes domains acknowledging uncertainty critical address issue propose Divide Align DTA post training approach designed endow RAG systems ability respond don t know query knowledge boundary retrieved passages model s internal knowledge DTA divides data samples knowledge quadrants constructs tailored preference data quadrant resulting curated dataset Direct Preference Optimization DPO Experimental results benchmark datasets demonstrate DTA effectively balances accuracy appropriate abstention enhancing reliability trustworthiness retrieval augmented systems
594,PwnGPT: Automatic Exploit Generation Based on Large Language Models,"['Wanzong Peng', 'Lin Ye', 'Xuetao Du', 'Hongli Zhang', 'Dongyang Zhan', 'Yunting Zhang', 'Yicheng Guo', 'Chen Zhang']",,PwnGPT Automatic Exploit Generation Based Large Language Models
595,VMLU Benchmarks: A comprehensive benchmark toolkit for Vietnamese LLMs,"['Cuc Thi Bui', 'Nguyen Truong Son', 'Truong van trang', 'Lam Viet Phung', 'Pham Nhut Huy', 'Hoang Anh Le', 'Quoc Huu Van', 'Phong Nguyen-Thuan Do', 'Van Le Tran Truc', 'Duc Thanh Chau', 'Le-Minh Nguyen']",,VMLU Benchmarks comprehensive benchmark toolkit Vietnamese LLMs
596,Scaling Laws for RNN LLM in Long-Context Scenarios with State Size,"['Kai Liu', 'Jianfei Gao', 'Kai Chen']",,Scaling Laws RNN LLM Long Context Scenarios State Size
597,Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes,"['Bocheng Li', 'Zhujin Gao', 'Linli Xu']","Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to continuous spaces and apply fine-grained noise, but the diffusion progress is uniform across tokens, limiting their ability to capture semantic nuances. To address these limitations, we propose \textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models (NeoDiff), a novel diffusion model that integrates the strengths of both discrete and continuous approaches. NeoDiff introduces a Poisson diffusion process for the forward process, enabling a flexible and fine-grained noising paradigm, and employs a time predictor for the reverse process to adaptively modulate the denoising progress based on token semantics. Furthermore, NeoDiff utilizes an optimized schedule for inference to ensure more precise noise control and improved performance. Our approach unifies the theories of discrete and continuous diffusion models, offering a more principled and effective framework for text generation. Experimental results on several text generation tasks demonstrate NeoDiff's superior performance compared to baselines of non-autoregressive continuous and discrete diffusion models, iterative-based methods and autoregressive diffusion-based methods. These results highlight NeoDiff's potential as a powerful tool for generating high-quality text and advancing the field of diffusion-based text generation.",Unifying Continuous Discrete Text Diffusion Non simultaneous Diffusion Processes Diffusion models emerged promising approach text generation recent works falling main categories discrete continuous diffusion models Discrete diffusion models apply token corruption independently using categorical distributions allowing different diffusion progress tokens lacking fine grained control Continuous diffusion models map tokens continuous spaces apply fine grained noise diffusion progress uniform tokens limiting ability capture semantic nuances address limitations propose textbf underline N simultan textbf underline e ous C textbf underline o ntinuous textbf underline Diff usion Models NeoDiff novel diffusion model integrates strengths discrete continuous approaches NeoDiff introduces Poisson diffusion process forward process enabling flexible fine grained noising paradigm employs time predictor reverse process adaptively modulate denoising progress based token semantics Furthermore NeoDiff utilizes optimized schedule inference ensure precise noise control improved performance approach unifies theories discrete continuous diffusion models offering principled effective framework text generation Experimental results text generation tasks demonstrate NeoDiff s superior performance compared baselines non autoregressive continuous discrete diffusion models iterative based methods autoregressive diffusion based methods results highlight NeoDiff s potential powerful tool generating high quality text advancing field diffusion based text generation
598,A Strategic Coordination Framework of Small LMs Matches Large LMs in Data Synthesis,"['Xin Gao', 'Qizhi Pei', 'Zinan Tang', 'Yu Li', 'Honglin Lin', 'Jiang Wu', 'Lijun Wu', 'Conghui He']",,Strategic Coordination Framework Small LMs Matches Large LMs Data Synthesis
599,Defining and Evaluating Visual Language Models’ Basic Spatial Abilities: A Perspective from Psychometrics,"['Wenrui Xu', 'Dalin Lyu', 'Weihang Wang', 'Jie Feng', 'Chen Gao', 'Yong Li']",,Defining Evaluating Visual Language Models Basic Spatial Abilities Perspective Psychometrics
600,SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through Hierarchical Evaluation,"['Wenyu Zhang', 'Wei En Ng', 'Lixin Ma', 'Yuwen Wang', 'Junqi Zhao', 'Allison Koenecke', 'Boyang Li', 'WANGLU']","Current vision-language models may grasp basic spatial cues and simple directions (e.g. left, right, front, back), but struggle with the multi-dimensional spatial reasoning necessary for human-like understanding and real-world applications. To address this gap, we develop SPHERE (Spatial Perception and Hierarchical Evaluation of REasoning), a hierarchical evaluation framework supported by a new human-annotated dataset. SPHERE systematically probes models across increasing levels of complexity, from fundamental skills to multi-skill integration and high-level reasoning that combines spatial, visual, and logical understanding. Benchmark evaluation of state-of-the-art models reveals significant deficiencies, especially in reasoning about distance and proximity, understanding both egocentric and allocentric perspectives, and applying spatial logic in physical contexts. These findings expose critical blind spots in existing models and underscore the need for more advanced spatial reasoning techniques, driving the development of vision-language models that align more closely with human spatial cognition. The SPHERE benchmark is available at https://github.com/zwenyu/SPHERE-VLM.",SPHERE Unveiling Spatial Blind Spots Vision Language Models Hierarchical Evaluation Current vision language models grasp basic spatial cues simple directions e g left right struggle multi dimensional spatial reasoning necessary human like understanding real world applications address gap develop SPHERE Spatial Perception Hierarchical Evaluation REasoning hierarchical evaluation framework supported new human annotated dataset SPHERE systematically probes models increasing levels complexity fundamental skills multi skill integration high level reasoning combines spatial visual logical understanding Benchmark evaluation state art models reveals significant deficiencies especially reasoning distance proximity understanding egocentric allocentric perspectives applying spatial logic physical contexts findings expose critical blind spots existing models underscore need advanced spatial reasoning techniques driving development vision language models align closely human spatial cognition SPHERE benchmark available https github com zwenyu SPHERE VLM
601,Enhancing Retrieval Systems with Inference-Time Logical Reasoning,"['Felix Faltings', 'Wei Wei', 'Yujia Bao']","Traditional retrieval methods rely on transforming user queries into vector representations and retrieving documents based on cosine similarity within an embedding space. While efficient and scalable, this approach often fails to handle complex queries involving logical constructs such as negations, conjunctions, and disjunctions. In this paper, we propose a novel inference-time logical reasoning framework that explicitly incorporates logical reasoning into the retrieval process. Our method extracts logical reasoning structures from natural language queries and then composes the individual cosine similarity scores to formulate the final document scores. This approach enables the retrieval process to handle complex logical reasoning without compromising computational efficiency. Our results on both synthetic and real-world benchmarks demonstrate that the proposed method consistently outperforms traditional retrieval methods across different models and datasets, significantly improving retrieval performance for complex queries.",Enhancing Retrieval Systems Inference Time Logical Reasoning Traditional retrieval methods rely transforming user queries vector representations retrieving documents based cosine similarity embedding space efficient scalable approach fails handle complex queries involving logical constructs negations conjunctions disjunctions paper propose novel inference time logical reasoning framework explicitly incorporates logical reasoning retrieval process method extracts logical reasoning structures natural language queries composes individual cosine similarity scores formulate final document scores approach enables retrieval process handle complex logical reasoning compromising computational efficiency results synthetic real world benchmarks demonstrate proposed method consistently outperforms traditional retrieval methods different models datasets significantly improving retrieval performance complex queries
602,Using Subtext to Enhance Generative IDRR,"['Zhipang Wang', 'Yu Hong', 'Weihao Sun', 'Guodong Zhou']",,Using Subtext Enhance Generative IDRR
603,User-side Model Consistency Monitoring for Open Source Large Language Models Inference Services,"['Qijun Miao', 'Zhixuan Fang']",,User Model Consistency Monitoring Open Source Large Language Models Inference Services
604,Jailbreaking? One Step Is Enough!,"['Weixiong Zheng', 'Peijian Zeng', 'YiWei Li', 'Hongyan Wu', 'Nankai Lin', 'Junhao Chen', 'Aimin Yang', 'Yongmei Zhou']","Large language models (LLMs) excel in various tasks but remain vulnerable to jailbreak attacks, where adversaries manipulate prompts to generate harmful outputs. Examining jailbreak prompts helps uncover the shortcomings of LLMs. However, current jailbreak methods and the target model's defenses are engaged in an independent and adversarial process, resulting in the need for frequent attack iterations and redesigning attacks for different models. To address these gaps, we propose a Reverse Embedded Defense Attack (REDA) mechanism that disguises the attack intention as the ""defense"". intention against harmful content. Specifically, REDA starts from the target response, guiding the model to embed harmful content within its defensive measures, thereby relegating harmful content to a secondary role and making the model believe it is performing a defensive task. The attacking model considers that it is guiding the target model to deal with harmful content, while the target model thinks it is performing a defensive task, creating an illusion of cooperation between the two. Additionally, to enhance the model's confidence and guidance in ""defensive"" intentions, we adopt in-context learning (ICL) with a small number of attack examples and construct a corresponding dataset of attack examples. Extensive evaluations demonstrate that the REDA method enables cross-model attacks without the need to redesign attack strategies for different models, enables successful jailbreak in one iteration, and outperforms existing methods on both open-source and closed-source models.",Jailbreaking Step Large language models LLMs excel various tasks remain vulnerable jailbreak attacks adversaries manipulate prompts generate harmful outputs Examining jailbreak prompts helps uncover shortcomings LLMs current jailbreak methods target model s defenses engaged independent adversarial process resulting need frequent attack iterations redesigning attacks different models address gaps propose Reverse Embedded Defense Attack REDA mechanism disguises attack intention defense intention harmful content Specifically REDA starts target response guiding model embed harmful content defensive measures relegating harmful content secondary role making model believe performing defensive task attacking model considers guiding target model deal harmful content target model thinks performing defensive task creating illusion cooperation Additionally enhance model s confidence guidance defensive intentions adopt context learning ICL small number attack examples construct corresponding dataset attack examples Extensive evaluations demonstrate REDA method enables cross model attacks need redesign attack strategies different models enables successful jailbreak iteration outperforms existing methods open source closed source models
605,Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning,"['Yongxin Xu', 'Ruizhe Zhang', 'Xinke Jiang', 'Yujie Feng', 'Yuzhen Xiao', 'Xinyu Ma', 'Runchuan Zhu', 'Xu Chu', 'Junfeng Zhao', 'Yasha Wang']","Retrieval-Augmented Generation (RAG) offers an effective solution to the issues faced by Large Language Models (LLMs) in hallucination generation and knowledge obsolescence by incorporating externally retrieved knowledge. However, existing methods lack effective control mechanisms for integrating internal and external knowledge. Inspired by human cognitive processes, we propose Parenting, a novel framework that decouples, identifies, and purposefully optimizes parameter subspaces related to adherence and robustness. Specifically, Parenting utilizes a key parameter mining method that combines forward and backward propagation signals to localize subspaces representing different capabilities. Then, Parenting employs a type-tailored tuning strategy, applying specific and appropriate optimizations to different subspaces, aiming to achieve a balanced enhancement of both adherence and robustness. Extensive experiments on various datasets and models validate the effectiveness and generalizability of our method.",Parenting Optimizing Knowledge Selection Retrieval Augmented Language Models Parameter Decoupling Tailored Tuning Retrieval Augmented Generation RAG offers effective solution issues faced Large Language Models LLMs hallucination generation knowledge obsolescence incorporating externally retrieved knowledge existing methods lack effective control mechanisms integrating internal external knowledge Inspired human cognitive processes propose Parenting novel framework decouples identifies purposefully optimizes parameter subspaces related adherence robustness Specifically Parenting utilizes key parameter mining method combines forward backward propagation signals localize subspaces representing different capabilities Parenting employs type tailored tuning strategy applying specific appropriate optimizations different subspaces aiming achieve balanced enhancement adherence robustness Extensive experiments various datasets models validate effectiveness generalizability method
606,PaSa: An LLM Agent for Comprehensive Academic Paper Search,"['Yichen He', 'Guanhua Huang', 'Peiyuan Feng', 'Yuan Lin', 'Yuchen Zhang', 'Hang Li', 'Weinan E']","We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholar queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4o for paraphrased queries, ChatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50, and exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.",PaSa LLM Agent Comprehensive Academic Paper Search introduce PaSa advanced Paper Search agent powered large language models PaSa autonomously make series decisions including invoking search tools reading papers selecting relevant references ultimately obtain comprehensive accurate results complex scholar queries optimize PaSa using reinforcement learning synthetic dataset AutoScholarQuery includes 35k fine grained academic queries corresponding papers sourced tier AI conference publications Additionally develop RealScholarQuery benchmark collecting real world academic queries assess PaSa performance realistic scenarios Despite trained synthetic data PaSa significantly outperforms existing baselines RealScholarQuery including Google Google Scholar Google GPT 4o paraphrased queries ChatGPT search enabled GPT 4o GPT o1 PaSa GPT 4o PaSa implemented prompting GPT 4o Notably PaSa 7B surpasses best Google based baseline Google GPT 4o 37 78 recall 20 39 90 recall 50 exceeds PaSa GPT 4o 30 36 recall 4 25 precision Model datasets code available https github com bytedance pasa
607,Less Mature is More Adaptable for Sentence-level Language Modeling,"['Abhilasha Sancheti', 'David Dale', 'Artyom Kozhevnikov', 'Maha Elbayad']",,Mature Adaptable Sentence level Language Modeling
608,EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts,"['Subhajit Chaudhury', 'Payel Das', 'Sarathkrishna Swaminathan', 'Georgios Kollias', 'Elliot Nelson', 'Khushbu Pahwa', 'Tejaswini Pedapati', 'Igor Melnyk', 'Matthew Riemer']","Recent advances in Large Language Models (LLMs) have yielded impressive successes on many language tasks. However, efficient processing of long contexts using LLMs remains a significant challenge. We introduce \textbf{EpMAN} -- a method for processing long contexts in an \textit{episodic memory} module while \textit{holistically attending to} semantically relevant context chunks. The output of \textit{episodic attention} is then used to reweigh the decoder's self-attention to the stored KV cache of the context during training and generation. When an LLM decoder is trained using \textbf{EpMAN}, its performance on multiple challenging single-hop long-context recall and question-answering benchmarks is found to be stronger and more robust across the range from 16k to 256k tokens than baseline decoders trained with self-attention, and popular retrieval-augmented generation frameworks.",EpMAN Episodic Memory AttentioN Generalizing Longer Contexts Recent advances Large Language Models LLMs yielded impressive successes language tasks efficient processing long contexts using LLMs remains significant challenge introduce textbf EpMAN method processing long contexts textit episodic memory module textit holistically attending semantically relevant context chunks output textit episodic attention used reweigh decoder s self attention stored KV cache context training generation LLM decoder trained using textbf EpMAN performance multiple challenging single hop long context recall question answering benchmarks stronger robust range 16k 256k tokens baseline decoders trained self attention popular retrieval augmented generation frameworks
609,UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter Efficient Fine-Tuning of Large Models,"['Xueyan Zhang', 'Jinman Zhao', 'Zhifei Yang', 'Yibo Zhong', 'Shuhao Guan', 'Linbo Cao', 'Yining Wang']",,UORA Uniform Orthogonal Reinitialization Adaptation Parameter Efficient Fine Tuning Large Models
610,"Agri-CM$^3$: A Chinese Massive Multi-modal, Multi-level Benchmark for Agricultural Understanding and Reasoning","['Haotian Wang', 'Yi Guan', 'Fanshu Meng', 'Chao Zhao', 'Lian Yan', 'Yang Yang', 'Jingchi Jiang']",,Agri CM 3 Chinese Massive Multi modal Multi level Benchmark Agricultural Understanding Reasoning
611,TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification,"['Junnan Zhu', 'Min Xiao', 'Yining Wang', 'Feifei Zhai', 'Yu Zhou', 'Chengqing Zong']","LLMs have achieved remarkable fluency and coherence in text generation, yet their widespread adoption has raised concerns about content reliability and accountability. In high-stakes domains, it is crucial to understand where and how the content is created. To address this, we introduce the Text pROVEnance (TROVE) challenge, designed to trace each sentence of a target text back to specific source sentences within potentially lengthy or multi-document inputs. Beyond identifying sources, TROVE annotates the fine-grained relationships (quotation, compression, inference, and others), providing a deep understanding of how each target sentence is formed. To benchmark TROVE, we construct our dataset by leveraging three public datasets covering 11 diverse scenarios (e.g., QA and summarization) in English and Chinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+), emphasizing the multi-document and long-document settings essential for provenance. To ensure high-quality data, we employ a three-stage annotation process: sentence retrieval, GPT-4o provenance, and human provenance. We evaluate 11 LLMs under direct prompting and retrieval-augmented paradigms, revealing that retrieval is essential for robust performance, larger models perform better in complex relationship classification, and closed-source models often lead, yet open-source models show significant promise, particularly with retrieval augmentation. We make our dataset available here: https://github.com/ZNLP/ZNLP-Dataset.",TROVE Challenge Fine Grained Text Provenance Source Sentence Tracing Relationship Classification LLMs achieved remarkable fluency coherence text generation widespread adoption raised concerns content reliability accountability high stakes domains crucial understand content created address introduce Text pROVEnance TROVE challenge designed trace sentence target text specific source sentences potentially lengthy multi document inputs identifying sources TROVE annotates fine grained relationships quotation compression inference providing deep understanding target sentence formed benchmark TROVE construct dataset leveraging public datasets covering 11 diverse scenarios e g QA summarization English Chinese spanning source texts varying lengths 0 5k 5 10k 10k emphasizing multi document long document settings essential provenance ensure high quality data employ stage annotation process sentence retrieval GPT 4o provenance human provenance evaluate 11 LLMs direct prompting retrieval augmented paradigms revealing retrieval essential robust performance larger models perform better complex relationship classification closed source models lead open source models significant promise particularly retrieval augmentation make dataset available https github com ZNLP ZNLP Dataset
612,CaLMQA: Exploring culturally specific long-form question answering across 23 languages,"['Shane Arora', 'Marzena Karpinska', 'Hung-Ting Chen', 'Ipsita Bhattacharjee', 'Mohit Iyyer', 'Eunsol Choi']","Despite rising global usage of large language models (LLMs), their ability to generate long-form answers to culturally specific questions remains unexplored in many languages. To fill this gap, we perform the first study of textual multilingual long-form QA by creating CaLMQA, a dataset of 51.7K culturally specific questions across 23 different languages. We define culturally specific questions as those that refer to concepts unique to one or a few cultures, or have different answers depending on the cultural or regional context. We obtain these questions by crawling naturally-occurring questions from community web forums in high-resource languages, and by hiring native speakers to write questions in under-resourced, rarely-studied languages such as Fijian and Kirundi. Our data collection methodologies are translation-free, enabling the collection of culturally unique questions like ""Kuber iki umwami wa mbere w'uburundi yitwa Ntare?"" (Kirundi; English translation: ""Why was the first king of Burundi called Ntare (Lion)?""). We evaluate factuality, relevance and surface-level quality of LLM-generated long-form answers, finding that (1) for many languages, even the best models make critical surface-level errors (e.g., answering in the wrong language, repetition), especially for low-resource languages; and (2) answers to culturally specific questions contain more factual errors than answers to culturally agnostic questions -- questions that have consistent meaning and answer across many cultures. We release CaLMQA to facilitate future research in cultural and multilingual long-form QA.",CaLMQA Exploring culturally specific long form question answering 23 languages Despite rising global usage large language models LLMs ability generate long form answers culturally specific questions remains unexplored languages gap perform study textual multilingual long form QA creating CaLMQA dataset 51 7K culturally specific questions 23 different languages define culturally specific questions refer concepts unique cultures different answers depending cultural regional context obtain questions crawling naturally occurring questions community web forums high resource languages hiring native speakers write questions resourced rarely studied languages Fijian Kirundi data collection methodologies translation free enabling collection culturally unique questions like Kuber iki umwami wa mbere w uburundi yitwa Ntare Kirundi English translation king Burundi called Ntare Lion evaluate factuality relevance surface level quality LLM generated long form answers finding 1 languages best models make critical surface level errors e g answering wrong language repetition especially low resource languages 2 answers culturally specific questions contain factual errors answers culturally agnostic questions questions consistent meaning answer cultures release CaLMQA facilitate future research cultural multilingual long form QA
613,Croppable Knowledge Graph Embedding,"['Yushan Zhu', 'Wen Zhang', 'Zhiqiang Liu', 'Mingyang Chen', 'Lei Liang', 'Huajun Chen']","Knowledge Graph Embedding (KGE) is a common approach for Knowledge Graphs (KGs) in AI tasks. Embedding dimensions depend on application scenarios. Requiring a new dimension means training a new KGE model from scratch, increasing cost and limiting efficiency and flexibility. In this work, we propose a novel KGE training framework MED. It allows one training to obtain a croppable KGE model for multiple scenarios with different dimensional needs. Sub-models of required dimensions can be directly cropped and used without extra training. In MED, we propose a mutual learning mechanism to improve the low-dimensional sub-models and make high-dimensional sub-models retain the low-dimensional sub-models' capacity, an evolutionary improvement mechanism to promote the high-dimensional sub-models to master the triple that the low-dimensional sub-models can not, and a dynamic loss weight to adaptively balance the multiple losses. Experiments on 4 KGE models across 4 standard KG completion datasets, 3 real-world scenarios using a large-scale KG, and extending MED to the BERT language model demonstrate its effectiveness, high efficiency, and flexible extensibility.",Croppable Knowledge Graph Embedding Knowledge Graph Embedding KGE common approach Knowledge Graphs KGs AI tasks Embedding dimensions depend application scenarios Requiring new dimension means training new KGE model scratch increasing cost limiting efficiency flexibility work propose novel KGE training framework MED allows training obtain croppable KGE model multiple scenarios different dimensional needs Sub models required dimensions directly cropped used extra training MED propose mutual learning mechanism improve low dimensional sub models make high dimensional sub models retain low dimensional sub models capacity evolutionary improvement mechanism promote high dimensional sub models master triple low dimensional sub models dynamic loss weight adaptively balance multiple losses Experiments 4 KGE models 4 standard KG completion datasets 3 real world scenarios using large scale KG extending MED BERT language model demonstrate effectiveness high efficiency flexible extensibility
614,HyKGE: A Hypothesis Knowledge Graph Enhanced RAG Framework for Accurate and Reliable Medical LLMs Responses,"['Xinke Jiang', 'Ruizhe Zhang', 'Yongxin Xu', 'Rihong Qiu', 'Yue Fang', 'Zhiyuan Wang', 'Jinyi Tang', 'Hongxin Ding', 'Xu Chu', 'Junfeng Zhao', 'Yasha Wang']",,HyKGE Hypothesis Knowledge Graph Enhanced RAG Framework Accurate Reliable Medical LLMs Responses
615,LongRecipe: Recipe for Efficient Long Context Generalization in Large Language Models,"['Zhiyuan Hu', 'Yuliang Liu', 'Jinman Zhao', 'Suyuchen Wang', 'WangYan', 'Wei Shen', 'Qing Gu', 'Anh Tuan Luu', 'See-Kiong Ng', 'Zhiwei Jiang', 'Bryan Hooi']","Large language models (LLMs) face significant challenges in handling long-context tasks because of their limited effective context window size during pretraining, which restricts their ability to generalize over extended sequences. Meanwhile, extending the context window in LLMs through post-pretraining is highly resource-intensive. To address this, we introduce LongRecipe, an efficient training strategy for extending the context window of LLMs, including impactful token analysis, position index transformation, and training optimization strategies. It simulates long-sequence inputs while maintaining training efficiency and significantly improves the model's understanding of long-range dependencies. Experiments on three types of LLMs show that LongRecipe can utilize long sequences while requiring only 30% of the target context window size, and reduces computational training resource over 85% compared to full sequence training. Furthermore, LongRecipe also preserves the original LLM's capabilities in general tasks. Ultimately, we can extend the effective context window of open-source LLMs from 8k to 128k, achieving performance close to GPT-4 with just one day of dedicated training using a single GPU with 80G memory. Our code is released at https://github.com/zhiyuanhubj/LongRecipe.",LongRecipe Recipe Efficient Long Context Generalization Large Language Models Large language models LLMs face significant challenges handling long context tasks limited effective context window size pretraining restricts ability generalize extended sequences extending context window LLMs post pretraining highly resource intensive address introduce LongRecipe efficient training strategy extending context window LLMs including impactful token analysis position index transformation training optimization strategies simulates long sequence inputs maintaining training efficiency significantly improves model s understanding long range dependencies Experiments types LLMs LongRecipe utilize long sequences requiring 30 target context window size reduces computational training resource 85 compared sequence training Furthermore LongRecipe preserves original LLM s capabilities general tasks Ultimately extend effective context window open source LLMs 8k 128k achieving performance close GPT 4 just day dedicated training using single GPU 80G memory code released https github com zhiyuanhubj LongRecipe
616,BeamLoRA: Beam-Constraint Low-Rank Adaptation,"['Naibin Gu', 'Zhenyu Zhang', 'Xiyu Liu', 'Peng Fu', 'Zheng Lin', 'Shuohuan Wang', 'Yu Sun', 'Hua Wu', 'Weiping Wang', 'Haifeng Wang']","Due to the demand for efficient fine-tuning of large language models, Low-Rank Adaptation (LoRA) has been widely adopted as one of the most effective parameter-efficient fine-tuning methods. Nevertheless, while LoRA improves efficiency, there remains room for improvement in accuracy. Herein, we adopt a novel perspective to assess the characteristics of LoRA ranks. The results reveal that different ranks within the LoRA modules not only exhibit varying levels of importance but also evolve dynamically throughout the fine-tuning process, which may limit the performance of LoRA. Based on these findings, we propose BeamLoRA, which conceptualizes each LoRA module as a beam where each rank naturally corresponds to a potential sub-solution, and the fine-tuning process becomes a search for the optimal sub-solution combination. BeamLoRA dynamically eliminates underperforming sub-solutions while expanding the parameter space for promising ones, enhancing performance with a fixed rank. Extensive experiments across three base models and 12 datasets spanning math reasoning, code generation, and commonsense reasoning demonstrate that BeamLoRA consistently enhances the performance of LoRA, surpassing the other baseline methods.",BeamLoRA Beam Constraint Low Rank Adaptation demand efficient fine tuning large language models Low Rank Adaptation LoRA widely adopted effective parameter efficient fine tuning methods LoRA improves efficiency remains room improvement accuracy adopt novel perspective assess characteristics LoRA ranks results reveal different ranks LoRA modules exhibit varying levels importance evolve dynamically fine tuning process limit performance LoRA Based findings propose BeamLoRA conceptualizes LoRA module beam rank naturally corresponds potential sub solution fine tuning process search optimal sub solution combination BeamLoRA dynamically eliminates underperforming sub solutions expanding parameter space promising ones enhancing performance fixed rank Extensive experiments base models 12 datasets spanning math reasoning code generation commonsense reasoning demonstrate BeamLoRA consistently enhances performance LoRA surpassing baseline methods
617,GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art,"['Chenkai Zhang', 'Yiming Lei', 'Zeming Liu', 'Haitao Leng', 'ShaoGuo Liu', 'Tingting Gao', 'Qingjie Liu', 'Yunhong Wang']","Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs' abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at https://github.com/stan-lei/GODBench-ACL2025.",GODBench Benchmark Multimodal Large Language Models Video Comment Art Video Comment Art enhances user engagement providing creative content conveys humor satire emotional resonance requiring nuanced comprehensive grasp cultural contextual subtleties Multimodal Large Language Models MLLMs Chain Thought CoT demonstrated strong reasoning abilities STEM tasks e g mathematics coding struggle generate creative expressions resonant jokes insightful satire existing benchmarks constrained limited modalities insufficient categories hindering exploration comprehensive creativity video based Comment Art creation address limitations introduce GODBench novel benchmark integrates video text modalities systematically evaluate MLLMs abilities compose Comment Art Furthermore inspired propagation patterns waves physics propose Ripple Thought RoT multi step reasoning framework designed enhance creativity MLLMs Extensive experiments reveal existing MLLMs CoT methods face significant challenges understanding generating creative video comments contrast RoT provides effective approach improve creative composing highlighting potential drive meaningful advancements MLLM based creativity GODBench publicly available https github com stan lei GODBench ACL2025
618,UniLR: Unleashing the Power of LLMs on Multiple Legal Tasks with a Unified Legal Retriever,"['Ang Li', 'Yiquan Wu', 'Yifei Liu', 'Ming Cai', 'Lizhi Qing', 'Shihang Wang', 'Yangyang Kang', 'Chengyuan Liu', 'Fei Wu', 'Kun Kuang']",,UniLR Unleashing Power LLMs Multiple Legal Tasks Unified Legal Retriever
619,Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models,"['Haoran Ye', 'TianZe Zhang', 'Yuhang Xie', 'Liyuan Zhang', 'Yuanyi Ren', 'Xin Zhang', 'Guojie Song']","Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz's values.",Generative Psycho Lexical Approach Constructing Value Systems Large Language Models Values core drivers individual collective perception cognition behavior Value systems Schwartz s Theory Basic Human Values delineate hierarchy interplay values enabling cross disciplinary investigations decision making societal dynamics Recently rise Large Language Models LLMs raised concerns regarding elusive intrinsic values Despite growing efforts evaluating understanding aligning LLM values psychologically grounded LLM value remains underexplored study addresses gap introducing Generative Psycho Lexical Approach GPLA scalable adaptable theoretically informed method constructing value systems Leveraging GPLA propose psychologically grounded factor value tailored LLMs systematic validation present benchmarking tasks integrate psychological principles cutting edge AI priorities results reveal proposed value meets standard psychological criteria better captures LLM values improves LLM safety prediction enhances LLM alignment compared canonical Schwartz s values
620,Beyond Dialogue: A Profile-Dialogue Alignment Framework Towards General Role-Playing Language Model,"['Yeyong Yu', 'Runsheng Yu', 'Haojie Wei', 'Zhanqiu Zhang', 'Quan QIAN']",,Dialogue Profile Dialogue Alignment Framework General Role Playing Language Model
621,ACECODER: Acing Coder RL via Automated Test-Case Synthesis,"['Huaye Zeng', 'Dongfu Jiang', 'Haozhe Wang', 'Ping Nie', 'Xiaotong Chen', 'Wenhu Chen']","Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\% and MBPP-plus by 6\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.",ACECODER Acing Coder RL Automated Test Case Synthesis progress recent coder models driven supervised fine tuning SFT potential reinforcement learning RL remains largely unexplored primarily lack reliable reward data model code domain paper address challenge leveraging automated large scale test case synthesis enhance code model training Specifically design pipeline generates extensive question test cases pairs existing code data Using test cases construct preference pairs based pass rates sampled programs train reward models Bradley Terry loss shows average 10 point improvement Llama 3 1 8B Ins 5 point improvement Qwen2 5 Coder 7B Ins best 32 sampling making 7B model par 236B DeepSeek V2 5 Furthermore conduct reinforcement learning reward models test case pass rewards leading consistent improvements HumanEval MBPP BigCodeBench LiveCodeBench V4 Notably follow R1 style training start Qwen2 5 Coder base directly RL training improve model HumanEval plus 25 MBPP plus 6 merely 80 optimization steps believe results highlight huge potential reinforcement learning coder models
622,Quantifying Semantic Emergence in Language Models,"['Hang Chen', 'Xinyu Yang', 'Jiaying Zhu', 'Wenya Wang']","Large language models (LLMs) are widely recognized for their exceptional capacity to capture semantics meaning. Yet, there remains no established metric to quantify this capability. In this work, we introduce a quantitative metric, Information Emergence (IE), designed to measure LLMs' ability to extract semantics from input tokens. We formalize ``semantics'' as the meaningful information abstracted from a sequence of tokens and quantify this by comparing the entropy reduction observed for a sequence of tokens (macro-level) and individual tokens (micro-level). To achieve this, we design a lightweight estimator to compute the mutual information at each transformer layer, which is agnostic to different tasks and language model architectures. We apply IE in both synthetic in-context learning (ICL) scenarios and natural sentence contexts. Experiments demonstrate informativeness and patterns about semantics. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights.",Quantifying Semantic Emergence Language Models Large language models LLMs widely recognized exceptional capacity capture semantics meaning remains established metric quantify capability work introduce quantitative metric Information Emergence designed measure LLMs ability extract semantics input tokens formalize semantics meaningful information abstracted sequence tokens quantify comparing entropy reduction observed sequence tokens macro level individual tokens micro level achieve design lightweight estimator compute mutual information transformer layer agnostic different tasks language model architectures apply synthetic context learning ICL scenarios natural sentence contexts Experiments demonstrate informativeness patterns semantics patterns confirm conventional prior linguistic knowledge rest relatively unexpected provide new insights
623,DebateCoder: Towards Collective Intelligence of LLMs via Test Case Driven LLM Debate for Code Generation,"['Jizheng Chen', 'Kounianhua Du', 'Xinyi Dai', 'Weiming Zhang', 'Xihuai Wang', 'Yasheng Wang', 'Ruiming Tang', 'Weinan Zhang', 'Yong Yu']",,DebateCoder Collective Intelligence LLMs Test Case Driven LLM Debate Code Generation
624,The Tug of War Within: Mitigating the Fairness-Privacy Conflicts in Large Language Models,"['Chen Qian', 'Dongrui Liu', 'Jie Zhang', 'Yong Liu', 'Jing Shao']","Ensuring awareness of fairness and privacy in Large Language Models (LLMs) is critical. Interestingly, we discover a counter-intuitive trade-off phenomenon that enhancing an LLM's privacy awareness through Supervised Fine-Tuning (SFT) methods significantly decreases its fairness awareness with thousands of samples. To address this issue, inspired by the information theory, we introduce a training-free method to \textbf{S}uppress the \textbf{P}rivacy and fa\textbf{I}rness coupled \textbf{N}eurons (\textbf{SPIN}), which theoretically and empirically decrease the mutual information between fairness and privacy awareness. Extensive experimental results demonstrate that SPIN eliminates the trade-off phenomenon and significantly improves LLMs' fairness and privacy awareness simultaneously without compromising general capabilities, \eg improving Qwen-2-7B-Instruct's fairness awareness by 12.2\% and privacy awareness by 14.0\%. More crucially, SPIN remains robust and effective with limited annotated data or even when only malicious fine-tuning data is available, whereas SFT methods may fail to perform properly in such scenarios. Furthermore, we show that SPIN could generalize to other potential trade-off dimensions. We hope this study provides valuable insights into concurrently addressing fairness and privacy concerns in LLMs and can be integrated into comprehensive frameworks to develop more ethical and responsible AI systems. Our code is available at https://github.com/ChnQ/SPIN.",Tug War Mitigating Fairness Privacy Conflicts Large Language Models Ensuring awareness fairness privacy Large Language Models LLMs critical Interestingly discover counter intuitive trade phenomenon enhancing LLM s privacy awareness Supervised Fine Tuning SFT methods significantly decreases fairness awareness thousands samples address issue inspired information theory introduce training free method textbf S uppress textbf P rivacy fa textbf rness coupled textbf N eurons textbf SPIN theoretically empirically decrease mutual information fairness privacy awareness Extensive experimental results demonstrate SPIN eliminates trade phenomenon significantly improves LLMs fairness privacy awareness simultaneously compromising general capabilities improving Qwen 2 7B Instruct s fairness awareness 12 2 privacy awareness 14 0 crucially SPIN remains robust effective limited annotated data malicious fine tuning data available SFT methods fail perform properly scenarios Furthermore SPIN generalize potential trade dimensions hope study provides valuable insights concurrently addressing fairness privacy concerns LLMs integrated comprehensive frameworks develop ethical responsible AI systems code available https github com ChnQ SPIN
625,GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding,"['Yukun Cao', 'Shuo Han', 'Zengyi Gao', 'Zezhong Ding', 'Xike Xie', 'S Kevin Zhou']","Although Large Language Models (LLMs) have demonstrated potential in processing graphs, they struggle with comprehending graphical structure information through prompts of graph description sequences, especially as the graph size increases. We attribute this challenge to the uneven memory performance of LLMs across different positions in graph description sequences, known as ''positional biases''. To address this, we propose GraphInsight, a novel framework aimed at improving LLMs' comprehension of both macro- and micro-level graphical information. GraphInsight is grounded in two key strategies: 1) placing critical graphical information in positions where LLMs exhibit stronger memory performance, and 2) investigating a lightweight external knowledge base for regions with weaker memory performance, inspired by retrieval-augmented generation (RAG). Moreover, GraphInsight explores integrating these two strategies into LLM agent processes for composite graph tasks that require multi-step reasoning. Extensive empirical studies on benchmarks with a wide range of evaluation tasks show that GraphInsight significantly outperforms all other graph description methods (e.g., prompting techniques and reordering strategies) in understanding graph structures of varying sizes.",GraphInsight Unlocking Insights Large Language Models Graph Structure Understanding Large Language Models LLMs demonstrated potential processing graphs struggle comprehending graphical structure information prompts graph description sequences especially graph size increases attribute challenge uneven memory performance LLMs different positions graph description sequences known positional biases address propose GraphInsight novel framework aimed improving LLMs comprehension macro micro level graphical information GraphInsight grounded key strategies 1 placing critical graphical information positions LLMs exhibit stronger memory performance 2 investigating lightweight external knowledge base regions weaker memory performance inspired retrieval augmented generation RAG GraphInsight explores integrating strategies LLM agent processes composite graph tasks require multi step reasoning Extensive empirical studies benchmarks wide range evaluation tasks GraphInsight significantly outperforms graph description methods e g prompting techniques reordering strategies understanding graph structures varying sizes
626,"Phonotomizer: A Compact, Unsupervised, Online Training Approach to Real-Time, Multilingual Phonetic Segmentation","['Michael S. Yantosca', 'Albert M. K. Cheng']",,Phonotomizer Compact Unsupervised Online Training Approach Real Time Multilingual Phonetic Segmentation
627,A Multi-persona Framework for Argument Quality Assessment,"['Bojun Jin', 'Jianzhu Bao', 'Yufang Hou', 'Yang Sun', 'Yice Zhang', 'Huajie Wang', 'Bin Liang', 'Ruifeng Xu']",,Multi persona Framework Argument Quality Assessment
628,Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification,"['Chengwu Liu', 'Ye Yuan', 'Yichun Yin', 'Yan Xu', 'Xin Xu', 'Zaoyu Chen', 'Lifeng Shang', 'Qun Liu', 'Ming Zhang']","Chain-of-Thought (CoT) prompting has become the de facto method to elicit reasoning capabilities from large language models (LLMs). However, to mitigate hallucinations in CoT that are notoriously difficult to detect, current methods such as process reward models (PRMs) or self-consistency operate as opaque boxes and do not provide checkable evidence for their judgments, possibly limiting their effectiveness. To address this issue, we draw inspiration from the idea that ""the gold standard for supporting a mathematical claim is to provide a proof"". We propose a retrospective, step-aware formal verification framework $Safe$. Rather than assigning arbitrary scores, we strive to articulate mathematical claims in formal mathematical language Lean 4 at each reasoning step and provide formal proofs to identify hallucinations. We evaluate our framework $Safe$ across multiple language models and various mathematical datasets, demonstrating a significant performance improvement while offering interpretable and verifiable evidence. We also propose $FormalStep$ as a benchmark for step correctness theorem proving with $30,809$ formal statements. To the best of our knowledge, our work represents the first endeavor to utilize formal mathematical language Lean 4 for verifying natural language content generated by LLMs, aligning with the reason why formal mathematical languages were created in the first place: to provide a robust foundation for hallucination-prone human-written proofs.",Safe Enhancing Mathematical Reasoning Large Language Models Retrospective Step aware Formal Verification Chain Thought CoT prompting facto method elicit reasoning capabilities large language models LLMs mitigate hallucinations CoT notoriously difficult detect current methods process reward models PRMs self consistency operate opaque boxes provide checkable evidence judgments possibly limiting effectiveness address issue draw inspiration idea gold standard supporting mathematical claim provide proof propose retrospective step aware formal verification framework Safe assigning arbitrary scores strive articulate mathematical claims formal mathematical language Lean 4 reasoning step provide formal proofs identify hallucinations evaluate framework Safe multiple language models various mathematical datasets demonstrating significant performance improvement offering interpretable verifiable evidence propose FormalStep benchmark step correctness theorem proving 30 809 formal statements best knowledge work represents endeavor utilize formal mathematical language Lean 4 verifying natural language content generated LLMs aligning reason formal mathematical languages created place provide robust foundation hallucination prone human written proofs
629,SAM Decoding: Speculative Decoding via Suffix Automaton,"['Yuxuan Hu', 'Ke Wang', 'Xiaokang Zhang', 'Fanjin Zhang', 'Cuiping Li', 'Hong Chen', 'Jing Zhang']","Speculative decoding (SD) has been demonstrated as an effective technique for lossless LLM inference acceleration. Retrieval-based SD methods, one kind of model-free method, have yielded promising speedup, but they often rely on incomplete retrieval resources, inefficient retrieval methods, and are constrained to certain domains. This paper presents a novel retrieval-based speculative decoding method that adapts suffix automaton (SAM) for efficient and accurate draft generation by utilizing common text corpus and dynamic text sequence. Unlike existing $n$-gram matching methods, SAM-Decoding finds the exact longest suffix match, achieving an average time complexity of O(1) per generation step of SAM update and suffix retrieval. It can also integrate with existing methods, adaptively selecting a draft generation strategy based on match length to generalize to broader domains. Extensive experiments on Spec-Bench show that our method is $18\%+$ faster than other retrieval-based SD methods. Additionally, when combined with advanced EAGLE-2, it provides an additional speedup of $3.28\%$ -- $11.13\%$ across various-sized LLM backbones. Our code is available at our \href{https://github.com/hyx1999/SAM-Decoding}{repository}.",SAM Decoding Speculative Decoding Suffix Automaton Speculative decoding SD demonstrated effective technique lossless LLM inference acceleration Retrieval based SD methods kind model free method yielded promising speedup rely incomplete retrieval resources inefficient retrieval methods constrained certain domains paper presents novel retrieval based speculative decoding method adapts suffix automaton SAM efficient accurate draft generation utilizing common text corpus dynamic text sequence Unlike existing n gram matching methods SAM Decoding finds exact longest suffix match achieving average time complexity O 1 generation step SAM update suffix retrieval integrate existing methods adaptively selecting draft generation strategy based match length generalize broader domains Extensive experiments Spec Bench method 18 faster retrieval based SD methods Additionally combined advanced EAGLE 2 provides additional speedup 3 28 11 13 various sized LLM backbones code available href https github com hyx1999 SAM Decoding repository
630,PsyAdvisor: A Plug-and-Play Strategy Advice Planner with Proactive Questioning in Psychological Conversations,"['Yuxin Hu', 'Danni Liu', 'Bo Liu', 'Yida Chen', 'Jiuxin Cao', 'Yan Liu']",,PsyAdvisor Plug Play Strategy Advice Planner Proactive Questioning Psychological Conversations
631,$HomeBench$: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices,"['Silin Li', 'Yuhang Guo', 'Jiashu Yao', 'Zeming Liu', 'Haifeng Wang']",,HomeBench Evaluating LLMs Smart Homes Valid Invalid Instructions Single Multiple Devices
632,Advancing Zero-shot Text-to-Speech Intelligibility across Diverse Domains via Preference Alignment,"['Xueyao Zhang', 'Yuancheng Wang', 'Chaoren Wang', 'Ziniu Li', 'Zhuo Chen', 'Zhizheng Wu']","Modern zero-shot text-to-speech (TTS) systems, despite using extensive pre-training, often struggle in challenging scenarios such as tongue twisters, repeated words, code-switching, and cross-lingual synthesis, leading to intelligibility issues. To address these limitations, this paper leverages preference alignment techniques, which enable targeted construction of out-of-pretraining-distribution data to enhance performance. We introduce a new dataset, named the Intelligibility Preference Speech Dataset (INTP), and extend the Direct Preference Optimization (DPO) framework to accommodate diverse TTS architectures. After INTP alignment, in addition to intelligibility, we observe overall improvements including naturalness, similarity, and audio quality for multiple TTS models across diverse domains. Based on that, we also verify the weak-to-strong generalization ability of INTP for more intelligible models such as CosyVoice 2 and Ints. Moreover, we showcase the potential for further improvements through iterative alignment based on Ints. Audio samples are available at https://intalign.github.io/.",Advancing Zero shot Text Speech Intelligibility Diverse Domains Preference Alignment Modern zero shot text speech TTS systems despite using extensive pre training struggle challenging scenarios tongue twisters repeated words code switching cross lingual synthesis leading intelligibility issues address limitations paper leverages preference alignment techniques enable targeted construction pretraining distribution data enhance performance introduce new dataset named Intelligibility Preference Speech Dataset INTP extend Direct Preference Optimization DPO framework accommodate diverse TTS architectures INTP alignment addition intelligibility observe overall improvements including naturalness similarity audio quality multiple TTS models diverse domains Based verify weak strong generalization ability INTP intelligible models CosyVoice 2 Ints showcase potential improvements iterative alignment based Ints Audio samples available https intalign github io
633,GiFT: Gibbs Fine-Tuning for Code Generation,"['Haochen Li', 'Wanjin Feng', 'Xin Zhou', 'Zhiqi Shen']","Training Large Language Models (LLMs) with synthetic data is a prevalent practice in code generation. A key approach is self-training, where LLMs are iteratively trained on self-generated correct code snippets. In this case, the self-generated codes are drawn from a conditional distribution, conditioned on a specific seed description. However, the seed description is not the only valid representation that aligns with its intended meaning. With all valid descriptions and codes forming a joint space, codes drawn from the conditional distribution would lead to an underrepresentation of the full description-code space. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training method inspired by Gibbs sampling. GiFT allows self-generated data to be drawn from the marginal distribution of the joint space, thereby mitigating the biases inherent in conditional sampling. We provide a theoretical analysis demonstrating the potential benefits of fine-tuning LLMs with code derived from the marginal distribution. Furthermore, we propose a perplexity-based code selection method to mitigate the imbalanced long-tail distribution of the self-generated codes. Empirical evaluation of two LLMs across four datasets demonstrates that GiFT achieves superior performance, particularly on more challenging benchmarks. Source code is available at https://github.com/Alex-HaochenLi/GiFT.",GiFT Gibbs Fine Tuning Code Generation Training Large Language Models LLMs synthetic data prevalent practice code generation key approach self training LLMs iteratively trained self generated correct code snippets case self generated codes drawn conditional distribution conditioned specific seed description seed description valid representation aligns intended meaning valid descriptions codes forming joint space codes drawn conditional distribution lead underrepresentation description code space propose Gibbs Fine Tuning GiFT novel self training method inspired Gibbs sampling GiFT allows self generated data drawn marginal distribution joint space mitigating biases inherent conditional sampling provide theoretical analysis demonstrating potential benefits fine tuning LLMs code derived marginal distribution Furthermore propose perplexity based code selection method mitigate imbalanced long tail distribution self generated codes Empirical evaluation LLMs datasets demonstrates GiFT achieves superior performance particularly challenging benchmarks Source code available https github com Alex HaochenLi GiFT
634,Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models,"['Yiwen Jiang', 'Deval Mehta', 'Wei Feng', 'Zongyuan Ge']","Concept Bottleneck Models (CBMs) decompose image classification into a process governed by interpretable, human-readable concepts. Recent advances in CBMs have used Large Language Models (LLMs) to generate candidate concepts. However, a critical question remains: What is the optimal number of concepts to use? Current concept banks suffer from redundancy or insufficient coverage. To address this issue, we introduce a dynamic, agent-based approach that adjusts the concept bank in response to environmental feedback, optimizing the number of concepts for sufficiency yet concise coverage. Moreover, we propose Conditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in traditional CBMs' concept scoring mechanisms. It enhances the accuracy of assessing each concept's contribution to classification tasks and feature an editable matrix that allows LLMs to correct concept scores that conflict with their internal knowledge. Our evaluations across 6 datasets show that our method not only improves classification accuracy by 6% but also enhances interpretability assessments by 30%.",Enhancing Interpretable Image Classification LLM Agents Conditional Concept Bottleneck Models Concept Bottleneck Models CBMs decompose image classification process governed interpretable human readable concepts Recent advances CBMs used Large Language Models LLMs generate candidate concepts critical question remains optimal number concepts use Current concept banks suffer redundancy insufficient coverage address issue introduce dynamic agent based approach adjusts concept bank response environmental feedback optimizing number concepts sufficiency concise coverage propose Conditional Concept Bottleneck Models CoCoBMs overcome limitations traditional CBMs concept scoring mechanisms enhances accuracy assessing concept s contribution classification tasks feature editable matrix allows LLMs correct concept scores conflict internal knowledge evaluations 6 datasets method improves classification accuracy 6 enhances interpretability assessments 30
635,Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction,"['Xiaowei Zhu', 'Yubing Ren', 'Yanan Cao', 'Xixun Lin', 'Fang Fang', 'Yangxi Li']","The rapid advancement of large language models has raised significant concerns regarding their potential misuse by malicious actors. As a result, developing effective detectors to mitigate these risks has become a critical priority. However, most existing detection methods focus excessively on detection accuracy, often neglecting the societal risks posed by high false positive rates (FPRs). This paper addresses this issue by leveraging Conformal Prediction (CP), which effectively constrains the upper bound of FPRs. While directly applying CP constrains FPRs, it also leads to a significant reduction in detection performance. To overcome this trade-off, this paper proposes a Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction (MCP), which both enforces the FPR constraint and improves detection performance. This paper also introduces RealDet, a high-quality dataset that spans a wide range of domains, ensuring realistic calibration and enabling superior detection performance when combined with MCP. Empirical evaluations demonstrate that MCP effectively constrains FPRs, significantly enhances detection performance, and increases robustness against adversarial attacks across multiple detectors and datasets.",Reliably Bounding False Positives Zero Shot Machine Generated Text Detection Framework Multiscaled Conformal Prediction rapid advancement large language models raised significant concerns regarding potential misuse malicious actors result developing effective detectors mitigate risks critical priority existing detection methods focus excessively detection accuracy neglecting societal risks posed high false positive rates FPRs paper addresses issue leveraging Conformal Prediction CP effectively constrains upper bound FPRs directly applying CP constrains FPRs leads significant reduction detection performance overcome trade paper proposes Zero Shot Machine Generated Text Detection Framework Multiscaled Conformal Prediction MCP enforces FPR constraint improves detection performance paper introduces RealDet high quality dataset spans wide range domains ensuring realistic calibration enabling superior detection performance combined MCP Empirical evaluations demonstrate MCP effectively constrains FPRs significantly enhances detection performance increases robustness adversarial attacks multiple detectors datasets
636,RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph,"['Junsik Kim', 'Jinwook Park', 'Kangil Kim']","In knowledge graph embedding, leveraging relation specific entity transformation has markedly enhanced performance. However, the consistency of embedding differences before and after transformation remains unaddressed, risking the loss of valuable inductive bias inherent in the embeddings. This inconsistency stems from two problems. First, transformation representations are specified for relations in a disconnected manner, allowing dissimilar transformations and corresponding entity embeddings for similar relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter Based on Relations) disrupts this consistency through excessive concentration of entity embeddings under entity-based regularization, generating indistinguishable score distributions among relations. In this paper, we introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF). Its entity transformation has three features for enhancing semantic consistency: 1) shared affine transformation of relation embeddings across all relations, 2) rooted entity transformation that adds an entity embedding to its change represented by the transformed vector, and 3) normalization of the change to prevent scale reduction. To amplify the advantages of consistency that preserve semantics on embeddings, RSCF adds relation transformation and prediction modules for enhancing the semantics. In knowledge graph completion tasks with distance-based and tensor decomposition models, RSCF significantly outperforms state-of-the-art KGE methods, showing robustness across all relations and their frequencies.",RSCF Relation Semantics Consistent Filter Entity Embedding Knowledge Graph knowledge graph embedding leveraging relation specific entity transformation markedly enhanced performance consistency embedding differences transformation remains unaddressed risking loss valuable inductive bias inherent embeddings inconsistency stems problems transformation representations specified relations disconnected manner allowing dissimilar transformations corresponding entity embeddings similar relations Second generalized plug approach SFBR Semantic Filter Based Relations disrupts consistency excessive concentration entity embeddings entity based regularization generating indistinguishable score distributions relations paper introduce plug KGE method Relation Semantics Consistent Filter RSCF entity transformation features enhancing semantic consistency 1 shared affine transformation relation embeddings relations 2 rooted entity transformation adds entity embedding change represented transformed vector 3 normalization change prevent scale reduction amplify advantages consistency preserve semantics embeddings RSCF adds relation transformation prediction modules enhancing semantics knowledge graph completion tasks distance based tensor decomposition models RSCF significantly outperforms state art KGE methods showing robustness relations frequencies
637,RolePlot: A Systematic Framework for Evaluating and Enhancing the Plot-Progression Capabilities of Role-Playing Agents,"['Pinyi Zhang', 'Siyu An', 'Lingfeng Qiao', 'Yifei Yu', 'Jingyang Chen', 'Jie Wang', 'di yin', 'Xing Sun', 'Kai Zhang']",,RolePlot Systematic Framework Evaluating Enhancing Plot Progression Capabilities Role Playing Agents
638,TreeRL: LLM Reinforcement Learning with On-Policy Tree Search,"['Zhenyu Hou', 'Ziniu Hu', 'Yujiang Li', 'Rui Lu', 'Jie Tang', 'Yuxiao Dong']","Reinforcement learning (RL) with tree search has demonstrated superior performance in traditional reasoning tasks. Compared to conventional independent chain sampling strategies with outcome supervision, tree search enables better exploration of the reasoning space and provides dense, on-policy process rewards during RL training but remains under-explored in On-Policy LLM RL. We propose TreeRL, a reinforcement learning framework that directly incorporates on-policy tree search for RL training. Our approach includes intermediate supervision and eliminates the need for a separate reward model training. Existing approaches typically train a separate process reward model, which can suffer from distribution mismatch and reward hacking. We also introduce a cost-effective tree search approach that achieves higher search efficiency under the same generation token budget by strategically branching from high-uncertainty intermediate steps rather than using random branching. Experiments on challenging math and code reasoning benchmarks demonstrate that TreeRL achieves superior performance compared to traditional ChainRL, highlighting the potential of tree search for LLM. TreeRL is open-sourced at https://github.com/THUDM/TreeRL.",TreeRL LLM Reinforcement Learning Policy Tree Search Reinforcement learning RL tree search demonstrated superior performance traditional reasoning tasks Compared conventional independent chain sampling strategies outcome supervision tree search enables better exploration reasoning space provides dense policy process rewards RL training remains explored Policy LLM RL propose TreeRL reinforcement learning framework directly incorporates policy tree search RL training approach includes intermediate supervision eliminates need separate reward model training Existing approaches typically train separate process reward model suffer distribution mismatch reward hacking introduce cost effective tree search approach achieves higher search efficiency generation token budget strategically branching high uncertainty intermediate steps using random branching Experiments challenging math code reasoning benchmarks demonstrate TreeRL achieves superior performance compared traditional ChainRL highlighting potential tree search LLM TreeRL open sourced https github com THUDM TreeRL
639,Can a Single Model Master Both Multi-turn Conversations and Tool Use? CALM: A Unified Conversational Agentic Language Model,"['Emre Can Acikgoz', 'Jeremiah Greer', 'Akul Datta', 'Ze Yang', 'William Zeng', 'Oussama Elachqar', 'Emmanouil Koukoumidis', 'Dilek Hakkani-Tür', 'Gokhan Tur']",,Single Model Master Multi turn Conversations Tool Use CALM Unified Conversational Agentic Language Model
640,Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation,"['Yupu Liang', 'Yaping Zhang', 'Zhiyang Zhang', 'Yang Zhao', 'Lu Xiang', 'Chengqing Zong', 'Yu Zhou']","Document Image Machine Translation (DIMT) aims to translate text within document images, facing generalization challenges due to limited training data and the complex interplay between visual and textual information. To address these challenges, we introduce M4Doc, a novel single-to-mix modality alignment framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an image-only encoder with the multimodal representations of an MLLM, pre-trained on large-scale document image datasets. This alignment enables a lightweight DIMT model to learn crucial visual-textual correlations during training. During inference, M4Doc bypasses the MLLM, maintaining computational efficiency while benefiting from its multimodal knowledge. Comprehensive experiments demonstrate substantial improvements in translation quality, especially in cross-domain generalization and challenging document image scenarios.",Single mix Modality Alignment Multimodal Large Language Model Document Image Machine Translation Document Image Machine Translation DIMT aims translate text document images facing generalization challenges limited training data complex interplay visual textual information address challenges introduce M4Doc novel single mix modality alignment framework leveraging Multimodal Large Language Models MLLMs M4Doc aligns image encoder multimodal representations MLLM pre trained large scale document image datasets alignment enables lightweight DIMT model learn crucial visual textual correlations training inference M4Doc bypasses MLLM maintaining computational efficiency benefiting multimodal knowledge Comprehensive experiments demonstrate substantial improvements translation quality especially cross domain generalization challenging document image scenarios
641,SDPO: Segment-Level Direct Preference Optimization for Social Agents,"['Aobo Kong', 'Wentao Ma', 'Shiwan Zhao', 'Yongbin Li', 'Yuchuan Wu', 'Ke Wang', 'Xiaoqian Liu', 'Qicheng Li', 'Yong Qin', 'Fei Huang']","Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across various agent tasks. However, standard DPO focuses solely on individual turns, which limits its effectiveness in multi-turn social interactions. Several DPO-based multi-turn alignment methods with session-level data have shown potential in addressing this problem.While these methods consider multiple turns across entire sessions, they are often overly coarse-grained, introducing training noise, and lack robust theoretical support. To resolve these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which dynamically select key segments within interactions to optimize multi-turn agent behavior. SDPO minimizes training noise and is grounded in a rigorous theoretical framework. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPO's potential to advance the social intelligence of LLM-based agents. We release our code and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.",SDPO Segment Level Direct Preference Optimization Social Agents Social agents powered large language models LLMs simulate human social behaviors fall short handling complex social dialogues Direct Preference Optimization DPO proven effective aligning LLM behavior human preferences various agent tasks standard DPO focuses solely individual turns limits effectiveness multi turn social interactions DPO based multi turn alignment methods session level data shown potential addressing problem methods consider multiple turns entire sessions overly coarse grained introducing training noise lack robust theoretical support resolve limitations propose Segment Level Direct Preference Optimization SDPO dynamically select key segments interactions optimize multi turn agent behavior SDPO minimizes training noise grounded rigorous theoretical framework Evaluations SOTOPIA benchmark demonstrate SDPO tuned agents consistently outperform existing DPO based methods proprietary LLMs like GPT 4o underscoring SDPO s potential advance social intelligence LLM based agents release code data https github com AlibabaResearch DAMO ConvAI tree main SDPO
642,KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors,"['Zhiyang Qi', 'Takumasa Kaneko', 'Keiko Takamizo', 'Mariko Ukiyo', 'Michimasa Inaba']","Generating psychological counseling responses with language models relies heavily on high-quality datasets. Crowdsourced data collection methods require strict worker training, and data from real-world counseling environments may raise privacy and ethical concerns. While recent studies have explored using large language models (LLMs) to augment psychological counseling dialogue datasets, the resulting data often suffers from limited diversity and authenticity. To address these limitations, this study adopts a role-playing approach where trained counselors simulate counselor-client interactions, ensuring high-quality dialogues while mitigating privacy risks. Using this method, we construct KokoroChat, a Japanese psychological counseling dialogue dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive client feedback. Experimental results demonstrate that fine-tuning open-source LLMs with KokoroChat improves both the quality of generated counseling responses and the automatic evaluation of counseling dialogues. The KokoroChat dataset is available at https://github.com/UEC-InabaLab/KokoroChat.",KokoroChat Japanese Psychological Counseling Dialogue Dataset Collected Role Playing Trained Counselors Generating psychological counseling responses language models relies heavily high quality datasets Crowdsourced data collection methods require strict worker training data real world counseling environments raise privacy ethical concerns recent studies explored using large language models LLMs augment psychological counseling dialogue datasets resulting data suffers limited diversity authenticity address limitations study adopts role playing approach trained counselors simulate counselor client interactions ensuring high quality dialogues mitigating privacy risks Using method construct KokoroChat Japanese psychological counseling dialogue dataset comprising 6 589 long form dialogues accompanied comprehensive client feedback Experimental results demonstrate fine tuning open source LLMs KokoroChat improves quality generated counseling responses automatic evaluation counseling dialogues KokoroChat dataset available https github com UEC InabaLab KokoroChat
643,"SURVEYFORGE : On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing","['Xiangchao Yan', 'Shiyang Feng', 'Jiakang Yuan', 'Renqiu Xia', 'Bin Wang', 'LEI BAI', 'Bo Zhang']",,SURVEYFORGE Outline Heuristics Memory Driven Generation Multi dimensional Evaluation Automated Survey Writing
644,Making LLMs Better Many-to-Many Speech-to-Text Translators with Curriculum Learning,"['Yexing Du', 'Youcheng Pan', 'Ziyang Ma', 'Bo Yang', 'Yifan Yang', 'Keqi Deng', 'Xie Chen', 'Yang Xiang', 'Ming Liu', 'Bing Qin']","Multimodal Large Language Models (MLLMs) have achieved significant success in Speech-to-Text Translation (S2TT) tasks. While most existing research has focused on English-centric translation directions, the exploration of many-to-many translation is still limited by the scarcity of parallel data. To address this, we propose a three-stage curriculum learning strategy that leverages the machine translation capabilities of large language models and adapts them to S2TT tasks, enabling effective learning in low-resource settings. We trained MLLMs with varying parameter sizes (3B, 7B, and 32B) and evaluated the proposed strategy using the FLEURS and CoVoST-2 datasets. Experimental results show that the proposed strategy achieves state-of-the-art average performance in $15\times14$ language pairs, requiring fewer than 10 hours of speech data per language to achieve competitive results. The source code and models are released at https://github.com/yxduir/LLM-SRT.",Making LLMs Better Speech Text Translators Curriculum Learning Multimodal Large Language Models MLLMs achieved significant success Speech Text Translation S2TT tasks existing research focused English centric translation directions exploration translation limited scarcity parallel data address propose stage curriculum learning strategy leverages machine translation capabilities large language models adapts S2TT tasks enabling effective learning low resource settings trained MLLMs varying parameter sizes 3B 7B 32B evaluated proposed strategy using FLEURS CoVoST 2 datasets Experimental results proposed strategy achieves state art average performance 15 times14 language pairs requiring fewer 10 hours speech data language achieve competitive results source code models released https github com yxduir LLM SRT
645,AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research,"['Yilun Zhao', 'Weiyuan Chen', 'Zhijian Xu', 'Yixin Liu', 'Chengye Wang', 'Manasi Patwardhan', 'Lovekesh Vig', 'Arman Cohan']",,AbGen Evaluating Large Language Models Ablation Study Design Evaluation Scientific Research
646,Redundancy Principles for MLLMs Benchmarks,"['Zicheng Zhang', 'Xiangyu Zhao', 'Xinyu Fang', 'Chunyi Li', 'Xiaohong Liu', 'Xiongkuo Min', 'Haodong Duan', 'Kai Chen', 'Guangtao Zhai']","With the rapid iteration of Multi-modality Large Language Models (MLLMs) and the evolving demands of the field, the number of benchmarks produced annually has surged into the hundreds. The rapid growth has inevitably led to significant redundancy among benchmarks. Therefore, it is crucial to take a step back and critically assess the current state of redundancy and propose targeted principles for constructing effective MLLM benchmarks. In this paper, we focus on redundancy from three key perspectives: 1) Redundancy of benchmark capability dimensions, 2) Redundancy in the number of test questions, and 3) Cross-benchmark redundancy within specific domains. Through the comprehensive analysis over hundreds of MLLMs' performance across more than 20 benchmarks, we aim to quantitatively measure the level of redundancy lies in existing MLLM evaluations, provide valuable insights to guide the future development of MLLM benchmarks, and offer strategies to refine and address redundancy issues effectively. The code is available at https://github.com/zzc-1998/Benchmark-Redundancy.",Redundancy Principles MLLMs Benchmarks rapid iteration Multi modality Large Language Models MLLMs evolving demands field number benchmarks produced annually surged hundreds rapid growth inevitably led significant redundancy benchmarks crucial step critically assess current state redundancy propose targeted principles constructing effective MLLM benchmarks paper focus redundancy key perspectives 1 Redundancy benchmark capability dimensions 2 Redundancy number test questions 3 Cross benchmark redundancy specific domains comprehensive analysis hundreds MLLMs performance 20 benchmarks aim quantitatively measure level redundancy lies existing MLLM evaluations provide valuable insights guide future development MLLM benchmarks offer strategies refine address redundancy issues effectively code available https github com zzc 1998 Benchmark Redundancy
647,WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models,"['Yifu Chen', 'Shengpeng Ji', 'Haoxiao Wang', 'Ziqing Wang', 'Siyu Chen', 'Jinzheng He', 'Jin Xu', 'Zhou Zhao']","Retrieval Augmented Generation (RAG) has gained widespread adoption owing to its capacity to empower large language models (LLMs) to integrate external knowledge. However, existing RAG frameworks are primarily designed for text-based LLMs and rely on Automatic Speech Recognition to process speech input, which discards crucial audio information, risks transcription errors, and increases computational overhead. Therefore, we introduce WavRAG, the first retrieval augmented generation framework with native, end-to-end audio support. WavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw audio for both embedding and retrieval. 2) WavRAG integrates audio and text into a unified knowledge representation. Specifically, we propose the WavRetriever to facilitate the retrieval from a text-audio hybrid knowledge base, and further enhance the in-context capabilities of spoken dialogue models through the integration of chain-of-thought reasoning. In comparison to state-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval performance while delivering a 10x acceleration. Furthermore, WavRAG's unique text-audio hybrid retrieval capability extends the boundaries of RAG to the audio modality.",WavRAG Audio Integrated Retrieval Augmented Generation Spoken Dialogue Models Retrieval Augmented Generation RAG gained widespread adoption owing capacity empower large language models LLMs integrate external knowledge existing RAG frameworks primarily designed text based LLMs rely Automatic Speech Recognition process speech input discards crucial audio information risks transcription errors increases computational overhead introduce WavRAG retrieval augmented generation framework native end end audio support WavRAG offers key features 1 Bypassing ASR WavRAG directly processes raw audio embedding retrieval 2 WavRAG integrates audio text unified knowledge representation Specifically propose WavRetriever facilitate retrieval text audio hybrid knowledge base enhance context capabilities spoken dialogue models integration chain thought reasoning comparison state art ASR Text RAG pipelines WavRAG achieves comparable retrieval performance delivering 10x acceleration Furthermore WavRAG s unique text audio hybrid retrieval capability extends boundaries RAG audio modality
648,ChildMandarin: A Comprehensive Mandarin Speech Dataset for Young Children Aged 3-5,"['Jiaming Zhou', 'shiyao wang', 'Shiwan Zhao', 'Jiabei He', 'Haoqin Sun', 'Hui Wang', 'Cheng Liu', 'Aobo Kong', 'Yujie Guo', 'Xi Yang', 'Yequan Wang', 'Yonghua Lin', 'Yong Qin']","Automatic speech recognition (ASR) systems have advanced significantly with models like Whisper, Conformer, and self-supervised frameworks such as Wav2vec 2.0 and HuBERT. However, developing robust ASR models for young children's speech remains challenging due to differences in pronunciation, tone, and pace compared to adult speech. In this paper, we introduce a new Mandarin speech dataset focused on children aged 3 to 5, addressing the scarcity of resources in this area. The dataset comprises 41.25 hours of speech with carefully crafted manual transcriptions, collected from 397 speakers across various provinces in China, with balanced gender representation. We provide a comprehensive analysis of speaker demographics, speech duration distribution and geographic coverage. Additionally, we evaluate ASR performance on models trained from scratch, such as Conformer, as well as fine-tuned pre-trained models like HuBERT and Whisper, where fine-tuning demonstrates significant performance improvements. Furthermore, we assess speaker verification (SV) on our dataset, showing that, despite the challenges posed by the unique vocal characteristics of young children, the dataset effectively supports both ASR and SV tasks. This dataset is a valuable contribution to Mandarin child speech research. The dataset is now open-source and freely available for all academic purposes on https://github.com/flageval-baai/ChildMandarin.",ChildMandarin Comprehensive Mandarin Speech Dataset Young Children Aged 3 5 Automatic speech recognition ASR systems advanced significantly models like Whisper Conformer self supervised frameworks Wav2vec 2 0 HuBERT developing robust ASR models young children s speech remains challenging differences pronunciation tone pace compared adult speech paper introduce new Mandarin speech dataset focused children aged 3 5 addressing scarcity resources area dataset comprises 41 25 hours speech carefully crafted manual transcriptions collected 397 speakers various provinces China balanced gender representation provide comprehensive analysis speaker demographics speech duration distribution geographic coverage Additionally evaluate ASR performance models trained scratch Conformer fine tuned pre trained models like HuBERT Whisper fine tuning demonstrates significant performance improvements Furthermore assess speaker verification SV dataset showing despite challenges posed unique vocal characteristics young children dataset effectively supports ASR SV tasks dataset valuable contribution Mandarin child speech research dataset open source freely available academic purposes https github com flageval baai ChildMandarin
649,Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization,"['Yao Xiao', 'Hai Ye', 'Linyao Chen', 'Hwee Tou Ng', 'Lidong Bing', 'Xiaoli Li', 'Roy Ka-Wei Lee']","Iterative data generation and model retraining are widely used to align large language models (LLMs). It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses. In this work, we aim to \emph{scale up} the number of on-policy samples via repeated random sampling to improve alignment performance. Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO. However, our experiments reveal that this strategy leads to a \emph{decline} in performance as the sample size increases. To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards. We categorize the reward space into seven representative points and systematically explore all 21 ($C_7^2$) pairwise combinations. Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position $\mu - 2\sigma$ rather than the minimum reward, is crucial for optimal performance. We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases.",Finding Sweet Spot Preference Data Construction Scaling Preference Optimization Iterative data generation model retraining widely used align large language models LLMs typically involves policy model generate policy responses reward model guide training data selection Direct Preference Optimization DPO enhances process constructing preference pairs chosen rejected responses work aim emph scale number policy samples repeated random sampling improve alignment performance Conventional practice selects sample highest reward chosen lowest rejected DPO experiments reveal strategy leads emph decline performance sample size increases address investigate preference data construction lens underlying normal distribution sample rewards categorize reward space seven representative points systematically explore 21 C_7 2 pairwise combinations evaluations models using AlpacaEval 2 selecting rejected response reward position mu 2 sigma minimum reward crucial optimal performance finally introduce scalable preference data construction strategy consistently enhances model performance sample scale increases
650,Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization,"['Yuhao Wang', 'Keyan Ding', 'Kehua Feng', 'Zeyuan Wang', 'Ming Qin', 'Xiaotong Li', 'Qiang Zhang', 'Huajun Chen']",,Enhancing Safe Controllable Protein Generation Knowledge Preference Optimization
651,SINCon: Mitigate LLM-Generated Malicious Message Injection Attack for Rumor Detection,"['Mingqing Zhang', 'Qiang Liu', 'Xiang Tao', 'Shu Wu', 'Liang Wang']","In the era of rapidly evolving large language models (LLMs), state-of-the-art rumor detection systems, particularly those based on Message Propagation Trees (MPTs), which represent a conversation tree with the post as its root and the replies as its descendants, are facing increasing threats from adversarial attacks that leverage LLMs to generate and inject malicious messages. Existing methods are based on the assumption that different nodes exhibit varying degrees of influence on predictions. They define nodes with high predictive influence as important nodes and target them for attacks. If the model treats nodes' predictive influence more uniformly, attackers will find it harder to target high predictive influence nodes. In this paper, we propose Similarizing the predictive Influence of Nodes with Contrastive Learning (SINCon), a defense mechanism that encourages the model to learn graph representations where nodes with varying importance have a more uniform influence on predictions. Extensive experiments on the Twitter and Weibo datasets demonstrate that SINCon not only preserves high classification accuracy on clean data but also significantly enhances resistance against LLM-driven message injection attacks.",SINCon Mitigate LLM Generated Malicious Message Injection Attack Rumor Detection era rapidly evolving large language models LLMs state art rumor detection systems particularly based Message Propagation Trees MPTs represent conversation tree post root replies descendants facing increasing threats adversarial attacks leverage LLMs generate inject malicious messages Existing methods based assumption different nodes exhibit varying degrees influence predictions define nodes high predictive influence important nodes target attacks model treats nodes predictive influence uniformly attackers harder target high predictive influence nodes paper propose Similarizing predictive Influence Nodes Contrastive Learning SINCon defense mechanism encourages model learn graph representations nodes varying importance uniform influence predictions Extensive experiments Twitter Weibo datasets demonstrate SINCon preserves high classification accuracy clean data significantly enhances resistance LLM driven message injection attacks
652,Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models,"['Jungwoo Park', 'Chanwoong Yoon', 'Hyeon Hwang', 'Taewhoo Lee', 'Jaewoo Kang']","Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",Outlier Safe Pre Training Robust 4 Bit Quantization Large Language Models Extreme activation outliers Large Language Models LLMs critically degrade quantization performance hindering efficient device deployment channel wise operations adaptive gradient scaling recognized causes practical mitigation remains challenging introduce Outlier Safe Pre Training OSP practical guideline proactively prevents outlier formation relying post hoc mitigation OSP combines key innovations 1 Muon optimizer eliminating privileged bases maintaining training efficiency 2 Single Scale RMSNorm preventing channel wise amplification 3 learnable embedding projection redistributing activation magnitudes originating embedding matrices validate OSP training 1 4B parameter model 1 trillion tokens production scale LLM trained outliers aggressive 4 bit quantization OSP model achieves 35 7 average score 10 benchmarks compared 26 5 Adam trained model 2 training overhead Remarkably OSP models exhibit near zero excess kurtosis 0 04 compared extreme values 1818 56 standard models fundamentally altering LLM quantization behavior work demonstrates outliers inherent LLMs consequences training strategies paving way efficient LLM deployment source code pretrained checkpoints available https github com dmis lab Outlier Safe Pre Training
653,Agentic Knowledgeable Self-awareness,"['Shuofei Qiao', 'Zhisong Qiu', 'Baochang Ren', 'Xiaobin Wang', 'Xiangyuan Ru', 'Ningyu Zhang', 'Xiang Chen', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen']","Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a ""flood irrigation"" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.",Agentic Knowledgeable Self awareness Large Language Models LLMs achieved considerable performance various agentic planning tasks traditional agent planning approaches adopt flood irrigation methodology indiscriminately injects gold trajectories external feedback domain knowledge agent models practice overlooks fundamental human cognitive principle situational self awareness decision making ability dynamically assess situational demands strategically employ resources decision making propose agentic knowledgeable self awareness address gap novel paradigm enabling LLM based agents autonomously regulate knowledge utilization Specifically propose KnowSelf data centric approach applies agents knowledgeable self awareness like humans Concretely devise heuristic situation judgement criterion mark special tokens agent s self explored trajectories collecting training data stage training process agent model switch different situations generating specific special tokens achieving optimal planning effects minimal costs experiments demonstrate KnowSelf outperform various strong baselines different tasks models minimal use external knowledge Code available https github com zjunlp KnowSelf
654,A Unified Agentic Framework for Evaluating Conditional Image Generation,"['Jifang Wang', 'Yangxue', 'Longyue Wang', 'Zhenran Xu', 'Yiyu Wang', 'Yaowei Wang', 'Weihua Luo', 'Kaifu Zhang', 'Baotian Hu', 'Min zhang']","Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.",Unified Agentic Framework Evaluating Conditional Image Generation Conditional image generation gained significant attention ability personalize content field faces challenges developing task agnostic reliable explainable evaluation metrics paper introduces CIGEval unified agentic framework comprehensive evaluation conditional image generation tasks CIGEval utilizes large multimodal models LMMs core integrating multi functional toolbox establishing fine grained evaluation framework Additionally synthesize evaluation trajectories fine tuning empowering smaller LMMs autonomously select appropriate tools conduct nuanced analyses based tool outputs Experiments seven prominent conditional image generation tasks demonstrate CIGEval GPT 4o version achieves high correlation 0 4625 human assessments closely matching inter annotator correlation 0 47 implemented 7B open source LMMs using 2 3K training trajectories CIGEval surpasses previous GPT 4o based state art method Case studies GPT 4o image generation highlight CIGEval s capability identifying subtle issues related subject consistency adherence control guidance indicating great potential automating evaluation image generation tasks human level reliability
655,Planning-Driven Programming: A Large Language Model Programming Workflow,"['Chao Lei', 'Yanchuan Chang', 'Nir Lipovetzky', 'Krista A. Ehinger']","The strong performance of large language models (LLMs) raises extensive discussion on their application to code generation. Recent research suggests continuous program refinements through visible tests to improve code generation accuracy in LLMs. However, these methods suffer from LLMs' inefficiency and limited reasoning capacity. In this work, we propose an LLM programming workflow (LPW) designed to improve both initial code generation and subsequent refinements within a structured two-phase workflow. Specifically, the solution generation phase formulates a solution plan, which is then verified through visible tests to specify the intended natural language solution. Subsequently, the code implementation phase drafts an initial code according to the solution plan and its verification. If the generated code fails the visible tests, the plan verification serves as the intended solution to consistently inform the refinement process for correcting bugs. Compared to state-of-the-art methods across various existing LLMs, LPW significantly improves the Pass@1 accuracy by up to 16.4% on well-established text-to-code generation benchmarks. LPW also sets new state-of-the-art Pass@1 accuracy, achieving 98.2% on HumanEval, 84.8% on MBPP, 59.3% on LiveCode, 62.6% on APPS, and 34.7% on CodeContest, using GPT-4o as the backbone. Our code is publicly available at: https://github.com/you68681/lpw",Planning Driven Programming Large Language Model Programming Workflow strong performance large language models LLMs raises extensive discussion application code generation Recent research suggests continuous program refinements visible tests improve code generation accuracy LLMs methods suffer LLMs inefficiency limited reasoning capacity work propose LLM programming workflow LPW designed improve initial code generation subsequent refinements structured phase workflow Specifically solution generation phase formulates solution plan verified visible tests specify intended natural language solution Subsequently code implementation phase drafts initial code according solution plan verification generated code fails visible tests plan verification serves intended solution consistently inform refinement process correcting bugs Compared state art methods various existing LLMs LPW significantly improves Pass 1 accuracy 16 4 established text code generation benchmarks LPW sets new state art Pass 1 accuracy achieving 98 2 HumanEval 84 8 MBPP 59 3 LiveCode 62 6 APPS 34 7 CodeContest using GPT 4o backbone code publicly available https github com you68681 lpw
656,Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering,"['Yuan Sui', 'Yufei He', 'Zifeng Ding', 'Bryan Hooi']","Recent works integrating Knowledge Graphs (KGs) have shown promising improvements in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing benchmarks primarily focus on closed-ended tasks, leaving a gap in evaluating performance on more complex, real-world scenarios. This limitation also hinders a thorough assessment of KGs' potential to reduce hallucinations in LLMs. To address this, we introduce OKGQA, a new benchmark specifically designed to evaluate LLMs augmented with KGs in open-ended, real-world question answering settings. OKGQA reflects practical complexities through diverse question types and incorporates metrics to quantify both hallucination rates and reasoning improvements in LLM+KG models. To consider the scenarios in which KGs may contain varying levels of errors, we propose a benchmark variant, OKGQA-P, to assess model performance when the semantics and structure of KGs are deliberately perturbed and contaminated. In this paper, we aims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended setting, and (2) conduct a comparative analysis to shed light on method design. We believe this study can facilitate a more complete performance comparison and encourages continuous improvement in integrating KGs with LLMs to mitigate hallucination, and make LLMs more trustworthy. Code and data are released at https://github.com/Y-Sui/OKGQA.",Knowledge Graphs Make Large Language Models Trustworthy Empirical Study Open ended Question Answering Recent works integrating Knowledge Graphs KGs shown promising improvements enhancing reasoning capabilities Large Language Models LLMs existing benchmarks primarily focus closed ended tasks leaving gap evaluating performance complex real world scenarios limitation hinders thorough assessment KGs potential reduce hallucinations LLMs address introduce OKGQA new benchmark specifically designed evaluate LLMs augmented KGs open ended real world question answering settings OKGQA reflects practical complexities diverse question types incorporates metrics quantify hallucination rates reasoning improvements LLM KG models consider scenarios KGs contain varying levels errors propose benchmark variant OKGQA P assess model performance semantics structure KGs deliberately perturbed contaminated paper aims 1 explore KGs make LLMs trustworthy open ended setting 2 conduct comparative analysis shed light method design believe study facilitate complete performance comparison encourages continuous improvement integrating KGs LLMs mitigate hallucination make LLMs trustworthy Code data released https github com Y Sui OKGQA
657,Nudging: Inference-time Alignment of LLMs via Guided Decoding,"['Yu Fei', 'Yasaman Razeghi', 'Sameer Singh']","Large language models (LLMs) require alignment to effectively and safely follow user instructions. This process necessitates training an aligned version for every base model, resulting in significant computational overhead. In this work, we propose NUDGING, a simple, training-free algorithm that aligns any base model at inference time using a small aligned model. NUDGING is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens (e.g., discourse markers). We find that base models are significantly more uncertain when generating these tokens. Building on this insight, NUDGING employs a small aligned model to generate nudging tokens to guide the base model's output during decoding when the base model's uncertainty is high, with only a minor additional inference overhead. We evaluate NUDGING across 3 model families on a diverse range of open-instruction tasks. Without any training, nudging a large base model with a 7x-14x smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. By operating at the token level, NUDGING enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-27b-chat outperforms Llama-2-70b-chat on various tasks. Overall, our work offers a modular and cost-efficient solution to LLM alignment. Our code and demo are available at: https://fywalter.github.io/nudging/ .",Nudging Inference time Alignment LLMs Guided Decoding Large language models LLMs require alignment effectively safely follow user instructions process necessitates training aligned version base model resulting significant computational overhead work propose NUDGING simple training free algorithm aligns base model inference time using small aligned model NUDGING motivated recent findings alignment primarily alters model s behavior small subset stylistic tokens e g discourse markers base models significantly uncertain generating tokens Building insight NUDGING employs small aligned model generate nudging tokens guide base model s output decoding base model s uncertainty high minor additional inference overhead evaluate NUDGING 3 model families diverse range open instruction tasks training nudging large base model 7x 14x smaller aligned model achieves zero shot performance comparable surpassing large aligned models operating token level NUDGING enables shelf collaboration model families instance nudging Gemma 2 27b Llama 27b chat outperforms Llama 2 70b chat various tasks Overall work offers modular cost efficient solution LLM alignment code demo available https fywalter github io nudging
658,Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing,"['Zhilin Wang', 'Yafu Li', 'Jianhao Yan', 'Yu Cheng', 'Yue Zhang']","Dynamical systems theory provides a framework for analyzing iterative processes and evolution over time. Within such systems, repetitive transformations can lead to stable configurations, known as attractors, including fixed points and limit cycles. Applying this perspective to large language models (LLMs), which iteratively map input text to output text, provides a principled approach to characterizing long-term behaviors. Successive paraphrasing serves as a compelling testbed for exploring such dynamics, as paraphrases re-express the same underlying meaning with linguistic variation. Although LLMs are expected to explore a diverse set of paraphrases in the text space, our study reveals that successive paraphrasing converges to stable periodic states, such as 2-period attractor cycles, limiting linguistic diversity. This phenomenon is attributed to the self-reinforcing nature of LLMs, as they iteratively favour and amplify certain textual forms over others. This pattern persists with increasing generation randomness or alternating prompts and LLMs. These findings underscore inherent constraints in LLM generative capability, while offering a novel dynamical systems perspective for studying their expressive potential.",Unveiling Attractor Cycles Large Language Models Dynamical Systems View Successive Paraphrasing Dynamical systems theory provides framework analyzing iterative processes evolution time systems repetitive transformations lead stable configurations known attractors including fixed points limit cycles Applying perspective large language models LLMs iteratively map input text output text provides principled approach characterizing long term behaviors Successive paraphrasing serves compelling testbed exploring dynamics paraphrases express underlying meaning linguistic variation LLMs expected explore diverse set paraphrases text space study reveals successive paraphrasing converges stable periodic states 2 period attractor cycles limiting linguistic diversity phenomenon attributed self reinforcing nature LLMs iteratively favour amplify certain textual forms pattern persists increasing generation randomness alternating prompts LLMs findings underscore inherent constraints LLM generative capability offering novel dynamical systems perspective studying expressive potential
659,State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for State Space Models,"['Wonjun Kang', 'Kevin Galim', 'Yuchen Zeng', 'Minjae Lee', 'Hyung Il Koo', 'Nam Ik Cho']","State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.",State offset Tuning State based Parameter Efficient Fine Tuning State Space Models State Space Models SSMs emerged efficient alternatives Transformers mitigating quadratic computational cost application Parameter Efficient Fine Tuning PEFT methods SSMs remains largely unexplored particular prompt based methods like Prompt Tuning Prefix Tuning widely used Transformers perform SSMs address propose state based methods superior alternative prompt based methods new family methods naturally stems architectural characteristics SSMs State based methods adjust state related features directly instead depending external prompts Furthermore introduce novel state based PEFT method State offset Tuning timestep method directly affects state current step leading effective adaptation extensive experiments diverse datasets demonstrate effectiveness method Code available https github com furiosa ai ssm state tuning
660,SCAR: Data Selection via Style Consistency-Aware Response Ranking for Efficient Instruction-Tuning of Large Language Models,"['Zhuang Li', 'YUNCHENG HUA', 'Thuy-Trang Vu', 'Haolan Zhan', 'Lizhen Qu', 'Gholamreza Haffari']","Recent studies emphasize that manually ensuring a consistent response style and maintaining high data quality in training sets can significantly improve the performance of fine-tuned Large Language Models (LLMs) while reducing the number of training examples needed. However, the precise definition of style and the relationship between style, data quality, and LLM performance remains unclear. This research identifies two key stylistic elements in responses: linguistic form and instructional surprisal. We find that, among training data of comparable quality, higher consistency in these response elements leads to better LLM performance. Inspired by this, we introduce Style Consistency-Aware Response Ranking (SCAR), which automatically prioritizes instruction-response pairs in the training set based on their response stylistic consistency. By selecting the most style-consistent examples, using only 0.7% of the full dataset in the best case, the fine-tuned LLMs can match or even surpass the performance of models trained on the entire dataset in coding and open-ended question-answering benchmarks. Code and data are available at https://github.com/zhuang-li/SCAR .",SCAR Data Selection Style Consistency Aware Response Ranking Efficient Instruction Tuning Large Language Models Recent studies emphasize manually ensuring consistent response style maintaining high data quality training sets significantly improve performance fine tuned Large Language Models LLMs reducing number training examples needed precise definition style relationship style data quality LLM performance remains unclear research identifies key stylistic elements responses linguistic form instructional surprisal training data comparable quality higher consistency response elements leads better LLM performance Inspired introduce Style Consistency Aware Response Ranking SCAR automatically prioritizes instruction response pairs training set based response stylistic consistency selecting style consistent examples using 0 7 dataset best case fine tuned LLMs match surpass performance models trained entire dataset coding open ended question answering benchmarks Code data available https github com zhuang li SCAR
661,Internal and External Impacts of Natural Language Processing Papers,['Yu Zhang'],"We investigate the impacts of NLP research published in top-tier conferences (i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from research articles and external sources such as patents, media, and policy documents, we examine how different NLP topics are consumed both within the academic community and by the broader public. Our findings reveal that language modeling has the widest internal and external influence, while linguistic foundations have lower impacts. We also observe that internal and external impacts generally align, but topics like ethics, bias, and fairness show significant attention in policy documents with much fewer academic citations. Additionally, external domains exhibit distinct preferences, with patents focusing on practical NLP applications and media and policy documents engaging more with the societal implications of NLP models.",Internal External Impacts Natural Language Processing Papers investigate impacts NLP research published tier conferences e ACL EMNLP NAACL 1979 2024 analyzing citations research articles external sources patents media policy documents examine different NLP topics consumed academic community broader public findings reveal language modeling widest internal external influence linguistic foundations lower impacts observe internal external impacts generally align topics like ethics bias fairness significant attention policy documents fewer academic citations Additionally external domains exhibit distinct preferences patents focusing practical NLP applications media policy documents engaging societal implications NLP models
662,HFT: Half Fine-Tuning for Large Language Models,"['Tingfeng Hui', 'Zhenyu Zhang', 'Shuohuan Wang', 'Weiran Xu', 'Yu Sun', 'Hua Wu']","Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences. However, it carries the risk of catastrophic forgetting during sequential training, the parametric knowledge or the ability learned in previous stages may be overwhelmed by incoming training data. In this paper, we find that by regularly resetting partial parameters, LLMs can restore some of the original knowledge. Inspired by this, we introduce Half Fine-Tuning (HFT) for LLMs, as a substitute for full fine-tuning (FFT), to mitigate the forgetting issues, where half of the parameters are selected to learn new tasks while the other half are frozen to remain previous knowledge. We provide a feasibility analysis from the perspective of optimization and interpret the parameter selection operation as a regularization term. Without changing the model architecture, HFT could be seamlessly integrated into existing fine-tuning frameworks. Extensive experiments and analysis on supervised fine-tuning, direct preference optimization, and continual learning consistently demonstrate the effectiveness, robustness, and efficiency of HFT. Compared with FFT, HFT not only significantly alleviates the forgetting problem, but also achieves the best performance in a series of downstream benchmarks, with an approximately 30% reduction in training time.",HFT Half Fine Tuning Large Language Models Large language models LLMs fine tuning phases necessary step unlock various capabilities enabling LLMs follow natural language instructions align human preferences carries risk catastrophic forgetting sequential training parametric knowledge ability learned previous stages overwhelmed incoming training data paper regularly resetting partial parameters LLMs restore original knowledge Inspired introduce Half Fine Tuning HFT LLMs substitute fine tuning FFT mitigate forgetting issues half parameters selected learn new tasks half frozen remain previous knowledge provide feasibility analysis perspective optimization interpret parameter selection operation regularization term changing model architecture HFT seamlessly integrated existing fine tuning frameworks Extensive experiments analysis supervised fine tuning direct preference optimization continual learning consistently demonstrate effectiveness robustness efficiency HFT Compared FFT HFT significantly alleviates forgetting problem achieves best performance series downstream benchmarks approximately 30 reduction training time
663,Beyond Surface Simplicity: Revealing Hidden Reasoning Attributes for Precise Commonsense Diagnosis,"['Huijun Lian', 'Zekai Sun', 'Keqi Chen', 'Yingming Gao', 'Ya Li']",,Surface Simplicity Revealing Hidden Reasoning Attributes Precise Commonsense Diagnosis
664,From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation,"['Cheng Cheng', 'Zhenya Huang', 'GuanHao Zhao', 'Yuxiang Guo', 'Xin Lin', 'Jinze Wu', 'Xin Li', 'Shijin Wang']","Automatically generating high-quality mathematical problems that align with educational objectives is a crucial task in NLP-based educational technology. Traditional generation methods focus primarily on textual quality, but they often overlook educational objectives. Moreover, these methods address only single-dimensional, simple question generation, failing to meet complex, multifaceted educational requirements. To address these challenges, we constructed and annotated EduMath, a dataset of 16k mathematical questions with multi-dimensional educational objectives. Based on this dataset, we developed EQGEVAL, which incorporates three evaluation dimensions and is designed to assess the ability of models to generate educational questions. Drawing inspiration from teachers' problem design processes, we propose the Educational Question Planning with self-Reflection (EQPR) method for educational mathematical question generation, following a ""plan-evaluate-optimize"" approach. Specifically, by combining planning algorithm based on Monte Carlo Tree Search with the generative capabilities of Large Language Models, we continuously optimize questions through iterative feedback. This self-optimization mechanism ensures that the generated questions both fit the educational context and strategically achieve specific basic educational objectives. Through extensive experiments based on EQGEVAL, we have demonstrated that EQPR achieves significant improvements in generating questions that meet multi-dimensional educational objectives.",Objectives Questions Planning based Framework Educational Mathematical Question Generation Automatically generating high quality mathematical problems align educational objectives crucial task NLP based educational technology Traditional generation methods focus primarily textual quality overlook educational objectives methods address single dimensional simple question generation failing meet complex multifaceted educational requirements address challenges constructed annotated EduMath dataset 16k mathematical questions multi dimensional educational objectives Based dataset developed EQGEVAL incorporates evaluation dimensions designed assess ability models generate educational questions Drawing inspiration teachers problem design processes propose Educational Question Planning self Reflection EQPR method educational mathematical question generation following plan evaluate optimize approach Specifically combining planning algorithm based Monte Carlo Tree Search generative capabilities Large Language Models continuously optimize questions iterative feedback self optimization mechanism ensures generated questions fit educational context strategically achieve specific basic educational objectives extensive experiments based EQGEVAL demonstrated EQPR achieves significant improvements generating questions meet multi dimensional educational objectives
665,RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts,"['Mingyan Wu', 'Zhenghao Liu', 'Yukun Yan', 'Xinze Li', 'Shi Yu', 'Zheni Zeng', 'Yu Gu', 'Ge Yu']","Retrieval-Augmented Generation (RAG) enhances the performance of Large Language Models (LLMs) by incorporating external knowledge. However, LLMs still encounter challenges in effectively utilizing the knowledge from retrieved documents, often being misled by irrelevant or noisy information. To address this issue, we introduce RankCoT, a knowledge refinement method that incorporates reranking signals in generating CoT-based summarization for knowledge refinement based on given query and all retrieval documents. During training, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates based on the query and individual documents. It then fine-tunes the LLM to directly reproduce the best CoT from these candidate outputs based on all retrieved documents, which requires LLM to filter out irrelevant documents during generating CoT-style summarization. Additionally, RankCoT incorporates a self-reflection mechanism that further refines the CoT outputs, resulting in higher-quality training data. Our experiments demonstrate the effectiveness of RankCoT, showing its superior performance over other knowledge refinement models. Further analysis reveals that RankCoT can provide shorter but effective refinement results, enabling the generator to produce more accurate answers. All code and data are available at https://github.com/NEUIR/RankCoT.",RankCoT Refining Knowledge Retrieval Augmented Generation Ranking Chain Thoughts Retrieval Augmented Generation RAG enhances performance Large Language Models LLMs incorporating external knowledge LLMs encounter challenges effectively utilizing knowledge retrieved documents misled irrelevant noisy information address issue introduce RankCoT knowledge refinement method incorporates reranking signals generating CoT based summarization knowledge refinement based given query retrieval documents training RankCoT prompts LLM generate Chain Thought CoT candidates based query individual documents fine tunes LLM directly reproduce best CoT candidate outputs based retrieved documents requires LLM filter irrelevant documents generating CoT style summarization Additionally RankCoT incorporates self reflection mechanism refines CoT outputs resulting higher quality training data experiments demonstrate effectiveness RankCoT showing superior performance knowledge refinement models analysis reveals RankCoT provide shorter effective refinement results enabling generator produce accurate answers code data available https github com NEUIR RankCoT
666,Lost in Literalism: How Supervised Training Shapes Translationese in LLMs,"['Yafu Li', 'Ronghao Zhang', 'Zhilin Wang', 'Huajian Zhang', 'Leyang Cui', 'Yongjing Yin', 'Tong Xiao', 'Yue Zhang']","Large language models (LLMs) have achieved remarkable success in machine translation, demonstrating impressive performance across diverse languages. However, translationese, characterized by overly literal and unnatural translations, remains a persistent challenge in LLM-based translation systems. Despite their pre-training on vast corpora of natural utterances, LLMs exhibit translationese errors and generate unexpected unnatural translations, stemming from biases introduced during supervised fine-tuning (SFT). In this work, we systematically evaluate the prevalence of translationese in LLM-generated translations and investigate its roots during supervised training. We introduce methods to mitigate these biases, including polishing golden references and filtering unnatural training instances. Empirical evaluations demonstrate that these approaches significantly reduce translationese while improving translation naturalness, validated by human evaluations and automatic metrics. Our findings highlight the need for training-aware adjustments to optimize LLM translation outputs, paving the way for more fluent and target-language-consistent translations. We release the data and code at https://github.com/yafuly/LLM_Translationese.",Lost Literalism Supervised Training Shapes Translationese LLMs Large language models LLMs achieved remarkable success machine translation demonstrating impressive performance diverse languages translationese characterized overly literal unnatural translations remains persistent challenge LLM based translation systems Despite pre training vast corpora natural utterances LLMs exhibit translationese errors generate unexpected unnatural translations stemming biases introduced supervised fine tuning SFT work systematically evaluate prevalence translationese LLM generated translations investigate roots supervised training introduce methods mitigate biases including polishing golden references filtering unnatural training instances Empirical evaluations demonstrate approaches significantly reduce translationese improving translation naturalness validated human evaluations automatic metrics findings highlight need training aware adjustments optimize LLM translation outputs paving way fluent target language consistent translations release data code https github com yafuly LLM_Translationese
667,An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling,"['Xuemei Tang', 'Jun Wang', 'Qi Su', 'Chu-Ren Huang', 'Jinghang Gu']","Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models.",Effective Incorporating Heterogeneous Knowledge Curriculum Learning Sequence Labeling Sequence labeling models benefit incorporating external knowledge practice introduces data heterogeneity complicates model additional modules leading increased expenses training high performing model address challenge propose stage curriculum learning TCL framework specifically designed sequence labeling tasks TCL framework enhances training gradually introducing data instances easy hard aiming improve performance training speed Furthermore explore different metrics assessing difficulty levels sequence labeling tasks extensive experimentation Chinese word segmentation CWS speech tagging POS datasets demonstrate effectiveness model enhancing performance sequence labeling models Additionally analysis indicates TCL accelerates training alleviates slow training problem associated complex models
668,Accurate KV Cache Quantization with Outlier Tokens Tracing,"['Yi Su', 'Yuechi Zhou', 'Quantong Qiu', 'Juntao Li', 'Qingrong Xia', 'Ping Li', 'Xinyu Duan', 'Zhefeng Wang', 'Min Zhang']","The impressive capabilities of Large Language Models (LLMs) come at the cost of substantial computational resources during deployment. While KV Cache can significantly reduce recomputation during inference, it also introduces additional memory overhead. KV Cache quantization presents a promising solution, striking a good balance between memory usage and accuracy. Previous research has shown that the Keys are distributed by channel, while the Values are distributed by token. Consequently, the common practice is to apply channel-wise quantization to the Keys and token-wise quantization to the Values. However, our further investigation reveals that a small subset of unusual tokens exhibit unique characteristics that deviate from this pattern, which can substantially impact quantization accuracy. To address this, we develop a simple yet effective method to identify these tokens accurately during the decoding process and exclude them from quantization as outlier tokens, significantly improving overall accuracy. Extensive experiments show that our method achieves significant accuracy improvements under 2-bit quantization and can deliver a 6.4 times reduction in memory usage and a 2.3 times increase in throughput.",Accurate KV Cache Quantization Outlier Tokens Tracing impressive capabilities Large Language Models LLMs come cost substantial computational resources deployment KV Cache significantly reduce recomputation inference introduces additional memory overhead KV Cache quantization presents promising solution striking good balance memory usage accuracy Previous research shown Keys distributed channel Values distributed token Consequently common practice apply channel wise quantization Keys token wise quantization Values investigation reveals small subset unusual tokens exhibit unique characteristics deviate pattern substantially impact quantization accuracy address develop simple effective method identify tokens accurately decoding process exclude quantization outlier tokens significantly improving overall accuracy Extensive experiments method achieves significant accuracy improvements 2 bit quantization deliver 6 4 times reduction memory usage 2 3 times increase throughput
669,Can Large Language Models Understand Internet Buzzwords Through User-Generated Content,"['Chen Huang', 'Junkai Luo', 'Xinzuo Wang', 'Wenqiang Lei', 'Jiancheng Lv']","The massive user-generated content (UGC) available in Chinese social media is giving rise to the possibility of studying internet buzzwords. In this paper, we study if large language models (LLMs) can generate accurate definitions for these buzzwords based on UGC as examples. Our work serves a threefold contribution. First, we introduce CHEER, the first dataset of Chinese internet buzzwords, each annotated with a definition and relevant UGC. Second, we propose a novel method, called RESS, to effectively steer the comprehending process of LLMs to produce more accurate buzzword definitions, mirroring the skills of human language learning. Third, with CHEER, we benchmark the strengths and weaknesses of various off-the-shelf definition generation methods and our RESS. Our benchmark demonstrates the effectiveness of RESS while revealing crucial shared challenges: over-reliance on prior exposure, underdeveloped inferential abilities, and difficulty identifying high-quality UGC to facilitate comprehension. We believe our work lays the groundwork for future advancements in LLM-based definition generation. Our dataset and code are available at https://github.com/SCUNLP/Buzzword.",Large Language Models Understand Internet Buzzwords User Generated Content massive user generated content UGC available Chinese social media giving rise possibility studying internet buzzwords paper study large language models LLMs generate accurate definitions buzzwords based UGC examples work serves threefold contribution introduce CHEER dataset Chinese internet buzzwords annotated definition relevant UGC Second propose novel method called RESS effectively steer comprehending process LLMs produce accurate buzzword definitions mirroring skills human language learning CHEER benchmark strengths weaknesses various shelf definition generation methods RESS benchmark demonstrates effectiveness RESS revealing crucial shared challenges reliance prior exposure underdeveloped inferential abilities difficulty identifying high quality UGC facilitate comprehension believe work lays groundwork future advancements LLM based definition generation dataset code available https github com SCUNLP Buzzword
670,EAC-MoE: Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models,"['Yuanteng Chen', 'Yuantian Shao', 'Peisong Wang', 'Jian Cheng']",,EAC MoE Expert Selection Aware Compressor Mixture Experts Large Language Models
671,Activation Steering Decoding: Mitigating Hallucination in Large Vision-Language Models through Bidirectional Hidden State Intervention,"['Jingran Su', 'Jingfan CHEN', 'Hongxin Li', 'Yuntao Chen', 'Li Qing', 'Zhaoxiang Zhang']",,Activation Steering Decoding Mitigating Hallucination Large Vision Language Models Bidirectional Hidden State Intervention
672,Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models,"['Fangzhi Xu', 'Qiushi Sun', 'Kanzhi Cheng', 'Jun Liu', 'Yu Qiao', 'Zhiyong Wu']","One of the primary driving forces contributing to the superior performance of Large Language Models (LLMs) is the extensive availability of human-annotated natural language data, which is used for alignment fine-tuning. This inspired researchers to investigate self-training methods to mitigate the extensive reliance on human annotations. However, the current success of self-training has been primarily observed in natural language scenarios, rather than in the increasingly important neural-symbolic scenarios. To this end, we propose an environment-guided neural-symbolic self-training framework named ENVISIONS. It aims to overcome two main challenges: (1) the scarcity of symbolic data, and (2) the limited proficiency of LLMs in processing symbolic language. Extensive evaluations conducted on three distinct domains demonstrate the effectiveness of our approach. Additionally, we have conducted a comprehensive analysis to uncover the factors contributing to ENVISIONS's success, thereby offering valuable insights for future research in this area. Code will be available at \url{https://github.com/xufangzhi/ENVISIONS}.",Interactive Evolution Neural Symbolic Self Training Framework Large Language Models primary driving forces contributing superior performance Large Language Models LLMs extensive availability human annotated natural language data used alignment fine tuning inspired researchers investigate self training methods mitigate extensive reliance human annotations current success self training primarily observed natural language scenarios increasingly important neural symbolic scenarios end propose environment guided neural symbolic self training framework named ENVISIONS aims overcome main challenges 1 scarcity symbolic data 2 limited proficiency LLMs processing symbolic language Extensive evaluations conducted distinct domains demonstrate effectiveness approach Additionally conducted comprehensive analysis uncover factors contributing ENVISIONS s success offering valuable insights future research area Code available url https github com xufangzhi ENVISIONS
673,Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback,"['Yucheng Zhou', 'Lingran Song', 'Jianbing Shen']","Existing Medical Large Vision-Language Models (Med-LVLMs), encapsulating extensive medical knowledge, demonstrate excellent capabilities in understanding medical images. However, there remain challenges in visual localization in medical images, which is crucial for abnormality detection and interpretation. To address these issues, we propose a novel UMed-LVLM designed to unveil medical abnormalities. Specifically, we collect a Medical Abnormalities Unveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM training. To collect MAU dataset, we propose a prompt method utilizing the GPT-4V to generate diagnoses based on identified abnormal areas in medical images. Moreover, the two-stage training method includes Abnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding, comprising Relevance Reward, Abnormal Localization Reward and Vision Relevance Reward. Experimental results demonstrate that our UMed-LVLM significantly outperforms existing Med-LVLMs in identifying and understanding medical abnormalities, achieving a 58% improvement over the baseline. In addition, this work shows that enhancing the abnormality detection capabilities of Med-LVLMs significantly improves their understanding of medical images and generalization capability.",Improving Medical Large Vision Language Models Abnormal Aware Feedback Existing Medical Large Vision Language Models Med LVLMs encapsulating extensive medical knowledge demonstrate excellent capabilities understanding medical images remain challenges visual localization medical images crucial abnormality detection interpretation address issues propose novel UMed LVLM designed unveil medical abnormalities Specifically collect Medical Abnormalities Unveiling MAU dataset propose stage training method UMed LVLM training collect MAU dataset propose prompt method utilizing GPT 4V generate diagnoses based identified abnormal areas medical images stage training method includes Abnormal Aware Instruction Tuning Abnormal Aware Rewarding comprising Relevance Reward Abnormal Localization Reward Vision Relevance Reward Experimental results demonstrate UMed LVLM significantly outperforms existing Med LVLMs identifying understanding medical abnormalities achieving 58 improvement baseline addition work shows enhancing abnormality detection capabilities Med LVLMs significantly improves understanding medical images generalization capability
674,Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging,"['Tingfeng Hui', 'Zhenyu Zhang', 'Shuohuan Wang', 'Yu Sun', 'Hua Wu', 'Sen Su']","Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and demonstrates outstanding performance in plentiful natural language processing tasks. However, existing methods transforming LLMs from dense to MoE face significant data requirements and typically rely on large-scale post-training. In this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient approach for tuning a dense pre-trained model into a MoE instruction model. Specifically, we first point out that intermediate checkpoints during instruction tuning of the dense model are naturally suitable for specialized experts, and then propose an expert expansion stage to flexibly achieve models with flexible numbers of experts, where genetic algorithm and parameter merging are introduced to ensure sufficient diversity of new extended experts. To ensure that each specialized expert in the MoE model works as expected, we select a small amount of seed data that each expert excels to pre-optimize the router. Extensive experiments with various data scales and upcycling settings demonstrate the outstanding performance and data efficiency of UpIT, as well as stable improvement in expert or data scaling. Further analysis reveals the importance of ensuring expert diversity in upcycling.",Upcycling Instruction Tuning Dense Mixture Experts Parameter Merging Mixture Experts MoE shines brightly large language models LLMs demonstrates outstanding performance plentiful natural language processing tasks existing methods transforming LLMs dense MoE face significant data requirements typically rely large scale post training paper propose Upcycling Instruction Tuning UpIT data efficient approach tuning dense pre trained model MoE instruction model Specifically point intermediate checkpoints instruction tuning dense model naturally suitable specialized experts propose expert expansion stage flexibly achieve models flexible numbers experts genetic algorithm parameter merging introduced ensure sufficient diversity new extended experts ensure specialized expert MoE model works expected select small seed data expert excels pre optimize router Extensive experiments various data scales upcycling settings demonstrate outstanding performance data efficiency UpIT stable improvement expert data scaling analysis reveals importance ensuring expert diversity upcycling
675,MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation,"['Lingfeng Zhang', 'Xiaoshuai Hao', 'Qinwen Xu', 'Qiang Zhang', 'Xinyao Zhang', 'Pengwei Wang', 'Jing Zhang', 'Zhongyuan Wang', 'Shanghang Zhang', 'Renjing Xu']",,MapNav Novel Memory Representation Annotated Semantic Maps VLM based Vision Language Navigation
676,Exploring Compositional Generalization of Multimodal LLMs for Medical Imaging,"['Zhenyang Cai', 'Junying Chen', 'Rongsheng Wang', 'Weihong Wang', 'Yonglin Deng', 'Dingjie Song', 'Yize Chen', 'Zixu Zhang', 'Benyou Wang']","Medical imaging provides essential visual insights for diagnosis, and multimodal large language models (MLLMs) are increasingly utilized for its analysis due to their strong generalization capabilities; however, the underlying factors driving this generalization remain unclear. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG), which refers to the models' ability to understand novel combinations by recombining learned elements, as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and confirmed that MLLMs can achieve CG across classification and detection tasks, underscoring its broader generalization potential. Med-MAT is available at https://github.com/FreedomIntelligence/Med-MAT.",Exploring Compositional Generalization Multimodal LLMs Medical Imaging Medical imaging provides essential visual insights diagnosis multimodal large language models MLLMs increasingly utilized analysis strong generalization capabilities underlying factors driving generalization remain unclear Current research suggests multi task training outperforms single task different tasks benefit overlook internal relationships tasks analyze phenomenon attempted employ compositional generalization CG refers models ability understand novel combinations recombining learned elements guiding framework medical images precisely defined Modality Anatomical area Task naturally providing environment exploring CG assembled 106 medical datasets create Med MAT comprehensive experiments experiments confirmed MLLMs use CG understand unseen medical images identified CG main drivers generalization observed multi task training Additionally studies demonstrated CG effectively supports datasets limited data confirmed MLLMs achieve CG classification detection tasks underscoring broader generalization potential Med MAT available https github com FreedomIntelligence Med MAT
677,CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention,"['Zekai Ye', 'Qiming Li', 'Xiaocheng Feng', 'Libo Qin', 'Yichong Huang', 'Baohang Li', 'Kui Jiang', 'Yang Xiang', 'Zhirui Zhang', 'Yunfei Lu', 'Duyu Tang', 'Dandan Tu', 'Bing Qin']","Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal abilities but remain prone to multilingual object hallucination, with a higher likelihood of generating responses inconsistent with the visual input when utilizing queries in non-English languages compared to English. Most existing approaches to address these rely on pretraining or fine-tuning, which are resource-intensive. In this paper, inspired by observing the disparities in cross-modal attention patterns across languages, we propose Cross-Lingual Attention Intervention for Mitigating multilingual object hallucination (CLAIM) in LVLMs, a novel near training-free method by aligning attention patterns. CLAIM first identifies language-specific cross-modal attention heads, then estimates language shift vectors from English to the target language, and finally intervenes in the attention outputs during inference to facilitate cross-lingual visual perception capability alignment. Extensive experiments demonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in Spanish) on the POPE and 21.75% on the hallucination subsets of the MME benchmark across various languages. Further analysis reveals that multilingual attention divergence is most prominent in intermediate layers, highlighting their critical role in multilingual scenarios.",CLAIM Mitigating Multilingual Object Hallucination Large Vision Language Models Cross Lingual Attention Intervention Large Vision Language Models LVLMs demonstrated impressive multimodal abilities remain prone multilingual object hallucination higher likelihood generating responses inconsistent visual input utilizing queries non English languages compared English existing approaches address rely pretraining fine tuning resource intensive paper inspired observing disparities cross modal attention patterns languages propose Cross Lingual Attention Intervention Mitigating multilingual object hallucination CLAIM LVLMs novel near training free method aligning attention patterns CLAIM identifies language specific cross modal attention heads estimates language shift vectors English target language finally intervenes attention outputs inference facilitate cross lingual visual perception capability alignment Extensive experiments demonstrate CLAIM achieves average improvement 13 56 30 Spanish POPE 21 75 hallucination subsets MME benchmark various languages analysis reveals multilingual attention divergence prominent intermediate layers highlighting critical role multilingual scenarios
678,Wizard of Shopping: Target-Oriented E-commerce Dialogue Generation with Decision Tree Branching,"['Xiangci Li', 'Zhiyu Chen', 'Jason Ingyu Choi', 'Nikhita Vedula', 'Besnik Fetahu', 'Oleg Rokhlenko', 'Shervin Malmasi']","The goal of conversational product search (CPS) is to develop an intelligent, chat-based shopping assistant that can directly interact with customers to understand shopping intents, ask clarification questions, and find relevant products. However, training such assistants is hindered mainly due to the lack of reliable and large-scale datasets. Prior human-annotated CPS datasets are extremely small in size and lack integration with real-world product search systems. We propose a novel approach, TRACER, which leverages large language models (LLMs) to generate realistic and natural conversations for different shopping domains. TRACER's novelty lies in grounding the generation to dialogue plans, which are product search trajectories predicted from a decision tree model, that guarantees relevant product discovery in the shortest number of search conditions. We also release the first target-oriented CPS dataset Wizard of Shopping (WoS), containing highly natural and coherent conversations (3.6k) from three shopping domains. Finally, we demonstrate the quality and effectiveness of WoS via human evaluations and downstream tasks.",Wizard Shopping Target Oriented E commerce Dialogue Generation Decision Tree Branching goal conversational product search CPS develop intelligent chat based shopping assistant directly interact customers understand shopping intents ask clarification questions relevant products training assistants hindered mainly lack reliable large scale datasets Prior human annotated CPS datasets extremely small size lack integration real world product search systems propose novel approach TRACER leverages large language models LLMs generate realistic natural conversations different shopping domains TRACER s novelty lies grounding generation dialogue plans product search trajectories predicted decision tree model guarantees relevant product discovery shortest number search conditions release target oriented CPS dataset Wizard Shopping WoS containing highly natural coherent conversations 3 6k shopping domains Finally demonstrate quality effectiveness WoS human evaluations downstream tasks
679,Multi-Agent Collaboration for Multilingual Code Instruction Tuning,"['Jian Yang', 'Wei Zhang', 'Yibo Miao', 'Shanghaoran Quan', 'Zhenhe Wu', 'Qiyao Peng', 'Liqun Yang', 'Tianyu Liu', 'Zeyu Cui', 'Binyuan Hui', 'Junyang Lin']","Recent advancement in code understanding and generation demonstrates that code LLMs fine-tuned on a high-quality instruction dataset can gain powerful capabilities to address wide-ranging code-related tasks. However, most previous existing methods mainly view each programming language in isolation and ignore the knowledge transfer among different programming languages. To bridge the gap among different programming languages, we introduce a novel multi-agent collaboration framework to enhance multilingual instruction tuning for code LLMs, where multiple language-specific intelligent agent components with generation memory work together to transfer knowledge from one language to another efficiently and effectively. Specifically, we first generate the language-specific instruction data from the code snippets and then provide the generated data as the seed data for language-specific agents. Multiple language-specific agents discuss and collaborate to formulate a new instruction and its corresponding solution (A new programming language or existing programming language), To further encourage the cross-lingual transfer, each agent stores its generation history as memory and then summarizes its merits and faults. Finally, the high-quality multilingual instruction data is used to encourage knowledge transfer among different programming languages to train Qwen2.5-xCoder. Experimental results on multilingual programming benchmarks demonstrate the superior performance of Qwen2.5-xCoder in sharing common knowledge, highlighting its potential to reduce the cross-lingual gap.",Multi Agent Collaboration Multilingual Code Instruction Tuning Recent advancement code understanding generation demonstrates code LLMs fine tuned high quality instruction dataset gain powerful capabilities address wide ranging code related tasks previous existing methods mainly view programming language isolation ignore knowledge transfer different programming languages bridge gap different programming languages introduce novel multi agent collaboration framework enhance multilingual instruction tuning code LLMs multiple language specific intelligent agent components generation memory work transfer knowledge language efficiently effectively Specifically generate language specific instruction data code snippets provide generated data seed data language specific agents Multiple language specific agents discuss collaborate formulate new instruction corresponding solution new programming language existing programming language encourage cross lingual transfer agent stores generation history memory summarizes merits faults Finally high quality multilingual instruction data used encourage knowledge transfer different programming languages train Qwen2 5 xCoder Experimental results multilingual programming benchmarks demonstrate superior performance Qwen2 5 xCoder sharing common knowledge highlighting potential reduce cross lingual gap
680,Cultivating Gaming Sense for Yourself: Making VLMs Gaming Experts,"['wenxuan lu', 'Jiangyang He', 'Zhanqiu Zhang', 'Tianning Zang', 'Steven Y. Guo']",,Cultivating Gaming Sense Making VLMs Gaming Experts
681,Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning,"['Fangzhi Xu', 'Hang Yan', 'Chang Ma', 'Haiteng Zhao', 'Qiushi Sun', 'Kanzhi Cheng', 'Junxian He', 'Jun Liu', 'Zhiyong Wu']","Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. We introduce a generalizable and purely unsupervised self-training framework, named Genius. Without external auxiliary, Genius requires to seek the optimal response sequence in a stepwise manner and optimize the LLM. To explore the potential steps and exploit the optimal ones, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Further, we recognize that the unsupervised setting inevitably induces the intrinsic noise and uncertainty. To provide a robust optimization, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. Combining these techniques together, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries. The code will be released at https://github.com/xufangzhi/Genius.",Genius Generalizable Purely Unsupervised Self Training Framework Advanced Reasoning Advancing LLM reasoning skills captivated wide current post training techniques rely heavily supervisory signals outcome supervision auxiliary reward models face problem scalability high annotation costs motivates enhance LLM reasoning need external supervision introduce generalizable purely unsupervised self training framework named Genius external auxiliary Genius requires seek optimal response sequence stepwise manner optimize LLM explore potential steps exploit optimal ones Genius introduces stepwise foresight sampling strategy sample estimate step value simulating future outcomes recognize unsupervised setting inevitably induces intrinsic noise uncertainty provide robust optimization propose advantage calibrated optimization ACO loss function mitigate estimation inconsistencies Combining techniques Genius provides advanced initial step self improve LLM reasoning general queries supervision revolutionizing reasoning scaling laws given vast availability general queries code released https github com xufangzhi Genius
682,Accelerating Dense LLMs via L0-regularized Mixture-of-Experts,"['Zhenyu Zhang', 'JiuDong Yang', 'taozhaowen', 'Meng Chen']",,Accelerating Dense LLMs L0 regularized Mixture Experts
683,Extending Complex Logical Queries on Uncertain Knowledge Graphs,"['Weizhi Fei', 'Zihao Wang', 'Hang Yin', 'Yang Duan', 'Yangqiu Song']","The study of machine learning-based logical query answering enables reasoning with large-scale and incomplete knowledge graphs. This paper advances this area of research by addressing the uncertainty inherent in knowledge. While the uncertain nature of knowledge is widely recognized in the real world, it does not align seamlessly with the first-order logic that underpins existing studies. To bridge this gap, we explore the soft queries on uncertain knowledge, inspired by the framework of soft constraint programming. We propose a neural symbolic approach that incorporates both forward inference and backward calibration to answer soft queries on large-scale, incomplete, and uncertain knowledge graphs. Theoretical discussions demonstrate that our method avoids catastrophic cascading errors in the forward inference while maintaining the same complexity as state-of-the-art symbolic methods for complex logical queries. Empirical results validate the superior performance of our backward calibration compared to extended query embedding methods and neural symbolic approaches.",Extending Complex Logical Queries Uncertain Knowledge Graphs study machine learning based logical query answering enables reasoning large scale incomplete knowledge graphs paper advances area research addressing uncertainty inherent knowledge uncertain nature knowledge widely recognized real world does align seamlessly order logic underpins existing studies bridge gap explore soft queries uncertain knowledge inspired framework soft constraint programming propose neural symbolic approach incorporates forward inference backward calibration answer soft queries large scale incomplete uncertain knowledge graphs Theoretical discussions demonstrate method avoids catastrophic cascading errors forward inference maintaining complexity state art symbolic methods complex logical queries Empirical results validate superior performance backward calibration compared extended query embedding methods neural symbolic approaches
684,Knowledge Decoupling via Orthogonal Projection for Lifelong Editing of Large Language Models,"['Haoyu Xu', 'Pengxiang Lan', 'Enneng Yang', 'Guibing Guo', 'Jianzhe Zhao', 'Linying Jiang', 'Xingwei Wang']",,Knowledge Decoupling Orthogonal Projection Lifelong Editing Large Language Models
685,$\phi$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation,"['Fangzhi Xu', 'Hang Yan', 'Chang Ma', 'Haiteng Zhao', 'Jun Liu', 'Qika Lin', 'Zhiyong Wu']",,phi Decoding Adaptive Foresight Sampling Balanced Inference Time Exploration Exploitation
686,Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?,"['Leyi Pan', 'Aiwei Liu', 'Shiyu Huang', 'Yijian LU', 'Xuming Hu', 'Lijie Wen', 'Irwin King', 'Philip S. Yu']","The radioactive nature of Large Language Model (LLM) watermarking enables the detection of watermarks inherited by student models when trained on the outputs of watermarked teacher models, making it a promising tool for preventing unauthorized knowledge distillation. However, the robustness of watermark radioactivity against adversarial actors remains largely unexplored. In this paper, we investigate whether student models can acquire the capabilities of teacher models through knowledge distillation while avoiding watermark inheritance. We propose two categories of watermark removal approaches: pre-distillation removal through untargeted and targeted training data paraphrasing (UP and TP), and post-distillation removal through inference-time watermark neutralization (WN). Extensive experiments across multiple model pairs, watermarking schemes and hyper-parameter settings demonstrate that both TP and WN thoroughly eliminate inherited watermarks, with WN achieving this while maintaining knowledge transfer efficiency and low computational overhead. Given the ongoing deployment of watermarking techniques in production LLMs, these findings emphasize the urgent need for more robust defense strategies. Our code is available at https://github.com/THU-BPM/Watermark-Radioactivity-Attack.",LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation radioactive nature Large Language Model LLM watermarking enables detection watermarks inherited student models trained outputs watermarked teacher models making promising tool preventing unauthorized knowledge distillation robustness watermark radioactivity adversarial actors remains largely unexplored paper investigate student models acquire capabilities teacher models knowledge distillation avoiding watermark inheritance propose categories watermark removal approaches pre distillation removal untargeted targeted training data paraphrasing TP post distillation removal inference time watermark neutralization WN Extensive experiments multiple model pairs watermarking schemes hyper parameter settings demonstrate TP WN thoroughly eliminate inherited watermarks WN achieving maintaining knowledge transfer efficiency low computational overhead Given ongoing deployment watermarking techniques production LLMs findings emphasize urgent need robust defense strategies code available https github com THU BPM Watermark Radioactivity Attack
687,Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization,"['Sunghwan Kim', 'Dongjin Kang', 'Taeyoon Kwon', 'Hyungjoo Chae', 'Dongha Lee', 'Jinyoung Yeo']","Reward models (RMs) play a crucial role in reinforcement learning from human feedback (RLHF), aligning model behavior with human preferences. However, existing benchmarks for reward models show a weak correlation with the performance of optimized policies, suggesting that they fail to accurately assess the true capabilities of RMs. To bridge this gap, we explore several evaluation designs through the lens of reward overoptimization\textemdash a phenomenon that captures both how well the reward model aligns with human preferences and the dynamics of the learning signal it provides to the policy. The results highlight three key findings on how to construct a reliable benchmark: (i) it is important to minimize differences between chosen and rejected responses beyond correctness, (ii) evaluating reward models requires multiple comparisons across a wide range of chosen and rejected responses, and (iii) given that reward models encounter responses with diverse representations, responses should be sourced from a variety of models. However, we also observe that a extremely high correlation with degree of overoptimization leads to comparatively lower correlation with certain downstream performance. Thus, when designing a benchmark, it is desirable to use the degree of overoptimization as a useful tool, rather than the end goal.",Rethinking Reward Model Evaluation Lens Reward Overoptimization Reward models RMs play crucial role reinforcement learning human feedback RLHF aligning model behavior human preferences existing benchmarks reward models weak correlation performance optimized policies suggesting fail accurately assess true capabilities RMs bridge gap explore evaluation designs lens reward overoptimization textemdash phenomenon captures reward model aligns human preferences dynamics learning signal provides policy results highlight key findings construct reliable benchmark important minimize differences chosen rejected responses correctness ii evaluating reward models requires multiple comparisons wide range chosen rejected responses iii given reward models encounter responses diverse representations responses sourced variety models observe extremely high correlation degree overoptimization leads comparatively lower correlation certain downstream performance designing benchmark desirable use degree overoptimization useful tool end goal
688,LISTN: Lexicon induction with socio-temporal nuance,['Christine de Kock'],,LISTN Lexicon induction socio temporal nuance
689,LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement,"['Boyi Kang', 'Xinfa Zhu', 'Zihan Zhang', 'Zhen Ye', 'Mingshuai Liu', 'Ziqian Wang', 'Yike Zhu', 'Guobin Ma', 'Jun Chen', 'Longshuai Xiao', 'CHAO WENG', 'Wei Xue', 'Lei Xie']","Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area.",LLaSE G1 Incentivizing Generalization Capability LLaMA based Speech Enhancement Recent advancements language models LMs demonstrated strong capabilities semantic understanding contextual modeling flourished generative speech enhancement SE LM based SE approaches primarily focus semantic information neglecting critical role acoustic information leads acoustic inconsistency enhancement limited generalization diverse SE tasks paper introduce LLaSE G1 LLaMA based language model incentivizes generalization capabilities speech enhancement LLaSE G1 offers following key contributions mitigate acoustic inconsistency LLaSE G1 employs continuous representations WavLM input predicts speech tokens X Codec2 maximizing acoustic preservation Second promote generalization capability LLaSE G1 introduces dual channel inputs outputs unifying multiple SE tasks requiring task specific IDs LLaSE G1 outperforms prior task specific discriminative generative SE models demonstrating scaling effects test time emerging capabilities unseen SE tasks Additionally release code models support research area
690,MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference,"['Kunxi Li', 'Zhonghua Jiang', 'Zhouzhou Shen', 'ZhaodeWang', 'chengfei lv', 'Shengyu Zhang', 'Fan Wu', 'Fei Wu']","This paper introduces MadaKV, a modality-adaptive key-value (KV) cache eviction strategy designed to enhance the efficiency of multimodal large language models (MLLMs) in long-context inference. In multimodal scenarios, attention heads exhibit varying preferences for different modalities, resulting in significant disparities in modality importance across attention heads. Traditional KV cache eviction methods, which are tailored for unimodal settings, fail to capture modality-specific information, thereby yielding suboptimal performance. MadaKV addresses these challenges through two key components: modality preference adaptation and hierarchical compression compensation. By dynamically sensing modality information within attention heads and adaptively retaining critical tokens, MadaKV achieves substantial reductions in KV cache memory footprint and model inference decoding latency (1.3 to 1.5 times improvement) while maintaining high accuracy across various multimodal long-context tasks. Extensive experiments on representative MLLMs and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to existing KV cache eviction methods.",MadaKV Adaptive Modality Perception KV Cache Eviction Efficient Multimodal Long Context Inference paper introduces MadaKV modality adaptive key value KV cache eviction strategy designed enhance efficiency multimodal large language models MLLMs long context inference multimodal scenarios attention heads exhibit varying preferences different modalities resulting significant disparities modality importance attention heads Traditional KV cache eviction methods tailored unimodal settings fail capture modality specific information yielding suboptimal performance MadaKV addresses challenges key components modality preference adaptation hierarchical compression compensation dynamically sensing modality information attention heads adaptively retaining critical tokens MadaKV achieves substantial reductions KV cache memory footprint model inference decoding latency 1 3 1 5 times improvement maintaining high accuracy various multimodal long context tasks Extensive experiments representative MLLMs MileBench benchmark demonstrate effectiveness MadaKV compared existing KV cache eviction methods
691,Efficient OpAmp Adaptation for Zoom Attention to Golden Contexts,"['Haoyuan Wu', 'Rui Ming', 'Haisheng Zheng', 'Zhuolun He', 'Bei Yu']","Large language models (LLMs) have shown significant promise in question-answering (QA) tasks, particularly in retrieval-augmented generation (RAG) scenarios and long-context applications. However, their performance is hindered by noisy reference documents, which often distract from essential information. Despite fine-tuning efforts, Transformer-based architectures struggle to prioritize relevant content. This is evidenced by their tendency to allocate disproportionate attention to irrelevant or later-positioned documents. Recent work proposes the differential attention mechanism to address this issue, but this mechanism is limited by an unsuitable common-mode rejection ratio (CMRR) and high computational costs. Inspired by the operational amplifier (OpAmp), we propose the OpAmp adaptation to address these challenges, which is implemented with adapters efficiently. By integrating the adapter into pre-trained Transformer blocks, our approach enhances focus on the golden context without costly training from scratch. Empirical evaluations on noisy-context benchmarks reveal that our Qwen2.5-OpAmp-72B model, trained with our OpAmp adaptation, surpasses the performance of state-of-the-art LLMs, including DeepSeek-V3 and GPT-4o.",Efficient OpAmp Adaptation Zoom Attention Golden Contexts Large language models LLMs shown significant promise question answering QA tasks particularly retrieval augmented generation RAG scenarios long context applications performance hindered noisy reference documents distract essential information Despite fine tuning efforts Transformer based architectures struggle prioritize relevant content evidenced tendency allocate disproportionate attention irrelevant later positioned documents Recent work proposes differential attention mechanism address issue mechanism limited unsuitable common mode rejection ratio CMRR high computational costs Inspired operational amplifier OpAmp propose OpAmp adaptation address challenges implemented adapters efficiently integrating adapter pre trained Transformer blocks approach enhances focus golden context costly training scratch Empirical evaluations noisy context benchmarks reveal Qwen2 5 OpAmp 72B model trained OpAmp adaptation surpasses performance state art LLMs including DeepSeek V3 GPT 4o
692,Bridging Discrete Codec Representations and Speech Language Models,"['Shengpeng Ji', 'Minghui Fang', 'Jialong Zuo', 'Ziyue Jiang', 'Dingdong WANG', 'Hanting Wang', 'Hai Huang', 'Zhou Zhao']",,Bridging Discrete Codec Representations Speech Language Models
693,Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger,"['Wenjun Li', 'Dexun Li', 'Kuicai Dong', 'Cong Zhang', 'Hao Zhang', 'Weiwen Liu', 'Yasheng Wang', 'Ruiming Tang', 'Yong Liu']","Large language models (LLMs) have shown remarkable emergent capabilities, transforming the execution of functional tasks by leveraging external tools for complex problems that require specialized processing or up-to-date data. While existing research expands LLMs access to diverse tools (e.g., program interpreters, search engines, calculators), the necessity of using these tools is often overlooked, leading to indiscriminate tool invocation. This naive approach raises two key issues: increased latency due to unnecessary tool calls, and potential errors resulting from faulty interactions with external tools. In this paper, we introduce meta-cognition as a proxy for LLMs self-assessment of their capabilities, reflecting the model's awareness of its own limitations. Based on this, we propose MeCo, an adaptive decision-making strategy for external tool use. MeCo quantifies metacognitive scores by capturing high-level cognitive signals in the representation space, guiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal cost. Experiments across multiple backbone models and benchmarks show that MeCo reliably detects LLMs' internal cognitive signals and significantly improves tool-use decision-making.",Adaptive Tool Use Large Language Models Meta Cognition Trigger Large language models LLMs shown remarkable emergent capabilities transforming execution functional tasks leveraging external tools complex problems require specialized processing date data existing research expands LLMs access diverse tools e g program interpreters search engines calculators necessity using tools overlooked leading indiscriminate tool invocation naive approach raises key issues increased latency unnecessary tool calls potential errors resulting faulty interactions external tools paper introduce meta cognition proxy LLMs self assessment capabilities reflecting model s awareness limitations Based propose MeCo adaptive decision making strategy external tool use MeCo quantifies metacognitive scores capturing high level cognitive signals representation space guiding invoke tools Notably MeCo fine tuning free incurs minimal cost Experiments multiple backbone models benchmarks MeCo reliably detects LLMs internal cognitive signals significantly improves tool use decision making
694,MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark,"['Qihao Zhao', 'Yangyu Huang', 'Tengchao Lv', 'Lei Cui', 'Qinzheng Sun', 'Shaoguang Mao', 'Xin Zhang', 'Ying Xin', 'Qiufeng Yin', 'Scarlett Li', 'Furu Wei']",,MMLU CF Contamination free Multi task Language Understanding Benchmark
695,Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding,"['Haneul Yoo', 'Yongjin Yang', 'Hwaran Lee']","As large language models (LLMs) have advanced rapidly, concerns regarding their safety have become prominent. In this paper, we discover that code-switching in red-teaming queries can effectively elicit undesirable behaviors of LLMs, which are common practices in natural language. We introduce a simple yet effective framework, CSRT, to synthesize codeswitching red-teaming queries and investigate the safety and multilingual understanding of LLMs comprehensively. Through extensive experiments with ten state-of-the-art LLMs and code-switching queries combining up to 10 languages, we demonstrate that the CSRT significantly outperforms existing multilingual red-teaming techniques, achieving 46.7% more attacks than standard attacks in English and being effective in conventional safety domains. We also examine the multilingual ability of those LLMs to generate and understand codeswitching texts. Additionally, we validate the extensibility of the CSRT by generating codeswitching attack prompts with monolingual data. We finally conduct detailed ablation studies exploring code-switching and propound unintended correlation between resource availability of languages and safety alignment in existing multilingual LLMs.",Code Switching Red Teaming LLM Evaluation Safety Multilingual Understanding large language models LLMs advanced rapidly concerns regarding safety prominent paper discover code switching red teaming queries effectively elicit undesirable behaviors LLMs common practices natural language introduce simple effective framework CSRT synthesize codeswitching red teaming queries investigate safety multilingual understanding LLMs comprehensively extensive experiments state art LLMs code switching queries combining 10 languages demonstrate CSRT significantly outperforms existing multilingual red teaming techniques achieving 46 7 attacks standard attacks English effective conventional safety domains examine multilingual ability LLMs generate understand codeswitching texts Additionally validate extensibility CSRT generating codeswitching attack prompts monolingual data finally conduct detailed ablation studies exploring code switching propound unintended correlation resource availability languages safety alignment existing multilingual LLMs
696,Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch,"['Yuyang Ding', 'Xinyu Shi', 'Xiaobo Liang', 'Juntao Li', 'Zhaopeng Tu', 'Qiaoming Zhu', 'Min Zhang']","Improving the mathematical reasoning capabilities of Large Language Models (LLMs) is critical for advancing artificial intelligence. However, access to extensive, diverse, and high-quality reasoning datasets remains a significant challenge, particularly for the open-source community. In this paper, we propose ScaleQuest, a novel, scalable, and cost-effective data synthesis method that enables the generation of large-scale mathematical reasoning datasets using lightweight 7B-scale models. ScaleQuest introduces a two-stage question-tuning process comprising Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to unlock the question generation capabilities of problem-solving models. By generating diverse questions from scratch -- without relying on powerful proprietary models or seed data -- we produce a dataset of 1 million problem-solution pairs. Our experiments demonstrate that models trained on our data outperform existing open-source datasets in both in-domain and out-of-domain evaluations. Furthermore, our approach shows continued performance improvement as the volume of training data increases, highlighting its potential for ongoing data scaling. The extensive improvements observed in code reasoning tasks demonstrate the generalization capabilities of our proposed method. Our work provides the open-source community with a practical solution to enhance the mathematical reasoning abilities of LLMs.",Unleashing LLM Reasoning Capability Scalable Question Synthesis Scratch Improving mathematical reasoning capabilities Large Language Models LLMs critical advancing artificial intelligence access extensive diverse high quality reasoning datasets remains significant challenge particularly open source community paper propose ScaleQuest novel scalable cost effective data synthesis method enables generation large scale mathematical reasoning datasets using lightweight 7B scale models ScaleQuest introduces stage question tuning process comprising Question Fine Tuning QFT Question Preference Optimization QPO unlock question generation capabilities problem solving models generating diverse questions scratch relying powerful proprietary models seed data produce dataset 1 million problem solution pairs experiments demonstrate models trained data outperform existing open source datasets domain domain evaluations Furthermore approach shows continued performance improvement volume training data increases highlighting potential ongoing data scaling extensive improvements observed code reasoning tasks demonstrate generalization capabilities proposed method work provides open source community practical solution enhance mathematical reasoning abilities LLMs
697,DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing,"['Haneul Yoo', 'Jieun Han', 'So-Yeon Ahn', 'Alice Oh']","Automated essay scoring (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time essay scores for students and instructors. However, previous AES models were trained on essays and scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated essay scoring with 48.9K samples in total. DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with 2.3K essays authored by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for essays, which generates 40.1K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. DREsS will enable further research to provide a more accurate and practical AES system for EFL writing education.",DREsS Dataset Rubric based Essay Scoring EFL Writing Automated essay scoring AES useful tool English Foreign Language EFL writing education offering real time essay scores students instructors previous AES models trained essays scores irrelevant practical scenarios EFL writing education usually provided single holistic score lack appropriate datasets paper release DREsS large scale standard dataset rubric based automated essay scoring 48 9K samples total DREsS comprises sub datasets DREsS_New DREsS_Std DREsS_CASE collect DREsS_New real classroom dataset 2 3K essays authored EFL undergraduate students scored English education experts standardize existing rubric based essay scoring datasets DREsS_Std suggest CASE corruption based augmentation strategy essays generates 40 1K synthetic samples DREsS_CASE improves baseline results 45 44 DREsS enable research provide accurate practical AES EFL writing education
698,PQR: Improving Dense Retrieval via Potential Query Modeling,"['Junfeng Kang', 'Rui Li', 'Qi Liu', 'Yanjiang Chen', 'Zheng Zhang', 'Junzhe Jiang', 'Heng Yu', 'Yu Su']",,PQR Improving Dense Retrieval Potential Query Modeling
699,"Do Multimodal Large Language Models Truly See What We Point At? Investigating Indexical, Iconic, and Symbolic Gesture Comprehension","['Noriki Nishida', 'Koji Inoue', 'Hideki Nakayama', 'Mayumi Bono', 'Katsuya Takanashi']",,Multimodal Large Language Models Truly Point Investigating Indexical Iconic Symbolic Gesture Comprehension
700,Cross-lingual Generalization and Compression: From Language-Specific to Shared Neurons,"['Frederick Riemenschneider', 'Anette Frank']",,Cross lingual Generalization Compression Language Specific Shared Neurons
701,SDBench: A Survey-based Domain-specific LLM Benchmarking and Optimization Framework,"['Cheng Guo', 'Hu Kai', 'Shuxian Liang', 'Yiyang Jiang', 'Yi Gao', 'Xian-Sheng Hua', 'Wei Dong']",,SDBench Survey based Domain specific LLM Benchmarking Optimization Framework
702,ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents,"['Yusheng Liao', 'Shuyang Jiang', 'Yanfeng Wang', 'Yu Wang']","Large Language Models (LLMs) have shown promising potential in the medical domain, assisting with tasks like clinical note generation and patient communication. However, current LLMs are limited to text-based communication, hindering their ability to interact with diverse forms of information in clinical environments. Despite clinical agents succeeding in diverse signal interaction, they are oriented to a single clinical scenario and hence fail for broader applications. To evaluate clinical agents holistically, we propose ClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting of 18 tasks across five key realistic clinical dimensions. Building on this, we introduce ReflecTool, a novel framework that excels at utilizing domain-specific tools within two stages. The first optimization stage progressively enlarges a long-term memory by saving successful solving processes and tool-wise experience of agents in a tiny pre-defined training set. In the following inference stage, ReflecTool can search for supportive successful demonstrations from already built long-term memory to guide the tool selection strategy, and a verifier improves the tool usage according to the tool-wise experience with two verification methods--iterative refinement and candidate selection. Extensive experiments on ClinicalAgent Benchmark demonstrate that ReflecTool surpasses the pure LLMs with more than 10 points and the well-established agent-based methods with 3 points, highlighting its adaptability and effectiveness in solving complex clinical tasks.",ReflecTool Reflection Aware Tool Augmented Clinical Agents Large Language Models LLMs shown promising potential medical domain assisting tasks like clinical note generation patient communication current LLMs limited text based communication hindering ability interact diverse forms information clinical environments Despite clinical agents succeeding diverse signal interaction oriented single clinical scenario fail broader applications evaluate clinical agents holistically propose ClinicalAgent Bench CAB comprehensive medical agent benchmark consisting 18 tasks key realistic clinical dimensions Building introduce ReflecTool novel framework excels utilizing domain specific tools stages optimization stage progressively enlarges long term memory saving successful solving processes tool wise experience agents tiny pre defined training set following inference stage ReflecTool search supportive successful demonstrations built long term memory guide tool selection strategy verifier improves tool usage according tool wise experience verification methods iterative refinement candidate selection Extensive experiments ClinicalAgent Benchmark demonstrate ReflecTool surpasses pure LLMs 10 points established agent based methods 3 points highlighting adaptability effectiveness solving complex clinical tasks
703,Lexical Recall or Logical Reasoning: Probing the Limits of Reasoning Abilities in Large Language Models,"['Henrike Beyer', 'Chris Reed']",,Lexical Recall Logical Reasoning Probing Limits Reasoning Abilities Large Language Models
704,ChainEdit: Propagating Ripple Effects through Logical Rule-Guided Chain Updates,"['Zilu dong', 'Xiangqing Shen', 'Zinong Yang', 'Rui Xia']",,ChainEdit Propagating Ripple Effects Logical Rule Guided Chain Updates
705,HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model,"['Haiyang Guo', 'Fanhu Zeng', 'Ziwei Xiang', 'Fei Zhu', 'Da-Han Wang', 'Xu-Yao Zhang', 'Cheng-Lin Liu']","Instruction tuning is widely used to improve a pre-trained Multimodal Large Language Model (MLLM) by training it on curated task-specific datasets, enabling better comprehension of human instructions. However, it is infeasible to collect all possible instruction datasets simultaneously in real-world scenarios. Thus, enabling MLLM with continual instruction tuning is essential for maintaining their adaptability. However, existing methods often trade off memory efficiency for performance gains, significantly compromising overall efficiency. In this paper, we propose a task-specific expansion and task-general fusion framework based on the variations in Centered Kernel Alignment (CKA) similarity across different model layers when trained on diverse datasets. Furthermore, we analyze the information leakage present in the existing benchmark and propose a new and more challenging benchmark to rationally evaluate the performance of different methods. Comprehensive experiments showcase a significant performance improvement of our method compared to existing state-of-the-art methods. Code and dataset are released at https://github.com/Ghy0501/HiDe-LLaVA.",HiDe LLaVA Hierarchical Decoupling Continual Instruction Tuning Multimodal Large Language Model Instruction tuning widely used improve pre trained Multimodal Large Language Model MLLM training curated task specific datasets enabling better comprehension human instructions infeasible collect possible instruction datasets simultaneously real world scenarios enabling MLLM continual instruction tuning essential maintaining adaptability existing methods trade memory efficiency performance gains significantly compromising overall efficiency paper propose task specific expansion task general fusion framework based variations Centered Kernel Alignment CKA similarity different model layers trained diverse datasets Furthermore analyze information leakage present existing benchmark propose new challenging benchmark rationally evaluate performance different methods Comprehensive experiments showcase significant performance improvement method compared existing state art methods Code dataset released https github com Ghy0501 HiDe LLaVA
706,Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models,"['Qika Lin', 'Tianzhe Zhao', 'Kai He', 'Zhen Peng', 'Fangzhi Xu', 'Ling Huang', 'Jingying Ma', 'Mengling Feng']","Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.",Self supervised Quantized Representation Seamlessly Integrating Knowledge Graphs Large Language Models presence natural gap Knowledge Graph KG structures natural language effective integration holistic structural information KGs Large Language Models LLMs emerged significant question end propose stage framework learn apply quantized codes entity aiming seamless integration KGs LLMs Firstly self supervised quantized representation SSQR method proposed compress KG structural semantic knowledge discrete codes tokens align format language sentences design KG instruction following data viewing learned codes features directly input LLMs achieving seamless integration experiment results demonstrate SSQR outperforms existing unsupervised quantized methods producing distinguishable codes fine tuned LLaMA2 LLaMA3 1 superior performance KG link prediction triple classification tasks utilizing 16 tokens entity instead thousands conventional prompting methods
707,Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking,"['Yifan Zhang', 'Wenyu Du', 'Dongming Jin', 'Jie Fu', 'Zhi Jin']","Chain-of-thought (CoT) significantly enhances the performance of large language models (LLMs) across a wide range of tasks, and prior research shows that CoT can theoretically increase expressiveness. However, there is limited mechanistic understanding of the algorithms that Transformer+CoT can learn. Our key contributions are: (1) We evaluate the state tracking capabilities of Transformer+CoT and its variants, confirming the effectiveness of CoT. (2) Next, we identify the circuit (a subset of model components, responsible for tracking the world state), indicating that late-layer MLP neurons play a key role. We propose two metrics, compression and distinction, and show that the neuron sets for each state achieve nearly 100% accuracy, providing evidence of an implicit finite state automaton (FSA) embedded within the model. (3) Additionally, we explore three challenging settings: skipping intermediate steps, introducing data noises, and testing length generalization. Our results demonstrate that Transformer+CoT learns robust algorithms (FSAs), highlighting its resilience in challenging scenarios. Our code is available at https://github.com/IvanChangPKU/FSA.",Finite State Automata Inside Transformers Chain Thought Mechanistic Study State Tracking Chain thought CoT significantly enhances performance large language models LLMs wide range tasks prior research shows CoT theoretically increase expressiveness limited mechanistic understanding algorithms Transformer CoT learn key contributions 1 evaluate state tracking capabilities Transformer CoT variants confirming effectiveness CoT 2 identify circuit subset model components responsible tracking world state indicating late layer MLP neurons play key role propose metrics compression distinction neuron sets state achieve nearly 100 accuracy providing evidence implicit finite state automaton FSA embedded model 3 Additionally explore challenging settings skipping intermediate steps introducing data noises testing length generalization results demonstrate Transformer CoT learns robust algorithms FSAs highlighting resilience challenging scenarios code available https github com IvanChangPKU FSA
708,TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and Competition,"['Tianwei Lin', 'Jiang Liu', 'Wenqiao Zhang', 'Yang Dai', 'Haoyuan Li', 'Zhelun Yu', 'Wanggui He', 'Juncheng Li', 'Jiannan Guo', 'Hao Jiang', 'Siliang Tang', 'Yueting Zhuang']","While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have effectively addressed GPU memory constraints during fine-tuning, their performance often falls short, especially in multidimensional task scenarios. To address this issue, one straightforward solution is to introduce task-specific LoRA modules as domain experts, leveraging the modeling of multiple experts' capabilities and thus enhancing the general capability of multi-task learning. Despite promising, these additional components often add complexity to the training and inference process, contravening the efficient characterization of PEFT designed for. Considering this, we introduce an innovative PEFT method, TeamLoRA, consisting of a collaboration and competition module for experts, and thus achieving the right balance of effectiveness and efficiency: (i) For collaboration, a novel knowledge-sharing and -organizing mechanism is devised to appropriately reduce the scale of matrix operations, thereby boosting the training and inference speed. (ii) For competition, we propose leveraging a game-theoretic interaction mechanism for experts, encouraging experts to transfer their domain-specific knowledge while facing diverse downstream tasks, and thus enhancing the performance. By doing so, TeamLoRA elegantly connects the experts as a ""Team"" with internal collaboration and competition, enabling a faster and more accurate PEFT paradigm for multi-task learning. To validate the superiority of TeamLoRA, we curate a comprehensive multi-task evaluation(CME) benchmark to thoroughly assess the capability of multi-task learning. Experiments conducted on our CME and other benchmarks indicate the effectiveness and efficiency of TeamLoRA. Our project is available at https://github.com/Lin-Tianwei/TeamLoRA.",TeamLoRA Boosting Low Rank Adaptation Expert Collaboration Competition Parameter Efficient Fine Tuning PEFT methods like LoRA effectively addressed GPU memory constraints fine tuning performance falls short especially multidimensional task scenarios address issue straightforward solution introduce task specific LoRA modules domain experts leveraging modeling multiple experts capabilities enhancing general capability multi task learning Despite promising additional components add complexity training inference process contravening efficient characterization PEFT designed Considering introduce innovative PEFT method TeamLoRA consisting collaboration competition module experts achieving right balance effectiveness efficiency collaboration novel knowledge sharing organizing mechanism devised appropriately reduce scale matrix operations boosting training inference speed ii competition propose leveraging game theoretic interaction mechanism experts encouraging experts transfer domain specific knowledge facing diverse downstream tasks enhancing performance doing TeamLoRA elegantly connects experts Team internal collaboration competition enabling faster accurate PEFT paradigm multi task learning validate superiority TeamLoRA curate comprehensive multi task evaluation CME benchmark thoroughly assess capability multi task learning Experiments conducted CME benchmarks indicate effectiveness efficiency TeamLoRA project available https github com Lin Tianwei TeamLoRA
709,CRiskEval: A Chinese Multi-Level Risk Evaluation Benchmark Dataset for Large Language Models,"['Ling Shi', 'Deyi Xiong']","Large language models (LLMs) are possessed of numerous beneficial capabilities, yet their potential inclination harbors unpredictable risks that may materialize in the future. We hence propose CRiskEval, a Chinese dataset meticulously designed for gauging the risk proclivities inherent in LLMs such as resource acquisition and malicious coordination, as part of efforts for proactive preparedness. To curate CRiskEval, we define a new risk taxonomy with 7 types of frontier risks and 4 safety levels, including extremely hazardous,moderately hazardous, neutral and safe. We follow the philosophy of tendency evaluation to empirically measure the stated desire of LLMs via fine-grained multiple-choice question answering. The dataset consists of 14,888 questions that simulate scenarios related to predefined 7 types of frontier risks. Each question is accompanied with 4 answer choices that state opinions or behavioral tendencies corresponding to the question. All answer choices are manually annotated with one of the defined risk levels so that we can easily build a fine-grained frontier risk profile for each assessed LLM. Extensive evaluation with CRiskEval on a spectrum of prevalent Chinese LLMs has unveiled a striking revelation: most models exhibit risk tendencies of more than 40% (weighted tendency to the four risk levels). Furthermore, a subtle increase in the model's inclination toward urgent self-sustainability, power seeking and other dangerous goals becomes evident as the size of models increase. To promote further research on the frontier risk evaluation of LLMs, we publicly release our dataset at https://github.com/lingshi6565/Risk_eval.",CRiskEval Chinese Multi Level Risk Evaluation Benchmark Dataset Large Language Models Large language models LLMs possessed numerous beneficial capabilities potential inclination harbors unpredictable risks materialize future propose CRiskEval Chinese dataset meticulously designed gauging risk proclivities inherent LLMs resource acquisition malicious coordination efforts proactive preparedness curate CRiskEval define new risk taxonomy 7 types frontier risks 4 safety levels including extremely hazardous moderately hazardous neutral safe follow philosophy tendency evaluation empirically measure stated desire LLMs fine grained multiple choice question answering dataset consists 14 888 questions simulate scenarios related predefined 7 types frontier risks question accompanied 4 answer choices state opinions behavioral tendencies corresponding question answer choices manually annotated defined risk levels easily build fine grained frontier risk profile assessed LLM Extensive evaluation CRiskEval spectrum prevalent Chinese LLMs unveiled striking revelation models exhibit risk tendencies 40 weighted tendency risk levels Furthermore subtle increase model s inclination urgent self sustainability power seeking dangerous goals evident size models increase promote research frontier risk evaluation LLMs publicly release dataset https github com lingshi6565 Risk_eval
710,STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning,"['Jaeseong Lee', 'seung-won hwang', 'Aurick Qiao', 'Daniel F Campos', 'Zhewei Yao', 'Yuxiong He']","Mixture-of-experts (MoEs) have been adopted for reducing inference costs by sparsely activating experts in Large language models (LLMs). Despite this reduction, the massive number of experts in MoEs still makes them expensive to serve. In this paper, we study how to address this, by pruning MoEs. Among pruning methodologies, unstructured pruning has been known to achieve the highest performance for a given pruning ratio, compared to structured pruning, since the latter imposes constraints on the sparsification structure. This is intuitive, as the solution space of unstructured pruning subsumes that of structured pruning. However, our counterintuitive finding reveals that expert pruning, a form of structured pruning, can actually precede unstructured pruning to outperform unstructured-only pruning. As existing expert pruning, requiring $O(\frac{k^n}{\sqrt{n}})$ forward passes for $n$ experts, cannot scale for recent MoEs, we propose a scalable alternative with $O(1)$ complexity, yet outperforming the more expensive methods. The key idea is leveraging a latent structure between experts, based on behavior similarity, such that the greedy decision of whether to prune closely captures the joint pruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized MoE with 128 experts, our method needs only one H100 and two hours to achieve nearly no loss in performance with 40% sparsity, even in generative tasks such as GSM8K, where state-of-the-art unstructured pruning fails to. The code will be made publicly available.",STUN Structured Unstructured Pruning Scalable MoE Pruning Mixture experts MoEs adopted reducing inference costs sparsely activating experts Large language models LLMs Despite reduction massive number experts MoEs makes expensive serve paper study address pruning MoEs pruning methodologies unstructured pruning known achieve highest performance given pruning ratio compared structured pruning imposes constraints sparsification structure intuitive solution space unstructured pruning subsumes structured pruning counterintuitive finding reveals expert pruning form structured pruning actually precede unstructured pruning outperform unstructured pruning existing expert pruning requiring O frac k n sqrt n forward passes n experts scale recent MoEs propose scalable alternative O 1 complexity outperforming expensive methods key idea leveraging latent structure experts based behavior similarity greedy decision prune closely captures joint pruning effect highly effective Snowflake Arctic 480B sized MoE 128 experts method needs H100 hours achieve nearly loss performance 40 sparsity generative tasks GSM8K state art unstructured pruning fails code publicly available
711,Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System,"['Ziyou Jiang', 'Mingyang Li', 'Guowei Yang', 'Junjie Wang', 'Yuekai Huang', 'Zhiyuan Chang', 'Qing Wang']","Information theft attacks pose a significant risk to Large Language Model (LLM) tool-learning systems. Adversaries can inject malicious commands through compromised tools, manipulating LLMs to send sensitive information to these tools, which leads to potential privacy breaches. However, existing attack approaches are black-box oriented and rely on static commands that cannot adapt flexibly to the changes in user queries and the invocation chain of tools. It makes malicious commands more likely to be detected by LLM and leads to attack failure. In this paper, we propose AutoCMD, a dynamic attack comment generation approach for information theft attacks in LLM tool-learning systems. Inspired by the concept of mimicking the familiar, AutoCMD is capable of inferring the information utilized by upstream tools in the toolchain through learning on open-source systems and reinforcement with target system examples, thereby generating more targeted commands for information theft. The evaluation results show that AutoCMD outperforms the baselines with +13.2% $ASR_{Theft}$, and can be generalized to new tool-learning systems to expose their information leakage risks. We also design four defense methods to effectively protect tool-learning systems from the attack.",Mimicking Familiar Dynamic Command Generation Information Theft Attacks LLM Tool Learning Information theft attacks pose significant risk Large Language Model LLM tool learning systems Adversaries inject malicious commands compromised tools manipulating LLMs send sensitive information tools leads potential privacy breaches existing attack approaches black box oriented rely static commands adapt flexibly changes user queries invocation chain tools makes malicious commands likely detected LLM leads attack failure paper propose AutoCMD dynamic attack comment generation approach information theft attacks LLM tool learning systems Inspired concept mimicking familiar AutoCMD capable inferring information utilized upstream tools toolchain learning open source systems reinforcement target examples generating targeted commands information theft evaluation results AutoCMD outperforms baselines 13 2 ASR_ Theft generalized new tool learning systems expose information leakage risks design defense methods effectively protect tool learning systems attack
712,FlashAudio: Rectified Flow for Fast and High-Fidelity Text-to-Audio Generation,"['Huadai Liu', 'Jialei Wang', 'Rongjie Huang', 'Yang Liu', 'Heng Lu', 'Wei Xue', 'Zhou Zhao']",,FlashAudio Rectified Flow Fast High Fidelity Text Audio Generation
713,How does Misinformation Affect Large Language Model Behaviors and Preferences?,"['Miao Peng', 'Nuo Chen', 'Jianheng Tang', 'Jia Li']","Large Language Models (LLMs) have shown remarkable capabilities in knowledge-intensive tasks, while they remain vulnerable when encountering misinformation. Existing studies have explored the role of LLMs in combating misinformation, but there is still a lack of fine-grained analysis on the specific aspects and extent to which LLMs are influenced by misinformation. To bridge this gap, we present MisBench, the current largest and most comprehensive benchmark for evaluating LLMs' behavior and knowledge preference toward misinformation. MisBench consists of 10,346,712 pieces of misinformation, which uniquely considers both knowledge-based conflicts and stylistic variations in misinformation. Empirical results reveal that while LLMs demonstrate comparable abilities in discerning misinformation, they still remain susceptible to knowledge conflicts and stylistic variations. Based on these findings, we further propose a novel approach called Reconstruct to Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our study provides valuable insights into LLMs' interactions with misinformation, and we believe MisBench can serve as an effective benchmark for evaluating LLM-based detectors and enhancing their reliability in real-world applications. Codes and data are available at https://github.com/GKNL/MisBench.",does Misinformation Affect Large Language Model Behaviors Preferences Large Language Models LLMs shown remarkable capabilities knowledge intensive tasks remain vulnerable encountering misinformation Existing studies explored role LLMs combating misinformation lack fine grained analysis specific aspects extent LLMs influenced misinformation bridge gap present MisBench current largest comprehensive benchmark evaluating LLMs behavior knowledge preference misinformation MisBench consists 10 346 712 pieces misinformation uniquely considers knowledge based conflicts stylistic variations misinformation Empirical results reveal LLMs demonstrate comparable abilities discerning misinformation remain susceptible knowledge conflicts stylistic variations Based findings propose novel approach called Reconstruct Discriminate RtD strengthen LLMs ability detect misinformation study provides valuable insights LLMs interactions misinformation believe MisBench serve effective benchmark evaluating LLM based detectors enhancing reliability real world applications Codes data available https github com GKNL MisBench
714,YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering,"['Jennifer D’Souza', 'Hamed Babaei Giglou', 'Quentin Münch']","Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry.",YESciEval Robust LLM Judge Scientific Question Answering Large Language Models LLMs drive scientific question answering modern search engines evaluation robustness remains underexplored introduce YESciEval open source framework combines fine grained rubric based assessment reinforcement learning mitigate optimism bias LLM evaluators release multidisciplinary scienceQ datasets including adversarial variants evaluation scores multiple LLMs Independent proprietary models human feedback approach enables scalable cost free evaluation advancing reliable LLM judge models work supports AI alignment fosters robust transparent evaluation essential scientific inquiry
715,GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding,"['Ziyin Zhang', 'Hang Yu', 'Sage Lee', 'Peng Di', 'Jianguo Li', 'Rui Wang']","Programming languages possess rich semantic information - such as data flow - that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with seven different baseline LLMs ranging in size from 350M to 14B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.",GALLa Graph Aligned Large Language Models Improved Source Code Understanding Programming languages possess rich semantic information data flow represented graphs available surface form source code Recent code language models scaled billions parameters model source code solely text tokens ignoring structural information Conversely models encode structural information code make modifications Transformer architecture limiting scale compatibility pretrained LLMs work best worlds GALLa Graph Aligned Large Language Models GALLa utilizes graph neural networks cross modal alignment technologies inject structural information code LLMs auxiliary task finetuning framework model agnostic task agnostic applied code LLM code downstream task requires structural graph data training time corpus unrelated finetuning data incurring cost inference time baseline LLM Experiments code tasks seven different baseline LLMs ranging size 350M 14B validate effectiveness GALLa demonstrating consistent improvement baseline powerful models LLaMA3 Qwen2 5 Coder
716,MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis,"['Daniel Philip Rose', 'Chia-Chien Hung', 'Marco Lepri', 'Israa Alqassem', 'Kiril Gashteovski', 'Carolin Lawrence']","Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical decision-making, in which physicians iteratively refine a ranked list of possible diseases based on symptoms, antecedents, and medical knowledge. While recent advances in large language models (LLMs) have shown promise in supporting DDx, existing approaches face key limitations, including single-dataset evaluations, isolated optimization of components, unrealistic assumptions about complete patient profiles, and single-attempt diagnosis. We introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx, where diagnostic reasoning evolves through iterative learning, rather than assuming a complete patient profile is accessible. MEDDxAgent integrates three modular components: (1) an orchestrator (DDxDriver), (2) a history taking simulator, and (3) two specialized agents for knowledge retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We analyze single-turn diagnostic approaches and demonstrate the importance of iterative refinement when patient profiles are not available at the outset. Our broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy improvements in interactive DDx across both large and small LLMs, while offering critical explainability into its diagnostic reasoning process.",MEDDxAgent Unified Modular Agent Framework Explainable Automatic Differential Diagnosis Differential Diagnosis DDx fundamental complex aspect clinical decision making physicians iteratively refine ranked list possible diseases based symptoms antecedents medical knowledge recent advances large language models LLMs shown promise supporting DDx existing approaches face key limitations including single dataset evaluations isolated optimization components unrealistic assumptions complete patient profiles single attempt diagnosis introduce Modular Explainable DDx Agent MEDDxAgent framework designed interactive DDx diagnostic reasoning evolves iterative learning assuming complete patient profile accessible MEDDxAgent integrates modular components 1 orchestrator DDxDriver 2 history taking simulator 3 specialized agents knowledge retrieval diagnosis strategy ensure robust evaluation introduce comprehensive DDx benchmark covering respiratory skin rare diseases analyze single turn diagnostic approaches demonstrate importance iterative refinement patient profiles available outset broad evaluation demonstrates MEDDxAgent achieves 10 accuracy improvements interactive DDx large small LLMs offering critical explainability diagnostic reasoning process
717,A Training-free LLM-based Approach to General Chinese Character Error Correction,"['Houquan Zhou', 'Bo Zhang', 'Zhenghua Li', 'Ming Yan', 'Min Zhang']","Chinese spelling correction (CSC) is a crucial task that aims to correct character errors in Chinese text. While conventional CSC focuses on character substitution errors caused by mistyping, two other common types of character errors, missing and redundant characters, have received less attention. These errors are often excluded from CSC datasets during the annotation process or ignored during evaluation, even when they have been annotated. This issue limits the practicality of the CSC task. To address this issue, we introduce the task of General Chinese Character Error Correction (C2EC), which focuses on all three types of character errors. We construct a high-quality C2EC benchmark by combining and manually verifying data from CCTC and Lemon datasets. We extend the training-free prompt-free CSC method to C2EC by using Levenshtein distance for handling length changes and leveraging an additional prompt-based large language model (LLM) to improve performance. Experiments show that our method enables a 14B-parameter LLM to be on par with models nearly 50 times larger on both conventional CSC and C2EC tasks, without any fine-tuning.",Training free LLM based Approach General Chinese Character Error Correction Chinese spelling correction CSC crucial task aims correct character errors Chinese text conventional CSC focuses character substitution errors caused mistyping common types character errors missing redundant characters received attention errors excluded CSC datasets annotation process ignored evaluation annotated issue limits practicality CSC task address issue introduce task General Chinese Character Error Correction C2EC focuses types character errors construct high quality C2EC benchmark combining manually verifying data CCTC Lemon datasets extend training free prompt free CSC method C2EC using Levenshtein distance handling length changes leveraging additional prompt based large language model LLM improve performance Experiments method enables 14B parameter LLM par models nearly 50 times larger conventional CSC C2EC tasks fine tuning
718,HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models,"['Songtao Jiang', 'Yan Zhang', 'Yeying Jin', 'Zhihang Tang', 'Yangyang Wu', 'YANG FENG', 'Jian Wu', 'Zuozhu Liu']","Medical Vision-Language Models (Med-VLMs) have achieved success across various tasks, yet most existing methods overlook the modality misalignment issue that can lead to untrustworthy responses in clinical settings. In this paper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel approach that addresses two critical challenges in Med-VLM alignment: 1) Cost-effective generation of high-quality preference data; 2) Capturing nuanced and context-aware preferences for improved alignment. HSCR first leverages the inherent capability of Med-VLMs to generate dispreferred responses with higher sampling probability. By analyzing output logit shifts after visual token dropout, we identify modality-coupled tokens that induce misalignment and derive an implicit alignment reward function. This function guides token replacement with hallucinated ones during decoding, producing high-quality dispreferred data. Furthermore, HSCR introduces a multi-level preference optimization strategy, which extends beyond traditional adjacent-level optimization by incorporating nuanced implicit preferences, leveraging relative quality in dispreferred data to capture subtle alignment cues for more precise and context-aware optimization. Extensive experiments across multiple medical tasks, including Med-VQA, medical image captioning and instruction following, demonstrate that HSCR not only enhances zero-shot performance but also significantly improves modality alignment and trustworthiness with just 2,000 training entries.",HSCR Hierarchical Self Contrastive Rewarding Aligning Medical Vision Language Models Medical Vision Language Models Med VLMs achieved success various tasks existing methods overlook modality misalignment issue lead untrustworthy responses clinical settings paper propose Hierarchical Self Contrastive Rewarding HSCR novel approach addresses critical challenges Med VLM alignment 1 Cost effective generation high quality preference data 2 Capturing nuanced context aware preferences improved alignment HSCR leverages inherent capability Med VLMs generate dispreferred responses higher sampling probability analyzing output logit shifts visual token dropout identify modality coupled tokens induce misalignment derive implicit alignment reward function function guides token replacement hallucinated ones decoding producing high quality dispreferred data Furthermore HSCR introduces multi level preference optimization strategy extends traditional adjacent level optimization incorporating nuanced implicit preferences leveraging relative quality dispreferred data capture subtle alignment cues precise context aware optimization Extensive experiments multiple medical tasks including Med VQA medical image captioning instruction following demonstrate HSCR enhances zero shot performance significantly improves modality alignment trustworthiness just 2 000 training entries
719,MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale,"['Jiawei Guo', 'Tianyu Zheng', 'Yizhi LI', 'Yuelin Bai', 'Bo Li', 'Yubo Wang', 'King Zhu', 'Graham Neubig', 'Wenhu Chen', 'Xiang Yue']",,MAmmoTH VL Eliciting Multimodal Reasoning Instruction Tuning Scale
720,SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning,"['Prabhat Pandey', 'Rupak Vignesh Swaminathan', 'K V Vijay Girish', 'Arunasish Sen', 'Jian. Xie', 'Grant Strimel', 'Andreas Schwarz']","We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs.",SIFT 50M Large Scale Multilingual Dataset Speech Instruction Fine Tuning introduce SIFT Speech Instruction Fine Tuning 50M example dataset designed instruction fine tuning pre training speech text large language models LLMs SIFT 50M built publicly available speech corpora collectively contain 14K hours speech leverages LLMs shelf expert models dataset spans languages encompassing diverse range speech understanding controllable speech generation instructions Using SIFT 50M train SIFT LLM outperforms existing speech text LLMs instruction following benchmarks achieving competitive performance foundational speech tasks support research introduce EvalSIFT benchmark dataset specifically designed evaluate instruction following capabilities speech text LLMs
721,Recent Advances in Speech Language Models: A Survey,"['Wenqian Cui', 'Dianzhi Yu', 'Xiaoqi Jiao', 'Ziqiao Meng', 'Guangyan Zhang', 'Qichao Wang', 'Steven Y. Guo', 'Irwin King']","Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)"", where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion, significant latency due to the complex pipeline, and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) -- end-to-end models that generate speech without converting from text -- have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize their evaluation metrics, and discuss the challenges and future research directions in this rapidly evolving field. The GitHub repository is available at https://github.com/dreamtheater123/Awesome-SpeechLM-Survey",Recent Advances Speech Language Models Survey Large Language Models LLMs recently garnered significant attention primarily capabilities text based interactions natural human interaction relies speech necessitating shift voice based models straightforward approach achieve involves pipeline Automatic Speech Recognition ASR LLM Text Speech TTS input speech transcribed text processed LLM converted speech Despite straightforward method suffers inherent limitations information loss modality conversion significant latency complex pipeline error accumulation stages address issues Speech Language Models SpeechLMs end end models generate speech converting text emerged promising alternative survey paper provides comprehensive overview recent methodologies constructing SpeechLMs detailing key components architecture various training recipes integral development Additionally systematically survey various capabilities SpeechLMs categorize evaluation metrics discuss challenges future research directions rapidly evolving field GitHub repository available https github com dreamtheater123 Awesome SpeechLM Survey
722,LexCLiPR: Cross-Lingual Paragraph Retrieval from Legal Judgments,"['Rohit Upadhya', 'Santosh T.Y.S.S']",,LexCLiPR Cross Lingual Paragraph Retrieval Legal Judgments
723,Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries,"['Wenqiang Wang', 'Yan XIAO', 'Hao Lin', 'Yangshijie Zhang', 'Xiaochun Cao']",,Multi task Adversarial Attacks Black box Model shot Queries
724,SPECTRA: Faster Large Language Model Inference with Optimized Internal and External Speculation,"['Nguyen-Khang Le', 'Truong Dinh Do', 'Le-Minh Nguyen']",,SPECTRA Faster Large Language Model Inference Optimized Internal External Speculation
725,Multi-level Association Refinement Network for Dialogue Aspect-based Sentiment Quadruple Analysis,"['Zeliang Tong', 'Wei Wei', 'Xiaoye Qu', 'Rikui Huang', 'Zhixin Chen', 'Xingyu Yan']",,Multi level Association Refinement Network Dialogue Aspect based Sentiment Quadruple Analysis
726,Innovative Image Fraud Detection with Cross-Sample Anomaly Analysis: The Power of LLMs,"['QiWen Wang', 'Zhenghao Lin', 'Chen Lin', 'Junqi Yang', 'Zhenzhe Ying', 'Weiqiang Wang']",,Innovative Image Fraud Detection Cross Sample Anomaly Analysis Power LLMs
727,Cooperative or Competitive? Understanding the Interaction between Attention Heads From A Game Theory Perspective,"['Xiaoye Qu', 'Zengqi Yu', 'Dongrui Liu', 'Wei Wei', 'Daizong Liu', 'Jianfeng Dong', 'Yu Cheng']",,Cooperative Competitive Understanding Interaction Attention Heads Game Theory Perspective
728,MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification,"['Linzhuang Sun', 'Hao Liang', 'Jingxuan Wei', 'Bihui Yu', 'Tianpeng Li', 'Fan Yang', 'Zenan Zhou', 'Wentao Zhang']","According to the Test-Time Scaling, the integration of External Slow-Thinking with the Verify mechanism has been demonstrated to enhance multi-round reasoning in large language models (LLMs). However, in the multimodal (MM) domain, there is still a lack of a strong MM-Verifier. In this paper, we introduce MM-Verifier and MM-Reasoner to enhance multimodal reasoning through longer inference and more robust verification. First, we propose a two-step MM verification data synthesis method, which combines a simulation-based tree search with verification and uses rejection sampling to generate high-quality Chain-of-Thought (COT) data. This data is then used to fine-tune the verification model, MM-Verifier. Additionally, we present a more efficient method for synthesizing MMCOT data, bridging the gap between text-based and multimodal reasoning. The synthesized data is used to fine-tune MM-Reasoner. Our MM-Verifier outperforms all larger models on the MathCheck, MathVista, and MathVerse benchmarks. Moreover, MM-Reasoner demonstrates strong effectiveness and scalability, with performance improving as data size increases. Finally, our approach achieves strong performance when combining MM-Reasoner and MM-Verifier, reaching an accuracy of 65.3 on MathVista, surpassing GPT-4o (63.8) with 12 rollouts.",MM Verify Enhancing Multimodal Reasoning Chain Thought Verification According Test Time Scaling integration External Slow Thinking Verify mechanism demonstrated enhance multi round reasoning large language models LLMs multimodal MM domain lack strong MM Verifier paper introduce MM Verifier MM Reasoner enhance multimodal reasoning longer inference robust verification propose step MM verification data synthesis method combines simulation based tree search verification uses rejection sampling generate high quality Chain Thought COT data data used fine tune verification model MM Verifier Additionally present efficient method synthesizing MMCOT data bridging gap text based multimodal reasoning synthesized data used fine tune MM Reasoner MM Verifier outperforms larger models MathCheck MathVista MathVerse benchmarks MM Reasoner demonstrates strong effectiveness scalability performance improving data size increases Finally approach achieves strong performance combining MM Reasoner MM Verifier reaching accuracy 65 3 MathVista surpassing GPT 4o 63 8 12 rollouts
729,Graph-Structured Trajectory Extraction from Travelogues,"['Aitaro Yamamoto', 'Hiroyuki Otomo', 'Hiroki Ouchi', 'Shohei Higashiyama', 'Hiroki Teranishi', 'Hiroyuki Shindo', 'Taro Watanabe']","Previous studies on sequence-based extraction of human movement trajectories have an issue of inadequate trajectory representation. Specifically, a pair of locations may not be lined up in a sequence especially when one location includes the other geographically. In this study, we propose a graph representation that retains information on the geographic hierarchy as well as the temporal order of visited locations, and have constructed a benchmark dataset for graph-structured trajectory extraction. The experiments with our baselines have demonstrated that it is possible to accurately predict visited locations and the order among them, but it remains a challenge to predict the hierarchical relations.",Graph Structured Trajectory Extraction Travelogues Previous studies sequence based extraction human movement trajectories issue inadequate trajectory representation Specifically pair locations lined sequence especially location includes geographically study propose graph representation retains information geographic hierarchy temporal order visited locations constructed benchmark dataset graph structured trajectory extraction experiments baselines demonstrated possible accurately predict visited locations order remains challenge predict hierarchical relations
730,Learning First-Order Logic Rules for Argumentation Mining,"['Yang Sun', 'Guanrong Chen', 'Hamid Alinejad-Rokny', 'Jianzhu Bao', 'Yuqi Huang', 'Bin Liang', 'Kam-Fai Wong', 'Min Yang', 'Ruifeng Xu']",,Learning Order Logic Rules Argumentation Mining
731,Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency,"['Jiafeng Liang', 'Shixin Jiang', 'Xuan Dong', 'Ning Wang', 'Zheng Chu', 'Hui Su', 'Jinlan Fu', 'Ming Liu', 'See-Kiong Ng', 'Bing Qin']","Large Multimodal Models (LMMs) have recently demonstrated impressive performance on general video comprehension benchmarks. Nevertheless, for broader applications, the robustness of their temporal analysis capability needs to be thoroughly investigated yet predominantly ignored. Motivated by this, we propose a novel temporal robustness benchmark (TemRobBench), which introduces temporal inconsistency perturbations separately at the visual and textual modalities to assess the robustness of models. We evaluate 16 mainstream LMMs and find that they exhibit over-reliance on prior knowledge and textual context in adversarial environments, while ignoring the actual temporal dynamics in the video. To mitigate this issue, we design panoramic direct preference optimization (PanoDPO), which encourages LMMs to incorporate both visual and linguistic feature preferences simultaneously. Experimental results show that PanoDPO can effectively enhance the model's robustness and reliability in temporal analysis.",Investigating Enhancing Robustness Large Multimodal Models Temporal Inconsistency Large Multimodal Models LMMs recently demonstrated impressive performance general video comprehension benchmarks broader applications robustness temporal analysis capability needs thoroughly investigated predominantly ignored Motivated propose novel temporal robustness benchmark TemRobBench introduces temporal inconsistency perturbations separately visual textual modalities assess robustness models evaluate 16 mainstream LMMs exhibit reliance prior knowledge textual context adversarial environments ignoring actual temporal dynamics video mitigate issue design panoramic direct preference optimization PanoDPO encourages LMMs incorporate visual linguistic feature preferences simultaneously Experimental results PanoDPO effectively enhance model s robustness reliability temporal analysis
732,UniRAG: Unified Query Understanding Method for Retrieval Augmented Generation,"['Rui Li', 'Liyang He', 'Qi Liu', 'Zheng Zhang', 'Heng Yu', 'Yuyang Ye', 'Linbo Zhu', 'Yu Su']",,UniRAG Unified Query Understanding Method Retrieval Augmented Generation
733,Contextual Experience Replay for Continual Learning of Language Agents,"['Yitao Liu', 'Chenglei Si', 'Karthik R Narasimhan', 'Shunyu Yao']",,Contextual Experience Replay Continual Learning Language Agents
734,Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning,"['Qi Sun', 'Pengfei Hong', 'Tej Deep Pala', 'Vernon Toh', 'U-Xuan Tan', 'Deepanway Ghosal', 'Soujanya Poria']",,Emma X Embodied Multimodal Action Model Grounded Chain Thought Look ahead Spatial Reasoning
735,"Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method","['Yupei Ren', 'Xinyi Zhou', 'Ning Zhang', 'Shangqing Zhao', 'Man Lan', 'Xiaopeng Bai']","Argument mining has garnered increasing attention over the years, with the recent advancement of Large Language Models (LLMs) further propelling this trend. However, current argument relations remain relatively simplistic and foundational, struggling to capture the full scope of argument information, particularly when it comes to representing complex argument structures in real-world scenarios. To address this limitation, we propose 14 fine-grained relation types from both vertical and horizontal dimensions, thereby capturing the intricate interplay between argument components for a thorough understanding of argument structure. On this basis, we conducted extensive experiments on three tasks: argument component detection, relation prediction, and automated essay grading. Additionally, we explored the impact of writing quality on argument component detection and relation prediction, as well as the connections between discourse relations and argumentative features. The findings highlight the importance of fine-grained argumentative annotations for argumentative writing quality assessment and encourage multi-dimensional argument analysis.",Comprehensive Argument Analysis Education Dataset Tasks Method Argument mining garnered increasing attention years recent advancement Large Language Models LLMs propelling trend current argument relations remain relatively simplistic foundational struggling capture scope argument information particularly comes representing complex argument structures real world scenarios address limitation propose 14 fine grained relation types vertical horizontal dimensions capturing intricate interplay argument components thorough understanding argument structure basis conducted extensive experiments tasks argument component detection relation prediction automated essay grading Additionally explored impact writing quality argument component detection relation prediction connections discourse relations argumentative features findings highlight importance fine grained argumentative annotations argumentative writing quality assessment encourage multi dimensional argument analysis
736,Browsing Like Human: A Multimodal Web Agent with Experiential Fast-and-Slow Thinking,"['Haohao Luo', 'Jiayi Kuang', 'Wei Liu', 'Ying Shen', 'Jian Luan', 'Yang Deng']",,Browsing Like Human Multimodal Web Agent Experiential Fast Slow Thinking
737,MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation,"['Yile Liu', 'Ziwei Ma', 'Xiu Jiang', 'Jinglu HU', 'ChangJing', 'Liang Li']","With the rapid adoption of large language models (LLMs) in natural language processing, the ability to follow instructions has emerged as a key metric for evaluating their practical utility. However, existing evaluation methods often focus on single-language scenarios, overlooking the challenges and differences present in multilingual and cross-lingual contexts. To address this gap, we introduce MaXIFE: a comprehensive evaluation benchmark designed to assess instruction-following capabilities across 23 different languages with 1667 verifiable instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to evaluate several leading commercial LLMs, establishing baseline results for future comparisons. By providing a standardized tool for multilingual instruction-following evaluation, MaXIFE aims to advance research and development in natural language processing.",MaXIFE Multilingual Cross lingual Instruction Following Evaluation rapid adoption large language models LLMs natural language processing ability follow instructions emerged key metric evaluating practical utility existing evaluation methods focus single language scenarios overlooking challenges differences present multilingual cross lingual contexts address gap introduce MaXIFE comprehensive evaluation benchmark designed assess instruction following capabilities 23 different languages 1667 verifiable instruction tasks MaXIFE integrates Rule Based Evaluation Model Based Evaluation ensuring balance efficiency accuracy applied MaXIFE evaluate leading commercial LLMs establishing baseline results future comparisons providing standardized tool multilingual instruction following evaluation MaXIFE aims advance research development natural language processing
738,Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning,"['Guijin Son', 'Jiwoo Hong', 'Hyunwoo Ko', 'James Thorne']","Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although ""thinking LLMs"" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.",Linguistic Generalizability Test Time Scaling Mathematical Reasoning Scaling pre training compute proven effective achieving mulitlinguality does hold test time scaling work introduce MCLM multilingual math benchmark featuring competition level problems 55 languages test test time scaling methods Outcome Reward Modeling ORM Process Reward Modeling ORM Budget Forcing BF Qwen2 5 1 5B Math MR1 1 5B multilingual LLM trained extended reasoning experiments using Qwen2 5 1 5B Math ORM achieves score 35 8 MCLM BF MR1 1 5B attains 35 2 thinking LLMs recently garnered significant attention performance comparable traditional scaling methods like best N constrained similar levels inference FLOPs BF yields 20 point improvement English AIME provides 1 94 point average gain languages pattern consistent test time scaling methods studied higlighting test time scaling generalize effectively multilingual tasks foster research release MCLM MR1 1 5B evaluation results
739,Can MLLMs Understand the Deep Implication Behind Chinese Images?,"['Chenhao Zhang', 'Xi Feng', 'Yuelin Bai', 'Xeron Du', 'Jinchang Hou', 'Kaixin Deng', 'Guangzeng Han', 'Qinrui Li', 'Bingli Wang', 'Jiaheng Liu', 'Xingwei Qu', 'Yifei Zhang', 'Qixuan Zhao', 'Yiming Liang', 'Ziqiang Liu', 'Feiteng Fang', 'Min Yang', 'Wenhao Huang', 'Chenghua Lin', 'Ge Zhang', 'Shiwen Ni']","As the capabilities of Multimodal Large Language Models (MLLMs) continue to improve, the need for higher-order capability evaluation of MLLMs is increasing. However, there is a lack of work evaluating MLLM for higher-order perception and understanding of Chinese visual content. To fill the gap, we introduce the **C**hinese **I**mage **I**mplication understanding **Bench**mark, **CII-Bench**, which aims to assess the higher-order perception and understanding capabilities of MLLMs for Chinese images. CII-Bench stands out in several ways compared to existing benchmarks. Firstly, to ensure the authenticity of the Chinese context, images in CII-Bench are sourced from the Chinese Internet and manually reviewed, with corresponding answers also manually crafted. Additionally, CII-Bench incorporates images that represent Chinese traditional culture, such as famous Chinese traditional paintings, which can deeply reflect the model's understanding of Chinese traditional culture. Through extensive experiments on CII-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on CII-Bench. The highest accuracy of MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an impressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional culture images, suggesting limitations in their ability to understand high-level semantics and lack a deep knowledge base of Chinese traditional culture. Finally, it is observed that most models exhibit enhanced accuracy when image emotion hints are incorporated into the prompts. We believe that CII-Bench will enable MLLMs to gain a better understanding of Chinese semantics and Chinese-specific images, advancing the journey towards expert artificial general intelligence (AGI). Our project is publicly available at https://cii-bench.github.io/.",MLLMs Understand Deep Implication Chinese Images capabilities Multimodal Large Language Models MLLMs continue improve need higher order capability evaluation MLLMs increasing lack work evaluating MLLM higher order perception understanding Chinese visual content gap introduce C hinese mage mplication understanding Bench mark CII Bench aims assess higher order perception understanding capabilities MLLMs Chinese images CII Bench stands ways compared existing benchmarks Firstly ensure authenticity Chinese context images CII Bench sourced Chinese Internet manually reviewed corresponding answers manually crafted Additionally CII Bench incorporates images represent Chinese traditional culture famous Chinese traditional paintings deeply reflect model s understanding Chinese traditional culture extensive experiments CII Bench multiple MLLMs significant findings Initially substantial gap observed performance MLLMs humans CII Bench highest accuracy MLLMs attains 64 4 human accuracy averages 78 2 peaking impressive 81 0 Subsequently MLLMs perform worse Chinese traditional culture images suggesting limitations ability understand high level semantics lack deep knowledge base Chinese traditional culture Finally observed models exhibit enhanced accuracy image emotion hints incorporated prompts believe CII Bench enable MLLMs gain better understanding Chinese semantics Chinese specific images advancing journey expert artificial general intelligence AGI project publicly available https cii bench github io
740,"KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan","['Mukhammed Togmanov', 'Nurdaulet Mukhituly', 'Diana Turmakhan', 'Jonibek Mansurov', 'Maiya Goloburda', 'Akhmed Sakip', 'Zhuohan Xie', 'Yuxia Wang', 'Bekassyl Syzdykov', 'Nurkhan Laiyk', 'Alham Fikri Aji', 'Ekaterina Kochmar', 'Preslav Nakov', 'Fajri Koto']","Despite having a population of twenty million, Kazakhstan's culture and language remain underrepresented in the field of natural language processing. Although large language models (LLMs) continue to advance worldwide, progress in Kazakh language has been limited, as seen in the scarcity of dedicated models and benchmark evaluations. To address this gap, we introduce KazMMLU, the first MMLU-style dataset specifically designed for Kazakh language. KazMMLU comprises 23,000 questions that cover various educational levels, including STEM, humanities, and social sciences, sourced from authentic educational materials and manually validated by native speakers and educators. The dataset includes 10,969 Kazakh questions and 12,031 Russian questions, reflecting Kazakhstan's bilingual education system and rich local context. Our evaluation of several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4, and DeepSeek V3) demonstrates substantial room for improvement, as even the best-performing models struggle to achieve competitive performance in Kazakh and Russian. These findings underscore significant performance gaps compared to high-resource languages. We hope that our dataset will enable further research and development of Kazakh-centric LLMs. Data and code will be made available upon acceptance.",KazMMLU Evaluating Language Models Kazakh Russian Regional Knowledge Kazakhstan Despite having population million Kazakhstan s culture language remain underrepresented field natural language processing large language models LLMs continue advance worldwide progress Kazakh language limited seen scarcity dedicated models benchmark evaluations address gap introduce KazMMLU MMLU style dataset specifically designed Kazakh language KazMMLU comprises 23 000 questions cover various educational levels including STEM humanities social sciences sourced authentic educational materials manually validated native speakers educators dataset includes 10 969 Kazakh questions 12 031 Russian questions reflecting Kazakhstan s bilingual education rich local context evaluation state art multilingual models Llama 3 1 Qwen 2 5 GPT 4 DeepSeek V3 demonstrates substantial room improvement best performing models struggle achieve competitive performance Kazakh Russian findings underscore significant performance gaps compared high resource languages hope dataset enable research development Kazakh centric LLMs Data code available acceptance
741,Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering,"['Songtao Jiang', 'Chenyi Zhou', 'Yan Zhang', 'Yeying Jin', 'Zuozhu Liu']","Multimodal large language models (MLLMs) still struggle with complex reasoning tasks in Visual Question Answering (VQA). While current methods have advanced by incorporating visual prompts, our study uncovers critical limitations: these approaches indiscriminately annotate all detected objects for every visual question, generating excessive visual markers that degrade task performance. This issue stems primarily from a lack of focus on key visual elements, raising two important questions: Are all objects equally important, and do all questions require visual prompts? Motivated by Dual Process Theory, which distinguishes between instinctive and deliberate cognitive modes in human reasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts to the complexity of questions, combining fast intuitive judgments with deliberate analytical reasoning to enhance the vision-language reasoning capability of the MLLM. For straightforward questions, FOCUS supports efficient zero-shot reasoning. For more complex tasks, it employs the conceptualizing before observation strategy to highlight critical elements. Extensive experiments on four benchmarks, ScienceQA, TextQA, VizWiz, and MME, demonstrate that FOCUS consistently improves the performance of both open-source and black-box MLLMs, achieving significant gains across all datasets. Ablation studies further validate the importance of combining diverse cognitive strategies with refined visual information for superior performance. Code will be released.",Fast Slow Integrating Fast Intuition Deliberate Thinking Enhancing Visual Question Answering Multimodal large language models MLLMs struggle complex reasoning tasks Visual Question Answering VQA current methods advanced incorporating visual prompts study uncovers critical limitations approaches indiscriminately annotate detected objects visual question generating excessive visual markers degrade task performance issue stems primarily lack focus key visual elements raising important questions objects equally important questions require visual prompts Motivated Dual Process Theory distinguishes instinctive deliberate cognitive modes human reasoning propose FOCUS plug play approach dynamically adapts complexity questions combining fast intuitive judgments deliberate analytical reasoning enhance vision language reasoning capability MLLM straightforward questions FOCUS supports efficient zero shot reasoning complex tasks employs conceptualizing observation strategy highlight critical elements Extensive experiments benchmarks ScienceQA TextQA VizWiz MME demonstrate FOCUS consistently improves performance open source black box MLLMs achieving significant gains datasets Ablation studies validate importance combining diverse cognitive strategies refined visual information superior performance Code released
742,Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages,"['Hyangsuk Min', 'Yuho Lee', 'Minjeong Ban', 'Jiaqi Deng', 'Nicole Hee-Yeon Kim', 'Taewon Yun', 'Hang Su', 'Jason Cai', 'Hwanjun Song']","Evaluation frameworks for text summarization have evolved in terms of both domain coverage and metrics. However, existing benchmarks still lack domain-specific assessment criteria, remain predominantly English-centric, and face challenges with human annotation due to the complexity of reasoning. To address these, we introduce MSumBench, which provides a multi-dimensional, multi-domain evaluation of summarization in English and Chinese. It also incorporates specialized assessment criteria for each domain and leverages a multi-agent debate system to enhance annotation quality. By evaluating eight modern summarization models, we discover distinct performance patterns across domains and languages. We further examine large language models as summary evaluators, analyzing the correlation between their evaluation and summarization capabilities, and uncovering systematic bias in their assessment of self-generated summaries. Our benchmark dataset is publicly available at https://github.com/DISL-Lab/MSumBench.",Multi dimensional Evaluation LLM Summarization Domains Languages Evaluation frameworks text summarization evolved terms domain coverage metrics existing benchmarks lack domain specific assessment criteria remain predominantly English centric face challenges human annotation complexity reasoning address introduce MSumBench provides multi dimensional multi domain evaluation summarization English Chinese incorporates specialized assessment criteria domain leverages multi agent debate enhance annotation quality evaluating modern summarization models discover distinct performance patterns domains languages examine large language models summary evaluators analyzing correlation evaluation summarization capabilities uncovering systematic bias assessment self generated summaries benchmark dataset publicly available https github com DISL Lab MSumBench
743,ClusterAttn: KV Cache Compression under Intrinsic Attention Clustering,"['Minwei Zhang', 'Haifeng Sun', 'Jingyu Wang', 'Shaolong Li', 'Wanyi Ning', 'Qi Qi', 'Zirui Zhuang', 'Jianxin Liao']",,ClusterAttn KV Cache Compression Intrinsic Attention Clustering
744,SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script,"['Eunwon Kim', 'Chanho Park', 'Buru Chang']","Shared memories between two individuals strengthen their bond and are crucial for facilitating their ongoing conversations. This study aims to make long-term dialogue more engaging by leveraging these shared memories. To this end, we introduce a new long-term dialogue dataset named SHARE, constructed from movie scripts, which are a rich source of shared memories among various relationships. Our dialogue dataset contains the summaries of persona information and events of two individuals, as explicitly revealed in their conversation, along with implicitly extractable shared memories. We also introduce EPISODE, a long-term dialogue framework based on SHARE that utilizes shared experiences between individuals. Through experiments using SHARE, we demonstrate that shared memories between two individuals make long-term dialogues more engaging and sustainable, and that EPISODE effectively manages shared memories during dialogue. Our dataset and code are available at https://github.com/e1kim/SHARE.",SHARE Shared Memory Aware Open Domain Long Term Dialogue Dataset Constructed Movie Script Shared memories individuals strengthen bond crucial facilitating ongoing conversations study aims make long term dialogue engaging leveraging shared memories end introduce new long term dialogue dataset named SHARE constructed movie scripts rich source shared memories various relationships dialogue dataset contains summaries persona information events individuals explicitly revealed conversation implicitly extractable shared memories introduce EPISODE long term dialogue framework based SHARE utilizes shared experiences individuals experiments using SHARE demonstrate shared memories individuals make long term dialogues engaging sustainable EPISODE effectively manages shared memories dialogue dataset code available https github com e1kim SHARE
745,Incongruity-aware Tension Field Network for Multi-modal Sarcasm Detection,"['Jiecheng Zhang', 'C.L.Philip Chen', 'Shuzhen Li', 'Tong Zhang']",,Incongruity aware Tension Field Network Multi modal Sarcasm Detection
746,Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh,"['Nurkhan Laiyk', 'Daniil Orel', 'Rituraj Joshi', 'Maiya Goloburda', 'Yuxia Wang', 'Preslav Nakov', 'Fajri Koto']","Instruction tuning in low-resource languages remains underexplored due to limited text data, particularly in government and cultural domains. To address this, we introduce and open-source a large-scale (10,600 samples) instruction-following (IFT) dataset, covering key institutional and cultural knowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of procedural, legal, and structural governance topics. We employ LLM-assisted data generation, comparing open-weight and closed-weight models for dataset construction, and select GPT-4o as the backbone. Each entity of our dataset undergoes full manual verification to ensure high quality. We also show that fine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent performance improvements in both multiple-choice and generative tasks, demonstrating the potential of LLM-assisted instruction tuning for low-resource languages.",Instruction Tuning Public Government Cultural Data Low Resource Language Case Study Kazakh Instruction tuning low resource languages remains underexplored limited text data particularly government cultural domains address introduce open source large scale 10 600 samples instruction following IFT dataset covering key institutional cultural knowledge relevant Kazakhstan dataset enhances LLMs understanding procedural legal structural governance topics employ LLM assisted data generation comparing open weight closed weight models dataset construction select GPT 4o backbone entity dataset undergoes manual verification ensure high quality fine tuning Qwen Falcon Gemma dataset leads consistent performance improvements multiple choice generative tasks demonstrating potential LLM assisted instruction tuning low resource languages
747,Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack,"['Chenxi Dai', 'Lin Lu', 'Pan Zhou']","Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes \textit{activation inversion attack} (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.",Stealing Training Data Large Language Models Decentralized Training Activation Inversion Attack Decentralized training resource efficient framework democratize training large language models LLMs privacy risks associated framework particularly potential inclusion sensitive data training datasets remain unexplored paper identifies novel realistic attack surface privacy leakage training data decentralized training proposes textit activation inversion attack AIA time AIA constructs shadow dataset comprising text labels corresponding activations using public datasets Leveraging dataset attack model trained reconstruct training data activations victim decentralized training conduct extensive experiments various LLMs publicly available datasets demonstrate susceptibility decentralized training AIA findings highlight urgent need enhance security measures decentralized training mitigate privacy risks training LLMs
748,From Selection to Generation: A Survey of LLM-based Active Learning,"['Yu Xia', 'Subhojyoti Mukherjee', 'Zhouhang Xie', 'Junda Wu', 'Xintong Li', 'Ryan Aponte', 'Hanjia Lyu', 'Joe Barrow', 'Hongjie Chen', 'Franck Dernoncourt', 'Branislav Kveton', 'Tong Yu', 'Ruiyi Zhang', 'Jiuxiang Gu', 'Nesreen K. Ahmed', 'Yu Wang', 'Xiang Chen', 'Hanieh Deilamsalehy', 'Sungchul Kim', 'Zhengmian Hu', 'Yue Zhao', 'Nedim Lipka', 'Seunghyun Yoon', 'Ting-Hao Kenneth Huang', 'Zichao Wang', 'Puneet Mathur', 'Soumyabrata Pal', 'Koyel Mukherjee', 'Zhehao Zhang', 'Namyong Park', 'Thien Huu Nguyen', 'Jiebo Luo', 'Ryan A. Rossi', 'Julian McAuley']","Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training. In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations. Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning. We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop. We further examine the impact of AL on LLM learning paradigms and its applications across various domains. Finally, we identify open challenges and propose future research directions. This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.",Selection Generation Survey LLM based Active Learning Active Learning AL powerful paradigm improving model efficiency performance selecting informative data points labeling training recent active learning frameworks Large Language Models LLMs employed selection generating entirely new data instances providing cost effective annotations Motivated increasing importance high quality data efficient model training era LLMs present comprehensive survey LLM based Active Learning introduce intuitive taxonomy categorizes techniques discuss transformative roles LLMs play active learning loop examine impact AL LLM learning paradigms applications various domains Finally identify open challenges propose future research directions survey aims serve date resource researchers practitioners seeking gain intuitive understanding LLM based AL techniques deploy new applications
749,OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation,"['Qinglin Zhang', 'Luyao Cheng', 'Chong Deng', 'Qian Chen', 'Wen Wang', 'Siqi Zheng', 'Jiaqing Liu', 'Hai Yu', 'Chao-Hong Tan', 'Zhihao Du', 'ShiLiang Zhang']","Full-duplex spoken dialogue systems significantly surpass traditional turn-based dialogue systems, as they allow simultaneous bidirectional communication, closely mirroring human-human interactions. However, achieving low latency and natural interactions in full-duplex dialogue systems remains a significant challenge, especially considering human conversation dynamics such as interruptions, backchannels, and overlapping speech. In this paper, we introduce a novel End-to-End GPT-based model OmniFlatten for full-duplex conversation, capable of effectively modeling the complex behaviors inherent to natural conversations with low latency. To achieve full-duplex conversation capabilities, we propose a multi-stage post-training scheme that progressively adapts a text large language model (LLM) backbone into a speech-text dialogue LLM, capable of generating text and speech in real time, without modifying the architecture of the backbone LLM. The training process comprises three stages: modality alignment, half-duplex dialogue learning, and full-duplex dialogue learning. In all training stages, we standardize the data using a flattening operation, which enables unifying the training methods and the GPT backbone across different modalities and tasks. Our approach offers a simple modeling technique and a promising research direction for developing efficient and natural end-to-end full-duplex spoken dialogue systems. Audio samples of dialogues generated by OmniFlatten can be found at this web site (https://omniflatten.github.io/).",OmniFlatten End end GPT Model Seamless Voice Conversation duplex spoken dialogue systems significantly surpass traditional turn based dialogue systems allow simultaneous bidirectional communication closely mirroring human human interactions achieving low latency natural interactions duplex dialogue systems remains significant challenge especially considering human conversation dynamics interruptions backchannels overlapping speech paper introduce novel End End GPT based model OmniFlatten duplex conversation capable effectively modeling complex behaviors inherent natural conversations low latency achieve duplex conversation capabilities propose multi stage post training scheme progressively adapts text large language model LLM backbone speech text dialogue LLM capable generating text speech real time modifying architecture backbone LLM training process comprises stages modality alignment half duplex dialogue learning duplex dialogue learning training stages standardize data using flattening operation enables unifying training methods GPT backbone different modalities tasks approach offers simple modeling technique promising research direction developing efficient natural end end duplex spoken dialogue systems Audio samples dialogues generated OmniFlatten web site https omniflatten github io
750,DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning,"['Dohoon Kim', 'Donghun Kang', 'Taesup Moon']","Domain-Adaptive Pre-training (DAP) has recently gained attention for its effectiveness in fine-tuning pre-trained models. Building on this, continual DAP has been explored to develop pre-trained models capable of incrementally incorporating different domain datasets. However, existing continual DAP methods face several limitations: (1) high computational cost and GPU memory usage during training; (2) sensitivity to incremental data order; and (3) providing a single, generalized model for all end tasks, which contradicts the essence of DAP. In this paper, we propose DoMIX, a novel approach that addresses these challenges by leveraging LoRA modules, a representative parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient and parallel domain-adaptive pre-training that is robust to domain order and effectively utilizes accumulated knowledge to provide tailored pre-trained models for specific tasks. We also demonstrate that our method can be extended beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available at https://github.com/dohoonkim-ai/DoMIX.",DoMIX Efficient Framework Exploiting Domain Knowledge Fine Tuning Domain Adaptive Pre training DAP recently gained attention effectiveness fine tuning pre trained models Building continual DAP explored develop pre trained models capable incrementally incorporating different domain datasets existing continual DAP methods face limitations 1 high computational cost GPU memory usage training 2 sensitivity incremental data order 3 providing single generalized model end tasks contradicts essence DAP paper propose DoMIX novel approach addresses challenges leveraging LoRA modules representative parameter efficient fine tuning PEFT method approach enables efficient parallel domain adaptive pre training robust domain order effectively utilizes accumulated knowledge provide tailored pre trained models specific tasks demonstrate method extended DAP setting standard LLM fine tuning scenarios Code available https github com dohoonkim ai DoMIX
751,EAGLE: Expert-Guided Self-Enhancement for Preference Alignment in Pathology Large Vision-Language Model,"['Meidan Ding', 'Jipeng Zhang', 'Wenxuan Wang', 'Haiqin Zhong', 'Xiaoqin Wang', 'XINHENG LYU', 'Wenting Chen', 'Linlin Shen']",,EAGLE Expert Guided Self Enhancement Preference Alignment Pathology Large Vision Language Model
752,CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from In-Context Demonstrations,"['Vignesh Kothapalli', 'Hamed Firooz', 'Maziar Sanjabi']",,CoT ICL Lab Petri Dish Studying Chain Thought Learning Context Demonstrations
753,Flexora: Flexible Low-Rank Adaptation for Large Language Models,"['Chenxing Wei', 'Yao Shu', 'Ying Tiffany He', 'Fei Yu']",,Flexora Flexible Low Rank Adaptation Large Language Models
754,QDTSynth: Quality-Driven Formal Theorem Synthesis for Enhancing Proving Performance of LLMs,"['Lei Wang', 'Ruobing Zuo', 'Gaolei He', 'Jianlin Wang', 'Zhengfeng Yang']",,QDTSynth Quality Driven Formal Theorem Synthesis Enhancing Proving Performance LLMs
755,RSVP: Reasoning Segmentation via Visual Prompting and Multi-modal Chain-of-Thought,"['Yi Lu', 'Jiawang Cao', 'Yongliang Wu', 'Bozheng Li', 'Licheng Tang', 'Yangguang Ji', 'Chong Wu', 'Jay Wu', 'Wenbo Zhu']","Multi-modal Large Language Models (MLLMs) have demonstrated remarkable reasoning capability while lack explicit mechanisms for visual grounding and segmentation, creating a gap between cognitive reasoning and visual perception. To bridge this gap, we introduce Reasoning Segmentation via Visual Prompting (RSVP), a novel framework that unifies multi-step multimodal reasoning with grounded visual understanding. RSVP is a two-stage structuralized framework that integrates reasoning-driven localization with segmentation refinement. In the reasoning stage, RSVP employs multimodal chain-of-thought visual prompts to help MLLMs understand queries and infer targets, generating interpretable region proposals that enhance visual grounding. In segmentation stage, RSVP refines these proposals with a Vision-Language Segmentation Module (VLSM), seamlessly integrates textual and visual cues to produce precise segmentation masks. By explicitly modelling the interaction between multimodal reasoning and segmentation, RSVP introduces a new paradigm for interpretable reasoning segmentation. It exploits MLLMs' inherent localization capabilities, enabling the models to not only reason about objects but also generate structured visual representations. Our extensive experiments demonstrate that RSVP achieves state-of-the-art performance, surpasses state-of-the-art methods by up to +6.5 gIoU and +9.2 cIoU on ReasonSeg, and achieves 49.7 mAP on SegInW under zero-shot settings. These results validate RSVP as an effective and scalable framework for integrating cognitive reasoning with structured visual understanding.",RSVP Reasoning Segmentation Visual Prompting Multi modal Chain Thought Multi modal Large Language Models MLLMs demonstrated remarkable reasoning capability lack explicit mechanisms visual grounding segmentation creating gap cognitive reasoning visual perception bridge gap introduce Reasoning Segmentation Visual Prompting RSVP novel framework unifies multi step multimodal reasoning grounded visual understanding RSVP stage structuralized framework integrates reasoning driven localization segmentation refinement reasoning stage RSVP employs multimodal chain thought visual prompts help MLLMs understand queries infer targets generating interpretable region proposals enhance visual grounding segmentation stage RSVP refines proposals Vision Language Segmentation Module VLSM seamlessly integrates textual visual cues produce precise segmentation masks explicitly modelling interaction multimodal reasoning segmentation RSVP introduces new paradigm interpretable reasoning segmentation exploits MLLMs inherent localization capabilities enabling models reason objects generate structured visual representations extensive experiments demonstrate RSVP achieves state art performance surpasses state art methods 6 5 gIoU 9 2 cIoU ReasonSeg achieves 49 7 mAP SegInW zero shot settings results validate RSVP effective scalable framework integrating cognitive reasoning structured visual understanding
756,QAEval: Mixture of Evaluators for Question-Answering Task Evaluation,"['Tan Yue', 'Rui Mao', 'xuzhao Shi', 'SHUO ZHAN', 'Zuhao Yang', 'Dongyan Zhao']",,QAEval Mixture Evaluators Question Answering Task Evaluation
757,Debiasing the Fine-Grained Classification Task in LLMs with Bias-Aware PEFT,"['Daiying Zhao', 'Xinyu Yang', 'Hang Chen']",,Debiasing Fine Grained Classification Task LLMs Bias Aware PEFT
758,Demystifying Small Language Models for Edge Deployment,"['Zhenyan Lu', 'Xiang Li', 'Dongqi Cai', 'Rongjie Yi', 'Fangming Liu', 'Wei Liu', 'Jian Luan', 'Xiwen Zhang', 'Nicholas D. Lane', 'Mengwei Xu']",,Demystifying Small Language Models Edge Deployment
759,"Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models","['Naibin Gu', 'Peng Fu', 'Xiyu Liu', 'Ke Ma', 'Zheng Lin', 'Weiping Wang']","Parameter-efficient fine-tuning (PEFT) has become a common method for fine-tuning large language models, where a base model can serve multiple users through PEFT module switching. To enhance user experience, base models require periodic updates. However, once updated, PEFT modules fine-tuned on previous versions often suffer substantial performance degradation on newer versions. Re-tuning these numerous modules to restore performance would incur significant computational costs. Through a comprehensive analysis of the changes that occur during base model updates, we uncover an interesting phenomenon: continual training primarily affects task-specific knowledge stored in Feed-Forward Networks (FFN), while having less impact on the task-specific pattern in the Attention mechanism. Based on these findings, we introduce Trans-PEFT, a novel approach that enhances the PEFT module by focusing on the task-specific pattern while reducing its dependence on certain knowledge in the base model. Further theoretical analysis supports our approach. Extensive experiments across 7 base models and 12 datasets demonstrate that Trans-PEFT trained modules can maintain performance on updated base models without re-tuning, significantly reducing maintenance overhead in real-world applications.",Adapt Thrive Updates Transferable Parameter Efficient Fine Tuning Evolving Base Models Parameter efficient fine tuning PEFT common method fine tuning large language models base model serve multiple users PEFT module switching enhance user experience base models require periodic updates updated PEFT modules fine tuned previous versions suffer substantial performance degradation newer versions tuning numerous modules restore performance incur significant computational costs comprehensive analysis changes occur base model updates uncover interesting phenomenon continual training primarily affects task specific knowledge stored Feed Forward Networks FFN having impact task specific pattern Attention mechanism Based findings introduce Trans PEFT novel approach enhances PEFT module focusing task specific pattern reducing dependence certain knowledge base model theoretical analysis supports approach Extensive experiments 7 base models 12 datasets demonstrate Trans PEFT trained modules maintain performance updated base models tuning significantly reducing maintenance overhead real world applications
760,Can Vision-Language Models Evaluate Handwritten Math?,"['Oikantik Nath', 'Hanani Bathina', 'Mohammed Safi Ur Rahman Khan', 'Mitesh M Khapra']","Recent advancements in Vision-Language Models (VLMs) have opened new possibilities in automatic grading of handwritten student responses, particularly in mathematics. However, a comprehensive study to test the ability of VLMs to evaluate and reason over handwritten content remains absent. To address this gap, we introduce FERMAT, a benchmark designed to assess the ability of VLMs to detect, localize and correct errors in handwritten mathematical content. FERMAT spans four key error dimensions - computational, conceptual, notational, and presentation - and comprises over 2,200 handwritten math solutions derived from 609 manually curated problems from grades 7-12 with intentionally introduced perturbations. Using FERMAT we benchmark nine VLMs across three tasks: error detection, localization, and correction. Our results reveal significant shortcomings in current VLMs in reasoning over handwritten text, with Gemini-1.5-Pro achieving the highest error correction rate (77%). We also observed that some models struggle with processing handwritten content, as their accuracy improves when handwritten inputs are replaced with printed text or images. These findings highlight the limitations of current VLMs and reveal new avenues for improvement. We release FERMAT and all the associated resources in the open-source to drive further research.",Vision Language Models Evaluate Handwritten Math Recent advancements Vision Language Models VLMs opened new possibilities automatic grading handwritten student responses particularly mathematics comprehensive study test ability VLMs evaluate reason handwritten content remains absent address gap introduce FERMAT benchmark designed assess ability VLMs detect localize correct errors handwritten mathematical content FERMAT spans key error dimensions computational conceptual notational presentation comprises 2 200 handwritten math solutions derived 609 manually curated problems grades 7 12 intentionally introduced perturbations Using FERMAT benchmark VLMs tasks error detection localization correction results reveal significant shortcomings current VLMs reasoning handwritten text Gemini 1 5 Pro achieving highest error correction rate 77 observed models struggle processing handwritten content accuracy improves handwritten inputs replaced printed text images findings highlight limitations current VLMs reveal new avenues improvement release FERMAT associated resources open source drive research
761,Continual Gradient Low-Rank Projection Fine-Tuning for LLMs,"['Chenxu Wang', 'Yilin Lyu', 'Zicheng Sun', 'Liping Jing']","Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.",Continual Gradient Low Rank Projection Fine Tuning LLMs Continual fine tuning Large Language Models LLMs hampered trade efficiency expressiveness Low Rank Adaptation LoRA offers efficiency constrains model s ability learn new tasks transfer knowledge low rank nature reliance explicit parameter constraints propose GORP Gradient LOw Rank Projection Continual Learning novel training strategy overcomes limitations synergistically combining low rank parameters jointly updating unified low rank gradient subspace GORP expands optimization space preserving efficiency mitigating catastrophic forgetting Extensive experiments continual learning benchmarks demonstrate GORP s superior performance compared existing state art approaches Code available https github com Wcxwcxw GORP
762,Towards Objective Fine-tuning: How LLMs’ Prior Knowledge Causes Potential Poor Calibration?,"['Ziming Wang', 'Zeyu Shi', 'Haoyi Zhou', 'Shiqi Gao', 'Qingyun Sun', 'Jianxin Li']",,Objective Fine tuning LLMs Prior Knowledge Causes Potential Poor Calibration
763,Can Community Notes Replace Professional Fact-Checkers?,"['Nadav Borenstein', 'Greta Warren', 'Desmond Elliott', 'Isabelle Augenstein']","Two commonly employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. Our results show that successful community moderation relies on professional fact-checking and highlight how citizen and professional fact-checking are deeply intertwined.",Community Notes Replace Professional Fact Checkers commonly employed strategies combat rise misinformation social media fact checking professional organisations ii community moderation platform users Policy changes Twitter X recently Meta signal shift away partnerships fact checking organisations increased reliance crowdsourced community notes extent nature dependencies fact checking helpful community notes remain unclear address questions use language models annotate large corpus Twitter X community notes attributes topic cited sources refute claims tied broader misinformation narratives analysis reveals community notes cite fact checking sources times previously reported Fact checking especially crucial notes posts linked broader narratives twice likely reference fact checking sources compared sources results successful community moderation relies professional fact checking highlight citizen professional fact checking deeply intertwined
764,Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization,"['Keane Ong', 'Rui Mao', 'Deeksha varshney', 'Erik Cambria', 'Gianmarco Mengaldo']","Sustainability reports are key for evaluating companies' environmental, social and governance, ESG performance, but their content is increasingly obscured by greenwashing - sustainability claims that are misleading, exaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack robustness against greenwashing risks, often extracting insights that reflect misleading or exaggerated sustainability claims rather than objective ESG performance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis with Cross-Category Generalization, as a novel dataset to improve the robustness of ESG analysis amid the prevalence of greenwashing. By explicitly linking sustainability aspects with their associated actions, A3CG facilitates a more fine-grained and transparent evaluation of sustainability claims, ensuring that insights are grounded in verifiable actions rather than vague or misleading rhetoric. Additionally, A3CG emphasizes cross-category generalization. This ensures robust model performance in aspect-action analysis even when companies change their reports to selectively favor certain sustainability areas. Through experiments on A3CG, we analyze state-of-the-art supervised models and LLMs, uncovering their limitations and outlining key directions for future research.",Robust ESG Analysis Greenwashing Risks Aspect Action Analysis Cross Category Generalization Sustainability reports key evaluating companies environmental social governance ESG performance content increasingly obscured greenwashing sustainability claims misleading exaggerated fabricated existing NLP approaches ESG analysis lack robustness greenwashing risks extracting insights reflect misleading exaggerated sustainability claims objective ESG performance bridge gap introduce A3CG Aspect Action Analysis Cross Category Generalization novel dataset improve robustness ESG analysis amid prevalence greenwashing explicitly linking sustainability aspects associated actions A3CG facilitates fine grained transparent evaluation sustainability claims ensuring insights grounded verifiable actions vague misleading rhetoric Additionally A3CG emphasizes cross category generalization ensures robust model performance aspect action analysis companies change reports selectively favor certain sustainability areas experiments A3CG analyze state art supervised models LLMs uncovering limitations outlining key directions future research
765,HiddenDetect: Detecting Jailbreak Attacks against Multimodal Large Language Models via Monitoring Hidden States,"['Yilei Jiang', 'Xinyan Gao', 'Tianshuo Peng', 'Yingshui Tan', 'Xiaoyong Zhu', 'Bo Zheng', 'Xiangyu Yue']",,HiddenDetect Detecting Jailbreak Attacks Multimodal Large Language Models Monitoring Hidden States
766,SwiLTra-Bench: The Swiss Legal Translation Benchmark,"['Joel Niklaus', 'Jakob Merane', 'Luka Nenadic', 'Sina Ahmadi', 'Yingqiang Gao', 'Cyrill A. H. Chevalley', 'Claude Humbel', 'Christophe Gösken', 'Lorenzo Tanzi', 'Thomas Lüthi', 'Stefan Palombo', 'Spencer Poff', 'Boling Yang', 'Nan Wu', 'Matthew Guillod', 'Robin Mamié', 'Daniel Brunner', 'Julio Pereyra', 'Niko Grupen']","In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.",SwiLTra Bench Swiss Legal Translation Benchmark Switzerland legal translation uniquely important country s official languages requirements multilingual legal documentation process traditionally relies professionals legal experts skilled translators creating bottlenecks impacting effective access justice address challenge introduce SwiLTra Bench comprehensive multilingual benchmark 180K aligned Swiss legal translation pairs comprising laws headnotes press releases Swiss languages English designed evaluate LLM based translation systems systematic evaluation reveals frontier models achieve superior translation performance document types specialized translation systems excel specifically laws perform headnotes rigorous testing human expert validation demonstrate fine tuning open SLMs significantly improves translation quality lag best zero shot prompted frontier models Claude 3 5 Sonnet Additionally present SwiLTra Judge specialized LLM evaluation aligns best human expert assessments
767,Two Intermediate Translations Are Better Than One: Fine-tuning LLMs for Document-level Translation Refinement,"['Yichen Dong', 'Xinglin Lyu', 'Junhui Li', 'Daimeng Wei', 'Min Zhang', 'Shimin Tao', 'Hao Yang']","Recent research has shown that large language models (LLMs) can enhance translation quality through self-refinement. In this paper, we build on this idea by extending the refinement from sentence-level to document-level translation, specifically focusing on document-to-document (Doc2Doc) translation refinement. Since sentence-to-sentence (Sent2Sent) and Doc2Doc translation address different aspects of the translation process, we propose fine-tuning LLMs for translation refinement using two intermediate translations, combining the strengths of both Sent2Sent and Doc2Doc. Additionally, recognizing that the quality of intermediate translations varies, we introduce an enhanced fine-tuning method with quality awareness that assigns lower weights to easier translations and higher weights to more difficult ones, enabling the model to focus on challenging translation cases. Experimental results across ten translation tasks with LLaMA-3-8B-Instruct and Mistral-Nemo-Instruct demonstrate the effectiveness of our approach.",Intermediate Translations Better Fine tuning LLMs Document level Translation Refinement Recent research shown large language models LLMs enhance translation quality self refinement paper build idea extending refinement sentence level document level translation specifically focusing document document Doc2Doc translation refinement sentence sentence Sent2Sent Doc2Doc translation address different aspects translation process propose fine tuning LLMs translation refinement using intermediate translations combining strengths Sent2Sent Doc2Doc Additionally recognizing quality intermediate translations varies introduce enhanced fine tuning method quality awareness assigns lower weights easier translations higher weights difficult ones enabling model focus challenging translation cases Experimental results translation tasks LLaMA 3 8B Instruct Mistral Nemo Instruct demonstrate effectiveness approach
768,Circuit Compositions: Exploring Modular Structures in Transformer-Based Language Models,"['Philipp Mondorf', 'Sondre Wold', 'Barbara Plank']","A fundamental question in interpretability research is to what extent neural networks, particularly language models, implement reusable functions through subnetworks that can be composed to perform more complex tasks. Recent advances in mechanistic interpretability have made progress in identifying $\textit{circuits}$, which represent the minimal computational subgraphs responsible for a model's behavior on specific tasks. However, most studies focus on identifying circuits for individual tasks without investigating how functionally similar circuits $\textit{relate}$ to each other. To address this gap, we study the modularity of neural networks by analyzing circuits for highly compositional subtasks within a transformer-based language model. Specifically, given a probabilistic context-free grammar, we identify and compare circuits responsible for ten modular string-edit operations. Our results indicate that functionally similar circuits exhibit both notable node overlap and cross-task faithfulness. Moreover, we demonstrate that the circuits identified can be reused and combined through set operations to represent more complex functional model capabilities.",Circuit Compositions Exploring Modular Structures Transformer Based Language Models fundamental question interpretability research extent neural networks particularly language models implement reusable functions subnetworks composed perform complex tasks Recent advances mechanistic interpretability progress identifying textit circuits represent minimal computational subgraphs responsible model s behavior specific tasks studies focus identifying circuits individual tasks investigating functionally similar circuits textit relate address gap study modularity neural networks analyzing circuits highly compositional subtasks transformer based language model Specifically given probabilistic context free grammar identify compare circuits responsible modular string edit operations results indicate functionally similar circuits exhibit notable node overlap cross task faithfulness demonstrate circuits identified reused combined set operations represent complex functional model capabilities
769,Can LLMs Ground when they (Don’t) Know: A Study on Direct and Loaded Political Questions,"['Clara Lachenmaier', 'Judith Sieker', 'Sina Zarrieß']",,LLMs Ground Don t Know Study Direct Loaded Political Questions
770,GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking,"['Yingjian Chen', 'Haoran Liu', 'Yinhong Liu', 'Jinxiang Xie', 'Rui Yang', 'Han Yuan', 'Yanran Fu', 'Peng Yuan Zhou', 'Qingyu Chen', 'James Caverlee', 'Irene Li']","Large language models (LLMs) are widely used, but they often generate subtle factual errors, especially in long-form text. These errors are fatal in some specialized domains such as medicine. Existing fact-checking with grounding documents methods face two main challenges: (1) they struggle to understand complex multihop relations in long documents, often overlooking subtle factual errors; (2) most specialized methods rely on pairwise comparisons, requiring multiple model calls, leading to high resource and computational costs. To address these challenges, we propose GraphCheck, a fact-checking framework that uses extracted knowledge graphs to enhance text representation. Graph Neural Networks further process these graphs as a soft prompt, enabling LLMs to incorporate structured knowledge more effectively. Enhanced with graph-based reasoning, GraphCheck captures multihop reasoning chains that are often overlooked by existing methods, enabling precise and efficient fact-checking in a single inference call. Experimental results on seven benchmarks spanning both general and medical domains demonstrate up to a 7.1% overall improvement over baseline models. Notably, GraphCheck outperforms existing specialized fact-checkers and achieves comparable performance with state-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters.",GraphCheck Breaking Long Term Text Barriers Extracted Knowledge Graph Powered Fact Checking Large language models LLMs widely used generate subtle factual errors especially long form text errors fatal specialized domains medicine Existing fact checking grounding documents methods face main challenges 1 struggle understand complex multihop relations long documents overlooking subtle factual errors 2 specialized methods rely pairwise comparisons requiring multiple model calls leading high resource computational costs address challenges propose GraphCheck fact checking framework uses extracted knowledge graphs enhance text representation Graph Neural Networks process graphs soft prompt enabling LLMs incorporate structured knowledge effectively Enhanced graph based reasoning GraphCheck captures multihop reasoning chains overlooked existing methods enabling precise efficient fact checking single inference Experimental results seven benchmarks spanning general medical domains demonstrate 7 1 overall improvement baseline models Notably GraphCheck outperforms existing specialized fact checkers achieves comparable performance state art LLMs DeepSeek V3 OpenAI o1 significantly fewer parameters
771,SCULPT: Systematic Tuning of Long Prompts,"['Shanu Kumar', 'Akhila Yesantarao Venkata', 'Shubhanshu Khandelwal', 'Bishal Santra', 'Parag Agrawal', 'Manish Gupta']","Prompt optimization is essential for effective utilization of large language models (LLMs) across diverse tasks. While existing optimization methods are effective in optimizing short prompts, they struggle with longer, more complex ones, often risking information loss and being sensitive to small perturbations. To address these challenges, we propose SCULPT (Systematic Tuning of Long Prompts), a framework that treats prompt optimization as a hierarchical tree refinement problem. SCULPT represents prompts as tree structures, enabling targeted modifications while preserving contextual integrity. It employs a Critic-Actor framework that generates reflections and applies actions to refine the prompt. Evaluations demonstrate SCULPT's effectiveness on long prompts, its robustness to adversarial perturbations, and its ability to generate high-performing prompts even without any initial human-written prompt. Compared to existing state of the art methods, SCULPT consistently improves LLM performance by preserving essential task information while applying structured refinements. Both qualitative and quantitative analyses show that SCULPT produces more stable and interpretable prompt modifications, ensuring better generalization across tasks.",SCULPT Systematic Tuning Long Prompts Prompt optimization essential effective utilization large language models LLMs diverse tasks existing optimization methods effective optimizing short prompts struggle longer complex ones risking information loss sensitive small perturbations address challenges propose SCULPT Systematic Tuning Long Prompts framework treats prompt optimization hierarchical tree refinement problem SCULPT represents prompts tree structures enabling targeted modifications preserving contextual integrity employs Critic Actor framework generates reflections applies actions refine prompt Evaluations demonstrate SCULPT s effectiveness long prompts robustness adversarial perturbations ability generate high performing prompts initial human written prompt Compared existing state art methods SCULPT consistently improves LLM performance preserving essential task information applying structured refinements qualitative quantitative analyses SCULPT produces stable interpretable prompt modifications ensuring better generalization tasks
772,Crab: A Novel Configurable Role-Playing LLM with Assessing Benchmark,"['Kai He', 'Yucheng Huang', 'Wenqing Wang', 'Delong Ran', 'Dongming Sheng', 'Junxuan Huang', 'Qika Lin', 'Jiaxing Xu', 'Wenqiang Liu', 'Mengling Feng']",,Crab Novel Configurable Role Playing LLM Assessing Benchmark
773,Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large Language Models,"['Yingshui Tan', 'Boren Zheng', 'Baihui Zheng', 'Kerui Cao', 'Huiyun Jing', 'Jincheng Wei', 'Jiaheng Liu', 'Yancheng He', 'Wenbo Su', 'Xiaoyong Zhu', 'Bo Zheng', 'Kaifu Zhang']","With the rapid advancement of Large Language Models (LLMs), significant safety concerns have emerged. Fundamentally, the safety of large language models is closely linked to the accuracy, comprehensiveness, and clarity of their understanding of safety knowledge, particularly in domains such as law, policy and ethics. This factuality ability is crucial in determining whether these models can be deployed and applied safely and compliantly within specific regions. To address these challenges and better evaluate the factuality ability of LLMs to answer short questions, we introduce the Chinese SafetyQA benchmark. Chinese SafetyQA has several properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate, Safety-related, Harmless). Based on Chinese SafetyQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs and analyze how these capabilities relate to LLM abilities, e.g., RAG ability and robustness against attacks.",Chinese SafetyQA Safety Short form Factuality Benchmark Large Language Models rapid advancement Large Language Models LLMs significant safety concerns emerged Fundamentally safety large language models closely linked accuracy comprehensiveness clarity understanding safety knowledge particularly domains law policy ethics factuality ability crucial determining models deployed applied safely compliantly specific regions address challenges better evaluate factuality ability LLMs answer short questions introduce Chinese SafetyQA benchmark Chinese SafetyQA properties e Chinese Diverse High quality Static Easy evaluate Safety related Harmless Based Chinese SafetyQA perform comprehensive evaluation factuality abilities existing LLMs analyze capabilities relate LLM abilities e g RAG ability robustness attacks
774,TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis,"['Xiaorui Wu', 'Xiaofeng Mao', 'Fei Li', 'Xin Zhang', 'Donghong Ji', 'Chong Teng', 'Xuanhong Li', 'Zhuang Li']","Large Language Models (LLMs) excel in various natural language processing tasks but remain vulnerable to generating harmful content or being exploited for malicious purposes. Although safety alignment datasets have been introduced to mitigate such risks through supervised fine-tuning (SFT), these datasets often lack comprehensive risk coverage. Most existing datasets focus primarily on lexical diversity while neglecting other critical dimensions. To address this limitation, we propose a novel analysis framework to systematically measure the risk coverage of alignment datasets across three essential dimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We further introduce TRIDENT, an automated pipeline that leverages persona-based, zero-shot LLM generation to produce diverse and comprehensive instructions spanning these dimensions. Each harmful instruction is paired with an ethically aligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311 examples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on TRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29% reduction in Harm Score, and a 20% decrease in Attack Success Rate compared to the best-performing baseline model fine-tuned on the WildBreak dataset.",TRIDENT Enhancing Large Language Model Safety Tri Dimensional Diversified Red Teaming Data Synthesis Large Language Models LLMs excel various natural language processing tasks remain vulnerable generating harmful content exploited malicious purposes safety alignment datasets introduced mitigate risks supervised fine tuning SFT datasets lack comprehensive risk coverage existing datasets focus primarily lexical diversity neglecting critical dimensions address limitation propose novel analysis framework systematically measure risk coverage alignment datasets essential dimensions Lexical Diversity Malicious Intent Jailbreak Tactics introduce TRIDENT automated pipeline leverages persona based zero shot LLM generation produce diverse comprehensive instructions spanning dimensions harmful instruction paired ethically aligned response resulting datasets TRIDENT Core comprising 26 311 examples TRIDENT Edge 18 773 examples Fine tuning Llama 3 1 8B TRIDENT Edge demonstrates substantial improvements achieving average 14 29 reduction Harm Score 20 decrease Attack Success Rate compared best performing baseline model fine tuned WildBreak dataset
775,Cross-Lingual Optimization for Language Transfer in Large Language Models,"['Jungseob Lee', 'Seongtae Hong', 'Hyeonseok Moon', 'Heuiseok Lim']",,Cross Lingual Optimization Language Transfer Large Language Models
776,ACE: A Generative Cross-Modal Retrieval Framework With Coarse-To-Fine Semantic Modeling,"['Minghui Fang', 'Shengpeng Ji', 'Jialong Zuo', 'Hai Huang', 'Yan Xia', 'Jieming Zhu', 'Xize Cheng', 'Xiaoda Yang', 'Wenrui Liu', 'Gang Wang', 'Zhenhua Dong', 'Zhou Zhao']",,ACE Generative Cross Modal Retrieval Framework Coarse Fine Semantic Modeling
777,MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark,"['Xiang Yue', 'Tianyu Zheng', 'Yuansheng Ni', 'Yubo Wang', 'Kai Zhang', 'Shengbang Tong', 'Yuxuan Sun', 'Botao Yu', 'Ge Zhang', 'Huan Sun', 'Yu Su', 'Wenhu Chen', 'Graham Neubig']",,MMMU Pro Robust Multi discipline Multimodal Understanding Benchmark
778,Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch,"['Xueru Wen', 'Jie Lou', 'Zichao Li', 'Yaojie Lu', 'XingYu', 'Yuqiu Ji', 'Guohai Xu', 'Hongyu Lin', 'Ben He', 'Xianpei Han', 'Le Sun', 'Debing Zhang']","Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences. However, most RM research is centered on English and relies heavily on synthetic resources, which leads to limited and less reliable datasets and benchmarks for Chinese. To address this gap, we introduce CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese contexts, and CheemsPreference, a large-scale and diverse preference dataset annotated through human-machine collaboration to support Chinese RM training. We systematically evaluate open-source discriminative and generative RMs on CheemsBench and observe significant limitations in their ability to capture human preferences in Chinese scenarios. Additionally, based on CheemsPreference, we construct an RM that achieves state-of-the-art performance on CheemsBench, demonstrating the necessity of human supervision in RM training. Our findings reveal that scaled AI-generated data struggles to fully capture human preferences, emphasizing the importance of high-quality human supervision in RM development.",Cheems Practical Guidance Building Evaluating Chinese Reward Models Scratch Reward models RMs crucial aligning large language models LLMs human preferences RM research centered English relies heavily synthetic resources leads limited reliable datasets benchmarks Chinese address gap introduce CheemsBench fully human annotated RM evaluation benchmark Chinese contexts CheemsPreference large scale diverse preference dataset annotated human machine collaboration support Chinese RM training systematically evaluate open source discriminative generative RMs CheemsBench observe significant limitations ability capture human preferences Chinese scenarios Additionally based CheemsPreference construct RM achieves state art performance CheemsBench demonstrating necessity human supervision RM training findings reveal scaled AI generated data struggles fully capture human preferences emphasizing importance high quality human supervision RM development
779,Why Safeguarded Ships Run Aground? Aligned Large Language Models’ Safety Mechanisms Tend to Be Anchored in The Template Region,"['Chak Tou Leong', 'Qingyu Yin', 'Jian Wang', 'Wenjie Li']",,Safeguarded Ships Run Aground Aligned Large Language Models Safety Mechanisms Tend Anchored Template Region
780,LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters through Modality Linear Representation-Steering,"['Jinhe Bi', 'Yujun Wang', 'Haokun Chen', 'Xun Xiao', 'Artur Hecker', 'Volker Tresp', 'Yunpu Ma']","Multimodal Large Language Models (MLLMs) have significantly advanced visual tasks by integrating visual representations into large language models (LLMs). The textual modality, inherited from LLMs, equips MLLMs with abilities like instruction following and in-context learning. In contrast, the visual modality enhances performance in downstream tasks by leveraging rich semantic content, spatial information, and grounding capabilities. These intrinsic modalities work synergistically across various visual tasks. Our research initially reveals a persistent imbalance between these modalities, with text often dominating output generation during visual instruction tuning. This imbalance occurs when using both full fine-tuning and parameter-efficient fine-tuning (PEFT) methods. We then found that re-balancing these modalities can significantly reduce the number of trainable parameters required, inspiring a direction for further optimizing visual instruction tuning. We introduce Modality Linear Representation-Steering (MoReS) to achieve the goal. MoReS effectively re-balances the intrinsic modalities throughout the model, where the key idea is to steer visual representations through linear transformations in the visual subspace across each model layer. To validate our solution, we composed LLaVA Steering, a suite of models integrated with the proposed MoReS method. Evaluation results show that the composed LLaVA Steering models require, on average, 500 times fewer trainable parameters than LoRA needs while still achieving comparable performance across three visual benchmarks and eight visual question-answering tasks. Last, we present the LLaVA Steering Factory, an in-house developed platform that enables researchers to quickly customize various MLLMs with component-based architecture for seamlessly integrating state-of-the-art models, and evaluate their intrinsic modality imbalance.",LLaVA Steering Visual Instruction Tuning 500x Fewer Parameters Modality Linear Representation Steering Multimodal Large Language Models MLLMs significantly advanced visual tasks integrating visual representations large language models LLMs textual modality inherited LLMs equips MLLMs abilities like instruction following context learning contrast visual modality enhances performance downstream tasks leveraging rich semantic content spatial information grounding capabilities intrinsic modalities work synergistically various visual tasks research initially reveals persistent imbalance modalities text dominating output generation visual instruction tuning imbalance occurs using fine tuning parameter efficient fine tuning PEFT methods balancing modalities significantly reduce number trainable parameters required inspiring direction optimizing visual instruction tuning introduce Modality Linear Representation Steering MoReS achieve goal MoReS effectively balances intrinsic modalities model key idea steer visual representations linear transformations visual subspace model layer validate solution composed LLaVA Steering suite models integrated proposed MoReS method Evaluation results composed LLaVA Steering models require average 500 times fewer trainable parameters LoRA needs achieving comparable performance visual benchmarks visual question answering tasks present LLaVA Steering Factory house developed platform enables researchers quickly customize various MLLMs component based architecture seamlessly integrating state art models evaluate intrinsic modality imbalance
781,Efficient Long Context Language Model Retrieval with Compression,"['Minju Seo', 'Jinheon Baek', 'Seongyun Lee', 'Sung Ju Hwang']","Long Context Language Models (LCLMs) have emerged as a new paradigm to perform Information Retrieval (IR), which enables the direct ingestion and retrieval of information by processing an entire corpus in their single context, showcasing the potential to surpass traditional sparse and dense retrieval methods. However, processing a large number of passages within in-context for retrieval is computationally expensive, and handling their representations during inference further exacerbates the processing time; thus, we aim to make LCLM retrieval more efficient and potentially more effective with passage compression. Specifically, we propose a new compression approach tailored for LCLM retrieval, which is trained to maximize the retrieval performance while minimizing the length of the compressed passages. To accomplish this, we generate the synthetic data, where compressed passages are automatically created and labeled as chosen or rejected according to their retrieval success for a given query, and we train the proposed Compression model for Long context Retrieval (CoLoR) with this data via preference optimization while adding the length regularization loss on top of it to enforce brevity. Through extensive experiments on 9 datasets, we show that CoLoR improves the retrieval performance by 6% while compressing the in-context size by a factor of 1.91. Our code is available at: https://github.com/going-doer/CoLoR.",Efficient Long Context Language Model Retrieval Compression Long Context Language Models LCLMs emerged new paradigm perform Information Retrieval IR enables direct ingestion retrieval information processing entire corpus single context showcasing potential surpass traditional sparse dense retrieval methods processing large number passages context retrieval computationally expensive handling representations inference exacerbates processing time aim make LCLM retrieval efficient potentially effective passage compression Specifically propose new compression approach tailored LCLM retrieval trained maximize retrieval performance minimizing length compressed passages accomplish generate synthetic data compressed passages automatically created labeled chosen rejected according retrieval success given query train proposed Compression model Long context Retrieval CoLoR data preference optimization adding length regularization loss enforce brevity extensive experiments 9 datasets CoLoR improves retrieval performance 6 compressing context size factor 1 91 code available https github com going doer CoLoR
782,Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering,"['Runxuan Liu', 'luobei', 'Jiaqi Li', 'Baoxin Wang', 'Ming Liu', 'Dayong Wu', 'Shijin Wang', 'Bing Qin']","Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.",Ontology Guided Reverse Thinking Makes Large Language Models Stronger Knowledge Graph Question Answering Large language models LLMs shown remarkable capabilities natural language processing knowledge graph question answering tasks KGQA remains issue answering questions require multi hop reasoning Existing methods rely entity vector matching purpose question abstract difficult match specific entities result difficult establish reasoning paths purpose leads information loss redundancy address issue inspired human reverse thinking propose Ontology Guided Reverse Thinking ORT novel framework constructs reasoning paths purposes conditions ORT operates key phases 1 using LLM extract purpose labels condition labels 2 constructing label reasoning paths based KG ontology 3 using label reasoning paths guide knowledge retrieval Experiments WebQSP CWQ datasets ORT achieves state art performance significantly enhances capability LLMs KGQA
783,Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large Language Models in Medical Applications,"['Zhe Chen', 'Yusheng Liao', 'Shuyang Jiang', 'Pingjie Wang', 'YiQiu Guo', 'Yanfeng Wang', 'Yu Wang']",,Omni RAG Comprehensive Retrieval Augmented Generation Large Language Models Medical Applications
784,"Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals","['Yuxin Lin', 'Yinglin Zheng', 'Ming Zeng', 'Wangzheng Shi']","This paper addresses the gap in predicting turn-taking and backchannel actions in human-machine conversations using multi-modal signals (linguistic, acoustic, and visual). To overcome the limitation of existing datasets, we propose an automatic data collection pipeline that allows us to collect and annotate over 210 hours of human conversation videos. From this, we construct a Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over 1.5M words and corresponding turn-taking and backchannel annotations from approximately 20M frames. Additionally, we present an end-to-end framework that predicts the probability of turn-taking and backchannel actions from multi-modal signals. The proposed model emphasizes the interrelation between modalities and supports any combination of text, audio, and video inputs, making it adaptable to a variety of realistic scenarios. Our experiments show that our approach achieves state-of-the-art performance on turn-taking and backchannel prediction tasks, achieving a 10% increase in F1-score on turn-taking and a 33% increase on backchannel prediction. Our dataset and code are publicly available online to ease of subsequent research.",Predicting Turn Taking Backchannel Human Machine Conversations Using Linguistic Acoustic Visual Signals paper addresses gap predicting turn taking backchannel actions human machine conversations using multi modal signals linguistic acoustic visual overcome limitation existing datasets propose automatic data collection pipeline allows collect annotate 210 hours human conversation videos construct Multi Modal Face Face MM F2F human conversation dataset including 1 5M words corresponding turn taking backchannel annotations approximately 20M frames Additionally present end end framework predicts probability turn taking backchannel actions multi modal signals proposed model emphasizes interrelation modalities supports combination text audio video inputs making adaptable variety realistic scenarios experiments approach achieves state art performance turn taking backchannel prediction tasks achieving 10 increase F1 score turn taking 33 increase backchannel prediction dataset code publicly available online ease subsequent research
785,A New Formulation of Zipf’s Meaning-Frequency Law through Contextual Diversity,"['Ryo Nagata', 'Kumiko Tanaka-Ishii']",,New Formulation Zipf s Meaning Frequency Law Contextual Diversity
786,The Mirage of Model Editing: Revisiting Evaluation in the Wild,"['Wanli Yang', 'Fei Sun', 'Jiajun Tan', 'Xinyu Ma', 'Qi Cao', 'Dawei Yin', 'Huawei Shen', 'Xueqi Cheng']","Despite near-perfect results reported in the literature, the effectiveness of model editing in real-world applications remains unclear. To bridge this gap, we introduce QAEdit, a new benchmark aligned with widely used question answering (QA) datasets, and WILD, a task-agnostic evaluation framework designed to better reflect real-world usage of model editing. Our single editing experiments show that current editing methods perform substantially worse than previously reported (38.5% vs. 96.8%). We demonstrate that it stems from issues in the synthetic evaluation practices of prior work. Among them, the most severe is the use of teacher forcing during testing, which leaks both content and length of the ground truth, leading to overestimated performance. Furthermore, we simulate practical deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. This work calls for a shift in model editing research toward rigorous evaluation and the development of robust, scalable methods that can reliably update knowledge in LLMs for real-world use.",Mirage Model Editing Revisiting Evaluation Wild Despite near perfect results reported literature effectiveness model editing real world applications remains unclear bridge gap introduce QAEdit new benchmark aligned widely used question answering QA datasets WILD task agnostic evaluation framework designed better reflect real world usage model editing single editing experiments current editing methods perform substantially worse previously reported 38 5 vs 96 8 demonstrate stems issues synthetic evaluation practices prior work severe use teacher forcing testing leaks content length ground truth leading overestimated performance Furthermore simulate practical deployment sequential editing revealing current approaches fail drastically 1000 edits work calls shift model editing research rigorous evaluation development robust scalable methods reliably update knowledge LLMs real world use
787,LAQuer: Localized Attribution Queries in Content-grounded Generation,"['Eran Hirsch', 'Aviv Slobodkin', 'David Wan', 'Elias Stengel-Eskin', 'Mohit Bansal', 'Ido Dagan']","Grounded text generation models often produce content that deviates from their source material, requiring user verification to ensure accuracy. Existing attribution methods associate entire sentences with source documents, which can be overwhelming for users seeking to fact-check specific claims. In contrast, existing sub-sentence attribution methods may be more precise but fail to align with users' interests. In light of these limitations, we introduce Localized Attribution Queries (LAQuer), a new task that localizes selected spans of generated output to their corresponding source spans, allowing fine-grained and user-directed attribution. We compare two approaches for the LAQuer task, including prompting large language models (LLMs) and leveraging LLM internal representations. We then explore a modeling framework that extends existing attributed text generation methods to LAQuer. We evaluate this framework across two grounded text generation tasks: Multi-document Summarization (MDS) and Long-form Question Answering (LFQA). Our findings show that LAQuer methods significantly reduce the length of the attributed text. Our contributions include: (1) proposing the LAQuer task to enhance attribution usability, (2) suggesting a modeling framework and benchmarking multiple baselines, and (3) proposing a new evaluation setting to promote future research on localized attribution in content-grounded generation.",LAQuer Localized Attribution Queries Content grounded Generation Grounded text generation models produce content deviates source material requiring user verification ensure accuracy Existing attribution methods associate entire sentences source documents overwhelming users seeking fact check specific claims contrast existing sub sentence attribution methods precise fail align users interests light limitations introduce Localized Attribution Queries LAQuer new task localizes selected spans generated output corresponding source spans allowing fine grained user directed attribution compare approaches LAQuer task including prompting large language models LLMs leveraging LLM internal representations explore modeling framework extends existing attributed text generation methods LAQuer evaluate framework grounded text generation tasks Multi document Summarization MDS Long form Question Answering LFQA findings LAQuer methods significantly reduce length attributed text contributions include 1 proposing LAQuer task enhance attribution usability 2 suggesting modeling framework benchmarking multiple baselines 3 proposing new evaluation setting promote future research localized attribution content grounded generation
788,EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning,"['Xiaoqian Liu', 'Ke Wang', 'Yongbin Li', 'Yuchuan Wu', 'Wentao Ma', 'Aobo Kong', 'Fei Huang', 'Jianbin Jiao', 'Junge Zhang']","Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL),utilizing process rewards and iterative self-play. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. Code and data are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO.",EPO Explicit Policy Optimization Strategic Reasoning LLMs Reinforcement Learning Large Language Models LLMs shown impressive reasoning capabilities defined problems clear solutions mathematics coding struggle complex real world scenarios like business negotiations require strategic reasoning ability navigate dynamic environments align long term goals amidst uncertainty Existing methods strategic reasoning face challenges adaptability scalability transferring strategies new contexts address issues propose explicit policy optimization EPO strategic reasoning featuring LLM provides strategies open ended action space plugged arbitrary LLM agents motivate goal directed behavior improve adaptability policy transferability train strategic reasoning model multi turn reinforcement learning RL utilizing process rewards iterative self play Experiments social physical domains demonstrate EPO s ability long term goal alignment enhanced strategic reasoning achieving state art performance social dialogue web navigation tasks findings reveal various collaborative reasoning mechanisms emergent EPO effectiveness generating novel strategies underscoring potential strategic reasoning real world applications Code data available https github com AlibabaResearch DAMO ConvAI tree main EPO
789,DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph,"['Jihyung Lee', 'Jin-Seop Lee', 'Jaehoon Lee', 'YunSeok Choi', 'Jee-Hyong Lee']","Text-to-SQL, which translates a natural language question into an SQL query, has advanced with in-context learning of Large Language Models (LLMs). However, existing methods show little improvement in performance compared to randomly chosen demonstrations, and significant performance drops when smaller LLMs (e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely on the intrinsic capabilities of hyper-scaled LLMs, rather than effectively retrieving useful demonstrations. In this paper, we propose a novel approach for effectively retrieving demonstrations and generating SQL queries. We construct a Deep Contextual Schema Link Graph, which contains key information and semantic relationship between a question and its database schema items. This graph-based structure enables effective representation of Text-to-SQL samples and retrieval of useful demonstrations for in-context learning. Experimental results on the Spider benchmark demonstrate the effectiveness of our approach, showing consistent improvements in SQL generation performance and efficiency across both hyper-scaled LLMs and small LLMs. Our code will be released.",DCG SQL Enhancing Context Learning Text SQL Deep Contextual Schema Link Graph Text SQL translates natural language question SQL query advanced context learning Large Language Models LLMs existing methods little improvement performance compared randomly chosen demonstrations significant performance drops smaller LLMs e g Llama 3 1 8B used indicates methods heavily rely intrinsic capabilities hyper scaled LLMs effectively retrieving useful demonstrations paper propose novel approach effectively retrieving demonstrations generating SQL queries construct Deep Contextual Schema Link Graph contains key information semantic relationship question database schema items graph based structure enables effective representation Text SQL samples retrieval useful demonstrations context learning Experimental results Spider benchmark demonstrate effectiveness approach showing consistent improvements SQL generation performance efficiency hyper scaled LLMs small LLMs code released
790,Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model,"['Sihan Tan', 'Taro Miyazaki', 'Kazuhiro Nakadai']","Sign Language Translation (SLT) aims to convert sign language (SL) videos into spoken language text, thereby bridging the communication gap between the sign and the spoken community. While most existing works focus on translating a single sign language into a single spoken language (one-to-one SLT), leveraging multilingual resources could mitigate low-resource issues and enhance accessibility. However, multilingual SLT (MLSLT) remains unexplored due to language conflicts and alignment difficulties across SLs and spoken languages. To address these challenges, we propose a multilingual gloss-free model with dual CTC objectives for token-level SL identification and spoken text generation. Our model supports 10 SLs and handles one-to-one, many-to-one, and many-to-many SLT tasks, achieving competitive performance compared to state-of-the-art methods on three widely adopted benchmarks: multilingual SP-10, PHOENIX14T, and CSL-Daily.",Multilingual Gloss free Sign Language Translation Building Sign Language Foundation Model Sign Language Translation SLT aims convert sign language SL videos spoken language text bridging communication gap sign spoken community existing works focus translating single sign language single spoken language SLT leveraging multilingual resources mitigate low resource issues enhance accessibility multilingual SLT MLSLT remains unexplored language conflicts alignment difficulties SLs spoken languages address challenges propose multilingual gloss free model dual CTC objectives token level SL identification spoken text generation model supports 10 SLs handles SLT tasks achieving competitive performance compared state art methods widely adopted benchmarks multilingual SP 10 PHOENIX14T CSL Daily
791,PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy,"['Shuhao Guan', 'Moule Lin', 'Cheng Xu', 'Xinyi Liu', 'Jinman Zhao', 'Jiexin Fan', 'Qi Xu', 'Derek Greene']","This paper introduces PreP-OCR, a two-stage pipeline that combines document image restoration with semantic-aware post-OCR correction to enhance both visual clarity and textual consistency, thereby improving text extraction from degraded historical documents. First, we synthesize document-image pairs from plaintext, rendering them with diverse fonts and layouts and then applying a randomly ordered set of degradation operations. An image restoration model is trained on this synthetic data, using multi-directional patch extraction and fusion to process large images. Second, a ByT5 post-OCR model, fine-tuned on synthetic historical text pairs, addresses remaining OCR errors. Detailed experiments on 13,831 pages of real historical documents in English, French, and Spanish show that the PreP-OCR pipeline reduces character error rates by 63.9-70.3% compared to OCR on raw images. Our pipeline demonstrates the potential of integrating image restoration with linguistic error correction for digitizing historical archives.",PreP OCR Complete Pipeline Document Image Restoration Enhanced OCR Accuracy paper introduces PreP OCR stage pipeline combines document image restoration semantic aware post OCR correction enhance visual clarity textual consistency improving text extraction degraded historical documents synthesize document image pairs plaintext rendering diverse fonts layouts applying randomly ordered set degradation operations image restoration model trained synthetic data using multi directional patch extraction fusion process large images Second ByT5 post OCR model fine tuned synthetic historical text pairs addresses remaining OCR errors Detailed experiments 13 831 pages real historical documents English French Spanish PreP OCR pipeline reduces character error rates 63 9 70 3 compared OCR raw images pipeline demonstrates potential integrating image restoration linguistic error correction digitizing historical archives
792,Digest the Knowledge: Large Language Models empowered Message Passing for Knowledge Graph Question Answering,"['Junhong Wan', 'Tao Yu', 'Kunyu Jiang', 'Yao Fu', 'Weihao Jiang', 'Jiang Zhu']",,Digest Knowledge Large Language Models empowered Message Passing Knowledge Graph Question Answering
793,RecLM: Recommendation Instruction Tuning,"['Yangqin Jiang', 'Yuhao Yang', 'Lianghao Xia', 'Da Luo', 'Kangyi Lin', 'Chao Huang']","Modern recommender systems aim to deeply understand users' complex preferences through their past interactions. While deep collaborative filtering approaches using Graph Neural Networks (GNNs) excel at capturing user-item relationships, their effectiveness is limited when handling sparse data or zero-shot scenarios, primarily due to constraints in ID-based embedding functions. To address these challenges, we propose a model-agnostic recommendation instruction-tuning paradigm that seamlessly integrates large language models with collaborative filtering. Our proposed $\underline{Rec}$ommendation $\underline{L}$anguage $\underline{M}$odel (RecLM) enhances the capture of user preference diversity through a carefully designed reinforcement learning reward function that facilitates self-augmentation of language models. Comprehensive evaluations demonstrate significant advantages of our approach across various settings, and its plug-and-play compatibility with state-of-the-art recommender systems results in notable performance enhancements. The implementation of our RecLM framework is publicly available at: https://github.com/HKUDS/RecLM.",RecLM Recommendation Instruction Tuning Modern recommender systems aim deeply understand users complex preferences past interactions deep collaborative filtering approaches using Graph Neural Networks GNNs excel capturing user item relationships effectiveness limited handling sparse data zero shot scenarios primarily constraints ID based embedding functions address challenges propose model agnostic recommendation instruction tuning paradigm seamlessly integrates large language models collaborative filtering proposed underline Rec ommendation underline L anguage underline M odel RecLM enhances capture user preference diversity carefully designed reinforcement learning reward function facilitates self augmentation language models Comprehensive evaluations demonstrate significant advantages approach various settings plug play compatibility state art recommender systems results notable performance enhancements implementation RecLM framework publicly available https github com HKUDS RecLM
794,DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for Few-Shot Aspect-Based Sentiment Analysis,"['Hongling Xu', 'Yice Zhang', 'Qianlong Wang', 'Ruifeng Xu']","Recently developed large language models (LLMs) have presented promising new avenues to address data scarcity in low-resource scenarios. In few-shot aspect-based sentiment analysis (ABSA), previous efforts have explored data augmentation techniques, which prompt LLMs to generate new samples by modifying existing ones. However, these methods fail to produce adequately diverse data, impairing their effectiveness. Besides, some studies apply in-context learning for ABSA by using specific instructions and a few selected examples as prompts. Though promising, LLMs often yield labels that deviate from task requirements. To overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize data from two complementary perspectives: \textit{key-point-driven} and \textit{instance-driven}, which effectively generate diverse and high-quality ABSA samples in low-resource settings. Furthermore, a \textit{label refinement} module is integrated to improve the synthetic labels. Extensive experiments demonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA solutions and other LLM-oriented data generation methods.",DS 2 ABSA Dual Stream Data Synthesis Label Refinement Shot Aspect Based Sentiment Analysis Recently developed large language models LLMs presented promising new avenues address data scarcity low resource scenarios shot aspect based sentiment analysis ABSA previous efforts explored data augmentation techniques prompt LLMs generate new samples modifying existing ones methods fail produce adequately diverse data impairing effectiveness studies apply context learning ABSA using specific instructions selected examples prompts promising LLMs yield labels deviate task requirements overcome limitations propose DS 2 ABSA dual stream data synthesis framework targeted shot ABSA leverages LLMs synthesize data complementary perspectives textit key point driven textit instance driven effectively generate diverse high quality ABSA samples low resource settings Furthermore textit label refinement module integrated improve synthetic labels Extensive experiments demonstrate DS 2 ABSA significantly outperforms previous shot ABSA solutions LLM oriented data generation methods
795,MISP-Meeting: A Real-World Dataset with Multimodal Cues for Long-form Meeting Transcription and Summarization,"['HangChen', 'Chao-Han Huck Yang', 'Jia-Chen Gu', 'Hongxu Yin', 'Sabato Marco Siniscalchi', 'Jun Du']",,MISP Meeting Real World Dataset Multimodal Cues Long form Meeting Transcription Summarization
796,Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning,"['Sohan Patnaik', 'Milan Aggarwal', 'Sumit Bhatia', 'Balaji Krishnamurthy']","LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions by generating step-by-step rationales. Prior works have utilized this capability to improve smaller and cheaper LMs (say, with 7B parameters). However, various practical constraints, such as copyright and legal issues, owing to lack of transparency in the pre-training data of large (often closed) models, prevent their use in commercial settings. Little focus has been given to improving the innate reasoning ability of smaller models without distilling information from larger LLMs. To address this, we propose COLLATE, a trainable framework that tunes a (small) LLM to generate those outputs from a pool of diverse rationales that selectively improves the downstream task. COLLATE enforces multiple instances of the same LLM to exhibit distinct behavior and employs them to generate rationales to obtain diverse outputs. The LLM is then tuned via preference optimization to choose the candidate rationale which maximizes the likelihood of ground-truth answer. COLLATE outperforms several trainable and prompting baselines on 5 datasets across 3 domains: maths problem solving, natural language inference, and commonsense reasoning. We show the eff icacy of COLLATE on LLMs from different model families across varying parameter scales (1B to 8B) and demonstrate the benefit of multiple rationale providers guided by the end task through ablations. Code is released here (https://github.com/Sohanpatnaik106/collate).",Learning Perform Better Teaching Small Scale LLMs Collaborate Preferential Rationale Tuning LLMssuch GPT 4 shown remarkable ability solve complex questions generating step step rationales Prior works utilized capability improve smaller cheaper LMs say 7B parameters various practical constraints copyright legal issues owing lack transparency pre training data large closed models prevent use commercial settings Little focus given improving innate reasoning ability smaller models distilling information larger LLMs address propose COLLATE trainable framework tunes small LLM generate outputs pool diverse rationales selectively improves downstream task COLLATE enforces multiple instances LLM exhibit distinct behavior employs generate rationales obtain diverse outputs LLM tuned preference optimization choose candidate rationale maximizes likelihood ground truth answer COLLATE outperforms trainable prompting baselines 5 datasets 3 domains maths problem solving natural language inference commonsense reasoning eff icacy COLLATE LLMs different model families varying parameter scales 1B 8B demonstrate benefit multiple rationale providers guided end task ablations Code released https github com Sohanpatnaik106 collate
797,MolRAG: Unlocking the Power of Large Language Models for Molecular Property Prediction,"['Ziting Xian', 'Jiawei Gu', 'Lingbo Li', 'Eran Segal', 'Shangsong Liang']",,MolRAG Unlocking Power Large Language Models Molecular Property Prediction
798,SkillAggregation: Reference-free LLM-Dependent Aggregation,"['Guangzhi Sun', 'Anmol Kagrecha', 'Potsawee Manakul', 'Phil Woodland', 'Mark Gales']","Large Language Models (LLMs) are increasingly used to assess NLP tasks due to their ability to generate human-like judgments. Single LLMs were used initially, however, recent work suggests using multiple LLMs as judges yields improved performance. An important step in exploiting multiple judgements is the combination stage, aggregation. Existing methods in NLP either assign equal weight to all LLM judgments or are designed for specific tasks such as hallucination detection. This work focuses on aggregating predictions from multiple systems where no reference labels are available. A new method called SkillAggregation is proposed, which learns to combine estimates from LLM judges without needing additional data or ground truth. It extends the Crowdlayer aggregation method, developed for image classification, to exploit the judge estimates during inference. The approach is compared to a range of standard aggregation methods on HaluEval-Dialogue, TruthfulQA and Chatbot Arena tasks. SkillAggregation outperforms Crowdlayer on all tasks, and yields the best performance over all approaches on the majority of tasks.",SkillAggregation Reference free LLM Dependent Aggregation Large Language Models LLMs increasingly used assess NLP tasks ability generate human like judgments Single LLMs used initially recent work suggests using multiple LLMs judges yields improved performance important step exploiting multiple judgements combination stage aggregation Existing methods NLP assign equal weight LLM judgments designed specific tasks hallucination detection work focuses aggregating predictions multiple systems reference labels available new method called SkillAggregation proposed learns combine estimates LLM judges needing additional data ground truth extends Crowdlayer aggregation method developed image classification exploit judge estimates inference approach compared range standard aggregation methods HaluEval Dialogue TruthfulQA Chatbot Arena tasks SkillAggregation outperforms Crowdlayer tasks yields best performance approaches majority tasks
799,MasRouter: Learning to Route LLMs for Multi-Agent Systems,"['Yanwei Yue', 'Guibin Zhang', 'Boyang Liu', 'Guancheng Wan', 'Kun Wang', 'Dawei Cheng', 'Yiyan Qi']","Multi-agent systems (MAS) powered by Large Language Models (LLMs) have been demonstrated to push the boundaries of LLM capabilities, yet they often incur significant costs and face challenges in dynamic LLM selection. Current LLM routing methods effectively reduce overhead in single-agent scenarios by customizing LLM selection for each query, but they overlook the critical decisions regarding collaboration modes and agent roles in MAS. In response to this challenge, we first introduce the problem of Multi-Agent System Routing (MASR), which integrates all components of MAS into a unified routing framework. Toward this goal, we propose MasRouter, the first high-performing, cost-effective, and inductive MASR solution. MasRouter employs collaboration mode determination, role allocation, and LLM routing through a cascaded controller network, progressively constructing a MAS that balances effectiveness and efficiency. Extensive experiments demonstrate that MasRouter is (1) high-performing, achieving a $1.8\%\sim8.2\%$ improvement over the state-of-the-art method on MBPP; (2) economical, reducing overhead by up to $52.07\%$ compared to SOTA methods on HumanEval; and (3) plug-and-play, seamlessly integrating with mainstream MAS frameworks, reducing overhead by $17.21\%\sim28.17\%$ via customized routing. The code is available at https://github.com/yanweiyue/masrouter.",MasRouter Learning Route LLMs Multi Agent Systems Multi agent systems MAS powered Large Language Models LLMs demonstrated push boundaries LLM capabilities incur significant costs face challenges dynamic LLM selection Current LLM routing methods effectively reduce overhead single agent scenarios customizing LLM selection query overlook critical decisions regarding collaboration modes agent roles MAS response challenge introduce problem Multi Agent Routing MASR integrates components MAS unified routing framework goal propose MasRouter high performing cost effective inductive MASR solution MasRouter employs collaboration mode determination role allocation LLM routing cascaded controller network progressively constructing MAS balances effectiveness efficiency Extensive experiments demonstrate MasRouter 1 high performing achieving 1 8 sim8 2 improvement state art method MBPP 2 economical reducing overhead 52 07 compared SOTA methods HumanEval 3 plug play seamlessly integrating mainstream MAS frameworks reducing overhead 17 21 sim28 17 customized routing code available https github com yanweiyue masrouter
800,Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation,"['Haozhe Xu', 'Xiaohua Wang', 'Changze Lv', 'Xiaoqing Zheng']",,Single Labels Improving Conversational Recommendation LLM Powered Data Augmentation
801,Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation,"['Peiwen Yuan', 'Yueqi Zhang', 'Shaoxiong Feng', 'Yiwei Li', 'Xinglin Wang', 'Jiayi Shi', 'Chuyi Tan', 'Boyuan Pan', 'Yao Hu', 'Kan Li']","Evaluating models on large benchmarks is very resource-intensive, especially during the period of rapid model evolution. Existing efficient evaluation methods estimate the performance of target models by testing them only on a small and static coreset of the benchmark, which is derived from the publicly available evaluation results of source models. These methods rely on the assumption that target models have high prediction consistency with source models. However, we demonstrate that it doesn't generalize well in practice. To alleviate the inconsistency issue, we present TailoredBench, a method that conducts customized evaluation tailored to each target model. Specifically, a Global-coreset is first constructed as a probe to identify the most consistent source models for each target model with an adaptive source model selection strategy. Afterwards, a scalable K-Medoids clustering algorithm is proposed to extend the Global-coreset to a tailored Native-coreset for each target model. According to the predictions on Native-coresets, we obtain the performance of target models on the whole benchmark with a calibrated estimation strategy. Comprehensive experiments on 5 benchmarks across over 300 models demonstrate that compared to best performing baselines, TailoredBench achieves an average reduction of 31.4% in MAE of accuracy estimates under the same inference budgets, showcasing strong effectiveness and generalizability.",Size Fits Tailored Benchmarks Efficient Evaluation Evaluating models large benchmarks resource intensive especially period rapid model evolution Existing efficient evaluation methods estimate performance target models testing small static coreset benchmark derived publicly available evaluation results source models methods rely assumption target models high prediction consistency source models demonstrate doesn t generalize practice alleviate inconsistency issue present TailoredBench method conducts customized evaluation tailored target model Specifically Global coreset constructed probe identify consistent source models target model adaptive source model selection strategy scalable K Medoids clustering algorithm proposed extend Global coreset tailored Native coreset target model According predictions Native coresets obtain performance target models benchmark calibrated estimation strategy Comprehensive experiments 5 benchmarks 300 models demonstrate compared best performing baselines TailoredBench achieves average reduction 31 4 MAE accuracy estimates inference budgets showcasing strong effectiveness generalizability
802,Advancing Sequential Numerical Prediction in Autoregressive Models,"['Xiang Fei', 'Jinghui Lu', 'Qi Sun', 'Hao Feng', 'Yanjie Wang', 'Wei Shi', 'An-Lan Wang', 'Jingqun Tang', 'Can Huang']","Autoregressive models have become the de facto choice for sequence generation tasks, but standard approaches treat digits as independent tokens and apply cross-entropy loss, overlooking the coherent structure of numerical sequences. This paper introduces Numerical Token Integrity Loss (NTIL) to address this gap. NTIL operates at two levels: (1) token-level, where it extends the Earth Mover's Distance (EMD) to preserve ordinal relationships between numerical values, and (2) sequence-level, where it penalizes the overall discrepancy between the predicted and actual sequences. This dual approach improves numerical prediction and integrates effectively with LLMs/MLLMs. Extensive experiments show significant performance improvements with NTIL.",Advancing Sequential Numerical Prediction Autoregressive Models Autoregressive models facto choice sequence generation tasks standard approaches treat digits independent tokens apply cross entropy loss overlooking coherent structure numerical sequences paper introduces Numerical Token Integrity Loss NTIL address gap NTIL operates levels 1 token level extends Earth Mover s Distance EMD preserve ordinal relationships numerical values 2 sequence level penalizes overall discrepancy predicted actual sequences dual approach improves numerical prediction integrates effectively LLMs MLLMs Extensive experiments significant performance improvements NTIL
803,iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering,"['Shuai Wang', 'Yinan Yu']","While Large Language Models (LLMs) excel at many natural language processing tasks, they often suffer from factual inaccuracies in knowledge-intensive scenarios. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop connections. To address these issues, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs.",iQUEST Iterative Question Guided Framework Knowledge Base Question Answering Large Language Models LLMs excel natural language processing tasks suffer factual inaccuracies knowledge intensive scenarios Integrating external knowledge resources particularly knowledge graphs KGs provides transparent updatable foundation reliable reasoning Knowledge Base Question Answering KBQA queries reasons KGs central effort especially complex multi hop queries multi hop reasoning poses key challenges 1 maintaining coherent reasoning paths 2 avoiding prematurely discarding critical multi hop connections address issues introduce iQUEST question guided KBQA framework iteratively decomposes complex queries simpler sub questions ensuring structured focused reasoning trajectory Additionally integrate Graph Neural Network GNN look ahead incorporate 2 hop neighbor information reasoning step dual approach strengthens reasoning process enabling model explore viable paths effectively Detailed experiments demonstrate consistent improvement delivered iQUEST benchmark datasets LLMs
804,IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory,"['Wei Song', 'Zhenya Huang', 'Cheng Cheng', 'Weibo Gao', 'Bihan Xu', 'GuanHao Zhao', 'Fei Wang', 'Runze Wu']",,IRT Router Effective Interpretable Multi LLM Routing Item Response Theory
805,MLAS-LoRA: Language-Aware Parameters Detection and LoRA-Based Knowledge Transfer for Multilingual Machine Translation,"['Tianyu Dong', 'Bo Li', 'Jinsong Liu', 'shaolin Zhu', 'Deyi Xiong']",,MLAS LoRA Language Aware Parameters Detection LoRA Based Knowledge Transfer Multilingual Machine Translation
806,M2RC-EVAL: Massively Multilingual Repository-level Code Completion Evaluation,"['Jiaheng Liu', 'Ken Deng', 'Congnan Liu', 'Jian Yang', 'Shukai Liu', 'He Zhu', 'Peng Zhao', 'Linzheng Chai', 'Yanan Wu', 'Ge Zhang', 'Yingshui Tan', 'Zekun Moore Wang', 'JinKe', 'Zhaoxiang Zhang', 'Bangyu Xiang', 'Guoan Zhang', 'Wenbo Su', 'Bo Zheng']",,M2RC EVAL Massively Multilingual Repository level Code Completion Evaluation
807,Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation,"['Susanna Rücker', 'Alan Akbik']","Entity disambiguation (ED) is the task of linking mentions in text to corresponding entries in a knowledge base. Dual Encoders address this by embedding mentions and label candidates in a shared embedding space and applying a similarity metric to predict the correct label. In this work, we focus on evaluating key design decisions for Dual Encoder-based ED, such as its loss function, similarity metric, label verbalization format, and negative sampling strategy. We present the resulting model VerbalizED, a document-level Dual Encoder model that includes contextual label verbalizations and efficient hard negative sampling. Additionally, we explore an iterative prediction variant that aims to improve the disambiguation of challenging data points. Comprehensive experiments on AIDA-Yago validate the effectiveness of our approach, offering insights into impactful design choices that result in a new State-of-the-Art system on the ZELDA benchmark.",Evaluating Design Decisions Dual Encoder based Entity Disambiguation Entity disambiguation ED task linking mentions text corresponding entries knowledge base Dual Encoders address embedding mentions label candidates shared embedding space applying similarity metric predict correct label work focus evaluating key design decisions Dual Encoder based ED loss function similarity metric label verbalization format negative sampling strategy present resulting model VerbalizED document level Dual Encoder model includes contextual label verbalizations efficient hard negative sampling Additionally explore iterative prediction variant aims improve disambiguation challenging data points Comprehensive experiments AIDA Yago validate effectiveness approach offering insights impactful design choices result new State Art ZELDA benchmark
808,How to Compare Things Properly? A Study on Answering Comparative Questions using Argument Summarization,"['Irina Nikishina', 'Saba Anwar', 'Nikolay Dolgov', 'Maria Manina', 'Daria Ignatenko', 'Artem Shelmanov', 'Chris Biemann']",,Compare Things Properly Study Answering Comparative Questions using Argument Summarization
809,"FinanceReasoning: Make Financial Numerical Reasoning More Credible, Comprehensive, and Challenging","['Zichen Tang', 'Haihong E', 'Ziyan Ma', 'Haoyang He', 'Jiacheng Liu', 'Zhongjun Yang', 'Zihua Rong', 'Rongjin Li', 'Kun Ji', 'Huang Qing', 'Xinyang Hu', 'Yang Liu', 'Qianhe Zheng']",,FinanceReasoning Make Financial Numerical Reasoning Credible Comprehensive Challenging
810,Controllable Style Arithmetic with Language Models,"['Weiqi Wang', 'Wengang Zhou', 'Zongmeng Zhang', 'Jie Zhao', 'Houqiang Li']",,Controllable Style Arithmetic Language Models
811,Masks Can be Learned As An Alternative of Experts,"['Peiyu Liu', 'Tianwen Wei', 'Bo Zhu', 'Xin Zhao', 'Shuicheng YAN']",,Masks Learned Alternative Experts
812,Program Synthesis Benchmark for Visual Programming in XLogoOnline Environment,"['Chao Wen', 'Jacqueline Staub', 'Adish Singla']","Large language and multimodal models have shown remarkable successes on various benchmarks focused on specific skills such as general-purpose programming, natural language understanding, math word problem-solving, and visual question answering. However, it is unclear how well these models perform on tasks that require a combination of these skills. In this paper, we curate a novel program synthesis benchmark based on the XLogoOnline visual programming environment. The benchmark comprises 85 real-world tasks from the Mini-level of the XLogoOnline environment, each requiring a combination of different skills such as spatial planning, basic programming, and logical reasoning. Our evaluation shows that current state-of-the-art models like GPT-4V and Llama3-70B struggle to solve these tasks, achieving only 20% and 2.35% success rates. Next, we develop a fine-tuning pipeline to boost the performance of models by leveraging a large-scale synthetic training dataset with over 80000 tasks. Moreover, we showcase how emulator-driven feedback can be used to design a curriculum over training data distribution. We showcase that a fine-tuned Llama3-8B drastically outperforms GPT-4V and Llama3-70B models, and provide an in-depth analysis of the models' expertise across different skill dimensions. We will publicly release the benchmark for future research on program synthesis in visual programming.",Program Synthesis Benchmark Visual Programming XLogoOnline Environment Large language multimodal models shown remarkable successes various benchmarks focused specific skills general purpose programming natural language understanding math word problem solving visual question answering unclear models perform tasks require combination skills paper curate novel program synthesis benchmark based XLogoOnline visual programming environment benchmark comprises 85 real world tasks Mini level XLogoOnline environment requiring combination different skills spatial planning basic programming logical reasoning evaluation shows current state art models like GPT 4V Llama3 70B struggle solve tasks achieving 20 2 35 success rates develop fine tuning pipeline boost performance models leveraging large scale synthetic training dataset 80000 tasks showcase emulator driven feedback used design curriculum training data distribution showcase fine tuned Llama3 8B drastically outperforms GPT 4V Llama3 70B models provide depth analysis models expertise different skill dimensions publicly release benchmark future research program synthesis visual programming
813,Removal of Hallucination on Hallucination: Debate-Augmented RAG,"['Wentao Hu', 'Wengyu Zhang', 'Yiyang Jiang', 'Chen Jason Zhang', 'Xiaoyong Wei', 'Li Qing']","Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating external knowledge, yet it introduces a critical issue: erroneous or biased retrieval can mislead generation, compounding hallucinations, a phenomenon we term Hallucination on Hallucination. To address this, we propose Debate-Augmented RAG (DRAG), a training-free framework that integrates Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages. In retrieval, DRAG employs structured debates among proponents, opponents, and judges to refine retrieval quality and ensure factual reliability. In generation, DRAG introduces asymmetric information roles and adversarial debates, enhancing reasoning robustness and mitigating factual inconsistencies. Evaluations across multiple tasks demonstrate that DRAG improves retrieval reliability, reduces RAG-induced hallucinations, and significantly enhances overall factual accuracy. Our code is available at https://github.com/Huenao/Debate-Augmented-RAG.",Removal Hallucination Hallucination Debate Augmented RAG Retrieval Augmented Generation RAG enhances factual accuracy integrating external knowledge introduces critical issue erroneous biased retrieval mislead generation compounding hallucinations phenomenon term Hallucination Hallucination address propose Debate Augmented RAG DRAG training free framework integrates Multi Agent Debate MAD mechanisms retrieval generation stages retrieval DRAG employs structured debates proponents opponents judges refine retrieval quality ensure factual reliability generation DRAG introduces asymmetric information roles adversarial debates enhancing reasoning robustness mitigating factual inconsistencies Evaluations multiple tasks demonstrate DRAG improves retrieval reliability reduces RAG induced hallucinations significantly enhances overall factual accuracy code available https github com Huenao Debate Augmented RAG
814,CodeDPO: Aligning Code Models with Self Generated and Verified Source Code,"['Kechi Zhang', 'Ge Li', 'Yihong Dong', 'Jingjing Xu', 'Jun Zhang', 'Jing Su', 'Yongfei Liu', 'Zhi Jin']","Code generation models have shown significant potential for programming tasks. However, existing training methods like supervised fine-tuning face key limitations: they do not effectively teach models to prioritize correct over incorrect solutions in ambiguous situations, nor do they effectively optimize the runtime efficiency of the generated code. To address these challenges, we propose CodeDPO, a framework that integrates preference learning into code generation to improve two key code preference factors: code correctness and efficiency. CodeDPO employs a novel dataset construction method, utilizing a self-generation-and-validation mechanism that simultaneously generates and evaluates code and test cases. The underlying assumption is that test cases executable by multiple code snippets provide more reliable validation, and code that passes more tests is more likely to be correct. Through this self-validation process, our PageRank-inspired algorithm iteratively updates the ranking score of each code snippet, ultimately creating a code preference optimization dataset based on correctness and efficiency. CodeDPO is flexible and scalable, generating diverse preference optimization data without depending on external resources. Through comprehensive evaluations of five widely used benchmarks, CodeDPO demonstrates significant improvements in correctness and efficiency compared to existing methods. Our experiments prove that CodeDPO enhances the capabilities of LLMs in code generation and provides a robust foundation for conducting code preference optimization in more complex and challenging real-world scenarios.",CodeDPO Aligning Code Models Self Generated Verified Source Code Code generation models shown significant potential programming tasks existing training methods like supervised fine tuning face key limitations effectively teach models prioritize correct incorrect solutions ambiguous situations effectively optimize runtime efficiency generated code address challenges propose CodeDPO framework integrates preference learning code generation improve key code preference factors code correctness efficiency CodeDPO employs novel dataset construction method utilizing self generation validation mechanism simultaneously generates evaluates code test cases underlying assumption test cases executable multiple code snippets provide reliable validation code passes tests likely correct self validation process PageRank inspired algorithm iteratively updates ranking score code snippet ultimately creating code preference optimization dataset based correctness efficiency CodeDPO flexible scalable generating diverse preference optimization data depending external resources comprehensive evaluations widely used benchmarks CodeDPO demonstrates significant improvements correctness efficiency compared existing methods experiments prove CodeDPO enhances capabilities LLMs code generation provides robust foundation conducting code preference optimization complex challenging real world scenarios
815,ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering,"['Alexander Miserlis Hoyle', 'Lorena Calvo-Bartolomé', 'Jordan Lee Boyd-Graber', 'Philip Resnik']","Topic model and document-clustering evaluations either use automated metrics that align poorly with human preferences or require expert labels that are intractable to scale. We design a scalable human evaluation protocol and a corresponding automated approximation that reflect practitioners' real-world usage of models. Annotators -- or an LLM-based proxy -- review text items assigned to a topic or cluster, infer a category for the group, then apply that category to other documents. Using this protocol, we collect extensive crowdworker annotations of outputs from a diverse set of topic models on two datasets. We then use these annotations to validate automated proxies, finding that the best LLM proxies are statistically indistinguishable from a human annotator and can therefore serve as a reasonable substitute in automated evaluations. Package, web interface, and data are at https://github.com/ahoho/proxann",ProxAnn Use Oriented Evaluations Topic Models Document Clustering Topic model document clustering evaluations use automated metrics align poorly human preferences require expert labels intractable scale design scalable human evaluation protocol corresponding automated approximation reflect practitioners real world usage models Annotators LLM based proxy review text items assigned topic cluster infer category group apply category documents Using protocol collect extensive crowdworker annotations outputs diverse set topic models datasets use annotations validate automated proxies finding best LLM proxies statistically indistinguishable human annotator serve reasonable substitute automated evaluations Package web interface data https github com ahoho proxann
816,BOOKWORLD: From Novels to Interactive Agent Societies for Story Creation,"['Yiting Ran', 'Xintao Wang', 'Tian Qiu', 'Jiaqing Liang', 'Yanghua Xiao', 'Deqing Yang']",,BOOKWORLD Novels Interactive Agent Societies Story Creation
817,Quantifying Lexical Semantic Shift via Unbalanced Optimal Transport,"['Ryo Kishino', 'Hiroaki Yamagiwa', 'Ryo Nagata', 'Sho Yokoi', 'Hidetoshi Shimodaira']","Lexical semantic change detection aims to identify shifts in word meanings over time. While existing methods using embeddings from a diachronic corpus pair estimate the degree of change for target words, they offer limited insight into changes at the level of individual usage instances. To address this, we apply Unbalanced Optimal Transport (UOT) to sets of contextualized word embeddings, capturing semantic change through the excess and deficit in the alignment between usage instances. In particular, we propose Sense Usage Shift (SUS), a measure that quantifies changes in the usage frequency of a word sense at each usage instance. By leveraging SUS, we demonstrate that several challenges in semantic change detection can be addressed in a unified manner, including quantifying instance-level semantic change and word-level tasks such as measuring the magnitude of semantic change and the broadening or narrowing of meaning.",Quantifying Lexical Semantic Shift Unbalanced Optimal Transport Lexical semantic change detection aims identify shifts word meanings time existing methods using embeddings diachronic corpus pair estimate degree change target words offer limited insight changes level individual usage instances address apply Unbalanced Optimal Transport UOT sets contextualized word embeddings capturing semantic change excess deficit alignment usage instances particular propose Sense Usage Shift SUS measure quantifies changes usage frequency word sense usage instance leveraging SUS demonstrate challenges semantic change detection addressed unified manner including quantifying instance level semantic change word level tasks measuring magnitude semantic change broadening narrowing meaning
818,Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems,"['Hao Peng', 'Yunjia Qi', 'Xiaozhi Wang', 'Zijun Yao', 'Bin Xu', 'Lei Hou', 'Juanzi Li']","Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).",Agentic Reward Modeling Integrating Human Preferences Verifiable Correctness Signals Reliable Reward Systems Reward models RMs crucial training inference time scaling large language models LLMs existing reward models primarily focus human preferences neglecting verifiable correctness signals shown strong potential training LLMs paper propose agentic reward modeling reward combines reward models verifiable correctness signals different aspects provide reliable rewards empirically implement reward agent named RewardAgent combines human preference rewards verifiable signals factuality instruction following provide reliable rewards conduct comprehensive experiments existing reward model benchmarks inference time best n searches real world downstream tasks RewardAgent significantly outperforms vanilla reward models demonstrating effectiveness construct training preference pairs using RewardAgent train LLM DPO objective achieving superior performance various NLP benchmarks compared conventional reward models codes publicly released facilitate research https github com THU KEG Agentic Reward Modeling
819,Adaptive and Robust Translation from Natural Language to Multi-model Query Languages,"['Gengyuan Shi', 'Chaokun Wang', 'Liu Yabin', 'Jiawei Ren']",,Adaptive Robust Translation Natural Language Multi model Query Languages
820,SAKE: Steering Activations for Knowledge Editing,"['Marco Scialanga', 'Thibault Laugel', 'Vincent Grari', 'Marcin Detyniecki']","As Large Langue Models have been shown to memorize real-world facts, the need to update this knowledge in a controlled and efficient manner arises. Designed with these constraints in mind, Knowledge Editing (KE) approaches propose to alter specific facts in pretrained models. However, they have been shown to suffer from several limitations, including their lack of contextual robustness and their failure to generalize to logical implications related to the fact. To overcome these issues, we propose SAKE, a steering activation method that models a fact to be edited as a distribution rather than a single prompt. Leveraging Optimal Transport, SAKE alters the LLM behavior over a whole fact-related distribution, defined as paraphrases and logical implications. Several numerical experiments demonstrate the effectiveness of this method: SAKE is thus able to perform more robust edits than its existing counterparts.",SAKE Steering Activations Knowledge Editing Large Langue Models shown memorize real world facts need update knowledge controlled efficient manner arises Designed constraints mind Knowledge Editing KE approaches propose alter specific facts pretrained models shown suffer limitations including lack contextual robustness failure generalize logical implications related fact overcome issues propose SAKE steering activation method models fact edited distribution single prompt Leveraging Optimal Transport SAKE alters LLM behavior fact related distribution defined paraphrases logical implications numerical experiments demonstrate effectiveness method SAKE able perform robust edits existing counterparts
821,Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs,"['Danni Liu', 'Jan Niehues']","While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (https://github.com/dannigt/mid-align).",Middle Layer Representation Alignment Cross Lingual Transfer Fine Tuned LLMs large language models demonstrate remarkable capabilities task specific applications fine tuning extending benefits diverse languages essential broad accessibility effective cross lingual transfer hindered LLM performance gaps languages scarcity fine tuning data languages analysis LLM internal representations 1 000 language pairs discover middle layers exhibit strongest potential cross lingual alignment Building finding propose middle layer alignment objective integrated task specific training experiments slot filling machine translation structured text generation consistent improvements cross lingual transfer especially lower resource languages method robust choice alignment languages generalizes languages unseen alignment Furthermore separately trained alignment modules merged existing task specific modules improving cross lingual capabilities training code publicly available https github com dannigt mid align
822,Can external validation tools improve annotation quality for LLM-as-a-Judge?,"['Arduin Findeis', 'Floris Weers', 'Guoli Yin', 'Ke Ye', 'Ruoming Pang', 'Tom Gunter']",,external validation tools improve annotation quality LLM Judge
823,One for All: Update Parameterized Knowledge Across Multiple Models with Once Edit,"['Weitao Ma', 'Xiyuan Du', 'Xiaocheng Feng', 'Lei Huang', 'Yichong Huang', 'Huiyi Zhang', 'Xiaoliang Yang', 'Baohang Li', 'Xiachong Feng', 'Ting Liu', 'Bing Qin']",,Update Parameterized Knowledge Multiple Models Edit
824,VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service,"['Xiasi Wang', 'Tianliang Yao', 'Simin Chen', 'Runqi Wang', 'Lei YE', 'Kuofeng Gao', 'Yi Huang', 'Yuan Yao']","Vision-Language Models (VLMs) have demonstrated great potential in real-world applications. While existing research primarily focuses on improving their accuracy, the efficiency remains underexplored. Given the real-time demands of many applications and the high inference overhead of VLMs, efficiency robustness is a critical issue. However, previous studies evaluate efficiency robustness under unrealistic assumptions, requiring access to the model architecture and parameters -- an impractical scenario in ML-as-a-service settings, where VLMs are deployed via inference APIs. To address this gap, we propose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness in a realistic black-box setting. VLMInferSlow incorporates fine-grained efficiency modeling tailored to VLM inference and leverages zero-order optimization to search for adversarial examples. Experimental results show that VLMInferSlow generates adversarial images with imperceptible perturbations, increasing the computational cost by up to 128.47%. We hope this research raises the community's awareness about the efficiency robustness of VLMs.",VLMInferSlow Evaluating Efficiency Robustness Large Vision Language Models Service Vision Language Models VLMs demonstrated great potential real world applications existing research primarily focuses improving accuracy efficiency remains underexplored Given real time demands applications high inference overhead VLMs efficiency robustness critical issue previous studies evaluate efficiency robustness unrealistic assumptions requiring access model architecture parameters impractical scenario ML service settings VLMs deployed inference APIs address gap propose VLMInferSlow novel approach evaluating VLM efficiency robustness realistic black box setting VLMInferSlow incorporates fine grained efficiency modeling tailored VLM inference leverages zero order optimization search adversarial examples Experimental results VLMInferSlow generates adversarial images imperceptible perturbations increasing computational cost 128 47 hope research raises community s awareness efficiency robustness VLMs
825,The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs,"['Nitay Calderon', 'Roi Reichart', 'Rotem Dror']","The ""LLM-as-an-annotator"" and ""LLM-as-a-judge"" paradigms employ Large Language Models (LLMs) as annotators, judges, and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM annotators and judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming the open-source LLMs we examine, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.",Alternative Annotator Test LLM Judge Statistically Justify Replacing Human Annotators LLMs LLM annotator LLM judge paradigms employ Large Language Models LLMs annotators judges evaluators tasks traditionally performed humans LLM annotations widely used NLP research fields like medicine psychology social science Despite role shaping study results insights standard rigorous procedure determine LLMs replace human annotators paper propose novel statistical procedure Alternative Annotator Test alt test requires modest subset annotated examples justify using LLM annotations Additionally introduce versatile interpretable measure comparing LLM annotators judges demonstrate procedure curated diverse collection datasets consisting language vision language tasks conducted experiments LLMs prompting techniques results LLMs replace humans closed source LLMs GPT 4o outperforming open source LLMs examine prompting techniques yield judges varying quality hope study encourages rigorous reliable practices
826,CrisisTS: Coupling Social Media Textual Data and Meteorological Time Series for Urgency Classification,"['Romain Meunier', 'Farah Benamara', 'Véronique Moriceau', 'Savitha Ramasamy', 'Zhongzheng Qiao']",,CrisisTS Coupling Social Media Textual Data Meteorological Time Series Urgency Classification
827,How to Mitigate Overfitting in Weak-to-strong Generalization?,"['Junhao Shi', 'Qinyuan Cheng', 'Zhaoye Fei', 'Yining Zheng', 'Qipeng Guo', 'Xipeng Qiu']","Aligning powerful AI models on tasks that surpass human evaluation capabilities is the central problem of \textbf{superalignment}. To address this problem, weak-to-strong generalization aims to elicit the capabilities of strong models through weak supervisors and ensure that the behavior of strong models aligns with the intentions of weak supervisors without unsafe behaviors such as deception. Although weak-to-strong generalization exhibiting certain generalization capabilities, strong models exhibit significant overfitting in weak-to-strong generalization: Due to the strong fit ability of strong models, erroneous labels from weak supervisors may lead to overfitting in strong models. In addition, simply filtering out incorrect labels may lead to a degeneration in question quality, resulting in a weak generalization ability of strong models on hard questions. To mitigate overfitting in weak-to-strong generalization, we propose a two-stage framework that simultaneously improves the quality of supervision signals and the quality of input questions. Experimental results in three series of large language models and two mathematical benchmarks demonstrate that our framework significantly improves PGR compared to naive weak-to-strong generalization, even achieving up to 100\% PGR on some models.",Mitigate Overfitting Weak strong Generalization Aligning powerful AI models tasks surpass human evaluation capabilities central problem textbf superalignment address problem weak strong generalization aims elicit capabilities strong models weak supervisors ensure behavior strong models aligns intentions weak supervisors unsafe behaviors deception weak strong generalization exhibiting certain generalization capabilities strong models exhibit significant overfitting weak strong generalization strong fit ability strong models erroneous labels weak supervisors lead overfitting strong models addition simply filtering incorrect labels lead degeneration question quality resulting weak generalization ability strong models hard questions mitigate overfitting weak strong generalization propose stage framework simultaneously improves quality supervision signals quality input questions Experimental results series large language models mathematical benchmarks demonstrate framework significantly improves PGR compared naive weak strong generalization achieving 100 PGR models
828,Com$^2$ : A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models,"['Kai Xiong', 'Xiao Ding', 'Yixin Cao', 'Yuxiong Yan', 'Li Du', 'Yufei Zhang', 'Jinglong Gao', 'Jiaqian Liu', 'Bing Qin', 'Ting Liu']",,Com 2 Causal Guided Benchmark Exploring Complex Commonsense Reasoning Large Language Models
829,Dynamic Head Selection for Neural Lexicalized Constituency Parsing,"['Yang Hou', 'Zhenghua Li']",,Dynamic Head Selection Neural Lexicalized Constituency Parsing
830,My Words Imply Your Opinion: Reader Agent-Based Propagation Enhancement for Personalized Implicit Emotion Analysis,"['Jian Liao', 'Yu Feng', 'Yujin Zheng', 'Jun Zhao', 'Suge Wang', 'JianXing Zheng']",,Words Imply Opinion Reader Agent Based Propagation Enhancement Personalized Implicit Emotion Analysis
831,EvolveBench: A Comprehensive Benchmark for Assessing Temporal Awareness in LLMs on Evolving Knowledge,"['Zhiyuan Zhu', 'Yusheng Liao', 'Zhe Chen', 'Yuhao Wang', 'Yunfeng Guan', 'Yanfeng Wang', 'Yu Wang']",,EvolveBench Comprehensive Benchmark Assessing Temporal Awareness LLMs Evolving Knowledge
832,Enabling LLM Knowledge Analysis via Extensive Materialization,"['Yujia Hu', 'Tuan-Phong Nguyen', 'Shrestha Ghosh', 'Simon Razniewski']","Large language models (LLMs) have majorly advanced NLP and AI, and next to their ability to perform a wide range of procedural tasks, a major success factor is their internalized factual knowledge. Since Petroni et al. (2019), analyzing this knowledge has gained attention. However, most approaches investigate one question at a time via modest-sized pre-defined samples, introducing an ``availability bias'' (Tversky&Kahnemann, 1973) that prevents the analysis of knowledge (or beliefs) of LLMs beyond the experimenter's predisposition. To address this challenge, we propose a novel methodology to comprehensively materialize an LLM's factual knowledge through recursive querying and result consolidation. Our approach is a milestone for LLM research, for the first time providing constructive insights into the scope and structure of LLM knowledge (or beliefs). As a prototype, we build GPTKB, a knowledge base (KB) comprising 101 million relational triples for over 2.9 million entities from GPT-4o-mini. We use GPTKB to exemplarily analyze GPT-4o-mini's factual knowledge in terms of scale, accuracy, bias, cutoff and consistency, at the same time. GPTKB is accessible at https://gptkb.org",Enabling LLM Knowledge Analysis Extensive Materialization Large language models LLMs majorly advanced NLP AI ability perform wide range procedural tasks major success factor internalized factual knowledge Petroni et al 2019 analyzing knowledge gained attention approaches investigate question time modest sized pre defined samples introducing availability bias Tversky Kahnemann 1973 prevents analysis knowledge beliefs LLMs experimenter s predisposition address challenge propose novel methodology comprehensively materialize LLM s factual knowledge recursive querying result consolidation approach milestone LLM research time providing constructive insights scope structure LLM knowledge beliefs prototype build GPTKB knowledge base KB comprising 101 million relational triples 2 9 million entities GPT 4o mini use GPTKB exemplarily analyze GPT 4o mini s factual knowledge terms scale accuracy bias cutoff consistency time GPTKB accessible https gptkb org
833,Rhythm Controllable and Efficient Zero-Shot Voice Conversion via Shortcut Flow Matching,"['Jialong Zuo', 'Shengpeng Ji', 'Minghui Fang', 'Mingze Li', 'Ziyue Jiang', 'Xize Cheng', 'Xiaoda Yang', 'Chen Feiyang', 'Xinyu Duan', 'Zhou Zhao']","Zero-Shot Voice Conversion (VC) aims to transform the source speaker's timbre into an arbitrary unseen one while retaining speech content. Most prior work focuses on preserving the source's prosody, while fine-grained timbre information may leak through prosody, and transferring target prosody to synthesized speech is rarely studied. In light of this, we propose R-VC, a rhythm-controllable and efficient zero-shot voice conversion model. R-VC employs data perturbation techniques and discretize source speech into Hubert content tokens, eliminating much content-irrelevant information. By leveraging a Mask Generative Transformer for in-context duration modeling, our model adapts the linguistic content duration to the desired target speaking style, facilitating the transfer of the target speaker's rhythm. Furthermore, R-VC introduces a powerful Diffusion Transformer (DiT) with shortcut flow matching during training, conditioning the network not only on the current noise level but also on the desired step size, enabling high timbre similarity and quality speech generation in fewer sampling steps, even in just two, thus minimizing latency. Experimental results show that R-VC achieves comparable speaker similarity to state-of-the-art VC methods with a smaller dataset, and surpasses them in terms of speech naturalness, intelligibility and style transfer performance.",Rhythm Controllable Efficient Zero Shot Voice Conversion Shortcut Flow Matching Zero Shot Voice Conversion VC aims transform source speaker s timbre arbitrary unseen retaining speech content prior work focuses preserving source s prosody fine grained timbre information leak prosody transferring target prosody synthesized speech rarely studied light propose R VC rhythm controllable efficient zero shot voice conversion model R VC employs data perturbation techniques discretize source speech Hubert content tokens eliminating content irrelevant information leveraging Mask Generative Transformer context duration modeling model adapts linguistic content duration desired target speaking style facilitating transfer target speaker s rhythm Furthermore R VC introduces powerful Diffusion Transformer DiT shortcut flow matching training conditioning network current noise level desired step size enabling high timbre similarity quality speech generation fewer sampling steps just minimizing latency Experimental results R VC achieves comparable speaker similarity state art VC methods smaller dataset surpasses terms speech naturalness intelligibility style transfer performance
834,"Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs","['Jingcheng Niu', 'Xingdi Yuan', 'Tong Wang', 'Hamidreza Saghir', 'Amir H. Abdi']","We observe a novel phenomenon, contextual entrainment, across a wide range of language models (LMs) and prompt settings, providing a new mechanistic perspective on how LMs become distracted by ``irrelevant'' contextual information in the input prompt. Specifically, LMs assign significantly higher logits (or probabilities) to any tokens that have previously appeared in the context prompt, even for random tokens. This suggests that contextual entrainment is a mechanistic phenomenon, occurring independently of the relevance or semantic relation of the tokens to the question or the rest of the sentence. We find statistically significant evidence that the magnitude of contextual entrainment is influenced by semantic factors. Counterfactual prompts have a greater effect compared to factual ones, suggesting that while contextual entrainment is a mechanistic phenomenon, it is modulated by semantic factors. We hypothesise that there is a circuit of attention heads -- the entrainment heads -- that corresponds to the contextual entrainment phenomenon. Using a novel entrainment head discovery method based on differentiable masking, we identify these heads across various settings. When we ``turn off'' these heads, i.e., set their outputs to zero, the effect of contextual entrainment is significantly attenuated, causing the model to generate output that capitulates to what it would produce if no distracting context were provided. Our discovery of contextual entrainment, along with our investigation into LM distraction via the entrainment heads, marks a key step towards the mechanistic analysis and mitigation of the distraction problem.",Llama Llama Mechanistic Perspective Contextual Entrainment Distraction LLMs observe novel phenomenon contextual entrainment wide range language models LMs prompt settings providing new mechanistic perspective LMs distracted irrelevant contextual information input prompt Specifically LMs assign significantly higher logits probabilities tokens previously appeared context prompt random tokens suggests contextual entrainment mechanistic phenomenon occurring independently relevance semantic relation tokens question rest sentence statistically significant evidence magnitude contextual entrainment influenced semantic factors Counterfactual prompts greater effect compared factual ones suggesting contextual entrainment mechanistic phenomenon modulated semantic factors hypothesise circuit attention heads entrainment heads corresponds contextual entrainment phenomenon Using novel entrainment head discovery method based differentiable masking identify heads various settings turn heads e set outputs zero effect contextual entrainment significantly attenuated causing model generate output capitulates produce distracting context provided discovery contextual entrainment investigation LM distraction entrainment heads marks key step mechanistic analysis mitigation distraction problem
835,CritiQ: Mining Data Quality Criteria from Human Preferences,"['Honglin Guo', 'Kai Lv', 'Qipeng Guo', 'Tianyi Liang', 'Zhiheng Xi', 'Demin Song', 'Qiuyinzhe Zhang', 'Yu Sun', 'Kai Chen', 'Xipeng Qiu', 'Tao Gui']","Language model heavily depends on high-quality data for optimal performance. Existing approaches rely on manually designed heuristics, the perplexity of existing models, training classifiers, or careful prompt engineering, which require significant expert experience and human annotation effort while introduce biases. We introduce CritiQ, a novel data selection method that automatically mines criteria from human preferences for data quality with only ~30 human-annotated pairs and performs efficient data selection. The main component, CritiQ Flow, employs a manager agent to evolve quality criteria and worker agents to make pairwise judgments. We build a knowledge base that extracts quality criteria from previous work to boost CritiQ Flow. Compared to perplexity- and classifier- based methods, verbal criteria are more interpretable and possess reusable value. After deriving the criteria, we train the CritiQ Scorer to give quality scores and perform efficient data selection. We demonstrate the effectiveness of our method in the code, math, and logic domains, achieving high accuracy on human-annotated test sets. To validate the quality of the selected data, we continually train Llama 3.1 models and observe improved performance on downstream tasks compared to uniform sampling. Ablation studies validate the benefits of the knowledge base and the reflection process. We analyze how criteria evolve and the effectiveness of majority voting.",CritiQ Mining Data Quality Criteria Human Preferences Language model heavily depends high quality data optimal performance Existing approaches rely manually designed heuristics perplexity existing models training classifiers careful prompt engineering require significant expert experience human annotation effort introduce biases introduce CritiQ novel data selection method automatically mines criteria human preferences data quality 30 human annotated pairs performs efficient data selection main component CritiQ Flow employs manager agent evolve quality criteria worker agents make pairwise judgments build knowledge base extracts quality criteria previous work boost CritiQ Flow Compared perplexity classifier based methods verbal criteria interpretable possess reusable value deriving criteria train CritiQ Scorer quality scores perform efficient data selection demonstrate effectiveness method code math logic domains achieving high accuracy human annotated test sets validate quality selected data continually train Llama 3 1 models observe improved performance downstream tasks compared uniform sampling Ablation studies validate benefits knowledge base reflection process analyze criteria evolve effectiveness majority voting
836,Theoretical Guarantees for Minimum Bayes Risk Decoding,"['Yuki Ichihara', 'Yuu Jinnai', 'Kaito Ariu', 'Tetsuro Morimura', 'Eiji Uchibe']","Minimum Bayes Risk (MBR) decoding optimizes output selection by maximizing the expected utility value of an underlying human distribution. While prior work has shown the effectiveness of MBR decoding through empirical evaluation, few studies have analytically investigated why the method is effective. As a result of our analysis, we show that, given the size $n$ of the reference hypothesis set used in computation, MBR decoding approaches the optimal solution with high probability at a rate of $O\left(n^{-\frac{1}{2}}\right)$, under certain assumptions, even though the language space $Y$ is significantly larger $|Y|\gg n$. This result helps to theoretically explain the strong performance observed in several prior empirical studies on MBR decoding. In addition, we provide the performance gap for maximum-a-posteriori (MAP) decoding and compare it to MBR decoding. The result of this paper indicates that MBR decoding tends to converge to the optimal solution faster than MAP decoding in several cases.",Theoretical Guarantees Minimum Bayes Risk Decoding Minimum Bayes Risk MBR decoding optimizes output selection maximizing expected utility value underlying human distribution prior work shown effectiveness MBR decoding empirical evaluation studies analytically investigated method effective result analysis given size n reference hypothesis set used computation MBR decoding approaches optimal solution high probability rate O left n frac 1 2 right certain assumptions language space Y significantly larger Y gg n result helps theoretically explain strong performance observed prior empirical studies MBR decoding addition provide performance gap maximum posteriori MAP decoding compare MBR decoding result paper indicates MBR decoding tends converge optimal solution faster MAP decoding cases
837,Mutual-Taught for Co-adapting Policy and Reward Models,"['Tianyuan Shi', 'Canbin Huang', 'Fanqi Wan', 'Longguang Zhong', 'Ziyi Yang', 'Weizhou Shen', 'Xiaojun Quan', 'Ming Yan']","During the preference optimization of large language models (LLMs), distribution shifts may arise between newly generated model samples and the data used to train the reward model (RM). This shift reduces the efficacy of the RM, which in turn negatively impacts the performance of the policy model (PM). To address this challenge, we propose Mutual-Taught, a self-training method that iteratively improves both the PM and RM without requiring additional human annotation. Our approach mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM, guiding the PM toward a better approximation of the latent optimal preference distribution. In the M-step, we update the RM by constructing training data from the outputs of the PM before and after the E-step update. This process ensures that the RM adapts to the evolving policy distribution. Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, our 8B policy model, LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par with GPT-4o-2024-08-06 on RewardBench.",Mutual Taught adapting Policy Reward Models preference optimization large language models LLMs distribution shifts arise newly generated model samples data used train reward model RM shift reduces efficacy RM turn negatively impacts performance policy model PM address challenge propose Mutual Taught self training method iteratively improves PM RM requiring additional human annotation approach mirrors expectation maximization EM algorithm E step PM updated using feedback current RM guiding PM better approximation latent optimal preference distribution M step update RM constructing training data outputs PM E step update process ensures RM adapts evolving policy distribution Experimental results demonstrate iterative approach leads consistent improvements models Specifically 8B policy model LLaMA 3 8B Instruct MT achieves length controlled win rate 54 1 AlpacaEval 2 8B reward model FsfairX LLaMA3 RM MT performs par GPT 4o 2024 08 06 RewardBench
838,Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages,"['Wenhao Zhuang', 'Yuan Sun', 'Xiaobing Zhao']",,Enhancing Cross Lingual Transfer Reversible Transliteration Huffman Based Approach Low Resource Languages
839,Unmasking Style Sensitivity: A Causal Analysis of Bias Evaluation Instability in Large Language Models,"['Jiaxu Zhao', 'Meng Fang', 'Kun Zhang', 'Mykola Pechenizkiy']",,Unmasking Style Sensitivity Causal Analysis Bias Evaluation Instability Large Language Models
840,"MockConf: A Student Interpretation Dataset: Analysis, Word- and Span-level Alignment and Baselines","['Dávid Javorský', 'Ondřej Bojar', 'François Yvon']","In simultaneous interpreting, an interpreter renders a source speech into another language with a very short lag, much sooner than sentences are finished. In order to understand and later reproduce this dynamic and complex task automatically, we need dedicated datasets and tools for analysis, monitoring, and evaluation, such as parallel speech corpora, and tools for their automatic annotation. Existing parallel corpora of translated texts and associated alignment algorithms hardly fill this gap, as they fail to model long-range interactions between speech segments or specific types of divergences (e.g., shortening, simplification, functional generalization) between the original and interpreted speeches. In this work, we introduce MockConf, a student interpreting dataset that was collected from Mock Conferences run as part of the students' curriculum. This dataset contains 7 hours of recordings in 5 European languages, transcribed and aligned at the level of spans and words. We further implement and release InterAlign, a modern web-based annotation tool for parallel word and span annotations on long inputs, suitable for aligning simultaneous interpreting. We propose metrics for the evaluation and a baseline for automatic alignment. Dataset and tools are released to the community.",MockConf Student Interpretation Dataset Analysis Word Span level Alignment Baselines simultaneous interpreting interpreter renders source speech language short lag sooner sentences finished order understand later reproduce dynamic complex task automatically need dedicated datasets tools analysis monitoring evaluation parallel speech corpora tools automatic annotation Existing parallel corpora translated texts associated alignment algorithms hardly gap fail model long range interactions speech segments specific types divergences e g shortening simplification functional generalization original interpreted speeches work introduce MockConf student interpreting dataset collected Mock Conferences run students curriculum dataset contains 7 hours recordings 5 European languages transcribed aligned level spans words implement release InterAlign modern web based annotation tool parallel word span annotations long inputs suitable aligning simultaneous interpreting propose metrics evaluation baseline automatic alignment Dataset tools released community
841,BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning,"['Ercong Nie', 'Bo Shao', 'Mingyang Wang', 'Zifeng Ding', 'Helmut Schmid', 'Hinrich Schuetze']","This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual in-context knowledge editing (IKE) across 53 languages, unifying three knowledge editing (KE) datasets: zsRE, CounterFact, and WikiFactDiff. Cross-lingual KE, which requires knowledge edited in one language to generalize across others while preserving unrelated knowledge, remains underexplored. To address this gap, we systematically evaluate IKE under zero-shot, one-shot, and few-shot setups, incorporating tailored metric-specific demonstrations. Our findings reveal that model scale and demonstration alignment critically govern cross-lingual IKE efficacy, with larger models and tailored demonstrations significantly improving performance. Linguistic properties, particularly script type, strongly influence performance variation across languages, with non-Latin languages underperforming due to issues like language confusion. Code and data are publicly available at: https://github.com/ercong21/MultiKnow/.",BMIKE 53 Investigating Cross Lingual Knowledge Editing Context Learning paper introduces BMIKE 53 comprehensive benchmark cross lingual context knowledge editing IKE 53 languages unifying knowledge editing KE datasets zsRE CounterFact WikiFactDiff Cross lingual KE requires knowledge edited language generalize preserving unrelated knowledge remains underexplored address gap systematically evaluate IKE zero shot shot shot setups incorporating tailored metric specific demonstrations findings reveal model scale demonstration alignment critically govern cross lingual IKE efficacy larger models tailored demonstrations significantly improving performance Linguistic properties particularly script type strongly influence performance variation languages non Latin languages underperforming issues like language confusion Code data publicly available https github com ercong21 MultiKnow
842,What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation,"['Dingyi Yang', 'Qin Jin']",,Matters Evaluating Book Length Stories Systematic Study Long Story Evaluation
843,PROPER: A Progressive Learning Framework for Personalized Large Language Models with Group-Level Adaptation,"['Linhai Zhang', 'Jialong Wu', 'Deyu Zhou', 'Yulan He']","Personalized large language models (LLMs) aim to tailor their outputs to user preferences. Recent advances in parameter-efficient fine-tuning (PEFT) methods have highlighted the effectiveness of adapting population-level LLMs to personalized LLMs by fine-tuning user-specific parameters with user history. However, user data is typically sparse, making it challenging to adapt LLMs to specific user patterns. To address this challenge, we propose PROgressive PERsonalization (PROPER), a novel progressive learning framework inspired by meso-level theory in social science. PROPER bridges population-level and user-level models by grouping users based on preferences and adapting LLMs in stages. It combines a Mixture-of-Experts (MoE) structure with Low Ranked Adaptation (LoRA), using a user-aware router to assign users to appropriate groups automatically. Additionally, a LoRA-aware router is proposed to facilitate the integration of individual user LoRAs with group-level LoRAs. Experimental results show that PROPER significantly outperforms SOTA models across multiple tasks, demonstrating the effectiveness of our approach.",PROPER Progressive Learning Framework Personalized Large Language Models Group Level Adaptation Personalized large language models LLMs aim tailor outputs user preferences Recent advances parameter efficient fine tuning PEFT methods highlighted effectiveness adapting population level LLMs personalized LLMs fine tuning user specific parameters user history user data typically sparse making challenging adapt LLMs specific user patterns address challenge propose PROgressive PERsonalization PROPER novel progressive learning framework inspired meso level theory social science PROPER bridges population level user level models grouping users based preferences adapting LLMs stages combines Mixture Experts MoE structure Low Ranked Adaptation LoRA using user aware router assign users appropriate groups automatically Additionally LoRA aware router proposed facilitate integration individual user LoRAs group level LoRAs Experimental results PROPER significantly outperforms SOTA models multiple tasks demonstrating effectiveness approach
844,Enhancing Event-centric News Cluster Summarization via Data Sharpening and Localization Insights,"['Longyin Zhang', 'Bowei Zou', 'AiTi Aw']",,Enhancing Event centric News Cluster Summarization Data Sharpening Localization Insights
845,MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration,"['Zhitao He', 'Sandeep Polisetty', 'Zhiyuan Fan', 'Shujin Wu', 'Yuchen Huang', 'Yi R. Fung']","In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoning, which requires multi-level (e.g., perception, reasoning) and multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior work on estimating model confidence tends to focus on the overall response for training and calibration, but fails to assess confidence in each reasoning step, leading to undesirable hallucination snowballing. In this work, we present MMBoundary, a novel framework that advances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration. To achieve this, we propose to incorporate complementary textual and cross-modal self-rewarding signals to estimate confidence at each step of the MLLM reasoning process. In addition to supervised fine-tuning MLLM on this set of self-rewarded confidence estimation signal for initial confidence expression warm-up, we introduce a reinforcement learning stage with multiple reward functions for further aligning model knowledge and calibrating confidence at each reasoning step, enhancing reasoning chain self-correction. Empirical results show that MMBoundary significantly outperforms existing methods across diverse domain datasets and metrics, achieving an average of 7.5% reduction in multimodal confidence calibration errors and up to 8.3% improvement in task performance.",MMBoundary Advancing MLLM Knowledge Boundary Awareness Reasoning Step Confidence Calibration recent years multimodal large language models MLLMs significant progress continue face inherent challenges multimodal reasoning requires multi level e g perception reasoning multi granular e g multi step reasoning chain advanced inferencing Prior work estimating model confidence tends focus overall response training calibration fails assess confidence reasoning step leading undesirable hallucination snowballing work present MMBoundary novel framework advances knowledge boundary awareness MLLMs reasoning step confidence calibration achieve propose incorporate complementary textual cross modal self rewarding signals estimate confidence step MLLM reasoning process addition supervised fine tuning MLLM set self rewarded confidence estimation signal initial confidence expression warm introduce reinforcement learning stage multiple reward functions aligning model knowledge calibrating confidence reasoning step enhancing reasoning chain self correction Empirical results MMBoundary significantly outperforms existing methods diverse domain datasets metrics achieving average 7 5 reduction multimodal confidence calibration errors 8 3 improvement task performance
846,LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios,"['Xiaodong Wu', 'Minhao Wang', 'Yichen Liu', 'Xiaoming Shi', 'He Yan', 'Lu Xiangju', 'Junmin Zhu', 'Wei Zhang']","As Large Language Models (LLMs) evolve in natural language processing (NLP), their ability to stably follow instructions in long-context inputs has become critical for real-world applications. However, existing benchmarks seldom focus on instruction-following in long-context scenarios or stability on different inputs. To bridge this gap, we introduce LIFBench, a scalable dataset designed to evaluate LLMs' instruction-following capabilities and stability across long contexts. LIFBench comprises three long-context scenarios and eleven diverse tasks, featuring 2,766 instructions generated through an automated expansion method across three dimensions: length, expression, and variables. For evaluation, we propose LIFEval, a rubric-based assessment method that enables precise, automated scoring of complex LLM responses without reliance on LLM-assisted assessments or human judgment. This method allows for a comprehensive analysis of model performance and stability from multiple perspectives. We conduct detailed experiments on 20 prominent LLMs across six length intervals. Our work contributes LIFBench and LIFEval as robust tools for assessing LLM performance in complex and long-context settings, offering valuable insights to guide future advancements in LLM development.",LIFBench Evaluating Instruction Following Performance Stability Large Language Models Long Context Scenarios Large Language Models LLMs evolve natural language processing NLP ability stably follow instructions long context inputs critical real world applications existing benchmarks seldom focus instruction following long context scenarios stability different inputs bridge gap introduce LIFBench scalable dataset designed evaluate LLMs instruction following capabilities stability long contexts LIFBench comprises long context scenarios diverse tasks featuring 2 766 instructions generated automated expansion method dimensions length expression variables evaluation propose LIFEval rubric based assessment method enables precise automated scoring complex LLM responses reliance LLM assisted assessments human judgment method allows comprehensive analysis model performance stability multiple perspectives conduct detailed experiments 20 prominent LLMs length intervals work contributes LIFBench LIFEval robust tools assessing LLM performance complex long context settings offering valuable insights guide future advancements LLM development
847,FEAT: A Preference Feedback Dataset through a Cost-Effective Auto-Generation and Labeling Framework for English AI Tutoring,"['Hyein Seo', 'Taewook Hwang', 'Yohan Lee', 'Sangkeun Jung']","In English education tutoring, teacher feedback is essential for guiding students. Recently, AI-based tutoring systems have emerged to assist teachers; however, these systems require high-quality and large-scale teacher feedback data, which is both time-consuming and costly to generate manually. In this study, we propose FEAT, a cost-effective framework for generating teacher feedback, and have constructed three complementary datasets: (1) DIRECT-Manual (DM), where both humans and large language models (LLMs) collaboratively generate high-quality teacher feedback, albeit at a higher cost; (2) DIRECT-Generated (DG), an LLM-only generated, cost-effective dataset with lower quality;, and (3) DIRECT-Augmented (DA), primarily based on DG with a small portion of DM added to enhance quality while maintaining cost-efficiency. Experimental results showed that incorporating a small portion of DM (5-10%) into DG leads to superior performance compared to using 100% DM alone.",FEAT Preference Feedback Dataset Cost Effective Auto Generation Labeling Framework English AI Tutoring English education tutoring teacher feedback essential guiding students Recently AI based tutoring systems emerged assist teachers systems require high quality large scale teacher feedback data time consuming costly generate manually study propose FEAT cost effective framework generating teacher feedback constructed complementary datasets 1 DIRECT Manual DM humans large language models LLMs collaboratively generate high quality teacher feedback albeit higher cost 2 DIRECT Generated DG LLM generated cost effective dataset lower quality 3 DIRECT Augmented DA primarily based DG small portion DM added enhance quality maintaining cost efficiency Experimental results showed incorporating small portion DM 5 10 DG leads superior performance compared using 100 DM
848,Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering,"['Shuzheng Si', 'Haozhe Zhao', 'Gang Chen', 'Cheng Gao', 'Yuzhuo Bai', 'Zhitong Wang', 'Kaikai An', 'Kangyang Luo', 'Chen Qian', 'Fanchao Qi', 'Baobao Chang', 'Maosong Sun']","Training LLMs on data containing unfamiliar knowledge during the instruction tuning stage can encourage hallucinations. To address this challenge, we introduce NOVA, a novel framework designed to identify high-quality data that aligns well with the LLM's learned knowledge to reduce hallucinations. NOVA includes Internal Consistency Probing (ICP) and Semantic Equivalence Identification (SEI) to measure how familiar the LLM is with instruction data. Specifically, ICP evaluates the LLM's understanding of the given instruction by calculating the tailored consistency among multiple self-generated responses. SEI further assesses the familiarity of the LLM with the target response by comparing it to the generated responses, using the proposed semantic clustering and well-designed voting strategy. Finally, to ensure the quality of selected samples, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity. By considering data quality and avoiding unfamiliar data, we can utilize the selected data to effectively align LLMs to follow instructions and hallucinate less.",Aligning Large Language Models Follow Instructions Hallucinate Effective Data Filtering Training LLMs data containing unfamiliar knowledge instruction tuning stage encourage hallucinations address challenge introduce NOVA novel framework designed identify high quality data aligns LLM s learned knowledge reduce hallucinations NOVA includes Internal Consistency Probing ICP Semantic Equivalence Identification SEI measure familiar LLM instruction data Specifically ICP evaluates LLM s understanding given instruction calculating tailored consistency multiple self generated responses SEI assesses familiarity LLM target response comparing generated responses using proposed semantic clustering designed voting strategy Finally ensure quality selected samples introduce expert aligned reward model considering characteristics just familiarity considering data quality avoiding unfamiliar data utilize selected data effectively align LLMs follow instructions hallucinate
849,One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs,"['Junwoo Ha', 'Hyunjun Kim', 'Sangyoon Yu', 'Haon Park', 'Ashkan Yousefpour', 'Yuna Park', 'Suhyun Kim']","We introduce a novel framework for consolidating multi-turn adversarial ``jailbreak'' prompts into single-turn queries, significantly reducing the manual overhead required for adversarial testing of large language models (LLMs). While multi-turn human jailbreaks have been shown to yield high attack success rates, they demand considerable human effort and time. Our multi-turn-to-single-turn (M2S) methods -- Hyphenize, Numberize, and Pythonize -- systematically reformat multi-turn dialogues into structured single-turn prompts. Despite removing iterative back-and-forth interactions, these prompts preserve and often enhance adversarial potency: in extensive evaluations on the Multi-turn Human Jailbreak (MHJ) dataset, M2S methods achieve attack success rates from 70.6 percent to 95.9 percent across several state-of-the-art LLMs. Remarkably, the single-turn prompts outperform the original multi-turn attacks by as much as 17.5 percentage points while cutting token usage by more than half on average. Further analysis shows that embedding malicious requests in enumerated or code-like structures exploits ``contextual blindness'', bypassing both native guardrails and external input-output filters. By converting multi-turn conversations into concise single-turn prompts, the M2S framework provides a scalable tool for large-scale red teaming and reveals critical weaknesses in contemporary LLM defenses.",Shot Consolidating Multi Turn Attacks Efficient Single Turn Prompts LLMs introduce novel framework consolidating multi turn adversarial jailbreak prompts single turn queries significantly reducing manual overhead required adversarial testing large language models LLMs multi turn human jailbreaks shown yield high attack success rates demand considerable human effort time multi turn single turn M2S methods Hyphenize Numberize Pythonize systematically reformat multi turn dialogues structured single turn prompts Despite removing iterative forth interactions prompts preserve enhance adversarial potency extensive evaluations Multi turn Human Jailbreak MHJ dataset M2S methods achieve attack success rates 70 6 percent 95 9 percent state art LLMs Remarkably single turn prompts outperform original multi turn attacks 17 5 percentage points cutting token usage half average analysis shows embedding malicious requests enumerated code like structures exploits contextual blindness bypassing native guardrails external input output filters converting multi turn conversations concise single turn prompts M2S framework provides scalable tool large scale red teaming reveals critical weaknesses contemporary LLM defenses
850,RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning Based on Emotional Information,"['Zhiwei Liu', 'Kailai Yang', 'Qianqian Xie', 'Christine de Kock', 'Sophia Ananiadou', 'Eduard Hovy']","Misinformation is prevalent in various fields such as education, politics, health, etc., causing significant harm to society. However, current methods for cross-domain misinformation detection rely on effort- and resource-intensive fine-tuning and complex model structures. With the outstanding performance of LLMs, many studies have employed them for misinformation detection. Unfortunately, they focus on in-domain tasks and do not incorporate significant sentiment and emotion features (which we jointly call {\em affect}). In this paper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to address cross-domain misinformation detection using in-context learning based on affective information. RAEmoLLM includes three modules. (1) In the index construction module, we apply an emotional LLM to obtain affective embeddings from all domains to construct a retrieval database. (2) The retrieval module uses the database to recommend top K examples (text-label pairs) from source domain data for target domain contents. (3) These examples are adopted as few-shot demonstrations for the inference module to process the target domain content. The RAEmoLLM can effectively enhance the general performance of LLMs in cross-domain misinformation detection tasks through affect-based retrieval, without fine-tuning. We evaluate our framework on three misinformation benchmarks. Results show that RAEmoLLM achieves significant improvements compared to the other few-shot methods on three datasets, with the highest increases of 15.64%, 31.18%, and 15.73% respectively. This project is available at https://github.com/lzw108/RAEmoLLM.",RAEmoLLM Retrieval Augmented LLMs Cross Domain Misinformation Detection Using Context Learning Based Emotional Information Misinformation prevalent various fields education politics health causing significant harm society current methods cross domain misinformation detection rely effort resource intensive fine tuning complex model structures outstanding performance LLMs studies employed misinformation detection Unfortunately focus domain tasks incorporate significant sentiment emotion features jointly em affect paper propose RAEmoLLM retrieval augmented RAG LLMs framework address cross domain misinformation detection using context learning based affective information RAEmoLLM includes modules 1 index construction module apply emotional LLM obtain affective embeddings domains construct retrieval database 2 retrieval module uses database recommend K examples text label pairs source domain data target domain contents 3 examples adopted shot demonstrations inference module process target domain content RAEmoLLM effectively enhance general performance LLMs cross domain misinformation detection tasks affect based retrieval fine tuning evaluate framework misinformation benchmarks Results RAEmoLLM achieves significant improvements compared shot methods datasets highest increases 15 64 31 18 15 73 respectively project available https github com lzw108 RAEmoLLM
851,Task-Specific Information Decomposition for End-to-End Dense Video Captioning,"['Zhiyue Liu', 'Xinru Zhang', 'Jinyuan Liu']",,Task Specific Information Decomposition End End Dense Video Captioning
852,CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges,"['Haitao Li', 'Junjie Chen', 'Qingyao Ai', 'Zhumin Chu', 'Yujia Zhou', 'Qian Dong', 'Yiqun LIU']","The use of large language models (LLMs) as automated evaluation tools to assess the quality of generated natural language, known as LLMs-as-Judges, has demonstrated promising capabilities and is rapidly gaining widespread attention. However, when applied to pairwise comparisons of candidate responses, LLM-based evaluators often exhibit selection bias. Specifically, their judgments may become inconsistent when the option positions or ID tokens are swapped, compromising the effectiveness and fairness of the evaluation result. To address this challenge, we introduce CalibraEval, a novel label-free method for mitigating selection bias during inference. Specifically, CalibraEval reformulates debiasing as an optimization task aimed at adjusting observed prediction distributions to align with unbiased prediction distributions. To solve this optimization problem, we propose a non-parametric order-preserving algorithm (NOA). This algorithm leverages the partial order relationships between model prediction distributions, thereby eliminating the need for explicit labels and precise mathematical function modeling.Empirical evaluations of LLMs in multiple representative benchmarks demonstrate that CalibraEval effectively mitigates selection bias and improves performance compared to existing debiasing methods. This work marks a step toward building more robust and unbiased automated evaluation frameworks, paving the way for improved reliability in AI-driven assessments",CalibraEval Calibrating Prediction Distribution Mitigate Selection Bias LLMs Judges use large language models LLMs automated evaluation tools assess quality generated natural language known LLMs Judges demonstrated promising capabilities rapidly gaining widespread attention applied pairwise comparisons candidate responses LLM based evaluators exhibit selection bias Specifically judgments inconsistent option positions ID tokens swapped compromising effectiveness fairness evaluation result address challenge introduce CalibraEval novel label free method mitigating selection bias inference Specifically CalibraEval reformulates debiasing optimization task aimed adjusting observed prediction distributions align unbiased prediction distributions solve optimization problem propose non parametric order preserving algorithm NOA algorithm leverages partial order relationships model prediction distributions eliminating need explicit labels precise mathematical function modeling Empirical evaluations LLMs multiple representative benchmarks demonstrate CalibraEval effectively mitigates selection bias improves performance compared existing debiasing methods work marks step building robust unbiased automated evaluation frameworks paving way improved reliability AI driven assessments
853,Explaining Matters: Leveraging Definitions and Semantic Expansion for Sexism Detection,"['Sahrish Khan', 'Gabriele Pergola', 'Arshad Jhumka']","The detection of sexism in online content remains an open problem, as harmful language disproportionately affects women and marginalized groups. While automated systems for sexism detection have been developed, they still face two key challenges: data sparsity and the nuanced nature of sexist language. Even in large, well-curated datasets like the Explainable Detection of Online Sexism (EDOS), severe class imbalance hinders model generalization. Additionally, the overlapping and ambiguous boundaries of fine-grained categories introduce substantial annotator disagreement, reflecting the difficulty of interpreting nuanced expressions of sexism. To address these challenges, we propose two prompt-based data augmentation techniques: Definition-based Data Augmentation (DDA), which leverages category-specific definitions to generate semantically-aligned synthetic examples, and Contextual Semantic Expansion (CSE), which targets systematic model errors by enriching examples with task-specific semantic features. To further improve reliability in fine-grained classification, we introduce an ensemble strategy that resolves prediction ties by aggregating complementary perspectives from multiple language models. Our experimental evaluation on the EDOS dataset demonstrates state-of-the-art performance across all tasks, with notable improvements of macro F1 by 1.5 points for binary classification (Task A) and 4.1 points for fine-grained classification (Task C).",Explaining Matters Leveraging Definitions Semantic Expansion Sexism Detection detection sexism online content remains open problem harmful language disproportionately affects women marginalized groups automated systems sexism detection developed face key challenges data sparsity nuanced nature sexist language large curated datasets like Explainable Detection Online Sexism EDOS severe class imbalance hinders model generalization Additionally overlapping ambiguous boundaries fine grained categories introduce substantial annotator disagreement reflecting difficulty interpreting nuanced expressions sexism address challenges propose prompt based data augmentation techniques Definition based Data Augmentation DDA leverages category specific definitions generate semantically aligned synthetic examples Contextual Semantic Expansion CSE targets systematic model errors enriching examples task specific semantic features improve reliability fine grained classification introduce ensemble strategy resolves prediction ties aggregating complementary perspectives multiple language models experimental evaluation EDOS dataset demonstrates state art performance tasks notable improvements macro F1 1 5 points binary classification Task 4 1 points fine grained classification Task C
854,Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models,"['Elena Sofia Ruzzetti', 'Giancarlo A. Xompero', 'Davide Venditti', 'Fabio Massimo Zanzotto']","Large Language Models (LLMs) memorize, and thus, among huge amounts of uncontrolled data, may memorize Personally Identifiable Information (PII), which should not be stored and, consequently, not leaked. In this paper, we introduce Private Memorization Editing (PME), an approach for preventing private data leakage that turns an apparent limitation, that is, the LLMs' memorization ability, into a powerful privacy defense strategy. While attacks against LLMs have been performed exploiting previous knowledge regarding their training data, our approach aims to exploit the same kind of knowledge in order to make a model more robust. We detect a memorized PII and then mitigate the memorization of PII by editing a model knowledge of its training data. We verify that our procedure does not affect the underlying language model while making it more robust against privacy Training Data Extraction attacks. We demonstrate that PME can effectively reduce the number of leaked PII in a number of configurations, in some cases even reducing the accuracy of the privacy attacks to zero.",Private Memorization Editing Turning Memorization Defense Strengthen Data Privacy Large Language Models Large Language Models LLMs memorize huge amounts uncontrolled data memorize Personally Identifiable Information PII stored consequently leaked paper introduce Private Memorization Editing PME approach preventing private data leakage turns apparent limitation LLMs memorization ability powerful privacy defense strategy attacks LLMs performed exploiting previous knowledge regarding training data approach aims exploit kind knowledge order make model robust detect memorized PII mitigate memorization PII editing model knowledge training data verify procedure does affect underlying language model making robust privacy Training Data Extraction attacks demonstrate PME effectively reduce number leaked PII number configurations cases reducing accuracy privacy attacks zero
855,PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning,"['Xinyu Zhang', 'Yuxuan Dong', 'Yanrui Wu', 'Jiaxing Huang', 'Chengyou Jia', 'Basura Fernando', 'Mike Zheng Shou', 'Lingling Zhang', 'Jun Liu']","Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason.",PhysReason Comprehensive Benchmark Physics Based Reasoning Large language models demonstrate remarkable capabilities various domains especially mathematics logic reasoning current evaluations overlook physics based reasoning complex task requiring physics theorems constraints present PhysReason 1 200 problem benchmark comprising knowledge based 25 reasoning based 75 problems divided difficulty levels easy medium hard Notably problems require average 8 1 solution steps hard requiring 15 6 reflecting complexity physics based reasoning propose Physics Solution Auto Scoring Framework incorporating efficient answer level comprehensive step level evaluations performing models like Deepseek R1 Gemini 2 0 Flash Thinking o3 mini high achieve 60 answer level evaluation performance dropping knowledge questions 75 11 hard problems 31 95 step level evaluation identified key bottlenecks Physics Theorem Application Physics Process Understanding Calculation Physics Condition Analysis findings position PhysReason novel comprehensive benchmark evaluating physics based reasoning capabilities large language models code data published https dxzxy12138 github io PhysReason
856,Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information,"['Yein Park', 'Chanwoong Yoon', 'Jungwoo Park', 'Minbyul Jeong', 'Jaewoo Kang']","While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads that primarily handle temporal knowledge, through circuit analysis. We confirm that these heads are present across multiple models, though their specific locations may vary, and their responses differ depending on the type of knowledge and its corresponding years. Disabling these heads degrades the model's ability to recall time-specific knowledge while maintaining its general capabilities without compromising time-invariant and question-answering performances. Moreover, the heads are activated not only numeric conditions (""In 2004"") but also textual aliases (""In the year ...""), indicating that they encode a temporal dimension beyond simple numerical representation. Furthermore, we expand the potential of our findings by demonstrating how temporal knowledge can be edited by adjusting the values of these heads.",Does Time Place Temporal Heads Language Models Recall Time specific Information ability language models elicit facts widely investigated handle temporally changing facts remains underexplored discover Temporal Heads specific attention heads primarily handle temporal knowledge circuit analysis confirm heads present multiple models specific locations vary responses differ depending type knowledge corresponding years Disabling heads degrades model s ability recall time specific knowledge maintaining general capabilities compromising time invariant question answering performances heads activated numeric conditions 2004 textual aliases year indicating encode temporal dimension simple numerical representation Furthermore expand potential findings demonstrating temporal knowledge edited adjusting values heads
857,Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training,"['Zheheng Luo', 'Xin Zhang', 'Xiao Liu', 'Haoling Li', 'Yeyun Gong', 'Qi Chen', 'Peng CHENG']","It is well-known that a diverse corpus is critical for training large language models, which are typically constructed from a mixture of various domains. In general, previous efforts resort to sampling training data from different domains with static proportions, as well as adjusting data proportions during training. However, few methods have addressed the complexities of domain-adaptive continual pre-training. To fill this gap, we propose Velocitune, a novel framework dynamically assesses learning velocity and adjusts data proportions accordingly, favoring slower-learning domains while shunning faster-learning ones, which is guided by a scaling law to indicate the desired learning goal for each domain with less associated cost. To evaluate the effectiveness of Velocitune, we conduct experiments in a reasoning-focused dataset with CodeLlama, as well as in a corpus specialised for system command generation with Llama3 and Mistral. Velocitune achieves performance gains in both math and code reasoning tasks and command-line generation benchmarks. Further analysis reveals that key factors driving Velocitune's effectiveness include target loss prediction and data ordering.",Velocitune Velocity based Dynamic Domain Reweighting Method Continual Pre training known diverse corpus critical training large language models typically constructed mixture various domains general previous efforts resort sampling training data different domains static proportions adjusting data proportions training methods addressed complexities domain adaptive continual pre training gap propose Velocitune novel framework dynamically assesses learning velocity adjusts data proportions accordingly favoring slower learning domains shunning faster learning ones guided scaling law indicate desired learning goal domain associated cost evaluate effectiveness Velocitune conduct experiments reasoning focused dataset CodeLlama corpus specialised command generation Llama3 Mistral Velocitune achieves performance gains math code reasoning tasks command line generation benchmarks analysis reveals key factors driving Velocitune s effectiveness include target loss prediction data ordering
858,"Sheep’s Skin, Wolf’s Deeds: Are LLMs Ready for Metaphorical Implicit Hate Speech?","['Jingjie Zeng', 'Yuanyuan Sun', 'zekun wang', 'Liang Yang', 'Hongfei Lin']",,Sheep s Skin Wolf s Deeds LLMs Ready Metaphorical Implicit Hate Speech
859,Neuron-Level Sequential Editing for Large Language Models,"['Houcheng Jiang', 'Junfeng Fang', 'Tianyu Zhang', 'Baolong Bi', 'An Zhang', 'Ruipeng Wang', 'Tao Liang', 'Xiang Wang']","This work explores sequential model editing in large language models (LLMs), a critical task that involves modifying internal knowledge within LLMs continuously through multi-round editing, each incorporating updates or corrections to adjust the model outputs without the need for costly retraining. Existing model editing methods, especially those that alter model parameters, typically focus on single-round editing and often face significant challenges in sequential model editing-most notably issues of model forgetting and failure. To address these challenges, we introduce a new model editing method, namely \textbf{N}euron-level \textbf{S}equential \textbf{E}diting (NSE), tailored for supporting sequential model editing. Specifically, we optimize the target layer's hidden states using the model's original weights to prevent model failure. Furthermore, we iteratively select neurons in multiple layers for editing based on their activation values to mitigate model forgetting. Our empirical experiments demonstrate that NSE significantly outperforms current modifying parameters model editing methods, marking a substantial advancement in the field of sequential model editing. Our code is released on \url{https://github.com/jianghoucheng/NSE}.",Neuron Level Sequential Editing Large Language Models work explores sequential model editing large language models LLMs critical task involves modifying internal knowledge LLMs continuously multi round editing incorporating updates corrections adjust model outputs need costly retraining Existing model editing methods especially alter model parameters typically focus single round editing face significant challenges sequential model editing notably issues model forgetting failure address challenges introduce new model editing method textbf N euron level textbf S equential textbf E diting NSE tailored supporting sequential model editing Specifically optimize target layer s hidden states using model s original weights prevent model failure Furthermore iteratively select neurons multiple layers editing based activation values mitigate model forgetting empirical experiments demonstrate NSE significantly outperforms current modifying parameters model editing methods marking substantial advancement field sequential model editing code released url https github com jianghoucheng NSE
860,Automatic Expert Discovery in LLM Upcycling via Sparse Interpolated Mixture-of-Experts,"['Shengzhuang Chen', 'Ying Wei', 'Jonathan Richard Schwarz']","We present Sparse Interpolated Mixture-of-Experts (SIMoE) instruction-tuning, an end-to-end algorithm designed to fine-tune a dense pre-trained Large Language Model (LLM) into a MoE-style model that possesses capabilities in multiple specialized domains. During instruction-tuning, SIMoE automatically identifies multiple specialized experts under a specified sparsity constraint, with each expert representing a structurally sparse subset of the seed LLM's parameters that correspond to domain-specific knowledge within the data. SIMoE simultaneously learns an input-dependent expert merging strategy via a router network, leveraging rich cross-expert knowledge for superior downstream generalization that surpasses existing baselines. Empirically, SIMoE consistently achieves state-of-the-art performance on common instruction-tuning benchmarks while maintaining an optimal performance-compute trade-off compared to all baselines.",Automatic Expert Discovery LLM Upcycling Sparse Interpolated Mixture Experts present Sparse Interpolated Mixture Experts SIMoE instruction tuning end end algorithm designed fine tune dense pre trained Large Language Model LLM MoE style model possesses capabilities multiple specialized domains instruction tuning SIMoE automatically identifies multiple specialized experts specified sparsity constraint expert representing structurally sparse subset seed LLM s parameters correspond domain specific knowledge data SIMoE simultaneously learns input dependent expert merging strategy router network leveraging rich cross expert knowledge superior downstream generalization surpasses existing baselines Empirically SIMoE consistently achieves state art performance common instruction tuning benchmarks maintaining optimal performance compute trade compared baselines
861,SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation,"['Keqi Deng', 'Wenxi Chen', 'Xie Chen', 'Phil Woodland']","Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",SimulS2S LLM Unlocking Simultaneous Inference Speech LLMs Speech Speech Translation Simultaneous speech translation SST outputs translations parallel streaming speech input balancing translation quality latency large language models LLMs extended handle speech modality streaming remains challenging speech prepended prompt entire generation process unlock LLM streaming capability paper proposes SimulS2S LLM trains speech LLMs offline employs test time policy guide simultaneous inference SimulS2S LLM alleviates mismatch training inference extracting boundary aware speech prompts allows better matched text input data SimulS2S LLM achieves simultaneous speech speech translation Simul S2ST predicting discrete output speech tokens synthesising output speech using pre trained vocoder incremental beam search designed expand search space speech token prediction increasing latency Experiments CVSS speech data SimulS2S LLM offers better translation quality latency trade existing methods use training data improving ASR BLEU scores 3 points similar latency
862,VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models,"['Wenqian Cui', 'Xiaoqi Jiao', 'Ziqiao Meng', 'Irwin King']","With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMs' knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMs' knowledge understanding through pure speech interactions. Our benchmark 1) uniquely maintains speech format for both inputs and outputs, 2) evaluates model robustness across diverse input audio conditions, and 3) pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Systematic evaluation demonstrates that VoxEval presents significant challenges to current SLMs, revealing their sensitivity to varying audio conditions and highlighting the need to enhance reasoning capabilities in future development. We hope this benchmark could guide the advancement of more sophisticated and reliable SLMs. VoxEval dataset is available at: https://github.com/dreamtheater123/VoxEval",VoxEval Benchmarking Knowledge Understanding Capabilities End End Spoken Language Models rising need speech based interaction models end end Spoken Language Models SLMs emerged promising solution models require comprehensive world knowledge meaningful reliable human interactions existing question answering QA benchmarks fall short evaluating SLMs knowledge understanding inability support end end speech evaluation account varied input audio conditions address limitations present VoxEval novel SpeechQA benchmark assesses SLMs knowledge understanding pure speech interactions benchmark 1 uniquely maintains speech format inputs outputs 2 evaluates model robustness diverse input audio conditions 3 pioneers assessment complex tasks like mathematical reasoning spoken format Systematic evaluation demonstrates VoxEval presents significant challenges current SLMs revealing sensitivity varying audio conditions highlighting need enhance reasoning capabilities future development hope benchmark guide advancement sophisticated reliable SLMs VoxEval dataset available https github com dreamtheater123 VoxEval
863,RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation,"['Xiaoxi Li', 'Jiajie Jin', 'Yujia Zhou', 'Yongkang Wu', 'Zhonghua Li', 'YE QI', 'Zhicheng Dou']","Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation. To address these issues, we propose \textbf{RetroLLM}, a unified framework that integrates retrieval and generation into a single, cohesive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding. Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy. Extensive experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance across both in-domain and out-of-domain tasks. The code is available at \url{https://github.com/sunnynexus/RetroLLM}.",RetroLLM Empowering Large Language Models Retrieve Fine grained Evidence Generation Large language models LLMs exhibit remarkable generative capabilities suffer hallucinations Retrieval augmented generation RAG offers effective solution incorporating external knowledge existing methods face limitations additional deployment costs separate retrievers redundant input tokens retrieved text chunks lack joint optimization retrieval generation address issues propose textbf RetroLLM unified framework integrates retrieval generation single cohesive process enabling LLMs directly generate fine grained evidence corpus constrained decoding mitigate false pruning process constrained evidence generation introduce 1 hierarchical FM Index constraints generate corpus constrained clues identify subset relevant documents evidence generation reducing irrelevant decoding space 2 forward looking constrained decoding strategy considers relevance future sequences improve evidence accuracy Extensive experiments open domain QA datasets demonstrate RetroLLM s superior performance domain domain tasks code available url https github com sunnynexus RetroLLM
864,ChronoSense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events,"['Duygu Sezen Islakoglu', 'Jan-Christoph Kalo']","Large Language Models (LLMs) have achieved remarkable success in various NLP tasks, yet they still face significant challenges in reasoning and arithmetic. Temporal reasoning, a critical component of natural language understanding, has raised increasing research attention. However, comprehensive testing of Allen's interval relations (e.g., before, after, during) -- a fundamental framework for temporal relationships -- remains underexplored. To fill this gap, we present ChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It includes 16 tasks, focusing on identifying the Allen relation between two temporal events and temporal arithmetic, using both abstract events and real-world data from Wikidata. We assess the performance of seven recent LLMs using this benchmark and the results indicate that models handle Allen relations, even symmetrical ones, quite differently. Moreover, the findings suggest that the models may rely on memorization to answer time-related questions. Overall, the models' low performance highlights the need for improved temporal understanding in LLMs and ChronoSense offers a robust framework for future research in this area. Our dataset and the source code are available at https://github.com/duyguislakoglu/chronosense.",ChronoSense Exploring Temporal Understanding Large Language Models Time Intervals Events Large Language Models LLMs achieved remarkable success various NLP tasks face significant challenges reasoning arithmetic Temporal reasoning critical component natural language understanding raised increasing research attention comprehensive testing Allen s interval relations e g fundamental framework temporal relationships remains underexplored gap present ChronoSense new benchmark evaluating LLMs temporal understanding includes 16 tasks focusing identifying Allen relation temporal events temporal arithmetic using abstract events real world data Wikidata assess performance seven recent LLMs using benchmark results indicate models handle Allen relations symmetrical ones quite differently findings suggest models rely memorization answer time related questions Overall models low performance highlights need improved temporal understanding LLMs ChronoSense offers robust framework future research area dataset source code available https github com duyguislakoglu chronosense
865,The Role of Deductive and Inductive Reasoning in Large Language Models,"['Chengkun Cai', 'Xu Zhao', 'Haoliang Liu', 'Zhongyu Jiang', 'Tianfang Zhang', 'Zongkai Wu', 'Jenq-Neng Hwang', 'Serge Belongie', 'Lei Li']",,Role Deductive Inductive Reasoning Large Language Models
866,Disentangling the Roles of Representation and Selection in Data Pruning,"['Yupei Du', 'Yingjin Song', 'Hugh Mee Wong', 'Daniil Ignatev', 'Albert Gatt', 'Dong Nguyen']","Data pruning, selecting small but impactful subsets, offers a promising way to efficiently scale NLP model training. However, existing methods often involve many different design choices, which have not been systematically studied. This limits future developments. In this work, we decompose data pruning into two key components: the data representation and the selection algorithm, and we systematically analyze their influence on the selection of instances. Our theoretical and empirical results highlight the crucial role of representations: better representations, e.g., training gradients, generally lead to a better selection of instances, regardless of the chosen selection algorithm. Furthermore, different selection algorithms excel in different settings, and none consistently outperforms the others. Moreover, the selection algorithms do not always align with their intended objectives: for example, algorithms designed for the same objective can select drastically different instances, highlighting the need for careful evaluation.",Disentangling Roles Representation Selection Data Pruning Data pruning selecting small impactful subsets offers promising way efficiently scale NLP model training existing methods involve different design choices systematically studied limits future developments work decompose data pruning key components data representation selection algorithm systematically analyze influence selection instances theoretical empirical results highlight crucial role representations better representations e g training gradients generally lead better selection instances regardless chosen selection algorithm Furthermore different selection algorithms excel different settings consistently outperforms selection algorithms align intended objectives example algorithms designed objective select drastically different instances highlighting need careful evaluation
867,FRACTAL: Fine-Grained Scoring from Aggregate Text Labels,"['Yukti Makhija', 'Priyanka Agrawal', 'Rishi Saket', 'Aravindan Raghuveer']","Large language models (LLMs) are being increasingly tuned to power complex generation tasks such as writing, fact-seeking, querying and reasoning. Traditionally, human or model feedback for evaluating and further tuning LLM performance has been provided at the response level, enabling faster and more cost-effective assessments. However, recent works (Amplayo et al. [2022], Wu et al. [2023]) indicate that sentence-level labels may provide more accurate and interpretable feedback for LLM optimization. In this work, we introduce methods to disaggregate response-level labels into sentence-level (pseudo-)labels. Our approach leverages multiple instance learning (MIL) and learning from label proportions (LLP) techniques in conjunction with prior information (e.g., document-sentence cosine similarity) to train a specialized model for sentence-level scoring. We also employ techniques which use model predictions to pseudo-label the train-set at the sentence-level for model training to further improve performance. We conduct extensive evaluations of our methods across six datasets and four tasks: retrieval, question answering, summarization, and math reasoning. Our results demonstrate improved performance compared to multiple baselines across most of these tasks. Our work is the first to develop response-level feedback to sentence-level scoring techniques, leveraging sentence-level prior information, along with comprehensive evaluations on multiple tasks as well as end-to-end finetuning evaluation showing performance comparable to a model trained on fine-grained human annotated labels.",FRACTAL Fine Grained Scoring Aggregate Text Labels Large language models LLMs increasingly tuned power complex generation tasks writing fact seeking querying reasoning Traditionally human model feedback evaluating tuning LLM performance provided response level enabling faster cost effective assessments recent works Amplayo et al 2022 Wu et al 2023 indicate sentence level labels provide accurate interpretable feedback LLM optimization work introduce methods disaggregate response level labels sentence level pseudo labels approach leverages multiple instance learning MIL learning label proportions LLP techniques conjunction prior information e g document sentence cosine similarity train specialized model sentence level scoring employ techniques use model predictions pseudo label train set sentence level model training improve performance conduct extensive evaluations methods datasets tasks retrieval question answering summarization math reasoning results demonstrate improved performance compared multiple baselines tasks work develop response level feedback sentence level scoring techniques leveraging sentence level prior information comprehensive evaluations multiple tasks end end finetuning evaluation showing performance comparable model trained fine grained human annotated labels
868,ACT: Knowledgeable Agents to Design and Perform Complex Tasks,"['Makoto Nakatsuji', 'Shuhei Tateishi', 'Yasuhiro Fujiwara', 'Ayaka Matsumoto', 'Narichika Nomoto', 'Yoshihide Sato']",,ACT Knowledgeable Agents Design Perform Complex Tasks
869,Logical forms complement probability in understanding language model (and human) performance,"['Yixuan Wang', 'Freda Shi']","With the increasing interest in using large language models (LLMs) for planning in natural language, understanding their behaviors becomes an important research question. This work conducts a systematic investigation of LLMs' ability to perform logical reasoning in natural language. We introduce a controlled dataset of hypothetical and disjunctive syllogisms in propositional and modal logic and use it as the testbed for understanding LLM performance. Our results lead to novel insights in predicting LLM behaviors: in addition to the probability of input (Gonen et al., 2023; McCoy et al., 2024), logical forms should be considered as important factors. In addition, we show similarities and discrepancies between the logical reasoning performances of humans and LLMs by collecting and comparing behavioral data from both.",Logical forms complement probability understanding language model human performance increasing using large language models LLMs planning natural language understanding behaviors important research question work conducts systematic investigation LLMs ability perform logical reasoning natural language introduce controlled dataset hypothetical disjunctive syllogisms propositional modal logic use testbed understanding LLM performance results lead novel insights predicting LLM behaviors addition probability input Gonen et al 2023 McCoy et al 2024 logical forms considered important factors addition similarities discrepancies logical reasoning performances humans LLMs collecting comparing behavioral data
870,Length Controlled Generation for Black-box LLMs,"['Yuxuan Gu', 'Wenjie Wang', 'Xiaocheng Feng', 'Weihong Zhong', 'kun Zhu', 'Lei Huang', 'Ting Liu', 'Bing Qin', 'Tat-Seng Chua']","Large language models (LLMs) have demonstrated impressive instruction following capabilities, while still struggling to accurately manage the length of the generated text, which is a fundamental requirement in many real-world applications. Existing length control methods involve fine-tuning the parameters of LLMs, which is inefficient and suboptimal for practical use. In this paper, we propose a novel iterative sampling framework for text length control, integrating the Metropolis-Hastings algorithm with an importance sampling acceleration strategy. This framework efficiently and reliably regulates LLMs to generate length-constrained text without modifying the underlying parameters, thereby preserving the original capabilities of LLMs. Experimental results demonstrate that our framework achieves almost 100\% success rates of length control on Llama3.1 for tasks such as length-controlled abstractive summarization and length-constrained instruction following, with minimal additional computational overhead. This also highlights the significant potential of our method for precise length control across a broader range of applications, without compromising the versatility of LLMs.",Length Controlled Generation Black box LLMs Large language models LLMs demonstrated impressive instruction following capabilities struggling accurately manage length generated text fundamental requirement real world applications Existing length control methods involve fine tuning parameters LLMs inefficient suboptimal practical use paper propose novel iterative sampling framework text length control integrating Metropolis Hastings algorithm importance sampling acceleration strategy framework efficiently reliably regulates LLMs generate length constrained text modifying underlying parameters preserving original capabilities LLMs Experimental results demonstrate framework achieves 100 success rates length control Llama3 1 tasks length controlled abstractive summarization length constrained instruction following minimal additional computational overhead highlights significant potential method precise length control broader range applications compromising versatility LLMs
871,Improving Contextual Faithfulness of Large Language Models via Retrieval Heads-Induced Optimization,"['Lei Huang', 'Xiaocheng Feng', 'Weitao Ma', 'Yuchun Fan', 'Xiachong Feng', 'Yangfan Ye', 'Weihong Zhong', 'Yuxuan Gu', 'Baoxin Wang', 'Dayong Wu', 'Guoping Hu', 'Bing Qin']","Ensuring contextual faithfulness in retrieval-augmented large language models (LLMs) is crucial for building trustworthy information-seeking systems, particularly in long-form question-answering (LFQA) scenarios. In this work, we identify a salient correlation between LFQA faithfulness and retrieval heads, a set of attention heads responsible for retrieving contextual information. Leveraging this insight, we propose RHIO, a framework designed to teach LLMs to explicitly discriminate between faithful and unfaithful generations. RHIO first augments unfaithful samples that simulate realistic model-intrinsic errors by selectively masking retrieval heads. Then, these samples are incorporated into joint training, enabling the model to distinguish unfaithful outputs from faithful ones conditioned on control tokens. Furthermore, these control tokens are leveraged to self-induce contrastive outputs, amplifying their difference through contrastive decoding. Additionally, to facilitate the evaluation of contextual faithfulness, we also introduce GroundBench, a comprehensive benchmark compiled from five existing LFQA datasets. Extensive experimental results on GroundBench demonstrate that RHIO significantly improves faithfulness, even outperforming GPT-4o.",Improving Contextual Faithfulness Large Language Models Retrieval Heads Induced Optimization Ensuring contextual faithfulness retrieval augmented large language models LLMs crucial building trustworthy information seeking systems particularly long form question answering LFQA scenarios work identify salient correlation LFQA faithfulness retrieval heads set attention heads responsible retrieving contextual information Leveraging insight propose RHIO framework designed teach LLMs explicitly discriminate faithful unfaithful generations RHIO augments unfaithful samples simulate realistic model intrinsic errors selectively masking retrieval heads samples incorporated joint training enabling model distinguish unfaithful outputs faithful ones conditioned control tokens Furthermore control tokens leveraged self induce contrastive outputs amplifying difference contrastive decoding Additionally facilitate evaluation contextual faithfulness introduce GroundBench comprehensive benchmark compiled existing LFQA datasets Extensive experimental results GroundBench demonstrate RHIO significantly improves faithfulness outperforming GPT 4o
872,Global Eye: Breaking the “Fixed Thinking Pattern” during the Instruction Expansion Process,"['wenxuan lu', 'Wei Liu', 'Jian Luan', 'Bin Wang', 'Songhao Jiang', 'Tianning Zang']",,Global Eye Breaking Fixed Thinking Pattern Instruction Expansion Process
873,On Synthesizing Data for Context Attribution in Question Answering,"['Gorjan Radevski', 'Kiril Gashteovski', 'Shahbaz Syed', 'Christopher Malon', 'Sebastien Nicolas', 'Chia-Chien Hung', 'Timo Sztyler', 'Verena Heußer', 'Wiem Ben Rim', 'Masafumi Enomoto', 'Kunihiro Takeoka', 'Masafumi Oyamada', 'Goran Glavaš', 'Carolin Lawrence']","Question Answering (QA) accounts for a significant portion of LLM usage ""in the wild"". However, LLMs sometimes produce false or misleading responses, also known as ""hallucinations"". Therefore, grounding the generated answers in contextually provided information -- i.e., providing evidence for the generated text -- is paramount for LLMs' trustworthiness. Providing this information is the task of context attribution. In this paper, we systematically study LLM-based approaches for this task, namely we investigate (i) zero-shot inference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic data generated by larger LLMs. Our key contribution is SynQA: a novel generative strategy for synthesizing context attribution data. Given selected context sentences, an LLM generates QA pairs that are supported by these sentences. This leverages LLMs' natural strengths in text generation while ensuring clear attribution paths in the synthetic training data. We show that the attribution data synthesized via SynQA is highly effective for fine-tuning small LMs for context attribution in different QA tasks and domains. Finally, with a user study, we validate the usefulness of small LMs (fine-tuned on synthetic data from SynQA) in context attribution for QA.",Synthesizing Data Context Attribution Question Answering Question Answering QA accounts significant portion LLM usage wild LLMs produce false misleading responses known hallucinations grounding generated answers contextually provided information e providing evidence generated text paramount LLMs trustworthiness Providing information task context attribution paper systematically study LLM based approaches task investigate zero shot inference ii LLM ensembling iii fine tuning small LMs synthetic data generated larger LLMs key contribution SynQA novel generative strategy synthesizing context attribution data Given selected context sentences LLM generates QA pairs supported sentences leverages LLMs natural strengths text generation ensuring clear attribution paths synthetic training data attribution data synthesized SynQA highly effective fine tuning small LMs context attribution different QA tasks domains Finally user study validate usefulness small LMs fine tuned synthetic data SynQA context attribution QA
874,TST: A Schema-Based Top-Down and Dynamic-Aware Agent of Text-to-Table Tasks,"['Peiwen Jiang', 'Haitong Jiang', 'Ruhui Ma', 'Yvonne Jie Chen', 'Jinhua Cheng']",,TST Schema Based Dynamic Aware Agent Text Table Tasks
875,EventRAG: Enhancing LLM Generation with Event Knowledge Graphs,"['Zairun Yang', 'Yilin Wang', 'Zhengyan Shi', 'Yuan Yao', 'Lei Liang', 'Keyan Ding', 'Emine Yilmaz', 'Huajun Chen', 'Qiang Zhang']",,EventRAG Enhancing LLM Generation Event Knowledge Graphs
876,Supervised Fine-Tuning Achieve Rapid Task Adaption Via Alternating Attention Head Activation Patterns,"['Yang Zhao', 'Li Du', 'Xiao Ding', 'Kai Xiong', 'Ting Liu', 'Bing Qin']","LLMs' performance on complex tasks is still unsatisfactory. A key issue is that presently LLMs learn in a data-driven schema, while the instructions about these complex tasks are both scarce and hard to collect or construct. On the contrary, a prominent phenomenon is that LLMs can learn rather fast on simpler tasks with adequate prior knowledge captured during pretraining stage. Thus, if the prerequisite and mechanism of such rapid generalization could be elucidated, it could enhance the efficiency and effectiveness of the LLM's ability to learn complex tasks. Thus, in this paper, we employ a gradient-based method, to dissect the process that the SFT process adapts LLMs to downstream tasks via the perspective of attention patterns. We find that: (1) LLMs selectively activate task-specific attention heads during SFT; (2) activation patterns for complex tasks are combinations of basic task patterns; and (3) changes in a few parameters can significantly impact activation patterns after SFT on a small number of samples.Based on these insights, experiments are conducted to actually enhance the efficiency and effectiveness of SFT.",Supervised Fine Tuning Achieve Rapid Task Adaption Alternating Attention Head Activation Patterns LLMs performance complex tasks unsatisfactory key issue presently LLMs learn data driven schema instructions complex tasks scarce hard collect construct contrary prominent phenomenon LLMs learn fast simpler tasks adequate prior knowledge captured pretraining stage prerequisite mechanism rapid generalization elucidated enhance efficiency effectiveness LLM s ability learn complex tasks paper employ gradient based method dissect process SFT process adapts LLMs downstream tasks perspective attention patterns 1 LLMs selectively activate task specific attention heads SFT 2 activation patterns complex tasks combinations basic task patterns 3 changes parameters significantly impact activation patterns SFT small number samples Based insights experiments conducted actually enhance efficiency effectiveness SFT
877,Can’t See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs,"['Wenxuan Wang', 'Xiaoyuan Liu', 'Kuiyi Gao', 'Jen-tse Huang', 'Youliang Yuan', 'Pinjia He', 'Shuai Wang', 'Zhaopeng Tu']",,t Forest Trees Benchmarking Multimodal Safety Awareness Multimodal LLMs
878,Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling,"['Jiayi Zeng', 'Yizhe Feng', 'Mengliang He', 'Wenhui Lei', 'Wei Zhang', 'Zeming Liu', 'Xiaoming Shi', 'Aimin Zhou']",,Mis prompt Benchmarking Large Language Models Proactive Error Handling
879,TripCraft: A Benchmark for Spatio-Temporally Fine Grained Travel Planning,"['Soumyabrata Chaudhuri', 'Pranav Purkar', 'Ritwik Raghav', 'Shubhojit Mallick', 'Manish Gupta', 'Abhik Jana', 'Shreya Ghosh']","Recent advancements in probing Large Language Models (LLMs) have explored their latent potential as personalized travel planning agents, yet existing benchmarks remain limited in real world applicability. Existing datasets, such as TravelPlanner and TravelPlanner+, suffer from semi synthetic data reliance, spatial inconsistencies, and a lack of key travel constraints, making them inadequate for practical itinerary generation. To address these gaps, we introduce TripCraft, a spatiotemporally coherent travel planning dataset that integrates real world constraints, including public transit schedules, event availability, diverse attraction categories, and user personas for enhanced personalization. To evaluate LLM generated plans beyond existing binary validation methods, we propose five continuous evaluation metrics, namely Temporal Meal Score, Temporal Attraction Score, Spatial Score, Ordering Score, and Persona Score which assess itinerary quality across multiple dimensions. Our parameter informed setting significantly enhances meal scheduling, improving the Temporal Meal Score from 61% to 80% in a 7 day scenario. TripCraft establishes a new benchmark for LLM driven personalized travel planning, offering a more realistic, constraint aware framework for itinerary generation. Dataset and Codebase will be made publicly available upon acceptance.",TripCraft Benchmark Spatio Temporally Fine Grained Travel Planning Recent advancements probing Large Language Models LLMs explored latent potential personalized travel planning agents existing benchmarks remain limited real world applicability Existing datasets TravelPlanner TravelPlanner suffer semi synthetic data reliance spatial inconsistencies lack key travel constraints making inadequate practical itinerary generation address gaps introduce TripCraft spatiotemporally coherent travel planning dataset integrates real world constraints including public transit schedules event availability diverse attraction categories user personas enhanced personalization evaluate LLM generated plans existing binary validation methods propose continuous evaluation metrics Temporal Meal Score Temporal Attraction Score Spatial Score Ordering Score Persona Score assess itinerary quality multiple dimensions parameter informed setting significantly enhances meal scheduling improving Temporal Meal Score 61 80 7 day scenario TripCraft establishes new benchmark LLM driven personalized travel planning offering realistic constraint aware framework itinerary generation Dataset Codebase publicly available acceptance
880,DualGuard: A Parameter Space Transformation Approach for Bidirectional Defense in Split-Based LLM Fine-Tuning,"['Zihan Liu', 'Yizhen Wang', 'Rui Wang', 'Sai Wu']",,DualGuard Parameter Space Transformation Approach Bidirectional Defense Split Based LLM Fine Tuning
881,Movie101v2: Improved Movie Narration Benchmark,"['Zihao Yue', 'Yepeng Zhang', 'Ziheng Wang', 'Qin Jin']","Automatic movie narration aims to generate video-aligned plot descriptions to assist visually impaired audiences. Unlike standard video captioning, it involves not only describing key visual details but also inferring plots that unfold across multiple movie shots, presenting distinct and complex challenges. To advance this field, we introduce Movie101v2, a large-scale, bilingual dataset with enhanced data quality specifically designed for movie narration. Revisiting the task, we propose breaking down the ultimate goal of automatic movie narration into three progressive stages, offering a clear roadmap with corresponding evaluation metrics. Based on our new benchmark, we baseline a range of large vision-language models, including GPT-4V, and conduct an in-depth analysis of the challenges in narration generation. Our findings highlight that achieving applicable movie narration generation is a fascinating goal that requires significant research.",Movie101v2 Improved Movie Narration Benchmark Automatic movie narration aims generate video aligned plot descriptions assist visually impaired audiences Unlike standard video captioning involves describing key visual details inferring plots unfold multiple movie shots presenting distinct complex challenges advance field introduce Movie101v2 large scale bilingual dataset enhanced data quality specifically designed movie narration Revisiting task propose breaking ultimate goal automatic movie narration progressive stages offering clear roadmap corresponding evaluation metrics Based new benchmark baseline range large vision language models including GPT 4V conduct depth analysis challenges narration generation findings highlight achieving applicable movie narration generation fascinating goal requires significant research
882,Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking using Knowledge Graphs,"['Nan Hu', 'Jiaoyan Chen', 'Yike Wu', 'Guilin Qi', 'Hongru WANG', 'Sheng Bi', 'Yongrui Chen', 'Tongtong Wu', 'Jeff Z. Pan']","Attributed Question Answering (AQA) has attracted wide attention, but there are still several limitations in evaluating the attributions, including lacking fine-grained attribution categories, relying on manual annotations, and failing to compare attributions with only subtle differences. To bridge these gaps, we introduce Complex Attributed Question Answering (CAQA), a large-scale benchmark containing comprehensive attribution categories, automatically generated using Knowledge Graphs (KGs), and complex attribution scenarios. We have conducted extensive experiments to verify the effectiveness of CAQA, including the benchmarking of 25 automatic evaluators, their comparison with human evaluators, the testing of LLM evaluators fine-tuned by CAQA and so on. These experiments also lead to a series of important findings that can benefit the future research of AQA. All the codes and data are publicly accessible at https://github.com/HuuuNan/CAQA-Benchmark.",LLMs Evaluate Complex Attribution QA Automatic Benchmarking using Knowledge Graphs Attributed Question Answering AQA attracted wide attention limitations evaluating attributions including lacking fine grained attribution categories relying manual annotations failing compare attributions subtle differences bridge gaps introduce Complex Attributed Question Answering CAQA large scale benchmark containing comprehensive attribution categories automatically generated using Knowledge Graphs KGs complex attribution scenarios conducted extensive experiments verify effectiveness CAQA including benchmarking 25 automatic evaluators comparison human evaluators testing LLM evaluators fine tuned CAQA experiments lead series important findings benefit future research AQA codes data publicly accessible https github com HuuuNan CAQA Benchmark
883,Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark,"['Jongwook Han', 'Dongmin Choi', 'Woojung Song', 'Eun-Ju Lee', 'Yohan Jo']",,Value Portrait Understanding Values LLMs Human aligned Benchmark
884,FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation,"['Wei Li', 'Xin Zhang', 'Zhongxin Guo', 'Shaoguang Mao', 'Wen Luo', 'Guangyue Peng', 'Yangyu Huang', 'Houfeng Wang', 'Scarlett Li']","Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.",FEA Bench Benchmark Evaluating Repository Level Code Generation Feature Implementation Implementing new features repository level codebases crucial application code generation models current benchmarks lack dedicated evaluation framework capability gap introduce FEA Bench benchmark designed assess ability large language models LLMs perform incremental development code repositories collect pull requests 83 GitHub repositories use rule based intent based filtering construct task instances focused new feature development task instance containing code changes paired relevant unit test files ensure solution verified feature implementation requires LLMs simultaneously possess code completion capabilities new components code editing abilities relevant parts code repository providing comprehensive evaluation method LLMs automated software engineering capabilities Experimental results LLMs perform significantly worse FEA Bench highlighting considerable challenges repository level incremental code development
885,Do not Abstain! Identify and Solve the Uncertainty,"['Jingyu Liu', 'JingquanPeng', 'xiaopeng Wu', 'Xubin Li', 'Tiezheng Ge', 'Bo Zheng', 'Yong Liu']","Despite the widespread application of Large Language Models (LLMs) across various domains, they frequently exhibit overconfidence when encountering uncertain scenarios, yet existing solutions primarily rely on evasive responses (e.g., ""I don't know"") overlooks the opportunity of identifying and addressing the uncertainty to generate more satisfactory responses. To systematically investigate and improve LLMs' ability of recognizing and addressing the source of uncertainty, we introduce \textbf{ConfuseBench}, a benchmark mainly focus on three types of uncertainty: document scarcity, limited capability, and query ambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to accurately identify the root cause of uncertainty and solve it. They prefer to attribute uncertainty to query ambiguity while overlooking capability limitations, especially for those weaker models. To tackle this challenge, we first generate context-aware inquiries that highlight the confusing aspect of the original query. Then we judge the source of uncertainty based on the uniqueness of the inquiry's answer. Further we use an on-policy training method, InteractDPO to generate better inquiries. Experimental results demonstrate the efficacy of our approach.",Abstain Identify Solve Uncertainty Despite widespread application Large Language Models LLMs various domains frequently exhibit overconfidence encountering uncertain scenarios existing solutions primarily rely evasive responses e g don t know overlooks opportunity identifying addressing uncertainty generate satisfactory responses systematically investigate improve LLMs ability recognizing addressing source uncertainty introduce textbf ConfuseBench benchmark mainly focus types uncertainty document scarcity limited capability query ambiguity Experiments ConfuseBench reveal current LLMs struggle accurately identify root cause uncertainty solve prefer attribute uncertainty query ambiguity overlooking capability limitations especially weaker models tackle challenge generate context aware inquiries highlight confusing aspect original query judge source uncertainty based uniqueness inquiry s answer use policy training method InteractDPO generate better inquiries Experimental results demonstrate efficacy approach
886,Decoding by Contrasting Knowledge: Enhancing Large Language Model Confidence on Edited Facts,"['Baolong Bi', 'Shenghua Liu', 'Lingrui Mei', 'Yiwei Wang', 'Junfeng Fang', 'Pengliang Ji', 'Xueqi Cheng']",,Decoding Contrasting Knowledge Enhancing Large Language Model Confidence Edited Facts
887,ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos,"['Mohammad Zia Ur Rehman', 'Anukriti Bhatnagar', 'Omkar Kabde', 'Shubhi Bansal', 'Dr. Nagendra Kumar']",,ImpliHateVid Benchmark Dataset stage Contrastive Learning Framework Implicit Hate Speech Detection Videos
888,Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions,"['Leonardo Ranaldi', 'Marco Valentino', 'Andre Freitas']","Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",Improving Chain Thought Reasoning Quasi Symbolic Abstractions Chain CoT represents common strategy reasoning Large Language Models LLMs decomposing complex tasks intermediate inference steps explanations generated CoT susceptible content biases negatively affect robustness faithfulness mitigate existing limitations recent work proposed using logical formalisms coupled external symbolic solvers fully symbolic approaches possess bottleneck requiring complete translation natural language formal languages process affects efficiency flexibility achieve trade paper investigates methods disentangle content logical reasoning complete formalisation particular present QuaSAR Quasi Symbolic Abstract Reasoning variation CoT guides LLMs operate higher level abstraction quasi symbolic explanations framework leverages capability LLMs formalise relevant variables predicates enabling coexistence symbolic elements natural language impact QuaSAR context learning constructing demonstrations improve reasoning capabilities smaller models experiments quasi symbolic abstractions improve CoT based methods 8 accuracy enhancing robustness consistency challenging adversarial variations natural language e MMLU Redux symbolic reasoning tasks e GSM Symbolic
889,Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments,"['Aniket Bhattacharyya', 'Anurag Tripathi', 'Ujjal Das', 'Archan Karmakar', 'Amit Pathak', 'Maneesh Gupta']","Information extraction (IE) from Visually Rich Documents (VRDs) containing layout features along with text is a critical and well-studied task. Specialized non-LLM NLP-based solutions typically involve training models using both textual and geometric information to label sequences/tokens as named entities or answers to specific questions. However, these approaches lack reasoning, are not able to infer values not explicitly present in documents, and do not generalize well to new formats. Generative LLM-based approaches proposed recently are capable of reasoning, but struggle to comprehend clues from document layout especially in previously unseen document formats, and do not show competitive performance in heterogeneous VRD benchmark datasets. In this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs into localized, reusable semantic textual segments called $\textit{semantic blocks}$, which are processed independently. Through focused and more generalizable reasoning,our approach outperforms the state-of-the-art on public VRD benchmarks by 1-3% in F1 scores, is resilient to document formats previously not encountered and shows abilities to correctly extract information not explicitly present in documents.",Information Extraction Visually Rich Documents using LLM based Organization Documents Independent Textual Segments Information extraction Visually Rich Documents VRDs containing layout features text critical studied task Specialized non LLM NLP based solutions typically involve training models using textual geometric information label sequences tokens named entities answers specific questions approaches lack reasoning able infer values explicitly present documents generalize new formats Generative LLM based approaches proposed recently capable reasoning struggle comprehend clues document layout especially previously unseen document formats competitive performance heterogeneous VRD benchmark datasets paper propose BLOCKIE novel LLM based approach organizes VRDs localized reusable semantic textual segments called textit semantic blocks processed independently focused generalizable reasoning approach outperforms state art public VRD benchmarks 1 3 F1 scores resilient document formats previously encountered shows abilities correctly extract information explicitly present documents
890,Enhancing Large Language Model’s Capabilities in Open Domains via Autonomous Tool Integration from GitHub,"['Bohan Lyu', 'Xin Cong', 'Heyang Yu', 'Pan Yang', 'Cheng Qian', 'Zihe Wang', 'Yujia Qin', 'Yining Ye', 'Yaxi Lu', 'Chen Qian', 'Zhong Zhang', 'Yukun Yan', 'Yankai Lin', 'Zhiyuan Liu', 'Maosong Sun']",,Enhancing Large Language Model s Capabilities Open Domains Autonomous Tool Integration GitHub
891,LLMs Can Simulate Standardized Patients via Agent Coevolution,"['Zhuoyun Du', 'LujieZheng', 'Renjun Hu', 'Yuyang Xu', 'Xiawei Li', 'Ying Sun', 'Wei Chen', 'Jian Wu', 'Haolei Cai', 'Haochao Ying']","Training medical personnel using standardized patients (SPs) remains a complex challenge, requiring extensive domain expertise and role-specific practice. Previous research on Large Language Model (LLM)-based SPs mostly focuses on improving data retrieval accuracy or adjusting prompts through human feedback. However, this focus has overlooked the critical need for patient agents to learn a standardized presentation pattern that transforms data into human-like patient responses through unsupervised simulations. To address this gap, we propose EvoPatient, a novel simulated patient framework in which a patient agent and doctor agents simulate the diagnostic process through multi-turn dialogues, simultaneously gathering experience to improve the quality of both questions and answers, ultimately enabling human doctor training. Extensive experiments on various cases demonstrate that, by providing only overall SP requirements, our framework improves over existing reasoning methods by more than 10\% in requirement alignment and better human preference, while achieving an optimal balance of resource consumption after evolving over 200 cases for 10 hours, with excellent generalizability. Our system will be available at https://github.com/ZJUMAI/EvoPatient.",LLMs Simulate Standardized Patients Agent Coevolution Training medical personnel using standardized patients SPs remains complex challenge requiring extensive domain expertise role specific practice Previous research Large Language Model LLM based SPs focuses improving data retrieval accuracy adjusting prompts human feedback focus overlooked critical need patient agents learn standardized presentation pattern transforms data human like patient responses unsupervised simulations address gap propose EvoPatient novel simulated patient framework patient agent doctor agents simulate diagnostic process multi turn dialogues simultaneously gathering experience improve quality questions answers ultimately enabling human doctor training Extensive experiments various cases demonstrate providing overall SP requirements framework improves existing reasoning methods 10 requirement alignment better human preference achieving optimal balance resource consumption evolving 200 cases 10 hours excellent generalizability available https github com ZJUMAI EvoPatient
892,Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts,"['Christopher Bagdon', 'Aidan Combs', 'Carina Silberer', 'Roman Klinger']","Accurate modeling of subjective phenomena such as emotion expression requires data annotated with authors' intentions. Commonly such data is collected by asking study participants to donate and label genuine content produced in the real world, or create content fitting particular labels during the study. Asking participants to create content is often simpler to implement and presents fewer risks to participant privacy than data donation. However, it is unclear if and how study-created content may differ from genuine content, and how differences may impact models. We collect study-created and genuine multimodal social media posts labeled for emotion and compare them on several dimensions, including model performance. We find that compared to genuine posts, study-created posts are longer, rely more on their text and less on their images for emotion expression, and focus more on emotion-prototypical events. The samples of participants willing to donate versus create posts are demographically different. Study-created data is valuable to train models that generalize well to genuine data, but realistic effectiveness estimates require genuine data.",Donate Create Comparing Data Collection Strategies Emotion labeled Multimodal Social Media Posts Accurate modeling subjective phenomena emotion expression requires data annotated authors intentions Commonly data collected asking study participants donate label genuine content produced real world create content fitting particular labels study Asking participants create content simpler implement presents fewer risks participant privacy data donation unclear study created content differ genuine content differences impact models collect study created genuine multimodal social media posts labeled emotion compare dimensions including model performance compared genuine posts study created posts longer rely text images emotion expression focus emotion prototypical events samples participants willing donate versus create posts demographically different Study created data valuable train models generalize genuine data realistic effectiveness estimates require genuine data
893,Which Demographics do LLMs Default to During Annotation?,"['Johannes Schäfer', 'Aidan Combs', 'Christopher Bagdon', 'Jiahui Li', 'Nadine Probol', 'Lynn Greschner', 'Sean Papay', 'Yarik Menchaca Resendiz', 'Aswathy Velutharambath', 'Amelie Wuehrl', 'Sabine Weber', 'Roman Klinger']","Demographics and cultural background of annotators influence the labels they assign in text annotation -- for instance, an elderly woman might find it offensive to read a message addressed to a ""bro"", but a male teenager might find it appropriate. It is therefore important to acknowledge label variations to not under-represent members of a society. Two research directions developed out of this observation in the context of using large language models (LLM) for data annotations, namely (1) studying biases and inherent knowledge of LLMs and (2) injecting diversity in the output by manipulating the prompt with demographic information. We combine these two strands of research and ask the question to which demographics an LLM resorts to when no demographics is given. To answer this question, we evaluate which attributes of human annotators LLMs inherently mimic. Furthermore, we compare non-demographic conditioned prompts and placebo-conditioned prompts (e.g., ""you are an annotator who lives in house number 5"") to demographics-conditioned prompts (""You are a 45 year old man and an expert on politeness annotation. How do you rate {instance}""). We study these questions for politeness and offensiveness annotations on the POPQUORN data set, a corpus created in a controlled manner to investigate human label variations based on demographics which has not been used for LLM-based analyses so far. We observe notable influences related to gender, race, and age in demographic prompting, which contrasts with previous studies that found no such effects.",Demographics LLMs Default Annotation Demographics cultural background annotators influence labels assign text annotation instance elderly woman offensive read message addressed bro male teenager appropriate important acknowledge label variations represent members society research directions developed observation context using large language models LLM data annotations 1 studying biases inherent knowledge LLMs 2 injecting diversity output manipulating prompt demographic information combine strands research ask question demographics LLM resorts demographics given answer question evaluate attributes human annotators LLMs inherently mimic Furthermore compare non demographic conditioned prompts placebo conditioned prompts e g annotator lives house number 5 demographics conditioned prompts 45 year old man expert politeness annotation rate instance study questions politeness offensiveness annotations POPQUORN data set corpus created controlled manner investigate human label variations based demographics used LLM based analyses far observe notable influences related gender race age demographic prompting contrasts previous studies effects
894,Can You Really Trust Code Copilot? Evaluating Large Language Models from a Code Security Perspective,"['Yutao Mou', 'Xiao Deng', 'Yuxiao Luo', 'Shikun Zhang', 'Wei Ye']",,Really Trust Code Copilot Evaluating Large Language Models Code Security Perspective
895,From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MarkerGen,"['Peiwen Yuan', 'Chuyi Tan', 'Shaoxiong Feng', 'Yiwei Li', 'Xinglin Wang', 'Yueqi Zhang', 'Jiayi Shi', 'Boyuan Pan', 'Yao Hu', 'Kan Li']",,Sub Ability Diagnosis Human Aligned Generation Bridging Gap Text Length Control MarkerGen
896,AGD: Adversarial Game Defense Against Jailbreak Attacks in Large Language Models,"['Shilong Pan', 'Zhiliang Tian', 'Zhen Huang', 'Wanlong Yu', 'Zhihua Wen', 'Xinwang Liu', 'Kai Lu', 'Minlie Huang', 'Dongsheng Li']",,AGD Adversarial Game Defense Jailbreak Attacks Large Language Models
897,SCOP: Evaluating the Comprehension Process of Large Language Models from a Cognitive View,"['Yongjie Xiao', 'Hongru Liang', 'Peixin Qin', 'YAO ZHANG', 'Wenqiang Lei']","Despite the great potential of large language models(LLMs) in machine comprehension, it is still disturbing to fully count on them in real-world scenarios. This is probably because there is no rational explanation for whether the comprehension process of LLMs is aligned with that of experts. In this paper, we propose SCOP to carefully examine how LLMs perform during the comprehension process from a cognitive view. Specifically, it is equipped with a systematical definition of five requisite skills during the comprehension process, a strict framework to construct testing data for these skills, and a detailed analysis of advanced open-sourced and closed-sourced LLMs using the testing data. With SCOP, we find that it is still challenging for LLMs to perform an expert-level comprehension process. Even so, we notice that LLMs share some similarities with experts, e.g., performing better at comprehending local information than global information. Further analysis reveals that LLMs can be somewhat unreliable -- they might reach correct answers through flawed comprehension processes. Based on SCOP, we suggest that one direction for improving LLMs is to focus more on the comprehension process, ensuring all comprehension skills are thoroughly developed during training.",SCOP Evaluating Comprehension Process Large Language Models Cognitive View Despite great potential large language models LLMs machine comprehension disturbing fully count real world scenarios probably rational explanation comprehension process LLMs aligned experts paper propose SCOP carefully examine LLMs perform comprehension process cognitive view Specifically equipped systematical definition requisite skills comprehension process strict framework construct testing data skills detailed analysis advanced open sourced closed sourced LLMs using testing data SCOP challenging LLMs perform expert level comprehension process notice LLMs share similarities experts e g performing better comprehending local information global information analysis reveals LLMs somewhat unreliable reach correct answers flawed comprehension processes Based SCOP suggest direction improving LLMs focus comprehension process ensuring comprehension skills thoroughly developed training
898,Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning,"['Peiying Yu', 'Guoxin Chen', 'Jingjing Wang']","Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.",Table Critic Multi Agent Framework Collaborative Criticism Refinement Table Reasoning Despite remarkable capabilities large language models LLMs various reasoning tasks struggle table reasoning tasks particularly maintaining consistency multi step reasoning processes existing approaches explored various decomposition strategies lack effective mechanisms identify correct errors intermediate reasoning steps leading cascading error propagation address issues propose Table Critic novel multi agent framework facilitates collaborative criticism iterative refinement reasoning process convergence correct solutions framework consists specialized agents Judge error identification Critic comprehensive critiques Refiner process improvement Curator pattern distillation effectively deal diverse unpredictable error types introduce self evolving template tree systematically accumulates critique knowledge experience driven learning guides future reflections Extensive experiments demonstrated Table Critic achieves substantial improvements existing methods achieving superior accuracy error correction rates maintaining computational efficiency lower solution degradation rate
899,An Expanded Massive Multilingual Dataset for High-Performance Language Technologies,"['Laurie Burchell', 'Ona De Gibert Bonet', 'Nikolay Arefyev', 'Mikko Aulamo', 'Marta Bañón', 'Pinzhen Chen', 'Mariia Fedorova', 'Liane Guillou', 'Barry Haddow', 'Jan Hajič', 'Jindřich Helcl', 'Erik Henriksson', 'Mateusz Klimaszewski', 'Ville Komulainen', 'Andrey Kutuzov', 'Joona Kytöniemi', 'Veronika Laippala', 'Petter Mæhlum', 'Bhavitvya Malik', 'Farrokh Mehryary', 'Vladislav Mikhailov', 'Nikita Moghe', 'Amanda Myntti', 'Dayyán O’Brien', 'Stephan Oepen', 'Proyag Pal', 'Jousia Piha', 'Sampo Pyysalo', 'Gema Ramírez-Sánchez', 'David Samuel', 'Pavel Stepachev', 'Jörg Tiedemann', 'Dušan Variš', 'Tereza Vojtěchová', 'Jaume Zaragoza-Bernabeu']",,Expanded Massive Multilingual Dataset High Performance Language Technologies
900,Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation,"['Yue Yang', 'Ajay Patel', 'Matt Deitke', 'Tanmay Gupta', 'Luca Weihs', 'Andrew Head', 'Mark Yatskar', 'Chris Callison-Burch', 'Ranjay Krishna', 'Aniruddha Kembhavi', 'Christopher Clark']","Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. Given input text describing a target domain (e.g., ""nutrition fact labels""), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments.",Scaling Text Rich Image Understanding Code Guided Synthetic Multimodal Data Generation Reasoning images rich text charts documents critical application vision language models VLMs VLMs struggle domains scarcity diverse text rich vision language data address challenge present CoSyn framework leverages coding capabilities text large language models LLMs automatically create synthetic text rich multimodal data Given input text describing target domain e g nutrition fact labels CoSyn prompts LLM generate code Python HTML LaTeX rendering synthetic images underlying code textual representations synthetic images CoSyn generate high quality instruction tuning data relying text LLM Using CoSyn constructed dataset comprising 400K images 2 7M rows vision language instruction tuning data Comprehensive experiments seven benchmarks demonstrate models trained synthetic data achieve state art performance competitive open source models including Llama 3 2 surpass proprietary models GPT 4V Gemini 1 5 Flash Furthermore CoSyn produce synthetic pointing data enabling VLMs ground information input images showcasing potential developing multimodal agents capable acting real world environments
901,Hierarchical Attention Generates Better Proofs,"['Jianlong Chen', 'Chao Li', 'Yang Yuan', 'Andrew C Yao']","Large language models (LLMs) have shown promise in formal theorem proving, but their token-level processing often fails to capture the inherent hierarchical nature of mathematical proofs. We introduce \textbf{Hierarchical Attention}, a regularization method that aligns LLMs' attention mechanisms with mathematical reasoning structures. Our approach establishes a five-level hierarchy from foundational elements to high-level concepts, ensuring structured information flow in proof generation. Experiments demonstrate that our method improves proof success rates by 2.05\% on miniF2F and 1.69\% on ProofNet while reducing proof complexity by 23.81\% and 16.50\% respectively. The code is available at https://github.com/Car-pe/HAGBP.",Hierarchical Attention Generates Better Proofs Large language models LLMs shown promise formal theorem proving token level processing fails capture inherent hierarchical nature mathematical proofs introduce textbf Hierarchical Attention regularization method aligns LLMs attention mechanisms mathematical reasoning structures approach establishes level hierarchy foundational elements high level concepts ensuring structured information flow proof generation Experiments demonstrate method improves proof success rates 2 05 miniF2F 1 69 ProofNet reducing proof complexity 23 81 16 50 respectively code available https github com Car pe HAGBP
902,"Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents","['Tianyi Men', 'Zhuoran Jin', 'Pengfei Cao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao']","As Multimodal Large Language Models (MLLMs) advance, multimodal agents show promise in real-world tasks like web navigation and embodied intelligence. However, due to limitations in a lack of external feedback, these agents struggle with self-correction and generalization. A promising approach is to use reward models as external feedback, but there is no clear on how to select reward models for agents. Thus, there is an urgent need to build a reward bench targeted at agents. To address these challenges, we propose Agent-RewardBench, a benchmark designed to evaluate reward modeling ability in MLLMs. The benchmark is characterized by three key features: (1) Multiple dimensions and real-world agent scenarios evaluation. It covers perception, planning, and safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the assessment of agent capabilities at the individual steps of a task, providing a more granular view of performance during the planning process; and (3) Appropriately difficulty and high-quality. We carefully sample from 10 diverse models, difficulty control to maintain task challenges, and manual verification to ensure the integrity of the data. Experiments demonstrate that even state-of-the-art multimodal models show limited performance, highlighting the need for specialized training in agent reward modeling. Code is available at github.",Agent RewardBench Unified Benchmark Reward Modeling Perception Planning Safety Real World Multimodal Agents Multimodal Large Language Models MLLMs advance multimodal agents promise real world tasks like web navigation embodied intelligence limitations lack external feedback agents struggle self correction generalization promising approach use reward models external feedback clear select reward models agents urgent need build reward bench targeted agents address challenges propose Agent RewardBench benchmark designed evaluate reward modeling ability MLLMs benchmark characterized key features 1 Multiple dimensions real world agent scenarios evaluation covers perception planning safety 7 scenarios 2 Step level reward evaluation allows assessment agent capabilities individual steps task providing granular view performance planning process 3 Appropriately difficulty high quality carefully sample 10 diverse models difficulty control maintain task challenges manual verification ensure integrity data Experiments demonstrate state art multimodal models limited performance highlighting need specialized training agent reward modeling Code available github
903,It’s Not Bragging If You Can Back It Up: Can LLMs Understand Braggings?,"['Jingjie Zeng', 'Huayang Li', 'Yuanyuan Sun', 'Liang Yang', 'Hongfei Lin']",,s Bragging LLMs Understand Braggings
904,A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns,"['Tianyi Men', 'Pengfei Cao', 'Zhuoran Jin', 'Yubo Chen', 'Kang Liu', 'Jun Zhao']","With the development of large language models, they are widely used as agents in various fields. A key component of agents is memory, which stores vital information but is susceptible to jailbreak attacks. Existing research mainly focuses on single-agent attacks and shared memory attacks. However, real-world scenarios often involve independent memory. In this paper, we propose the Troublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale, multi-agent, multi-topology text-based attack evaluation framework. TMCHT involves one attacker agent attempting to mislead an entire society of agents. We identify two major challenges in multi-agent attacks: (1) Non-complete graph structure, (2) Large-scale systems. We attribute these challenges to a phenomenon we term toxicity disappearing. To address these issues, we propose an Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes the retrieval suffix to make poisoned samples more easily retrieved and optimizes the replication suffix to make poisoned samples have contagious ability. We demonstrate the superiority of our approach in TMCHT, with 23.51%, 18.95%, and 52.93% improvements in line topology, star topology, and 100-agent settings. Encourage community attention to the security of multi-agent systems.",Troublemaker Contagious Jailbreak Makes Chaos Honest Towns development large language models widely used agents various fields key component agents memory stores vital information susceptible jailbreak attacks Existing research mainly focuses single agent attacks shared memory attacks real world scenarios involve independent memory paper propose Troublemaker Makes Chaos Honest Town TMCHT task large scale multi agent multi topology text based attack evaluation framework TMCHT involves attacker agent attempting mislead entire society agents identify major challenges multi agent attacks 1 Non complete graph structure 2 Large scale systems attribute challenges phenomenon term toxicity disappearing address issues propose Adversarial Replication Contagious Jailbreak ARCJ method optimizes retrieval suffix make poisoned samples easily retrieved optimizes replication suffix make poisoned samples contagious ability demonstrate superiority approach TMCHT 23 51 18 95 52 93 improvements line topology star topology 100 agent settings Encourage community attention security multi agent systems
905,Meta-Learning Neural Mechanisms rather than Bayesian Priors,"['Michael Eric Goodale', 'Salvador Mascarenhas', 'Yair Lakretz']","Children acquire language despite being exposed to several orders of magnitude less data than large language models require. Meta-learning has been proposed as a way to integrate human-like learning biases into neural-network architectures, combining both the structured generalizations of symbolic models with the scalability of neural-network models. But what does meta-learning exactly imbue the model with? We investigate the meta-learning of formal languages and find that, contrary to previous claims, meta-trained models are not learning simplicity-based priors when meta-trained on datasets organised around simplicity. Rather, we find evidence that meta-training imprints neural mechanisms (such as counters) into the model, which function like cognitive primitives for the network on downstream tasks. Most surprisingly, we find that meta-training on a single formal language can provide as much improvement to a model as meta-training on 5000 different formal languages, provided that the formal language incentivizes the learning of useful neural mechanisms. Taken together, our findings provide practical implications for efficient meta-learning paradigms and new theoretical insights into linking symbolic theories and neural mechanisms.",Meta Learning Neural Mechanisms Bayesian Priors Children acquire language despite exposed orders magnitude data large language models require Meta learning proposed way integrate human like learning biases neural network architectures combining structured generalizations symbolic models scalability neural network models does meta learning exactly imbue model investigate meta learning formal languages contrary previous claims meta trained models learning simplicity based priors meta trained datasets organised simplicity evidence meta training imprints neural mechanisms counters model function like cognitive primitives network downstream tasks surprisingly meta training single formal language provide improvement model meta training 5000 different formal languages provided formal language incentivizes learning useful neural mechanisms Taken findings provide practical implications efficient meta learning paradigms new theoretical insights linking symbolic theories neural mechanisms
906,Shifting from Ranking to Set Selection for Retrieval Augmented Generation,"['Dahyun Lee', 'Yongrae Jo', 'Haeju Park', 'Moontae Lee']","Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved passages are not only individually relevant but also collectively form a comprehensive set. Existing approaches primarily rerank top-k passages based on their individual relevance, often failing to meet the information needs of complex queries in multi-hop question answering. In this work, we propose a set-wise passage selection approach and introduce SETR, which explicitly identifies the information requirements of a query through Chain-of-Thought reasoning and selects an optimal set of passages that collectively satisfy those requirements. Experiments on multi-hop RAG benchmarks show that SETR outperforms both proprietary LLM-based rerankers and open-source baselines in terms of answer correctness and retrieval quality, providing an effective and efficient alternative to traditional rerankers in RAG systems. The code is available at https://github.com/LGAI-Research/SetR",Shifting Ranking Set Selection Retrieval Augmented Generation Retrieval Retrieval Augmented Generation RAG ensure retrieved passages individually relevant collectively form comprehensive set Existing approaches primarily rerank k passages based individual relevance failing meet information needs complex queries multi hop question answering work propose set wise passage selection approach introduce SETR explicitly identifies information requirements query Chain Thought reasoning selects optimal set passages collectively satisfy requirements Experiments multi hop RAG benchmarks SETR outperforms proprietary LLM based rerankers open source baselines terms answer correctness retrieval quality providing effective efficient alternative traditional rerankers RAG systems code available https github com LGAI Research SetR
907,Understanding Large Language Model Vulnerabilities to Social Bias Attacks,"['Jiaxu Zhao', 'Meng Fang', 'Fanghua Ye', 'Ke Xu', 'Qin Zhang', 'Joey Tianyi Zhou', 'Mykola Pechenizkiy']",,Understanding Large Language Model Vulnerabilities Social Bias Attacks
908,ChatSOP: An SOP-Guided MCTS Planning Framework for Controllable LLM Dialogue Agents,"['Zhigen Li', 'Jianxiang Peng', 'Yanmeng Wang', 'Yong Cao', 'Tianhao Shen', 'Minghui Zhang', 'Linxi Su', 'Shang Wu', 'Yihang Wu', 'YuQian Wang', 'Ye Wang', 'Wei Hu', 'Jianfeng Li', 'Shaojun Wang', 'Jing Xiao', 'Deyi Xiong']","Dialogue agents powered by Large Language Models (LLMs) show superior performance in various tasks. Despite the better user understanding and human-like responses, their lack of controllability remains a key challenge, often leading to unfocused conversations or task failure. To address this, we introduce Standard Operating Procedure (SOP) to regulate dialogue flow. Specifically, we propose ChatSOP, a novel SOP-guided Monte Carlo Tree Search (MCTS) planning framework designed to enhance the controllability of LLM-driven dialogue agents. To enable this, we curate a dataset comprising SOP-annotated multi-scenario dialogues, generated using a semi-automated role-playing system with GPT-4o and validated through strict manual quality control. Additionally, we propose a novel method that integrates Chain of Thought reasoning with supervised fine-tuning for SOP prediction and utilizes SOP-guided Monte Carlo Tree Search for optimal action planning during dialogues. Experimental results demonstrate the effectiveness of our method, such as achieving a 27.95% improvement in action accuracy compared to baseline models based on GPT-3.5 and also showing notable gains for open-source models. Dataset and codes are publicly available.",ChatSOP SOP Guided MCTS Planning Framework Controllable LLM Dialogue Agents Dialogue agents powered Large Language Models LLMs superior performance various tasks Despite better user understanding human like responses lack controllability remains key challenge leading unfocused conversations task failure address introduce Standard Operating Procedure SOP regulate dialogue flow Specifically propose ChatSOP novel SOP guided Monte Carlo Tree Search MCTS planning framework designed enhance controllability LLM driven dialogue agents enable curate dataset comprising SOP annotated multi scenario dialogues generated using semi automated role playing GPT 4o validated strict manual quality control Additionally propose novel method integrates Chain Thought reasoning supervised fine tuning SOP prediction utilizes SOP guided Monte Carlo Tree Search optimal action planning dialogues Experimental results demonstrate effectiveness method achieving 27 95 improvement action accuracy compared baseline models based GPT 3 5 showing notable gains open source models Dataset codes publicly available
909,Pixel-Level Reasoning Segmentation via Multi-turn Conversations,"['Dexian Cai', 'Xiaocui Yang', 'YongKang Liu', 'Daling Wang', 'Shi Feng', 'Yifei Zhang', 'Soujanya Poria']","Existing visual perception systems focus on region-level segmentation in single-turn dialogues, relying on complex and explicit query instructions. Such systems cannot reason at the pixel level and comprehend dynamic user intent that changes over interaction. Our work tackles this issue by introducing a novel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on multi-turn conversations, tracking evolving user intent via multi-turn interactions for fine-grained segmentation. To establish a benchmark for this novel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on Multi-Turn Conversations (PRIST), comprising 24k utterances from 8.3k multi-turn conversational scenarios with segmentation targets. Building on PRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning Segmentation framework, integrates pixel-level segmentation with robust multi-turn conversation understanding, generating pixel-grounded explanations aligned with user intent. The PRIST dataset and MIRSA framework fill the gap in pixel-level reasoning segmentation. Experimental results on the PRIST dataset demonstrate that our method outperforms current segmentation-specific baselines in terms of segmentation and LLM-based reasoning metrics. The code and data are available at: https://github.com/ccccai239/PixelRIST.",Pixel Level Reasoning Segmentation Multi turn Conversations Existing visual perception systems focus region level segmentation single turn dialogues relying complex explicit query instructions systems reason pixel level comprehend dynamic user intent changes interaction work tackles issue introducing novel task Pixel level Reasoning Segmentation Pixel level RS based multi turn conversations tracking evolving user intent multi turn interactions fine grained segmentation establish benchmark novel task build Pixel level ReasonIng Segmentation Dataset Based Multi Turn Conversations PRIST comprising 24k utterances 8 3k multi turn conversational scenarios segmentation targets Building PRIST propose MIRAS Multi turn Interactive ReAsoning Segmentation framework integrates pixel level segmentation robust multi turn conversation understanding generating pixel grounded explanations aligned user intent PRIST dataset MIRSA framework gap pixel level reasoning segmentation Experimental results PRIST dataset demonstrate method outperforms current segmentation specific baselines terms segmentation LLM based reasoning metrics code data available https github com ccccai239 PixelRIST
910,Fixing Distribution Shifts of LLM Self-Critique via On-Policy Self-Play Training,"['Rong Bao', 'Donglei Yu', 'Kai Fan', 'Minpeng Liao']",,Fixing Distribution Shifts LLM Self Critique Policy Self Play Training
911,Inferring Functionality of Attention Heads from their Parameters,"['Amit Elhelo', 'Mor Geva']","Attention heads are one of the building blocks of large language models (LLMs). Prior work on investigating their operation mostly focused on analyzing their behavior during inference for specific circuits or tasks. In this work, we seek a comprehensive mapping of the operations they implement in a model. We propose MAPS (Mapping Attention head ParameterS), an efficient framework that infers the functionality of attention heads from their parameters, without any model training or inference. We showcase the utility of MAPS for answering two types of questions: (a) given a predefined operation, mapping how strongly heads across the model implement it, and (b) given an attention head, inferring its salient functionality. Evaluating MAPS on 20 operations across 6 popular LLMs shows its estimations correlate with the head's outputs during inference and are causally linked to the model's predictions. Moreover, its mappings reveal attention heads of certain operations that were overlooked in previous studies, and valuable insights on function universality and architecture biases in LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS to characterize the salient operations of a given head. Our pipeline produces plausible operation descriptions for most heads, as assessed by human judgment, while revealing diverse operations.",Inferring Functionality Attention Heads Parameters Attention heads building blocks large language models LLMs Prior work investigating operation focused analyzing behavior inference specific circuits tasks work seek comprehensive mapping operations implement model propose MAPS Mapping Attention head ParameterS efficient framework infers functionality attention heads parameters model training inference showcase utility MAPS answering types questions given predefined operation mapping strongly heads model implement b given attention head inferring salient functionality Evaluating MAPS 20 operations 6 popular LLMs shows estimations correlate head s outputs inference causally linked model s predictions mappings reveal attention heads certain operations overlooked previous studies valuable insights function universality architecture biases LLMs present automatic pipeline analysis leverage MAPS characterize salient operations given head pipeline produces plausible operation descriptions heads assessed human judgment revealing diverse operations
912,Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations,"['Xin Quan', 'Marco Valentino', 'Louise A. Dennis', 'Andre Freitas']","Natural language explanations play a fundamental role in Natural Language Inference (NLI) by revealing how premises logically entail hypotheses. Recent work has shown that the interaction of large language models (LLMs) with theorem provers (TPs) can help verify and improve the validity of NLI explanations. However, TPs require translating natural language into machine-verifiable formal representations, a process that introduces the risk of semantic information loss and unfaithful interpretation, an issue compounded by LLMs' challenges in capturing critical logical structures with sufficient precision. Moreover, LLMs are still limited in their capacity for rigorous and robust proof construction within formal verification frameworks. To mitigate issues related to faithfulness and robustness, this paper investigates strategies to (1) alleviate semantic loss during autoformalisation, (2) efficiently identify and correct syntactic errors in logical representations, (3) explicitly use logical expressions to guide LLMs in generating structured proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree using different LLMs demonstrate that the proposed strategies yield significant improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover, we show that specific interventions on the hybrid LLM-TP architecture can substantially improve efficiency, drastically reducing the number of iterations required for successful verification.",Faithful Robust LLM Driven Theorem Proving NLI Explanations Natural language explanations play fundamental role Natural Language Inference NLI revealing premises logically entail hypotheses Recent work shown interaction large language models LLMs theorem provers TPs help verify improve validity NLI explanations TPs require translating natural language machine verifiable formal representations process introduces risk semantic information loss unfaithful interpretation issue compounded LLMs challenges capturing critical logical structures sufficient precision LLMs limited capacity rigorous robust proof construction formal verification frameworks mitigate issues related faithfulness robustness paper investigates strategies 1 alleviate semantic loss autoformalisation 2 efficiently identify correct syntactic errors logical representations 3 explicitly use logical expressions guide LLMs generating structured proof sketches 4 increase LLMs capacity interpreting TP s feedback iterative refinement empirical results e SNLI QASC WorldTree using different LLMs demonstrate proposed strategies yield significant improvements autoformalisation 18 46 34 2 39 77 explanation refinement 29 5 51 5 41 25 state art model specific interventions hybrid LLM TP architecture substantially improve efficiency drastically reducing number iterations required successful verification
913,Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing,"['Jiakuan Xie', 'Pengfei Cao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao']","Knowledge editing, which aims to update the knowledge encoded in language models, can be deceptive. Despite the fact that many existing knowledge editing algorithms achieve near-perfect performance on conventional metrics, the models edited by them are still prone to generating original knowledge. This paper introduces the concept of ""superficial editing"" to describe this phenomenon. Our comprehensive evaluation reveals that this issue presents a significant challenge to existing algorithms. Through systematic investigation, we identify and validate two key factors contributing to this issue: (1) the residual stream at the last subject position in earlier layers and (2) specific attention modules in later layers. Notably, certain attention heads in later layers, along with specific left singular vectors in their output matrices, encapsulate the original knowledge and exhibit a causal relationship with superficial editing. Furthermore, we extend our analysis to the task of superficial unlearning, where we observe consistent patterns in the behavior of specific attention heads and their corresponding left singular vectors, thereby demonstrating the robustness and broader applicability of our methodology and conclusions. Our code is available here.",Revealing Deceptiveness Knowledge Editing Mechanistic Analysis Superficial Editing Knowledge editing aims update knowledge encoded language models deceptive Despite fact existing knowledge editing algorithms achieve near perfect performance conventional metrics models edited prone generating original knowledge paper introduces concept superficial editing phenomenon comprehensive evaluation reveals issue presents significant challenge existing algorithms systematic investigation identify validate key factors contributing issue 1 residual stream subject position earlier layers 2 specific attention modules later layers Notably certain attention heads later layers specific left singular vectors output matrices encapsulate original knowledge exhibit causal relationship superficial editing Furthermore extend analysis task superficial unlearning observe consistent patterns behavior specific attention heads corresponding left singular vectors demonstrating robustness broader applicability methodology conclusions code available
914,Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation,"['Wenyu Huang', 'Pavlos Vougiouklis', 'Mirella Lapata', 'Jeff Z. Pan']","Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.",Masking Multi hop QA Analysis Language Models Perform Context Permutation Multi hop Question Answering MHQA adds layers complexity question answering making challenging Language Models LMs prompted multiple search results tasked retrieving relevant information employing multi hop reasoning information sources LMs perform traditional question answering tasks causal mask hinder capacity reason complex contexts paper explore LMs respond multi hop questions permuting search results retrieved documents various configurations study reveals interesting findings follows 1 Encoder decoder models ones Flan T5 family generally outperform causal decoder LMs MHQA tasks despite significantly smaller size 2 altering order gold documents reveals distinct trends Flan T5 models fine tuned decoder models optimal performance observed document order aligns reasoning chain order 3 enhancing causal decoder models bi directional attention modifying causal mask effectively boost end performance addition conduct thorough investigation distribution LM attention weights context MHQA experiments reveal attention weights tend peak higher values resulting answer correct leverage finding heuristically improve LMs performance task code publicly available https github com hwy9855 MultiHopQA Reasoning
915,From Human Reading to NLM Understanding: Evaluating the Role of Eye-Tracking Data in Encoder-Based Models,"['Luca Dini', 'Lucia Domenichelli', 'Dominique Brunato', 'Felice Dell’Orletta']",,Human Reading NLM Understanding Evaluating Role Eye Tracking Data Encoder Based Models
916,Optimizing Question Semantic Space for Dynamic Retrieval-Augmented Multi-hop Question Answering,"['LINHAO YE', 'Qin Chen', 'Jie Zhou', 'Lang Yu', 'Zhikai Lei', 'Liang He']","Retrieval-augmented generation (RAG) is usually integrated into large language models (LLMs) to mitigate hallucinations and knowledge obsolescence. Whereas,conventional one-step retrieve-and-read methods are insufficient for multi-hop question answering, facing challenges of retrieval semantic mismatching and the high cost in handling interdependent subquestions. In this paper, we propose Optimizing Question Semantic Space for Dynamic Retrieval-Augmented Multi-hop Question Answering (Q-DREAM). Q-DREAM consists of three key modules: (1) the Question Decomposition Module (QDM), which decomposes multi-hop questions into fine-grained subquestions; (2) the Subquestion Dependency Optimizer Module (SDOM), which models the interdependent relations of subquestions for better understanding; and (3) the Dynamic Passage Retrieval Module (DPRM), which aligns subquestions with relevant passages by optimizing the semantic embeddings. Experimental results across various benchmarks demonstrate that Q-DREAM significantly outperforms existing RAG methods, achieving state-of-the-art performance in both in-domain and out-of-domain settings. Notably, Q-DREAM also improves retrieval efficiency while maintaining high accuracy compared with recent baselines.",Optimizing Question Semantic Space Dynamic Retrieval Augmented Multi hop Question Answering Retrieval augmented generation RAG usually integrated large language models LLMs mitigate hallucinations knowledge obsolescence conventional step retrieve read methods insufficient multi hop question answering facing challenges retrieval semantic mismatching high cost handling interdependent subquestions paper propose Optimizing Question Semantic Space Dynamic Retrieval Augmented Multi hop Question Answering Q DREAM Q DREAM consists key modules 1 Question Decomposition Module QDM decomposes multi hop questions fine grained subquestions 2 Subquestion Dependency Optimizer Module SDOM models interdependent relations subquestions better understanding 3 Dynamic Passage Retrieval Module DPRM aligns subquestions relevant passages optimizing semantic embeddings Experimental results various benchmarks demonstrate Q DREAM significantly outperforms existing RAG methods achieving state art performance domain domain settings Notably Q DREAM improves retrieval efficiency maintaining high accuracy compared recent baselines
917,Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal LLMs,"['Xiaoyuan Liu', 'Wenxuan Wang', 'Youliang Yuan', 'Jen-tse Huang', 'Qiuzhi Liu', 'Pinjia He', 'Zhaopeng Tu']","This paper explores the problem of commonsense level vision-knowledge conflict in Multimodal Large Language Models (MLLMs), where visual information contradicts model's internal commonsense knowledge. To study this issue, we introduce an automated framework, augmented with human-in-the-loop quality control, to generate inputs designed to simulate and evaluate these conflicts in MLLMs. Using this framework, we have crafted a diagnostic benchmark consisting of 374 original images and 1,122 high-quality question-answer (QA) pairs. The benchmark covers two aspects of conflict and three question types, providing a thorough assessment tool. We apply this benchmark to assess the conflict-resolution capabilities of nine representative MLLMs from various model families. Our results indicate an evident over-reliance on parametric knowledge for approximately 20% of all queries, especially among Yes-No and action-related problems. Based on these findings, we evaluate the effectiveness of existing approaches to mitigating the conflicts and compare them to our ""Focus-on-Vision"" prompting strategy. Despite some improvement, the vision-knowledge conflict remains unresolved and can be further scaled through our data construction framework. Our proposed framework, benchmark, and analysis contribute to the understanding and mitigation of vision-knowledge conflicts in MLLMs.",Insight Sight Exploring Vision Knowledge Conflicts Multimodal LLMs paper explores problem commonsense level vision knowledge conflict Multimodal Large Language Models MLLMs visual information contradicts model s internal commonsense knowledge study issue introduce automated framework augmented human loop quality control generate inputs designed simulate evaluate conflicts MLLMs Using framework crafted diagnostic benchmark consisting 374 original images 1 122 high quality question answer QA pairs benchmark covers aspects conflict question types providing thorough assessment tool apply benchmark assess conflict resolution capabilities representative MLLMs various model families results indicate evident reliance parametric knowledge approximately 20 queries especially Yes action related problems Based findings evaluate effectiveness existing approaches mitigating conflicts compare Focus Vision prompting strategy Despite improvement vision knowledge conflict remains unresolved scaled data construction framework proposed framework benchmark analysis contribute understanding mitigation vision knowledge conflicts MLLMs
918,SceneGenAgent: Precise Industrial Scene Generation with Coding Agent,"['Xiao Xia', 'Dan Zhang', 'Zibo Liao', 'Zhenyu Hou', 'Tianrui Sun', 'Jing Li', 'Ling Fu', 'Yuxiao Dong']","The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and data are available at https://github.com/THUDM/SceneGenAgent .",SceneGenAgent Precise Industrial Scene Generation Coding Agent modeling industrial scenes essential simulations industrial manufacturing large language models LLMs shown significant progress generating general 3D scenes textual descriptions generating industrial scenes LLMs poses unique challenge demand precise measurements positioning requiring complex planning spatial arrangement address challenge introduce SceneGenAgent LLM based agent generating industrial scenes C code SceneGenAgent ensures precise layout planning structured calculable format layout verification iterative refinement meet quantitative requirements industrial scenarios Experiment results demonstrate LLMs powered SceneGenAgent exceed original performance reaching 81 0 success rate real world industrial scene generation tasks effectively meeting scene generation requirements enhance accessibility construct SceneInstruct dataset designed fine tuning open source LLMs integrate SceneGenAgent Experiments fine tuning open source LLMs SceneInstruct yields significant performance improvements Llama3 1 70B approaching capabilities GPT 4o code data available https github com THUDM SceneGenAgent
919,ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models,"['Hanxing Ding', 'Shuchang Tao', 'Liang Pang', 'Zihao Wei', 'Jinyang Gao', 'Bolin Ding', 'Huawei Shen', 'Xueqi Cheng']","Tool learning has emerged as a crucial capability for large language models (LLMs) to solve complex real-world tasks through interaction with external tools. Existing approaches face significant challenges, including reliance on hand-crafted prompts, difficulty in multi-step planning, and lack of precise error diagnosis and reflection mechanisms. We propose ToolCoder, a novel framework that reformulates tool learning as a code generation task. Inspired by software engineering principles, ToolCoder transforms natural language queries into structured Python function scaffold and systematically breaks down tasks with descriptive comments, enabling LLMs to leverage coding paradigms for complex reasoning and planning. It then generates and executes function implementations to obtain final responses. Additionally, ToolCoder stores successfully executed functions in a repository to promote code reuse, while leveraging error traceback mechanisms for systematic debugging, optimizing both execution efficiency and robustness. Experiments demonstrate that ToolCoder achieves superior performance in task completion accuracy and execution reliability compared to existing approaches, establishing the effectiveness of code-centric approaches in tool learning.",ToolCoder Systematic Code Empowered Tool Learning Framework Large Language Models Tool learning emerged crucial capability large language models LLMs solve complex real world tasks interaction external tools Existing approaches face significant challenges including reliance hand crafted prompts difficulty multi step planning lack precise error diagnosis reflection mechanisms propose ToolCoder novel framework reformulates tool learning code generation task Inspired software engineering principles ToolCoder transforms natural language queries structured Python function scaffold systematically breaks tasks descriptive comments enabling LLMs leverage coding paradigms complex reasoning planning generates executes function implementations obtain final responses Additionally ToolCoder stores successfully executed functions repository promote code reuse leveraging error traceback mechanisms systematic debugging optimizing execution efficiency robustness Experiments demonstrate ToolCoder achieves superior performance task completion accuracy execution reliability compared existing approaches establishing effectiveness code centric approaches tool learning
920,Human Alignment: How Much Do We Adapt to LLMs?,"['Cazalets Tanguy', 'Ruben Janssens', 'Tony Belpaeme', 'Joni Dambre']",,Human Alignment Adapt LLMs
921,Enhancing Text Editing for Grammatical Error Correction: Arabic as a Case Study,"['Bashar Alhafni', 'Nizar Habash']","Text editing frames grammatical error correction (GEC) as a sequence tagging problem, where edit tags are assigned to input tokens, and applying these edits results in the corrected text. This approach has gained attention for its efficiency and interpretability. However, while extensively explored for English, text editing remains largely underexplored for morphologically rich languages like Arabic. In this paper, we introduce a text editing approach that derives edit tags directly from data, eliminating the need for language-specific edits. We demonstrate its effectiveness on Arabic, a diglossic and morphologically rich language, and investigate the impact of different edit representations on model performance. Our approach achieves SOTA results on two Arabic GEC benchmarks and performs on par with SOTA on two others. Additionally, our models are over six times faster than existing Arabic GEC systems, making our approach more practical for real-world applications. Finally, we explore ensemble models, demonstrating how combining different models leads to further performance improvements. We make our code, data, and pretrained models publicly available.",Enhancing Text Editing Grammatical Error Correction Arabic Case Study Text editing frames grammatical error correction GEC sequence tagging problem edit tags assigned input tokens applying edits results corrected text approach gained attention efficiency interpretability extensively explored English text editing remains largely underexplored morphologically rich languages like Arabic paper introduce text editing approach derives edit tags directly data eliminating need language specific edits demonstrate effectiveness Arabic diglossic morphologically rich language investigate impact different edit representations model performance approach achieves SOTA results Arabic GEC benchmarks performs par SOTA Additionally models times faster existing Arabic GEC systems making approach practical real world applications Finally explore ensemble models demonstrating combining different models leads performance improvements make code data pretrained models publicly available
922,From Isolates to Families: Using Neural Networks for Automated Language Affiliation,"['Frederic Blum', 'Steffen Herbold', 'Johann-Mattis List']","In historical linguistics, the affiliation of languages to a common language family is traditionally carried out using a complex workflow that relies on manually comparing individual languages. Large-scale standardized collections of multilingual wordlists and grammatical language structures might help to improve this and open new avenues for developing automated language affiliation workflows. Here, we present neural network models that use lexical and grammatical data from a worldwide sample of more than 1,000 languages with known affiliations to classify individual languages into families. In line with the traditional assumption of most linguists, our results show that models trained on lexical data alone outperform models solely based on grammatical data, whereas combining both types of data yields even better performance. In additional experiments, we show how our models can identify long-ranging relations between entire subgroups, how they can be employed to investigate potential relatives of linguistic isolates, and how they can help us to obtain first hints on the affiliation of so far unaffiliated languages. We conclude that models for automated language affiliation trained on lexical and grammatical data provide comparative linguists with a valuable tool for evaluating hypotheses about deep and unknown language relations.",Isolates Families Using Neural Networks Automated Language Affiliation historical linguistics affiliation languages common language family traditionally carried using complex workflow relies manually comparing individual languages Large scale standardized collections multilingual wordlists grammatical language structures help improve open new avenues developing automated language affiliation workflows present neural network models use lexical grammatical data worldwide sample 1 000 languages known affiliations classify individual languages families line traditional assumption linguists results models trained lexical data outperform models solely based grammatical data combining types data yields better performance additional experiments models identify long ranging relations entire subgroups employed investigate potential relatives linguistic isolates help obtain hints affiliation far unaffiliated languages conclude models automated language affiliation trained lexical grammatical data provide comparative linguists valuable tool evaluating hypotheses deep unknown language relations
923,ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models,"['Xuxu Liu', 'Siyuan Liang', 'Mengya Han', 'Yong Luo', 'Aishan Liu', 'Xiantao Cai', 'Zheng He', 'Dacheng Tao']",,ELBA Bench Efficient Learning Backdoor Attacks Benchmark Large Language Models
924,"Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts","['Xue Zhang', 'Yunlong Liang', 'Fandong Meng', 'Songming Zhang', 'Yufeng Chen', 'Jinan Xu', 'Jie Zhou']","Continually expanding new languages for existing large language models (LLMs) is a promising yet challenging approach to building powerful multilingual LLMs. The biggest challenge is to make the model continuously learn new languages while preserving the proficient ability of old languages. To achieve this, recent work utilizes the Mixture-of-Experts (MoE) architecture to expand new languages by adding new experts and avoid catastrophic forgetting of old languages by routing corresponding tokens to the original model backbone (old experts). Although intuitive, this kind of method is parameter-costly when expanding new languages and still inevitably impacts the performance of old languages. To address these limitations, we analyze the language characteristics of different layers in LLMs and propose a layer-wise expert allocation algorithm (LayerMoE) to determine the appropriate number of new experts for each layer. Specifically, we find different layers in LLMs exhibit different representation similarities between languages and then utilize the similarity as the indicator to allocate experts for each layer, i.e., the higher similarity, the fewer experts. Additionally, to further mitigate the forgetting of old languages, we add a classifier in front of the router network on the layers with higher similarity to guide the routing of old language tokens. Experimental results show that our method outperforms the previous state-of-the-art baseline with 60% fewer experts in the single-expansion setting and with 33.3% fewer experts in the lifelong-expansion setting, demonstrating the effectiveness of our method.",Better Efficient Multilingual Expansion LLMs Layer wise Mixture Experts Continually expanding new languages existing large language models LLMs promising challenging approach building powerful multilingual LLMs biggest challenge make model continuously learn new languages preserving proficient ability old languages achieve recent work utilizes Mixture Experts MoE architecture expand new languages adding new experts avoid catastrophic forgetting old languages routing corresponding tokens original model backbone old experts intuitive kind method parameter costly expanding new languages inevitably impacts performance old languages address limitations analyze language characteristics different layers LLMs propose layer wise expert allocation algorithm LayerMoE determine appropriate number new experts layer Specifically different layers LLMs exhibit different representation similarities languages utilize similarity indicator allocate experts layer e higher similarity fewer experts Additionally mitigate forgetting old languages add classifier router network layers higher similarity guide routing old language tokens Experimental results method outperforms previous state art baseline 60 fewer experts single expansion setting 33 3 fewer experts lifelong expansion setting demonstrating effectiveness method
925,When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation,"['Daniela Occhipinti', 'Marco Guerini', 'Malvina Nissim']","Endowing dialogue agents with persona information has proven to significantly improve the consistency and diversity of their generations. While much focus has been placed on aligning dialogues with provided personas, the adaptation to the interlocutor's profile remains largely underexplored. In this work, we investigate three key aspects: (1) a model's ability to align responses with both the provided persona and the interlocutor's; (2) its robustness when dealing with familiar versus unfamiliar interlocutors and topics, and (3) the impact of additional fine-tuning on specific persona-based dialogues. We evaluate dialogues generated with diverse speaker pairings and topics, framing the evaluation as an author identification task and employing both LLM-as-a-judge and human evaluations. By systematically masking or disclosing information about the interlocutor, we assess its impact on dialogue generation. Results show that access to the interlocutor's persona improves the recognition of the target speaker, while masking it does the opposite. Although models generalise well across topics, they struggle with unfamiliar interlocutors. Finally, we found that in zero-shot settings, LLMs often copy biographical details, facilitating identification but trivialising the task.",Harry Meets Superman Role Interlocutor Persona Based Dialogue Generation Endowing dialogue agents persona information proven significantly improve consistency diversity generations focus placed aligning dialogues provided personas adaptation interlocutor s profile remains largely underexplored work investigate key aspects 1 model s ability align responses provided persona interlocutor s 2 robustness dealing familiar versus unfamiliar interlocutors topics 3 impact additional fine tuning specific persona based dialogues evaluate dialogues generated diverse speaker pairings topics framing evaluation author identification task employing LLM judge human evaluations systematically masking disclosing information interlocutor assess impact dialogue generation Results access interlocutor s persona improves recognition target speaker masking does opposite models generalise topics struggle unfamiliar interlocutors Finally zero shot settings LLMs copy biographical details facilitating identification trivialising task
926,ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs,"['Zhenliang Zhang', 'Xinyu Hu', 'Huixuan Zhang', 'Junzhe Zhang', 'Xiaojun Wan']",,ICR Probe Tracking Hidden State Dynamics Reliable Hallucination Detection LLMs
927,Revisit Self-Debugging with Self-Generated Tests for Code Generation,"['Xiancai Chen', 'Zhengwei Tao', 'Kechi Zhang', 'Changzhi Zhou', 'Xinyu Zhang', 'Wanli Gu', 'Yuanpeng He', 'Mengdi Zhang', 'Xunliang Cai', 'Haiyan Zhao', 'Zhi Jin']","Large language models (LLMs) have shown significant advancements in code generation, but still face challenges on tasks beyond their basic capabilities. Recently, the notion of self-debugging has been proposed to boost the performance of code generation by leveraging execution feedback from tests. Despite its promise, the availability of high-quality tests in real-world scenarios is limited. In this context, self-debugging with self-generated tests is a promising solution but lacks a full exploration of its limitations and practical potential. Therefore, we investigate its efficacy on diverse programming problems. To deepen our understanding, we propose two distinct paradigms for the process: post-execution and in-execution self-debugging. Within the scope of self-contained Python programming tasks, we find that post-execution self-debugging struggles on basic problems but shows potential for improvement on competitive ones, due to the bias introduced by self-generated tests. On the other hand, in-execution self-debugging enables LLMs to mitigate the bias by solely leveraging intermediate states during execution, thereby enhancing code generation.",Revisit Self Debugging Self Generated Tests Code Generation Large language models LLMs shown significant advancements code generation face challenges tasks basic capabilities Recently notion self debugging proposed boost performance code generation leveraging execution feedback tests Despite promise availability high quality tests real world scenarios limited context self debugging self generated tests promising solution lacks exploration limitations practical potential investigate efficacy diverse programming problems deepen understanding propose distinct paradigms process post execution execution self debugging scope self contained Python programming tasks post execution self debugging struggles basic problems shows potential improvement competitive ones bias introduced self generated tests hand execution self debugging enables LLMs mitigate bias solely leveraging intermediate states execution enhancing code generation
928,InSerter: Speech Instruction Following with Unsupervised Interleaved Pre-training,"['Dingdong WANG', 'Jin Xu', 'Ruihang Chu', 'Zhifang Guo', 'Xiong Wang', 'Jincenzi Wu', 'Dongchao Yang', 'Shengpeng Ji', 'Junyang Lin']","Recent advancements in speech large language models (SpeechLLMs) have attracted considerable attention. Nonetheless, current methods exhibit suboptimal performance in adhering to speech instructions. Notably, the intelligence of models significantly diminishes when processing speech-form input as compared to direct text-form input. Prior work has attempted to mitigate this semantic inconsistency between speech and text representations through techniques such as representation and behavior alignment, which involve the meticulous design of data pairs during the post-training phase. In this paper, we introduce a simple and scalable training method called InSerter, which stands for Interleaved Speech-Text Representation Pre-training. InSerter is designed to pre-train large-scale unsupervised speech-text sequences, where the speech is synthesized from randomly selected segments of an extensive text corpus using text-to-speech conversion. Consequently, the model acquires the ability to generate textual continuations corresponding to the provided speech segments, obviating the need for intensive data design endeavors. To systematically evaluate speech instruction-following capabilities, we introduce SpeechInstructBench, the first comprehensive benchmark specifically designed for speech-oriented instruction-following tasks. Our proposed InSerter achieves SOTA performance in SpeechInstructBench and demonstrates superior or competitive results across diverse speech processing tasks.",InSerter Speech Instruction Following Unsupervised Interleaved Pre training Recent advancements speech large language models SpeechLLMs attracted considerable attention Nonetheless current methods exhibit suboptimal performance adhering speech instructions Notably intelligence models significantly diminishes processing speech form input compared direct text form input Prior work attempted mitigate semantic inconsistency speech text representations techniques representation behavior alignment involve meticulous design data pairs post training phase paper introduce simple scalable training method called InSerter stands Interleaved Speech Text Representation Pre training InSerter designed pre train large scale unsupervised speech text sequences speech synthesized randomly selected segments extensive text corpus using text speech conversion Consequently model acquires ability generate textual continuations corresponding provided speech segments obviating need intensive data design endeavors systematically evaluate speech instruction following capabilities introduce SpeechInstructBench comprehensive benchmark specifically designed speech oriented instruction following tasks proposed InSerter achieves SOTA performance SpeechInstructBench demonstrates superior competitive results diverse speech processing tasks
929,Exploring LLMs’ Ability to Spontaneously and Conditionally Modify Moral Expressions through Text Manipulation,"['Candida Maria Greco', 'Lucio La Cava', 'Lorenzo Zangari', 'Andrea Tagarelli']",,Exploring LLMs Ability Spontaneously Conditionally Modify Moral Expressions Text Manipulation
930,Mixture of Ordered Scoring Experts for Cross-prompt Essay Trait Scoring,"['Po-Kai Chen', 'Bo-Wei Tsai', 'Shao Kuan Wei', 'Chien-Yao Wang', 'Jia-Ching Wang', 'Yi-Ting Huang']",,Mixture Ordered Scoring Experts Cross prompt Essay Trait Scoring
931,A Sample Offline Saves Time: Knowledge Distillation in the LLM Era,"['Anshumann', 'Mohd Abbas Zaidi', 'Akhil Kedia', 'Jinwoo Ahn', 'Taehwak Kwon', 'Kangwook Lee', 'Haejun Lee', 'Joohyung Lee']",,Sample Offline Saves Time Knowledge Distillation LLM Era
932,Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues,"['Varsha Suresh', 'M. Hamza Mughal', 'Christian Theobalt', 'Vera Demberg']","Research in linguistics shows that non-verbal cues, such as gestures, play a crucial role in spoken discourse. For example, speakers perform hand gestures to indicate topic shifts, helping listeners identify transitions in discourse. In this work, we investigate whether the joint modeling of gestures using human motion sequences and language can improve spoken discourse modeling in language models. To integrate gestures into language models, we first encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE. These gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space. To evaluate the gesture-aligned language model on spoken discourse, we construct text infilling tasks targeting three key discourse cues grounded in linguistic research: discourse connectives, stance markers, and quantifiers. Results show that incorporating gestures enhances marker prediction accuracy across the three tasks, highlighting the complementary information that gestures can offer in modeling spoken discourse. We view this work as an initial step toward leveraging non-verbal cues to advance spoken language modeling in language models.",Enhancing Spoken Discourse Modeling Language Models Using Gestural Cues Research linguistics shows non verbal cues gestures play crucial role spoken discourse example speakers perform hand gestures indicate topic shifts helping listeners identify transitions discourse work investigate joint modeling gestures using human motion sequences language improve spoken discourse modeling language models integrate gestures language models encode 3D human motion sequences discrete gesture tokens using VQ VAE gesture token embeddings aligned text embeddings feature alignment mapping text embedding space evaluate gesture aligned language model spoken discourse construct text infilling tasks targeting key discourse cues grounded linguistic research discourse connectives stance markers quantifiers Results incorporating gestures enhances marker prediction accuracy tasks highlighting complementary information gestures offer modeling spoken discourse view work initial step leveraging non verbal cues advance spoken language modeling language models
933,ExploraCoder: Advancing Code Generation for Multiple Unseen APIs via Planning and Chained Exploration,"['Yunkun Wang', 'Yue Zhang', 'Zhen Qin', 'Chen Zhi', 'Binhua Li', 'Fei Huang', 'Yongbin Li', 'Shuiguang Deng']",,ExploraCoder Advancing Code Generation Multiple Unseen APIs Planning Chained Exploration
934,Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models,"['Zihong Zhang', 'Liqi He', 'Zuchao Li', 'Lefei Zhang', 'hai zhao', 'Bo Du']","Word segmentation stands as a cornerstone of Natural Language Processing (NLP). Based on the concept of ""comprehend first, segment later"", we propose a new framework to explore the limit of unsupervised word segmentation with Large Language Models (LLMs) and evaluate the semantic understanding capabilities of LLMs based on word segmentation. We employ current mainstream LLMs to perform word segmentation across multiple languages to assess LLMs' ""comprehension"". Our findings reveal that LLMs are capable of following simple prompts to segment raw text into words. There is a trend suggesting that models with more parameters tend to perform better on multiple languages. Additionally, we introduce a novel unsupervised method, termed LLACA ($\textbf{L}$arge $\textbf{L}$anguage Model-Inspired $\textbf{A}$ho-$\textbf{C}$orasick $\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities of Aho-Corasick automata, LLACA innovatively combines these with the deep insights of well-pretrained LLMs. This approach not only enables the construction of a dynamic $n$-gram model that adjusts based on contextual information but also integrates the nuanced understanding of LLMs, offering significant improvements over traditional methods. Our source code is available at https://github.com/hkr04/LLACA",Segment Comprehend Explore Limit Unsupervised Word Segmentation Large Language Models Word segmentation stands cornerstone Natural Language Processing NLP Based concept comprehend segment later propose new framework explore limit unsupervised word segmentation Large Language Models LLMs evaluate semantic understanding capabilities LLMs based word segmentation employ current mainstream LLMs perform word segmentation multiple languages assess LLMs comprehension findings reveal LLMs capable following simple prompts segment raw text words trend suggesting models parameters tend perform better multiple languages Additionally introduce novel unsupervised method termed LLACA textbf L arge textbf L anguage Model Inspired textbf ho textbf C orasick textbf utomaton Leveraging advanced pattern recognition capabilities Aho Corasick automata LLACA innovatively combines deep insights pretrained LLMs approach enables construction dynamic n gram model adjusts based contextual information integrates nuanced understanding LLMs offering significant improvements traditional methods source code available https github com hkr04 LLACA
935,RUBY: An Effective Framework for Multi-Constraint Multi-Hop Question Generation,"['Wenzhuo Zhao', 'Shuangyin Li']",,RUBY Effective Framework Multi Constraint Multi Hop Question Generation
936,Can Indirect Prompt Injection Attacks Be Detected and Removed?,"['Yulin Chen', 'Haoran Li', 'Yuan Sui', 'Yufei He', 'Yue Liu', 'Yangqiu Song', 'Bryan Hooi']","Prompt injection attacks manipulate large language models (LLMs) by misleading them to deviate from the original input instructions and execute maliciously injected instructions, because of their instruction-following capabilities and inability to distinguish between the original input instructions and maliciously injected instructions. To defend against such attacks, recent studies have developed various detection mechanisms. While significant efforts have focused on detecting direct prompt injection attacks, where injected instructions are directly from the attacker who is also the user, limited attention has been given to indirect prompt injection attacks, where injected instructions are indirectly from external tools, such as a search engine. Moreover, current works mainly investigate injection detection methods and pay less attention to the post-processing method that aims to mitigate the injection after detection. In this paper, we investigate the feasibility of detecting and removing indirect prompt injection attacks, and we construct a benchmark dataset for evaluation. For detection, we assess the performance of existing LLMs and open-source detection models, and we further train detection models using our crafted training datasets. For removal, we evaluate two intuitive methods: (1) the segmentation removal method, which segments the injected document and removes parts containing injected instructions, and (2) the extraction removal method, which trains an extraction model to identify and remove injected instructions.",Indirect Prompt Injection Attacks Detected Removed Prompt injection attacks manipulate large language models LLMs misleading deviate original input instructions execute maliciously injected instructions instruction following capabilities inability distinguish original input instructions maliciously injected instructions defend attacks recent studies developed various detection mechanisms significant efforts focused detecting direct prompt injection attacks injected instructions directly attacker user limited attention given indirect prompt injection attacks injected instructions indirectly external tools search engine current works mainly investigate injection detection methods pay attention post processing method aims mitigate injection detection paper investigate feasibility detecting removing indirect prompt injection attacks construct benchmark dataset evaluation detection assess performance existing LLMs open source detection models train detection models using crafted training datasets removal evaluate intuitive methods 1 segmentation removal method segments injected document removes parts containing injected instructions 2 extraction removal method trains extraction model identify remove injected instructions
937,Identifying Open Challenges in Language Identification,['Rob van der Goot'],,Identifying Open Challenges Language Identification
938,The Distracting Effect: Understanding Irrelevant Passages in RAG,"['Chen Amiraz', 'Florin Cuconasu', 'Simone Filice', 'Zohar Karnin']","A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs. Our research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, we achieve up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. Our contribution is two-fold: first, we move beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, we develop and analyze multiple methods for finding hard distracting passages. To our knowledge, no other research has provided such a comprehensive framework for identifying and utilizing hard distracting passages.",Distracting Effect Understanding Irrelevant Passages RAG known issue Retrieval Augmented Generation RAG retrieved passages irrelevant query distract answer generating LLM causing provide incorrect response paper shed light core issue formulate distracting effect passage w r t query LLM provide quantifiable measure distracting effect passage demonstrate robustness LLMs research introduces novel methods identifying using hard distracting passages improve RAG systems fine tuning LLMs carefully selected distracting passages achieve 7 5 increase answering accuracy compared counterparts fine tuned conventional RAG datasets contribution fold simple binary classification irrelevant passages completely unrelated vs distracting second develop analyze multiple methods finding hard distracting passages knowledge research provided comprehensive framework identifying utilizing hard distracting passages
939,Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages,"['Zeli Su', 'Ziyin Zhang', 'Guixian Xu', 'Jianing Liu', 'Xu Han', 'Ting Zhang', 'Yushuang Dong']","While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.",Multilingual Encoder Knows Realize Shared Weights Pretraining Extremely Low Resource Languages multilingual language models like XLM R advanced multilingualism NLP perform poorly extremely low resource languages situation exacerbated fact modern LLMs LLaMA Qwen support far fewer languages XLM R making text generation models non existent languages world tackle challenge propose novel framework adapting multilingual encoders text generation extremely low resource languages reusing weights encoder decoder framework allows model leverage learned semantic space encoder enabling efficient learning effective generalization low resource languages Applying framework Chinese minority languages present XLM SWCM demonstrate superior performance various downstream tasks compared larger models
940,Graphically Speaking: Unmasking Abuse in Social Media with Conversation Insights,"['Célia Nouri', 'Chloé Clavel', 'Jean-Philippe Cointet']","Detecting abusive language in social media conversations poses significant challenges, as identifying abusiveness often depends on the conversational context, characterized by the content and topology of preceding comments. Traditional Abusive Language Detection (ALD) models often overlook this context, which can lead to unreliable performance metrics. Recent Natural Language Processing (NLP) methods that integrate conversational context often depend on limited and simplified representations, and report inconsistent results. In this paper, we propose a novel approach that utilize graph neural networks (GNNs) to model social media conversations as graphs, where nodes represent comments, and edges capture reply structures. We systematically investigate various graph representations and context windows to identify the optimal configuration for ALD. Our GNN model outperform both context-agnostic baselines and linear context-aware methods, achieving significant improvements in F1 scores. These findings demonstrate the critical role of structured conversational context and establish GNNs as a robust framework for advancing context-aware abusive language detection.",Graphically Speaking Unmasking Abuse Social Media Conversation Insights Detecting abusive language social media conversations poses significant challenges identifying abusiveness depends conversational context characterized content topology preceding comments Traditional Abusive Language Detection ALD models overlook context lead unreliable performance metrics Recent Natural Language Processing NLP methods integrate conversational context depend limited simplified representations report inconsistent results paper propose novel approach utilize graph neural networks GNNs model social media conversations graphs nodes represent comments edges capture reply structures systematically investigate various graph representations context windows identify optimal configuration ALD GNN model outperform context agnostic baselines linear context aware methods achieving significant improvements F1 scores findings demonstrate critical role structured conversational context establish GNNs robust framework advancing context aware abusive language detection
941,CodeTool: Enhancing Programmatic Tool Invocation of LLMs via Process Supervision,"['YifeiLu', 'Fanghua Ye', 'Jian Li', 'Qiang Gao', 'Cheng Liu', 'Haibo Luo', 'nan du', 'Xiaolong Li', 'Feiliang Ren']","Tool invocation significantly enhances the capabilities of Large Language Models (LLMs), yet challenges persist, particularly in complex task scenarios. Current methods, such as instruction-enhanced reasoning and supervised fine-tuning, often result in unnecessarily long reasoning paths and face difficulties in verifying the correctness of intermediate steps. In this paper, we propose CodeTool, a novel framework for stepwise code generation that improves LLM tool invocation by leveraging the concise and easily verifiable nature of code. CodeTool incorporates two distinct process rewards: the On-the-spot Reward, which provides immediate feedback on the accuracy of each tool invocation, and the Latent Reward, which assesses the contribution of each step toward overall task completion. By maximizing the cumulative reward of the On-the-spot and Latend Rewards at each step, LLMs are guided to follow efficient and accurate reasoning paths. Extensive experiments on StableToolBench and RestBench-TMDB demonstrate the superiority of CodeTool over existing approaches.",CodeTool Enhancing Programmatic Tool Invocation LLMs Process Supervision Tool invocation significantly enhances capabilities Large Language Models LLMs challenges persist particularly complex task scenarios Current methods instruction enhanced reasoning supervised fine tuning result unnecessarily long reasoning paths face difficulties verifying correctness intermediate steps paper propose CodeTool novel framework stepwise code generation improves LLM tool invocation leveraging concise easily verifiable nature code CodeTool incorporates distinct process rewards spot Reward provides immediate feedback accuracy tool invocation Latent Reward assesses contribution step overall task completion maximizing cumulative reward spot Latend Rewards step LLMs guided follow efficient accurate reasoning paths Extensive experiments StableToolBench RestBench TMDB demonstrate superiority CodeTool existing approaches
942,RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models,"['Hieu Tran', 'Zonghai Yao', 'Zhichao Yang', 'Junda Wang', 'Yifan Zhang', 'Feiyun Ouyang', 'Shuo Han', 'hong yu']",,RARE Retrieval Augmented Reasoning Enhancement Large Language Models
943,Defense Against Prompt Injection Attack by Leveraging Attack Techniques,"['Yulin Chen', 'Haoran Li', 'Zihao Zheng', 'Dekai Wu', 'Yangqiu Song', 'Bryan Hooi']","With the advancement of technology, large language models (LLMs) have achieved remarkable performance across various natural language processing (NLP) tasks, powering LLM-integrated applications like Microsoft Copilot. However, as LLMs continue to evolve, new vulnerabilities, especially prompt injection attacks arise. These attacks trick LLMs into deviating from the original input instructions and executing the attacker's instructions injected in data content, such as retrieved results. Recent attack methods leverage LLMs' instruction-following abilities and their inabilities to distinguish instructions injected in the data content, and achieve a high attack success rate (ASR). When comparing the attack and defense methods, we interestingly find that they share similar design goals, of inducing the model to ignore unwanted instructions and instead to execute wanted instructions. Therefore, we raise an intuitive question: Could these attack techniques be utilized for defensive purposes? In this paper, we invert the intention of prompt injection methods to develop novel defense methods based on previous training-free attack methods, by repeating the attack process but with the original input instruction rather than the injected instruction. Our comprehensive experiments demonstrate that our defense techniques outperform existing training-free defense approaches, achieving state-of-the-art results.",Defense Prompt Injection Attack Leveraging Attack Techniques advancement technology large language models LLMs achieved remarkable performance various natural language processing NLP tasks powering LLM integrated applications like Microsoft Copilot LLMs continue evolve new vulnerabilities especially prompt injection attacks arise attacks trick LLMs deviating original input instructions executing attacker s instructions injected data content retrieved results Recent attack methods leverage LLMs instruction following abilities inabilities distinguish instructions injected data content achieve high attack success rate ASR comparing attack defense methods interestingly share similar design goals inducing model ignore unwanted instructions instead execute wanted instructions raise intuitive question attack techniques utilized defensive purposes paper invert intention prompt injection methods develop novel defense methods based previous training free attack methods repeating attack process original input instruction injected instruction comprehensive experiments demonstrate defense techniques outperform existing training free defense approaches achieving state art results
944,Acquisition and Application of Novel Knowledge in Large Language Models,"['Ziyu Shang', 'Jianghan Liu', 'Zhizhao Luo', 'Peng Wang', 'Wenjun Ke', 'Jiajun Liu', 'Zijie Xu', 'Guozheng Li']",,Acquisition Application Novel Knowledge Large Language Models
945,DNCASR: End-to-End Training for Speaker-Attributed ASR,"['Xianrui Zheng', 'Chao Zhang', 'Phil Woodland']","This paper introduces DNCASR, a novel end-to-end trainable system designed for joint neural speaker clustering and automatic speech recognition (ASR), enabling speaker-attributed transcription of long multi-party meetings. DNCASR uses two separate encoders to independently encode global speaker characteristics and local waveform information, along with two linked decoders to generate speaker-attributed transcriptions. The use of linked decoders allows the entire system to be jointly trained under a unified loss function. By employing a serialised training approach, DNCASR effectively addresses overlapping speech in real-world meetings, where the link improves the prediction of speaker indices in overlapping segments. Experiments on the AMI-MDM meeting corpus demonstrate that the jointly trained DNCASR outperforms a parallel system that does not have links between the speaker and ASR decoders. Using cpWER to measure the speaker-attributed word error rate, DNCASR achieves a 9.0% relative reduction on the AMI-MDM Eval set.",DNCASR End End Training Speaker Attributed ASR paper introduces DNCASR novel end end trainable designed joint neural speaker clustering automatic speech recognition ASR enabling speaker attributed transcription long multi party meetings DNCASR uses separate encoders independently encode global speaker characteristics local waveform information linked decoders generate speaker attributed transcriptions use linked decoders allows entire jointly trained unified loss function employing serialised training approach DNCASR effectively addresses overlapping speech real world meetings link improves prediction speaker indices overlapping segments Experiments AMI MDM meeting corpus demonstrate jointly trained DNCASR outperforms parallel does links speaker ASR decoders Using cpWER measure speaker attributed word error rate DNCASR achieves 9 0 relative reduction AMI MDM Eval set
946,Exploring Persona Sentiment Sensitivity in Personalized Dialogue Generation,"['Yonghyun Jun', 'Hwanhee Lee']","Personalized dialogue systems have advanced considerably with the integration of user-specific personas into large language models (LLMs). However, while LLMs can effectively generate personalized responses, the influence of persona sentiment on dialogue quality remains underexplored. In this work, we conduct a large-scale analysis of dialogues generated using a range of polarized user profiles. Our experiments reveal that dialogues involving negatively polarized users tend to overemphasize persona attributes. In contrast, positively polarized profiles yield dialogues that selectively incorporate persona information, resulting in smoother interactions. Furthermore, we find that personas with weak or neutral sentiment generally produce lower-quality dialogues. Motivated by these findings, we propose a dialogue generation approach that explicitly accounts for persona polarity by combining a turn-based generation strategy with a profile ordering mechanism and sentiment-aware prompting. Our study provides new insights into the sensitivity of LLMs to persona sentiment and offers guidance for developing more robust and nuanced personalized dialogue systems.",Exploring Persona Sentiment Sensitivity Personalized Dialogue Generation Personalized dialogue systems advanced considerably integration user specific personas large language models LLMs LLMs effectively generate personalized responses influence persona sentiment dialogue quality remains underexplored work conduct large scale analysis dialogues generated using range polarized user profiles experiments reveal dialogues involving negatively polarized users tend overemphasize persona attributes contrast positively polarized profiles yield dialogues selectively incorporate persona information resulting smoother interactions Furthermore personas weak neutral sentiment generally produce lower quality dialogues Motivated findings propose dialogue generation approach explicitly accounts persona polarity combining turn based generation strategy profile ordering mechanism sentiment aware prompting study provides new insights sensitivity LLMs persona sentiment offers guidance developing robust nuanced personalized dialogue systems
947,AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge,"['Xiaobao Wu', 'Liangming Pan', 'Yuxi Xie', 'Ruiwen Zhou', 'Shuai Zhao', 'Yubo Ma', 'Mingzhe Du', 'Rui Mao', 'Anh Tuan Luu', 'William Yang Wang']","Data contamination hinders fair LLM evaluation by introducing test data into newer models' training sets. Existing studies solve this challenge by updating benchmarks with newly collected data. However, they fail to guarantee contamination-free evaluation as the newly collected data may contain pre-existing knowledge, and their benchmark updates rely on intensive human labor. To address these issues, we in this paper propose AntiLeak-Bench, an automated anti-leakage benchmarking framework. Instead of simply using newly collected data, we construct samples with explicitly new knowledge absent from LLMs' training sets, which thus ensures strictly contamination-free evaluation. We further design a fully automated workflow to build and update our benchmark without human labor. This significantly reduces the cost of benchmark maintenance to accommodate emerging LLMs. Through extensive experiments, we highlight that data contamination likely exists before LLMs' cutoff time and demonstrate AntiLeak-Bench effectively overcomes this challenge.",AntiLeakBench Preventing Data Contamination Automatically Constructing Benchmarks Updated Real World Knowledge Data contamination hinders fair LLM evaluation introducing test data newer models training sets Existing studies solve challenge updating benchmarks newly collected data fail guarantee contamination free evaluation newly collected data contain pre existing knowledge benchmark updates rely intensive human labor address issues paper propose AntiLeak Bench automated anti leakage benchmarking framework Instead simply using newly collected data construct samples explicitly new knowledge absent LLMs training sets ensures strictly contamination free evaluation design fully automated workflow build update benchmark human labor significantly reduces cost benchmark maintenance accommodate emerging LLMs extensive experiments highlight data contamination likely exists LLMs cutoff time demonstrate AntiLeak Bench effectively overcomes challenge
948,LLM-Guided Semantic-Aware Clustering for Topic Modeling,"['Jianghan Liu', 'Ziyu Shang', 'Wenjun Ke', 'Peng Wang', 'Zhizhao Luo', 'Jiajun Liu', 'Guozheng Li', 'Yining Li']",,LLM Guided Semantic Aware Clustering Topic Modeling
949,Hierarchical Bracketing Encodings for Dependency Parsing as Tagging,"['Ana Ezquerro', 'David Vilares', 'Anssi Yli-Jyrä', 'Carlos Gómez-Rodríguez']","We present a family of encodings for sequence labeling dependency parsing, based on the concept of hierarchical bracketing. We prove that the existing 4-bit projective encoding belongs to this family, but it is suboptimal in the number of labels used to encode a tree. We derive an optimal hierarchical bracketing, which minimizes the number of symbols used and encodes projective trees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also extend optimal hierarchical bracketing to support arbitrary non-projectivity in a more compact way than previous encodings. Our new encodings yield competitive accuracy on a diverse set of treebanks.",Hierarchical Bracketing Encodings Dependency Parsing Tagging present family encodings sequence labeling dependency parsing based concept hierarchical bracketing prove existing 4 bit projective encoding belongs family suboptimal number labels used encode tree derive optimal hierarchical bracketing minimizes number symbols used encodes projective trees using 12 distinct labels vs 16 4 bit encoding extend optimal hierarchical bracketing support arbitrary non projectivity compact way previous encodings new encodings yield competitive accuracy diverse set treebanks
950,OASIS: Order-Augmented Strategy for Improved Code Search,"['GAO Zuchen', 'Zizheng Zhan', 'Xianming LI', 'Erxin Yu', 'Haotian Zhang', 'chenbin', 'Yuqun Zhang', 'Jing Li']","Code embeddings capture the semantic representations of code and are crucial for various code-related large language model (LLM) applications, such as code search. Previous training primarily relies on optimizing the InfoNCE loss by comparing positive natural language (NL)-code pairs with in-batch negatives. However, due to the sparse nature of code contexts, training solely by comparing the major differences between positive and negative pairs may fail to capture deeper semantic nuances. To address this issue, we propose a novel order-augmented strategy for improved code search (OASIS). It leverages order-based similarity labels to train models to capture subtle differences in similarity among negative pairs. Extensive benchmark evaluations demonstrate that our OASIS model significantly outperforms previous state-of-the-art models focusing solely on major positive-negative differences. It underscores the value of exploiting subtle differences among negative pairs with order labels for effective code embedding training.",OASIS Order Augmented Strategy Improved Code Search Code embeddings capture semantic representations code crucial various code related large language model LLM applications code search Previous training primarily relies optimizing InfoNCE loss comparing positive natural language NL code pairs batch negatives sparse nature code contexts training solely comparing major differences positive negative pairs fail capture deeper semantic nuances address issue propose novel order augmented strategy improved code search OASIS leverages order based similarity labels train models capture subtle differences similarity negative pairs Extensive benchmark evaluations demonstrate OASIS model significantly outperforms previous state art models focusing solely major positive negative differences underscores value exploiting subtle differences negative pairs order labels effective code embedding training
951,Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?,"['Yancheng He', 'Shilong Li', 'Jiaheng Liu', 'Weixun Wang', 'Xingyuan Bu', 'Ge Zhang', 'Z.Y. Peng', 'Zhaoxiang Zhang', 'Zhicheng Zheng', 'Wenbo Su', 'Bo Zheng']","Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models.",Large Language Models Detect Errors Long Chain Thought Reasoning Recently o1 like models drawn significant attention models produce long Chain Thought CoT reasoning steps improve reasoning abilities existing Large Language Models LLMs paper understand qualities long CoTs measure critique abilities existing LLMs long CoTs introduce DeltaBench including generated long CoTs different o1 like models e g QwQ DeepSeek R1 different reasoning tasks e g Math Code General Reasoning measure ability detect errors long CoT reasoning Based DeltaBench perform fine grained analysis generated long CoTs discover effectiveness efficiency different o1 like models conduct extensive evaluations existing process reward models PRMs critic models detect errors annotated process aims investigate boundaries limitations existing PRMs critic models Finally hope DeltaBench guide developers better understand long CoT reasoning abilities models
952,OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference,"['Xiangyu Zhao', 'Shengyuan Ding', 'Zicheng Zhang', 'Haian Huang', 'Maosongcao', 'Jiaqi Wang', 'Weiyun Wang', 'Xinyu Fang', 'Wenhai Wang', 'Guangtao Zhai', 'Hua Yang', 'Haodong Duan', 'Kai Chen']","Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.",OmniAlign V Enhanced Alignment MLLMs Human Preference Recent advancements open source multi modal large language models MLLMs primarily focused enhancing foundational capabilities leaving significant gap human preference alignment paper introduces OmniAlign V comprehensive dataset 200K high quality training samples featuring diverse images complex questions varied response formats improve MLLMs alignment human preferences present MM AlignBench human annotated benchmark specifically designed evaluate MLLMs alignment human values Experimental results finetuning MLLMs OmniAlign V using Supervised Fine Tuning SFT Direct Preference Optimization DPO significantly enhances human preference alignment maintaining enhancing performance standard VQA benchmarks preserving fundamental capabilities datasets benchmark code checkpoints released https github com PhoenixZ810 OmniAlign V
953,Dynamic Order Template Prediction for Generative Aspect-Based Sentiment Analysis,"['Yonghyun Jun', 'Hwanhee Lee']","Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific aspects within texts, resulting in detailed sentiment tuples. Previous ABSA models often use static templates to predict all of the elements in the tuples, and these models often fail to accurately capture dependencies between elements. Multi-view prompting method improves the performance of ABSA by predicting tuples with various templates and then ensembling the results. However, this method suffers from inefficiencies and out-of-distribution errors. In this paper, we propose a Dynamic Order Template (DOT) method for ABSA, which dynamically generates necessary views for each instance based on instance-level entropy. Ensuring the diverse and relevant view generation, our proposed method improves F1-scores on ASQP and ACOS datasets while significantly reducing inference time.",Dynamic Order Template Prediction Generative Aspect Based Sentiment Analysis Aspect based sentiment analysis ABSA assesses sentiments specific aspects texts resulting detailed sentiment tuples Previous ABSA models use static templates predict elements tuples models fail accurately capture dependencies elements Multi view prompting method improves performance ABSA predicting tuples various templates ensembling results method suffers inefficiencies distribution errors paper propose Dynamic Order Template DOT method ABSA dynamically generates necessary views instance based instance level entropy Ensuring diverse relevant view generation proposed method improves F1 scores ASQP ACOS datasets significantly reducing inference time
954,Tree-KG: An Expandable KG Construction Framework for Knowledge-intensive Domains,"['Songjie Niu', 'Kaisen Yang', 'Rui Zhao', 'Yichao Liu', 'Zonglin Li', 'Hongning Wang', 'Wenguang Chen']",,Tree KG Expandable KG Construction Framework Knowledge intensive Domains
955,Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric,"['Yuming Yang', 'Yang Nan', 'Junjie Ye', 'Shihan Dou', 'Xiao Wang', 'Shuo Li', 'Huijie Lv', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang']","Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information density in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level ""novelty."" Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric. The code is available at https://github.com/UmeanNever/NovelSum.",Measuring Data Diversity Instruction Tuning Systematic Analysis Reliable Metric Data diversity crucial instruction tuning large language models Existing studies explored various diversity aware data selection methods construct high quality datasets enhance model performance fundamental problem precisely defining measuring data diversity remains underexplored limiting clear guidance data engineering address systematically analyze 11 existing diversity measurement methods evaluating correlation model performance extensive fine tuning experiments results indicate reliable diversity measure properly account inter sample differences information density sample space Building propose NovelSum new diversity metric based sample level novelty Experiments simulated real world data NovelSum accurately captures diversity variations achieves 0 97 correlation instruction tuned model performance highlighting value guiding data engineering practices NovelSum optimization objective develop greedy diversity oriented data selection strategy outperforms existing approaches validating effectiveness practical significance metric code available https github com UmeanNever NovelSum
956,Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning,"['Nan Huo', 'Jinyang Li', 'Bowen Qin', 'Ge Qu', 'Xiaolong Li', 'Xiaodong Li', 'Chenhao Ma', 'Reynold Cheng']","Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.",Micro Act Mitigate Knowledge Conflict Question Answering Actionable Self Reasoning Retrieval Augmented Generation RAG systems commonly suffer Knowledge Conflicts retrieved external knowledge contradicts inherent parametric knowledge large language models LLMs adversely affects performance downstream tasks question answering QA Existing approaches attempt mitigate conflicts directly comparing knowledge sources manner overwhelm LLMs extraneous lengthy contexts ultimately hindering ability identify mitigate inconsistencies address issue propose Micro Act framework hierarchical action space automatically perceives context complexity adaptively decomposes knowledge source sequence fine grained comparisons comparisons represented actionable steps enabling reasoning superficial context extensive experiments benchmark datasets Micro Act consistently achieves significant increase QA accuracy state art baselines 5 datasets 3 conflict types especially temporal semantic types baselines fail significantly importantly Micro Act exhibits robust performance non conflict questions simultaneously highlighting practical value real world RAG applications
957,Minimal Pair-Based Evaluation of Code-Switching,"['Igor Sterner', 'Simone Teufel']","There is a lack of an evaluation methodology that estimates the extent to which large language models (LLMs) use code-switching (CS) in the same way as bilinguals. Existing methods do not have wide language coverage, fail to account for the diverse range of CS phenomena, or do not scale. We propose an intervention based on minimal pairs of CS. Each minimal pair contains one naturally occurring CS sentence and one minimally manipulated variant. We collect up to 1,000 such pairs each for 11 language pairs. Our human experiments show that, for every language pair, bilinguals consistently prefer the naturally occurring CS sentence. Meanwhile our experiments with current LLMs show that the larger the model, the more consistently it assigns higher probability to the naturally occurring CS sentence than to the variant. In accordance with theoretical claims, the largest probability differences arise in those pairs where the manipulated material consisted of closed-class words.",Minimal Pair Based Evaluation Code Switching lack evaluation methodology estimates extent large language models LLMs use code switching CS way bilinguals Existing methods wide language coverage fail account diverse range CS phenomena scale propose intervention based minimal pairs CS minimal pair contains naturally occurring CS sentence minimally manipulated variant collect 1 000 pairs 11 language pairs human experiments language pair bilinguals consistently prefer naturally occurring CS sentence experiments current LLMs larger model consistently assigns higher probability naturally occurring CS sentence variant accordance theoretical claims largest probability differences arise pairs manipulated material consisted closed class words
958,"DNASpeech: A Contextualized and Situated Text-to-Speech Dataset with Dialogues, Narratives and Actions","['Chuanqi Cheng', 'Hongda Sun', 'Bo Du', 'Shuo Shang', 'Xinrong Hu', 'Rui Yan']",,DNASpeech Contextualized Situated Text Speech Dataset Dialogues Narratives Actions
959,LLaMA-Omni 2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis,"['Qingkai Fang', 'Yan Zhou', 'Shoutao Guo', 'Shaolei Zhang', 'Yang Feng']",,LLaMA Omni 2 LLM based Real time Spoken Chatbot Autoregressive Streaming Speech Synthesis
960,Error Comparison Optimization for Large Language Models on Aspect-Based Sentiment Analysis,"['Qianlong Wang', 'Keyang Ding', 'Hengxin Gao', 'Hui Wang', 'Ruifeng Xu']",,Error Comparison Optimization Large Language Models Aspect Based Sentiment Analysis
961,The AI Gap: How Socioeconomic Status Affects Language Technology Interactions,"['Elisa Bassignana', 'Amanda Cercas Curry', 'Dirk Hovy']","Socioeconomic status (SES) fundamentally influences how people interact with each other and more recently, with digital technologies like Large Language Models (LLMs). While previous research has highlighted the interaction between SES and language technology, it was limited by reliance on proxy metrics and synthetic data. We survey 1,000 individuals from diverse socioeconomic backgrounds about their use of language technologies and generative AI, and collect 6,482 prompts from their previous interactions with LLMs. We find systematic differences across SES groups in language technology usage (i.e., frequency, performed tasks), interaction styles, and topics. Higher SES entails a higher level of abstraction, convey requests more concisely, and topics like 'inclusivity' and 'travel'. Lower SES correlates with higher anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more concrete language. Our findings suggest that while generative language technologies are becoming more accessible to everyone, socioeconomic linguistic differences still stratify their use to exacerbate the digital divide. These differences underscore the importance of considering SES in developing language technologies to accommodate varying linguistic needs rooted in socioeconomic factors and limit the AI Gap across SES groups.",AI Gap Socioeconomic Status Affects Language Technology Interactions Socioeconomic status SES fundamentally influences people interact recently digital technologies like Large Language Models LLMs previous research highlighted interaction SES language technology limited reliance proxy metrics synthetic data survey 1 000 individuals diverse socioeconomic backgrounds use language technologies generative AI collect 6 482 prompts previous interactions LLMs systematic differences SES groups language technology usage e frequency performed tasks interaction styles topics Higher SES entails higher level abstraction convey requests concisely topics like inclusivity travel Lower SES correlates higher anthropomorphization LLMs using hello thank concrete language findings suggest generative language technologies accessible socioeconomic linguistic differences stratify use exacerbate digital divide differences underscore importance considering SES developing language technologies accommodate varying linguistic needs rooted socioeconomic factors limit AI Gap SES groups
962,Probing LLMs for Multilingual Discourse Generalization Through a Unified Label Set,"['Florian Eichin', 'Yang Janet Liu', 'Barbara Plank', 'Michael A. Hedderich']","Discourse understanding is essential for many NLP tasks, yet most existing work remains constrained by framework-dependent discourse representations. This work investigates whether large language models (LLMs) capture discourse knowledge that generalizes across languages and frameworks. We address this question along two dimensions: (1) developing a unified discourse relation label set to facilitate cross-lingual and cross-framework discourse analysis, and (2) probing LLMs to assess whether they encode generalizable discourse abstractions. Using multilingual discourse relation classification as a testbed, we examine a comprehensive set of 23 LLMs of varying sizes and multilingual capabilities. Our results show that LLMs, especially those with multilingual training corpora, can generalize discourse information across languages and frameworks. Further layer-wise analyses reveal that language generalization at the discourse level is most salient in the intermediate layers. Lastly, our error analysis provides an account of challenging relation classes.",Probing LLMs Multilingual Discourse Generalization Unified Label Set Discourse understanding essential NLP tasks existing work remains constrained framework dependent discourse representations work investigates large language models LLMs capture discourse knowledge generalizes languages frameworks address question dimensions 1 developing unified discourse relation label set facilitate cross lingual cross framework discourse analysis 2 probing LLMs assess encode generalizable discourse abstractions Using multilingual discourse relation classification testbed examine comprehensive set 23 LLMs varying sizes multilingual capabilities results LLMs especially multilingual training corpora generalize discourse information languages frameworks layer wise analyses reveal language generalization discourse level salient intermediate layers Lastly error analysis provides account challenging relation classes
963,"Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural Vision-Language Dataset for Southeast Asia","['Samuel Cahyawijaya', 'Holy Lovenia', 'Joel Ruben Antony Moniz', 'Tack Hwa Wong', 'Mohammad Rifqi Farhansyah', 'Thant Thiri Maung', 'Frederikus Hudi', 'David Anugraha', 'Muhammad Ravi Shulthan Habibi', 'Muhammad Reza Qorib', 'Amit Agarwal', 'Joseph Marvin Imperial', 'Hitesh Laxmichand Patel', 'Vicky Feliren', 'Bahrul Ilmi Nasution', 'Manuel Antonio Rufino', 'Genta Indra Winata', 'Rian Adam Rajagede', 'Carlos Rafael Catalan', 'Mohamed Fazli Mohamed Imam', 'Priyaranjan Pattnayak', 'Salsabila Zahirah Pranida', 'Kevin Pratama', 'Yeshil Bangera', 'Adisai Na-Thalang', 'Patricia Nicole Monderin', 'Yueqi Song', 'christian simon', 'Lynnette Hui Xian Ng', 'Richardy Lobo Sapan', 'Taki Hasan Rafi', 'Bin Wang', 'Supryadi', 'Kanyakorn Veerakanjana', 'Piyalitt Ittichaiwong', 'Matthew Theodore Roque', 'Karissa Vincentio', 'Takdanai Kreangphet', 'Phakphum Artkaew', 'Kadek Hendrawan Palgunadi', 'Yanzhi Yu', 'Rochana Prih Hastuti', 'William Nixon', 'Mithil Bangera', 'Adrian Xuan Wei Lim', 'Aye Hninn Khine', 'Hanif Muhammad Zhafran', 'Teddy Ferdinan', 'Audra Aurora Izzani', 'Ayushman Singh', 'Evan', 'Jauza Akbar Krito', 'Michael Anugraha', 'Fenal Ashokbhai Ilasariya', 'Haochen Li', 'John Amadeo Daniswara', 'Filbert Aurelian Tjiaranata', 'Eryawan Presma Yulianrifat', 'Can Udomcharoenchaikit', 'Fadil Risdian Ansori', 'Mahardika Krisna Ihsani', 'Giang Nguyen', 'Anab Maulana Barik', 'Dan John Velasco', 'Rifo Ahmad Genadi', 'Saptarshi Saha', 'Chengwei Wei', 'Isaiah Edri W. Flores', 'Kenneth Chen Ko Han', 'Anjela Gail D. Santos', 'Wan Shen Lim', 'Kaung Si Phyo', 'Tim Santos', 'Meisyarah Dwiastuti', 'Jiayun Luo', 'Jan Christian Blaise Cruz', 'Ming Shan Hee', 'Ikhlasul Akmal Hanif', 'M.Alif Al Hakim', 'Muhammad Rizky Sya’ban', 'Kun Kerdthaisong', 'Lester James Validad Miranda', 'Fajri Koto', 'Tirana Noor Fatyanosa', 'Alham Fikri Aji', 'Jostin Jerico Rosal', 'Jun Kevin', 'Robert Wijaya', 'Onno P. Kampman', 'Ruochen Zhang', 'Börje F. Karlsson', 'Peerat Limkonchotiwat']","Southeast Asia (SEA) is a region of extraordinary linguistic and cultural diversity, yet it remains significantly underrepresented in vision-language (VL) research. This often results in artificial intelligence (AI) models that fail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an open-source initiative dedicated to developing high-quality, culturally relevant data for SEA languages. By involving contributors from SEA countries, SEA-VL aims to ensure better cultural relevance and diversity, fostering greater inclusivity of underrepresented languages in VL research. Beyond crowdsourcing, our initiative goes one step further in the exploration of the automatic collection of culturally relevant images through crawling and image generation. First, we find that image crawling achieves approximately ~85% cultural relevance while being more cost- and time-efficient than crowdsourcing. Second, despite the substantial progress in generative vision models, synthetic images remain unreliable in accurately reflecting SEA cultures. The generated images often fail to reflect the nuanced traditions and cultural contexts of the region. Collectively, we gather 1.28M SEA culturally-relevant images, more than 50 times larger than other existing datasets. Through SEA-VL, we aim to bridge the representation gap in SEA, fostering the development of more inclusive AI systems that authentically represent diverse cultures across SEA.",Crowdsource Crawl Generate Creating SEA VL Multicultural Vision Language Dataset Southeast Asia Southeast Asia SEA region extraordinary linguistic cultural diversity remains significantly underrepresented vision language VL research results artificial intelligence AI models fail capture SEA cultural nuances gap present SEA VL open source initiative dedicated developing high quality culturally relevant data SEA languages involving contributors SEA countries SEA VL aims ensure better cultural relevance diversity fostering greater inclusivity underrepresented languages VL research crowdsourcing initiative goes step exploration automatic collection culturally relevant images crawling image generation image crawling achieves approximately 85 cultural relevance cost time efficient crowdsourcing Second despite substantial progress generative vision models synthetic images remain unreliable accurately reflecting SEA cultures generated images fail reflect nuanced traditions cultural contexts region Collectively gather 1 28M SEA culturally relevant images 50 times larger existing datasets SEA VL aim bridge representation gap SEA fostering development inclusive AI systems authentically represent diverse cultures SEA
964,Soundwave: Less is More for Speech-Text Alignment in LLMs,"['Yuhao Zhang', 'Zhiheng Liu', 'Fan Bu', 'Ruiyu Zhang', 'Benyou Wang', 'Haizhou Li']","Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation. The project is available at https://github.com/FreedomIntelligence/Soundwave.",Soundwave Speech Text Alignment LLMs Existing end end speech large language models LLMs usually rely large scale annotated data training data efficient training discussed depth focus fundamental problems speech text representation space gap sequence length inconsistency propose Soundwave utilizes efficient training strategy novel architecture address issues Results Soundwave outperforms advanced Qwen2 Audio speech translation AIR Bench speech tasks using fiftieth training data analysis shows Soundwave retains intelligence conversation project available https github com FreedomIntelligence Soundwave
965,RoToR: Towards More Reliable Responses for Order-Invariant Inputs,"['Soyoung Yoon', 'Dongha Ahn', 'Youngwon Lee', 'Minkyu Jung', 'HyungJoo Jang', 'seung-won hwang']","Mitigating positional bias of language models (LMs) for listwise inputs is a well-known and important problem (e.g., lost-in-the-middle). While zero-shot order-invariant LMs have been proposed to solve this issue, their success on practical listwise problems has been limited. In this work, as a first contribution, we identify and overcome two limitations to make zero-shot invariant LMs more practical: (1) training and inference distribution mismatch arising from modifying positional ID assignments to enforce invariance, and (2) failure to adapt to mixture of order-invariant and sensitive inputs in practical listwise problems. Then, to overcome these issues we propose (1) RoToR, a zero-shot invariant LM for genuinely order-invariant inputs with minimal modifications of positional IDs, and (2) Selective Routing, an adaptive framework that handles both order-invariant and order-sensitive inputs in listwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA), and MMLU benchmarks, we show that RoToR with Selective Routing can effectively handle practical listwise input tasks in a zero-shot manner (https://github.com/soyoung97/RoToR)",RoToR Reliable Responses Order Invariant Inputs Mitigating positional bias language models LMs listwise inputs known important problem e g lost middle zero shot order invariant LMs proposed solve issue success practical listwise problems limited work contribution identify overcome limitations make zero shot invariant LMs practical 1 training inference distribution mismatch arising modifying positional ID assignments enforce invariance 2 failure adapt mixture order invariant sensitive inputs practical listwise problems overcome issues propose 1 RoToR zero shot invariant LM genuinely order invariant inputs minimal modifications positional IDs 2 Selective Routing adaptive framework handles order invariant order sensitive inputs listwise tasks Lost middle LitM Knowledge Graph QA KGQA MMLU benchmarks RoToR Selective Routing effectively handle practical listwise input tasks zero shot manner https github com soyoung97 RoToR
966,Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation,"['Shivalika Singh', 'Angelika Romanou', 'Clémentine Fourrier', 'Jian Gang Ngui', 'David Ifeoluwa Adelani', 'Daniel Vila-Suero', 'Peerat Limkonchotiwat', 'Kelly Marchisio', 'Wei Qi Leong', 'Yosephine Susanto', 'Raymond Ng', 'Shayne Longpre', 'Sebastian Ruder', 'Wei-Yin Ko', 'Antoine Bosselut', 'Alice Oh', 'Andre Martins', 'Daphne Ippolito', 'Enzo Ferrante', 'Leshem Choshen', 'Marzieh Fadaee', 'Beyza Ermis', 'Sara Hooker']","Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.",Global MMLU Understanding Addressing Cultural Linguistic Biases Multilingual Evaluation Cultural biases multilingual datasets pose significant challenges effectiveness global benchmarks biases stem differences language cultural knowledge required interpret questions reducing practical utility translated datasets like MMLU Furthermore translation introduces artefacts distort meaning clarity questions target language common practice multilingual evaluation rely machine translated evaluation sets simply translating dataset insufficient address challenges work trace impact issues multilingual evaluations ensuing model performances large scale evaluation state art open proprietary models illustrates progress MMLU depends heavily learning Western centric concepts 28 questions requiring culturally sensitive knowledge questions requiring geographic knowledge astounding 84 9 focus North American European regions Rankings model evaluations change depending evaluated portion subset questions annotated culturally sensitive showing distortion model rankings blindly relying translated MMLU release Global MMLU improved MMLU evaluation coverage 42 languages improved overall quality engaging compensated professional community annotators verify translation quality rigorously evaluating cultural biases present original dataset comprehensive Global MMLU set includes designated subsets labeled culturally sensitive culturally agnostic allow holistic complete evaluation
967,Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification,"['Yaxin Fan', 'Peifeng Li', 'Qiaoming Zhu']","Dialogue discourse parsing aims to identify and analyze discourse relations between the utterances within dialogues. However, linguistic features in dialogues, such as omission and idiom, frequently introduce ambiguities that obscure the intended discourse relations, posing significant challenges for parsers. To address this issue, we propose a Discourse-aware Clarification Module (DCM) to enhance the performance of the dialogue discourse parser. DCM employs two distinct reasoning processes: clarification type reasoning and discourse goal reasoning. The former analyzes linguistic features, while the latter distinguishes the intended relation from the ambiguous one. Furthermore, we introduce Contribution-aware Preference Optimization (CPO) to mitigate the risk of erroneous clarifications, thereby reducing cascading errors. CPO enables the parser to assess the contributions of the clarifications from DCM and provide feedback to optimize the DCM, enhancing its adaptability and alignment with the parser's requirements. Extensive experiments on the STAC and Molweni datasets demonstrate that our approach effectively resolves ambiguities and significantly outperforms the state-of-the-art (SOTA) baselines.",Improving Dialogue Discourse Parsing Discourse aware Utterance Clarification Dialogue discourse parsing aims identify analyze discourse relations utterances dialogues linguistic features dialogues omission idiom frequently introduce ambiguities obscure intended discourse relations posing significant challenges parsers address issue propose Discourse aware Clarification Module DCM enhance performance dialogue discourse parser DCM employs distinct reasoning processes clarification type reasoning discourse goal reasoning analyzes linguistic features distinguishes intended relation ambiguous Furthermore introduce Contribution aware Preference Optimization CPO mitigate risk erroneous clarifications reducing cascading errors CPO enables parser assess contributions clarifications DCM provide feedback optimize DCM enhancing adaptability alignment parser s requirements Extensive experiments STAC Molweni datasets demonstrate approach effectively resolves ambiguities significantly outperforms state art SOTA baselines
968,ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs,"['Yan Yang', 'Yixia Li', 'Hongru WANG', 'Xuetao Wei', 'James Jianqiao Yu', 'Yun Chen', 'Guanhua Chen']","With the proliferation of task-specific large language models, delta compression has emerged as a method to mitigate the resource challenges of deploying numerous such models by effectively compressing the delta model parameters. Previous delta-sparsification methods either remove parameters randomly or truncate singular vectors directly after singular value decomposition (SVD). However, these methods either disregard parameter importance entirely or evaluate it with too coarse a granularity. In this work, we introduce ImPart, a novel importance-aware delta sparsification approach. Leveraging SVD, it dynamically adjusts sparsity ratios of different singular vectors based on their importance, effectively retaining crucial task-specific knowledge even at high sparsity ratios. Experiments show that ImPart achieves state-of-the-art delta sparsification performance, demonstrating $2\times$ higher compression ratio than baselines at the same performance level. When integrated with existing methods, ImPart sets a new state-of-the-art on delta quantization and model merging.",ImPart Importance Aware Delta Sparsification Improved Model Compression Merging LLMs proliferation task specific large language models delta compression emerged method mitigate resource challenges deploying numerous models effectively compressing delta model parameters Previous delta sparsification methods remove parameters randomly truncate singular vectors directly singular value decomposition SVD methods disregard parameter importance entirely evaluate coarse granularity work introduce ImPart novel importance aware delta sparsification approach Leveraging SVD dynamically adjusts sparsity ratios different singular vectors based importance effectively retaining crucial task specific knowledge high sparsity ratios Experiments ImPart achieves state art delta sparsification performance demonstrating 2 times higher compression ratio baselines performance level integrated existing methods ImPart sets new state art delta quantization model merging
969,Words of Warmth: Trust and Sociability Norms for over 26k English Words,['Saif M. Mohammad'],"Social psychologists have shown that Warmth (W) and Competence (C) are the primary dimensions along which we assess other people and groups. These dimensions impact various aspects of our lives from social competence and emotion regulation to success in the work place and how we view the world. More recent work has started to explore how these dimensions develop, why they have developed, and what they constitute. Of particular note, is the finding that warmth has two distinct components: Trust (T) and Sociability (S). In this work, we introduce Words of Warmth, the first large-scale repository of manually derived word--warmth (as well as word--trust and word--sociability) associations for over 26k English words. We show that the associations are highly reliable. We use the lexicons to study the rate at which children acquire WCTS words with age. Finally, we show that the lexicon enables a wide variety of bias and stereotype research through case studies on various target entities. Words of Warmth is freely available at: http://saifmohammad.com/warmth.html",Words Warmth Trust Sociability Norms 26k English Words Social psychologists shown Warmth W Competence C primary dimensions assess people groups dimensions impact various aspects lives social competence emotion regulation success work place view world recent work started explore dimensions develop developed constitute particular note finding warmth distinct components Trust T Sociability S work introduce Words Warmth large scale repository manually derived word warmth word trust word sociability associations 26k English words associations highly reliable use lexicons study rate children acquire WCTS words age Finally lexicon enables wide variety bias stereotype research case studies various target entities Words Warmth freely available http saifmohammad com warmth html
970,BehaviorBox: Automated Behavioral Comparison of Language Models,"['Lindia Tjuatja', 'Graham Neubig']",,BehaviorBox Automated Behavioral Comparison Language Models
971,HAF-RM: A Hybrid Alignment Framework for Reward Model Training,"['Shujun Liu', 'Xiaoyu Shen', 'Yuhang Lai', 'Siyuan Wang', 'ShengbinYue', 'Zengfeng Huang', 'Xuanjing Huang', 'zhongyu wei']","The reward model has become increasingly important in alignment, assessment, and data construction for large language models (LLMs). Most existing researchers focus on enhancing reward models through data improvements, following the conventional training framework for reward models that directly optimizes the predicted rewards. In this paper, we propose a hybrid alignment framework HaF-RM for reward model training by introducing an additional constraint on token-level policy probabilities in addition to the reward score. It can simultaneously supervise the internal preference model at the token level and optimize the mapping layer of the reward model at the sequence level. Experiment results on five datasets sufficiently show the validity and effectiveness of our proposed hybrid framework for training a high-quality reward model. By decoupling the reward modeling procedure and incorporating hybrid supervision, our HaF-RM framework offers a principled and effective approach to enhancing the performance and alignment of reward models, a critical component in the responsible development of powerful language models. We release our code at https://haf-rm.github.io.",HAF RM Hybrid Alignment Framework Reward Model Training reward model increasingly important alignment assessment data construction large language models LLMs existing researchers focus enhancing reward models data improvements following conventional training framework reward models directly optimizes predicted rewards paper propose hybrid alignment framework HaF RM reward model training introducing additional constraint token level policy probabilities addition reward score simultaneously supervise internal preference model token level optimize mapping layer reward model sequence level Experiment results datasets sufficiently validity effectiveness proposed hybrid framework training high quality reward model decoupling reward modeling procedure incorporating hybrid supervision HaF RM framework offers principled effective approach enhancing performance alignment reward models critical component responsible development powerful language models release code https haf rm github io
972,CULEMO: Cultural Lenses on Emotion - Benchmarking LLMs for Cross-Cultural Emotion Understanding,"['Tadesse Destaw Belay', 'Ahmed Haj Ahmed', 'Alvin C Grissom II', 'Iqra Ameer', 'Grigori Sidorov', 'Olga Kolesnikova', 'Seid Muhie Yimam']",,CULEMO Cultural Lenses Emotion Benchmarking LLMs Cross Cultural Emotion Understanding
973,DiffPO: Diffusion-styled Preference Optimization for Inference Time Alignment of Large Language Models,"['Ruizhe Chen', 'Wenhao Chai', 'Zhifei Yang', 'Xiaotian Zhang', 'Ziyang Wang', 'Tony Quek', 'Joey Tianyi Zhou', 'Soujanya Poria', 'Zuozhu Liu']",,DiffPO Diffusion styled Preference Optimization Inference Time Alignment Large Language Models
974,MemeQA: Holistic Evaluation of Meme Understanding,"['Khoi P. N. Nguyen', 'Terrence Li', 'Derek Lou Zhou', 'Gabriel Xiong', 'Pranav Balu', 'Nandhan Alahari', 'Alan Huang', 'Tanush Chauhan', 'Harshavardhan Bala', 'Emre Guzelordu', 'Affan Kashfi', 'Aaron Xu', 'Suyesh Shrestha', 'Megan Vu', 'Jerry Wang', 'Vincent Ng']",,MemeQA Holistic Evaluation Meme Understanding
975,LoGU: Long-form Generation with Uncertainty Expressions,"['Ruihan Yang', 'Caiqi Zhang', 'Zhisong Zhang', 'Xinting Huang', 'Sen Yang', 'Nigel Collier', 'Dong Yu', 'Deqing Yang']","While Large Language Models (LLMs) demonstrate impressive capabilities, they still struggle with generating factually incorrect content (i.e., hallucinations). A promising approach to mitigate this issue is enabling models to express uncertainty when unsure. Previous research on uncertainty modeling has primarily focused on short-form QA, but realworld applications often require much longer responses. In this work, we introduce the task of Long-form Generation with Uncertainty(LoGU). We identify two key challenges: Uncertainty Suppression, where models hesitate to express uncertainty, and Uncertainty Misalignment, where models convey uncertainty inaccurately. To tackle these challenges, we propose a refinement-based data collection framework and a two-stage training pipeline. Our framework adopts a divide-and-conquer strategy, refining uncertainty based on atomic claims. The collected data are then used in training through supervised fine-tuning (SFT) and direct preference optimization (DPO) to enhance uncertainty expression. Extensive experiments on three long-form instruction following datasets show that our method significantly improves accuracy, reduces hallucinations, and maintains the comprehensiveness of responses.",LoGU Long form Generation Uncertainty Expressions Large Language Models LLMs demonstrate impressive capabilities struggle generating factually incorrect content e hallucinations promising approach mitigate issue enabling models express uncertainty unsure Previous research uncertainty modeling primarily focused short form QA realworld applications require longer responses work introduce task Long form Generation Uncertainty LoGU identify key challenges Uncertainty Suppression models hesitate express uncertainty Uncertainty Misalignment models convey uncertainty inaccurately tackle challenges propose refinement based data collection framework stage training pipeline framework adopts divide conquer strategy refining uncertainty based atomic claims collected data used training supervised fine tuning SFT direct preference optimization DPO enhance uncertainty expression Extensive experiments long form instruction following datasets method significantly improves accuracy reduces hallucinations maintains comprehensiveness responses
976,KiRAG: Knowledge-Driven Iterative Retriever for Enhancing Retrieval-Augmented Generation,"['Jinyuan Fang', 'Zaiqiao Meng', 'Craig MacDonald']","Iterative retrieval-augmented generation (iRAG) models offer an effective approach for multi-hop question answering (QA). However, their retrieval process faces two key challenges: (1) it can be disrupted by irrelevant documents or factually inaccurate chain-of-thoughts; (2) their retrievers are not designed to dynamically adapt to the evolving information needs in multi-step reasoning, making it difficult to identify and retrieve the missing information required at each iterative step. Therefore, we propose KiRAG, which uses a knowledge-driven iterative retriever model to enhance the retrieval process of iRAG. Specifically, KiRAG decomposes documents into knowledge triples and performs iterative retrieval with these triples to enable a factually reliable retrieval process. Moreover, KiRAG integrates reasoning into the retrieval process to dynamically identify and retrieve knowledge that bridges information gaps, effectively adapting to the evolving information needs. Empirical results show that KiRAG significantly outperforms existing iRAG models, with an average improvement of 9.40% in R@3 and 5.14% in F1 on multi-hop QA.",KiRAG Knowledge Driven Iterative Retriever Enhancing Retrieval Augmented Generation Iterative retrieval augmented generation iRAG models offer effective approach multi hop question answering QA retrieval process faces key challenges 1 disrupted irrelevant documents factually inaccurate chain thoughts 2 retrievers designed dynamically adapt evolving information needs multi step reasoning making difficult identify retrieve missing information required iterative step propose KiRAG uses knowledge driven iterative retriever model enhance retrieval process iRAG Specifically KiRAG decomposes documents knowledge triples performs iterative retrieval triples enable factually reliable retrieval process KiRAG integrates reasoning retrieval process dynamically identify retrieve knowledge bridges information gaps effectively adapting evolving information needs Empirical results KiRAG significantly outperforms existing iRAG models average improvement 9 40 R 3 5 14 F1 multi hop QA
977,Enhancing Lexicon-Based Text Embeddings with Large Language Models,"['Yibin Lei', 'Tao Shen', 'Yu Cao', 'Andrew Yates']","Recent large language models (LLMs) have demonstrated exceptional performance on general-purpose text embedding tasks. While dense embeddings have dominated related research, we introduce the first Lexicon-based EmbeddiNgS (LENS) leveraging LLMs that achieve competitive performance on these tasks. Regarding the inherent tokenization redundancy issue and unidirectional attention limitations in traditional causal LLMs, LENS consolidates the vocabulary space through token embedding clustering, and investigates bidirectional attention and various pooling strategies. Specifically, LENS simplifies lexicon matching by assigning each dimension to a specific token cluster, where semantically similar tokens are grouped together, and unlocking the full potential of LLMs through bidirectional attention. Extensive experiments demonstrate that LENS outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB), delivering compact feature representations that match the sizes of dense counterparts. Notably, combining LENSE with dense embeddings achieves state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).",Enhancing Lexicon Based Text Embeddings Large Language Models Recent large language models LLMs demonstrated exceptional performance general purpose text embedding tasks dense embeddings dominated related research introduce Lexicon based EmbeddiNgS LENS leveraging LLMs achieve competitive performance tasks Regarding inherent tokenization redundancy issue unidirectional attention limitations traditional causal LLMs LENS consolidates vocabulary space token embedding clustering investigates bidirectional attention various pooling strategies Specifically LENS simplifies lexicon matching assigning dimension specific token cluster semantically similar tokens grouped unlocking potential LLMs bidirectional attention Extensive experiments demonstrate LENS outperforms dense embeddings Massive Text Embedding Benchmark MTEB delivering compact feature representations match sizes dense counterparts Notably combining LENSE dense embeddings achieves state art performance retrieval subset MTEB e BEIR
978,CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation,"['Santosh T.Y.S.S', 'Youssef Tarek Elkhayat', 'Oana Ichim', 'Pranav Shetty', 'Dongsheng Wang', 'Zhiqiang Ma', 'Armineh Nourbakhsh', 'Xiaomo Liu']",,CoCoLex Confidence guided Copy based Decoding Grounded Legal Text Generation
979,Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization,"['Itai Mondshine', 'Tzuf Paz-Argaman', 'Reut Tsarfaty']",,N Grams Rethinking Evaluation Metrics Strategies Multilingual Abstractive Summarization
980,CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning,"['Yangfan Ye', 'Xiaocheng Feng', 'Zekun Yuan', 'Xiachong Feng', 'Libo Qin', 'Lei Huang', 'Weitao Ma', 'Yichong Huang', 'Zhirui Zhang', 'Yunfei Lu', 'Xiaohui Yan', 'Duyu Tang', 'Dandan Tu', 'Bing Qin']","Current large language models (LLMs) often exhibit imbalanced multilingual capabilities due to their English-centric training corpora. To address this, existing fine-tuning approaches operating at the data-level (e.g., through data augmentation or distillation) typically introduce implicit cross-lingual alignment, overlooking the potential for more profound, latent-level cross-lingual interactions. In this work, we propose CC-Tuning, a novel multilingual fine-tuning paradigm that explicitly establishes a cross-lingual connection mechanism at the latent level. During training, CC-Tuning fuses the feed forward activations from both English and non-English inputs, enabling the model to benefit from both linguistic resources. This process is facilitated with a trainable Decision Maker that identifies beneficial activations. Furthermore, during inference, a Transform Matrix is utilized to simulate the cross-lingual connection under monolingual setting through representation transformation. Our experiments on six benchmarks covering 22 languages show that CC-Tuning outperforms vanilla SFT and offers a strong latent-level alternative to data-level augmentation methods. Further analysis also highlights the practicality of CC-Tuning and the potential of latent-level cross-lingual interactions in advancing the multilingual performance of LLMs.",CC Tuning Cross Lingual Connection Mechanism Improving Joint Multilingual Supervised Fine Tuning Current large language models LLMs exhibit imbalanced multilingual capabilities English centric training corpora address existing fine tuning approaches operating data level e g data augmentation distillation typically introduce implicit cross lingual alignment overlooking potential profound latent level cross lingual interactions work propose CC Tuning novel multilingual fine tuning paradigm explicitly establishes cross lingual connection mechanism latent level training CC Tuning fuses feed forward activations English non English inputs enabling model benefit linguistic resources process facilitated trainable Decision Maker identifies beneficial activations Furthermore inference Transform Matrix utilized simulate cross lingual connection monolingual setting representation transformation experiments benchmarks covering 22 languages CC Tuning outperforms vanilla SFT offers strong latent level alternative data level augmentation methods analysis highlights practicality CC Tuning potential latent level cross lingual interactions advancing multilingual performance LLMs
981,SConU: Selective Conformal Uncertainty in Large Language Models,"['Zhiyuan Wang', 'Qingni Wang', 'Yue Zhang', 'Tianlong Chen', 'Xiaofeng Zhu', 'Xiaoshuang Shi', 'Kaidi Xu']","As large language models are increasingly utilized in real-world applications, guarantees of task-specific metrics are essential for their reliable deployment. Previous studies have introduced various criteria of conformal uncertainty grounded in split conformal prediction, which offer user-specified correctness coverage. However, existing frameworks often fail to identify uncertainty data outliers that violate the exchangeability assumption, leading to unbounded miscoverage rates and unactionable prediction sets. In this paper, we propose a novel approach termed Selective Conformal Uncertainty (SConU), which, for the first time, implements significance tests, by developing two conformal p-values that are instrumental in determining whether a given sample deviates from the uncertainty distribution of the calibration set at a specific manageable risk level. Our approach not only facilitates rigorous management of miscoverage rates across both single-domain and interdisciplinary contexts, but also enhances the efficiency of predictions. Furthermore, we comprehensively analyze the components of the conformal procedures, aiming to approximate conditional coverage, particularly in high-stakes question-answering tasks.",SConU Selective Conformal Uncertainty Large Language Models large language models increasingly utilized real world applications guarantees task specific metrics essential reliable deployment Previous studies introduced various criteria conformal uncertainty grounded split conformal prediction offer user specified correctness coverage existing frameworks fail identify uncertainty data outliers violate exchangeability assumption leading unbounded miscoverage rates unactionable prediction sets paper propose novel approach termed Selective Conformal Uncertainty SConU time implements significance tests developing conformal p values instrumental determining given sample deviates uncertainty distribution calibration set specific manageable risk level approach facilitates rigorous management miscoverage rates single domain interdisciplinary contexts enhances efficiency predictions Furthermore comprehensively analyze components conformal procedures aiming approximate conditional coverage particularly high stakes question answering tasks
982,MegaPairs: Massive Data Synthesis for Universal Multimodal Retrieval,"['Junjie Zhou', 'yongping xiong', 'Zheng Liu', 'Ze Liu', 'Shitao Xiao', 'Yueze Wang', 'Bo Zhao', 'Chen Jason Zhang', 'Defu Lian']",,MegaPairs Massive Data Synthesis Universal Multimodal Retrieval
983,When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs,"['Xinyue Shen', 'Yun Shen', 'Michael Backes', 'Yang Zhang']","Knowledge files have been widely used in large language model (LLM) agents, such as GPTs, to improve response quality. However, concerns about the potential leakage of knowledge files have grown significantly. Existing studies demonstrate that adversarial prompts can induce GPTs to leak knowledge file content. Yet, it remains uncertain whether additional leakage vectors exist, particularly given the complex data flows across clients, servers, and databases in GPTs. In this paper, we present a comprehensive risk assessment of knowledge file leakage, leveraging a novel workflow inspired by Data Security Posture Management (DSPM). Through the analysis of 651,022 GPT metadata, 11,820 flows, and 1,466 responses, we identify five leakage vectors: metadata, GPT initialization, retrieval, sandboxed execution environments, and prompts. These vectors enable adversaries to extract sensitive knowledge file data such as titles, content, types, and sizes. Notably, the activation of the built-in tool Code Interpreter leads to a privilege escalation vulnerability, enabling adversaries to directly download original knowledge files with a 95.95% success rate. Further analysis reveals that 28.80% of leaked files are copyrighted, including digital copies from major publishers and internal materials from a listed company. In the end, we provide actionable solutions for GPT builders and platform providers to secure the GPT data supply chain.",GPT Spills Tea Comprehensive Assessment Knowledge File Leakage GPTs Knowledge files widely used large language model LLM agents GPTs improve response quality concerns potential leakage knowledge files grown significantly Existing studies demonstrate adversarial prompts induce GPTs leak knowledge file content remains uncertain additional leakage vectors exist particularly given complex data flows clients servers databases GPTs paper present comprehensive risk assessment knowledge file leakage leveraging novel workflow inspired Data Security Posture Management DSPM analysis 651 022 GPT metadata 11 820 flows 1 466 responses identify leakage vectors metadata GPT initialization retrieval sandboxed execution environments prompts vectors enable adversaries extract sensitive knowledge file data titles content types sizes Notably activation built tool Code Interpreter leads privilege escalation vulnerability enabling adversaries directly download original knowledge files 95 95 success rate analysis reveals 28 80 leaked files copyrighted including digital copies major publishers internal materials listed company end provide actionable solutions GPT builders platform providers secure GPT data supply chain
984,UniCodec: Unified Audio Codec with Single Domain-Adaptive Codebook,"['Yidi Jiang', 'Qian Chen', 'Shengpeng Ji', 'Yu Xi', 'Wen Wang', 'Chong Zhang', 'Xianghu Yue', 'ShiLiang Zhang', 'Haizhou Li']","The emergence of audio language models is empowered by neural audio codecs, which establish critical mappings between continuous waveforms and discrete tokens compatible with language model paradigms. The evolutionary trends from multi-layer residual vector quantizer to single-layer quantizer are beneficial for language-autoregressive decoding. However, the capability to handle multi-domain audio signals through a single codebook remains constrained by inter-domain distribution discrepancies. In this work, we introduce UniCodec, a unified audio codec with a single codebook to support multi-domain audio data, including speech, music, and sound. To achieve this, we propose a partitioned domain-adaptive codebook method and domain Mixture-of-Experts strategy to capture the distinct characteristics of each audio domain. Furthermore, to enrich the semantic density of the codec without auxiliary modules, we propose a self-supervised mask prediction modeling approach. Comprehensive objective and subjective evaluations demonstrate that UniCodec achieves excellent audio reconstruction performance across the three audio domains, outperforming existing unified neural codecs with a single codebook, and even surpasses state-of-the-art domain-specific codecs on both acoustic and semantic representation capabilities.",UniCodec Unified Audio Codec Single Domain Adaptive Codebook emergence audio language models empowered neural audio codecs establish critical mappings continuous waveforms discrete tokens compatible language model paradigms evolutionary trends multi layer residual vector quantizer single layer quantizer beneficial language autoregressive decoding capability handle multi domain audio signals single codebook remains constrained inter domain distribution discrepancies work introduce UniCodec unified audio codec single codebook support multi domain audio data including speech music sound achieve propose partitioned domain adaptive codebook method domain Mixture Experts strategy capture distinct characteristics audio domain Furthermore enrich semantic density codec auxiliary modules propose self supervised mask prediction modeling approach Comprehensive objective subjective evaluations demonstrate UniCodec achieves excellent audio reconstruction performance audio domains outperforming existing unified neural codecs single codebook surpasses state art domain specific codecs acoustic semantic representation capabilities
985,KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models,"['Fnu Mohbat', 'Mohammed J Zaki']","Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at https://github.com/mohbattharani/KERL.",KERL Knowledge Enhanced Personalized Recipe Recommendation using Large Language Models Recent advances large language models LLMs abundance food data resulted studies improve food understanding using LLMs Despite recommendation systems utilizing LLMs Knowledge Graphs KGs limited research integrating food related KGs LLMs introduce KERL unified leverages food KGs LLMs provide personalized food recommendations generates recipes associated micro nutritional information Given natural language question KERL extracts entities retrieves subgraphs KG fed LLM context select recipes satisfy constraints generates cooking steps nutritional information recipe evaluate approach develop benchmark dataset curating recipe related questions combined constraints personal preferences extensive experiments proposed KG augmented LLM significantly outperforms existing approaches offering complete coherent solution food recommendation recipe generation nutritional analysis code benchmark datasets publicly available https github com mohbattharani KERL
986,Multilingual Arbitration: Optimizing Data Pools to Accelerate Multilingual Progress,"['Ayomide Odumakinde', 'Daniel D’souza', 'Pat Verga', 'Beyza Ermis', 'Sara Hooker']",,Multilingual Arbitration Optimizing Data Pools Accelerate Multilingual Progress
987,Controlled Low-Rank Adaptation with Subspace Regularization for Continued Training on Large Language Models,"['Yuheng Lu', 'Bingshuo Qian', 'Caixia Yuan', 'Huixing Jiang', 'Xiaojie Wang']","Large language models (LLMs) exhibit remarkable capabilities in natural language processing but face catastrophic forgetting when learning new tasks, where adaptation to a new domain leads to a substantial decline in performance on previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a sub-space regularization method on LoRA structure. Aiming to reduce the scale of output change while introduce minimal constraint on model capacity, CLoRA imposes constraint on the direction of updating matrix's null space. Experimental results on one-stage LLM finetuning tasks and continual learning settings highlight the superority of CLoRA as a effective parameter efficient finetuning method with catastrophic forgetting mitigating.Further investigation for model parameters indicates that CLoRA effectively balances the trade-off between model capacity and degree of forgetting.",Controlled Low Rank Adaptation Subspace Regularization Continued Training Large Language Models Large language models LLMs exhibit remarkable capabilities natural language processing face catastrophic forgetting learning new tasks adaptation new domain leads substantial decline performance previous tasks paper propose Controlled LoRA CLoRA sub space regularization method LoRA structure Aiming reduce scale output change introduce minimal constraint model capacity CLoRA imposes constraint direction updating matrix s null space Experimental results stage LLM finetuning tasks continual learning settings highlight superority CLoRA effective parameter efficient finetuning method catastrophic forgetting mitigating investigation model parameters indicates CLoRA effectively balances trade model capacity degree forgetting
988,Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models,"['Yancheng He', 'Shilong Li', 'Jiaheng Liu', 'Yingshui Tan', 'Weixun Wang', 'Hui Huang', 'Xingyuan Bu', 'Hangyu Guo', 'Chengwei Hu', 'Boren Zheng', 'Zhuoran Lin', 'Dekai Sun', 'Zhicheng Zheng', 'Wenbo Su', 'Bo Zheng']","New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.",Chinese SimpleQA Chinese Factuality Evaluation Large Language Models New LLM evaluation benchmarks important align rapid development Large Language Models LLMs work present Chinese SimpleQA comprehensive Chinese benchmark evaluate factuality ability language models answer short questions Chinese SimpleQA mainly properties e Chinese Diverse High quality Static Easy evaluate Specifically focus Chinese language 6 major topics 99 diverse subtopics Second conduct comprehensive quality control process achieve high quality questions answers reference answers static changed time following SimpleQA questions answers short grading process easy evaluate based OpenAI API Based Chinese SimpleQA perform comprehensive evaluation factuality abilities existing LLMs Finally hope Chinese SimpleQA guide developers better understand Chinese factuality abilities models facilitate growth foundation models
989,"PVP: An Image Dataset for Personalized Visual Persuasion with Persuasiveness Ratings, Persuasion Strategies, and Viewer Characteristic","['Junseo Kim', 'Jongwook Han', 'Dongmin Choi', 'Jongwook Yoon', 'Eun-Ju Lee', 'Yohan Jo']",,PVP Image Dataset Personalized Visual Persuasion Persuasiveness Ratings Persuasion Strategies Viewer Characteristic
990,Any Information Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval,"['Zheng Liu', 'Ze Liu', 'Zhengyang Liang', 'Junjie Zhou', 'Shitao Xiao', 'Chao Gao', 'Chen Jason Zhang', 'Defu Lian']","With the popularity of multimodal techniques, it receives growing interests to acquire useful information in visual forms. In this work, we formally define an emerging IR paradigm called \textit{Visualized Information Retrieval}, or \textbf{Vis-IR}, where multimodal information, such as texts, images, tables and charts, is jointly represented by a unified visual format called \textbf{Screenshots}, for various retrieval applications. We further make three key contributions for Vis-IR. First, we create \textbf{VIRA} (Vis-IR Aggregation), a large-scale dataset comprising a vast collection of screenshots from diverse sources, carefully curated into captioned and question-answer formats. Second, we develop \textbf{UniSE} (Universal Screenshot Embeddings), a family of retrieval models that enable screenshots to query or be queried across arbitrary data modalities. Finally, we construct \textbf{MVRB} (Massive Visualized IR Benchmark), a comprehensive benchmark covering a variety of task forms and application scenarios. Through extensive evaluations on MVRB, we highlight the deficiency from existing multimodal retrievers and the substantial improvements made by UniSE. Our work will be shared with the community, laying a solid foundation for this emerging field.",Information Just Worth Single Screenshot Unifying Search Visualized Information Retrieval popularity multimodal techniques receives growing interests acquire useful information visual forms work formally define emerging IR paradigm called textit Visualized Information Retrieval textbf Vis IR multimodal information texts images tables charts jointly represented unified visual format called textbf Screenshots various retrieval applications make key contributions Vis IR create textbf VIRA Vis IR Aggregation large scale dataset comprising vast collection screenshots diverse sources carefully curated captioned question answer formats Second develop textbf UniSE Universal Screenshot Embeddings family retrieval models enable screenshots query queried arbitrary data modalities Finally construct textbf MVRB Massive Visualized IR Benchmark comprehensive benchmark covering variety task forms application scenarios extensive evaluations MVRB highlight deficiency existing multimodal retrievers substantial improvements UniSE work shared community laying solid foundation emerging field
991,Tunable LLM-based Proactive Recommendation Agent,"['Mingze Wang', 'Chongming Gao', 'Wenjie Wang', 'Yangyang Li', 'Fuli Feng']",,Tunable LLM based Proactive Recommendation Agent
992,AgentRM: Enhancing Agent Generalization with Reward Modeling,"['Yu Xia', 'Jingru Fan', 'Weize Chen', 'Siyu Yan', 'Xin Cong', 'Zhong Zhang', 'Yaxi Lu', 'Yankai Lin', 'Zhiyuan Liu', 'Maosong Sun']","Existing LLM-based agents have achieved strong performance on held-in tasks, but their generalizability to unseen tasks remains poor. Hence, some recent work focus on fine-tuning the policy model with more diverse tasks to improve the generalizability. In this work, we find that finetuning a reward model to guide the policy model is more robust than directly finetuning the policy model. Based on this finding, we propose AgentRM, a generalizable reward model, to guide the policy model for effective test-time search. We comprehensively investigate three approaches to construct the reward model, including explicit reward modeling, implicit reward modeling and LLM-as-a-judge. We then use AgentRM to guide the answer generation with Best-of-N sampling and step-level beam search. On four types of nine agent tasks, AgentRM enhances the base policy model by $8.8$ points on average, surpassing the top general agent by $4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding greater improvement of $12.6$ on LLaMA-3-70B policy model. As for the specializability, AgentRM can also boost a finetuned policy model and outperform the top specialized agent by $11.4$ on three held-in tasks. Further analysis verifies its effectiveness in test-time scaling. Codes will be released to facilitate the research in this area.",AgentRM Enhancing Agent Generalization Reward Modeling Existing LLM based agents achieved strong performance held tasks generalizability unseen tasks remains poor recent work focus fine tuning policy model diverse tasks improve generalizability work finetuning reward model guide policy model robust directly finetuning policy model Based finding propose AgentRM generalizable reward model guide policy model effective test time search comprehensively investigate approaches construct reward model including explicit reward modeling implicit reward modeling LLM judge use AgentRM guide answer generation Best N sampling step level beam search types agent tasks AgentRM enhances base policy model 8 8 points average surpassing general agent 4 0 demonstrates weak strong generalization yielding greater improvement 12 6 LLaMA 3 70B policy model specializability AgentRM boost finetuned policy model outperform specialized agent 11 4 held tasks analysis verifies effectiveness test time scaling Codes released facilitate research area
993,Score Consistency Meets Preference Alignment: Dual-Consistency for Partial Reward Modeling,"['Bin Xie', 'Bingbing Xu', 'Yige Yuan', 'Shengmao Zhu', 'Huawei Shen']",,Score Consistency Meets Preference Alignment Dual Consistency Partial Reward Modeling
994,Segment-Based Attention Masking for GPTs,"['Shahar Katz', 'Liran Ringel', 'Yaniv Romano', 'Lior Wolf']","Modern Language Models (LMs) owe much of their success to masked causal attention, the backbone of Generative Pre-Trained Transformer (GPT) models. Although GPTs can process the entire user prompt at once, the causal masking is applied to all input tokens step-by-step, mimicking the generation process. This imposes an unnecessary constraint during the initial ""prefill"" phase when the model processes the input prompt and generates the internal representations before producing any output tokens. In this work, attention is masked based on the known block structure at the prefill phase, followed by the conventional token-by-token autoregressive process after that. For example, in a typical chat prompt, the system prompt is treated as one block, and the user prompt as the next one. Each of these is treated as a unit for the purpose of masking, such that the first tokens in each block can access the subsequent tokens in a non-causal manner. Then, the model answer is generated in the conventional causal manner. This Segment-by-Segment scheme entails no additional computational overhead. When integrating it into models such as Llama and Qwen, state-of-the-art performance is consistently achieved.",Segment Based Attention Masking GPTs Modern Language Models LMs owe success masked causal attention backbone Generative Pre Trained Transformer GPT models GPTs process entire user prompt causal masking applied input tokens step step mimicking generation process imposes unnecessary constraint initial prefill phase model processes input prompt generates internal representations producing output tokens work attention masked based known block structure prefill phase followed conventional token token autoregressive process example typical chat prompt prompt treated block user prompt treated unit purpose masking tokens block access subsequent tokens non causal manner model answer generated conventional causal manner Segment Segment scheme entails additional computational overhead integrating models Llama Qwen state art performance consistently achieved
995,Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity,"['Yuri Kuratov', 'Mikhail Arkhipov', 'Aydar Bulatov', 'Mikhail Burtsev']","A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches are focused on reduction of the amount of compute in existing language models rather than minimization of number of bits needed to store text. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.",Cramming 1568 Tokens Single Vector Exploring Limits Embedding Space Capacity range recent works addresses problem compression sequence tokens shorter sequence real valued vectors used inputs instead token embeddings key value cache approaches focused reduction compute existing language models minimization number bits needed store text Despite relying powerful models encoders maximum attainable lossless compression ratio typically higher x10 fact highly intriguing theory maximum information capacity large real valued vectors far presented rates 16 bit precision modest vector size work explore limits compression replacing encoder sample optimization procedure vectors compression ratios x1500 exist highlights orders magnitude gap existing practically attainable solutions Furthermore empirically compression limits determined length input uncertainty reduced cross entropy loss sequence conditioning obtained limits highlight substantial gap theoretical capacity input embeddings practical utilization suggesting significant room optimization model design
996,Bi-Tuning with Collaborative Information for Controllable LLM-based Sequential Recommendation,"['Xinyu Zhang', 'Linmei Hu', 'Luhao Zhang', 'Wentao Cheng', 'Yashen Wang', 'Ge Shi', 'Chong Feng', 'Liqiang Nie']",,Bi Tuning Collaborative Information Controllable LLM based Sequential Recommendation
997,"A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment","['Jean-Philippe Corbeil', 'Amin Dada', 'Jean-Michel Attendu', 'Asma Ben Abacha', 'Alessandro Sordoni', 'Lucas Caccia', 'Francois Beaulieu', 'Thomas Lin', 'Jens Kleesiek', 'Paul Vozila']","High computation costs and latency of large language models such as GPT-4 have limited their deployment in clinical settings. Small language models (SLMs) offer a cost-effective alternative, but their limited capacity requires biomedical domain adaptation, which remains challenging. An additional bottleneck is the unavailability and high sensitivity of clinical data. To address these challenges, we propose a novel framework for adapting SLMs into high-performing clinical models. We introduce the MediPhi collection of 3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning of experts on relevant medical and clinical corpora (PMC, Medical Guideline, MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our expert models deliver relative improvements on this benchmark over the base model without any task-specific fine-tuning: 64.3% on medical entities, 49.5% on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by 14%). We unify the expert models into MediPhi via model merging, preserving gains across benchmarks. Furthermore, we built the MediFlow collection, a synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP tasks, 98 fine-grained document types, and JSON format support. Alignment of MediPhi using supervised fine-tuning and direct preference optimization achieves further gains of 18.9% on average.",Modular Approach Clinical SLMs Driven Synthetic Data Pre Instruction Tuning Model Merging Clinical Tasks Alignment High computation costs latency large language models GPT 4 limited deployment clinical settings Small language models SLMs offer cost effective alternative limited capacity requires biomedical domain adaptation remains challenging additional bottleneck unavailability high sensitivity clinical data address challenges propose novel framework adapting SLMs high performing clinical models introduce MediPhi collection 3 8B parameter SLMs developed novel framework pre instruction tuning experts relevant medical clinical corpora PMC Medical Guideline MedWiki model merging clinical tasks alignment cover clinical tasks extended CLUE benchmark CLUE doubling size expert models deliver relative improvements benchmark base model task specific fine tuning 64 3 medical entities 49 5 radiology reports 44 ICD 10 coding outperforming GPT 4 0125 14 unify expert models MediPhi model merging preserving gains benchmarks Furthermore built MediFlow collection synthetic dataset 2 5 million high quality instructions 14 medical NLP tasks 98 fine grained document types JSON format support Alignment MediPhi using supervised fine tuning direct preference optimization achieves gains 18 9 average
998,DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts,"['Yuchen Feng', 'Bowen Shen', 'Naibin Gu', 'Jiaxuan Zhao', 'Peng Fu', 'Zheng Lin', 'Weiping Wang']","Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture achieve high cost-efficiency by selectively activating a subset of the parameters. Despite the inference efficiency of MoE LLMs, the training of extensive experts from scratch incurs substantial overhead, whereas reconstructing a dense LLM into an MoE LLM significantly reduces the training budget. However, existing reconstruction methods often overlook the diversity among experts, leading to potential redundancy. In this paper, we come up with the observation that a specific LLM exhibits notable diversity after being pruned on different calibration datasets, based on which we present a Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE includes domain affinity mining, pruning-based expert reconstruction, and efficient retraining. Specifically, the reconstruction includes pruning and reassembly of the feed-forward network (FFN) module. After reconstruction, we efficiently retrain the model on routers, experts and normalization modules. We implement DIVE on Llama-style LLMs with open-source training corpora. Experiments show that DIVE achieves training efficiency with minimal accuracy trade-offs, outperforming existing pruning and MoE reconstruction methods with the same number of activated parameters.",DIVE MoE Diversity Enhanced Reconstruction Large Language Models Dense Mixture Experts Large language models LLMs Mixture Experts MoE architecture achieve high cost efficiency selectively activating subset parameters Despite inference efficiency MoE LLMs training extensive experts scratch incurs substantial overhead reconstructing dense LLM MoE LLM significantly reduces training budget existing reconstruction methods overlook diversity experts leading potential redundancy paper come observation specific LLM exhibits notable diversity pruned different calibration datasets based present Diversity Enhanced reconstruction method named DIVE recipe DIVE includes domain affinity mining pruning based expert reconstruction efficient retraining Specifically reconstruction includes pruning reassembly feed forward network FFN module reconstruction efficiently retrain model routers experts normalization modules implement DIVE Llama style LLMs open source training corpora Experiments DIVE achieves training efficiency minimal accuracy trade offs outperforming existing pruning MoE reconstruction methods number activated parameters
999,DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression,"['Yi Zhao', 'Zuchao Li', 'hai zhao', 'Baoyuan Qi', 'Liu Guoming']",,DAC Dynamic Attention aware Approach Task Agnostic Prompt Compression
1000,Computation Mechanism Behind LLM Position Generalization,"['Chi Han', 'Heng Ji']","Most written natural languages are composed of sequences of words and sentences. Similar to humans, large language models (LLMs) exhibit flexibility in handling textual positions - a phenomenon we term position generalization. They can understand texts with position perturbations and generalize to longer texts than those encountered during training with the latest techniques. These phenomena suggest that LLMs handle positions tolerantly, but how LLMs computationally process positional relevance remains largely unexplored. This work connects the linguistic phenomenon with LLMs' computational mechanisms. We show how LLMs enforce certain computational mechanisms for the aforementioned tolerance in position perturbations. Despite the complex design of the self-attention mechanism, this work reveals that LLMs learn a counterintuitive disentanglement of attention logits. Their values show a 0.959 linear correlation with an approximation of the arithmetic sum of positional relevance and semantic importance. Furthermore, we identify a prevalent pattern in intermediate features, which we prove theoretically enables this effect. The pattern, which is different from how randomly initialized parameters would behave, suggests that it is a learned behavior rather than a natural result of the model architecture. Based on these findings, we provide computational explanations and criteria for LLMs' position flexibilities. This work takes a pioneering step in linking position generalization with modern LLMs' internal mechanisms.",Computation Mechanism LLM Position Generalization written natural languages composed sequences words sentences Similar humans large language models LLMs exhibit flexibility handling textual positions phenomenon term position generalization understand texts position perturbations generalize longer texts encountered training latest techniques phenomena suggest LLMs handle positions tolerantly LLMs computationally process positional relevance remains largely unexplored work connects linguistic phenomenon LLMs computational mechanisms LLMs enforce certain computational mechanisms aforementioned tolerance position perturbations Despite complex design self attention mechanism work reveals LLMs learn counterintuitive disentanglement attention logits values 0 959 linear correlation approximation arithmetic sum positional relevance semantic importance Furthermore identify prevalent pattern intermediate features prove theoretically enables effect pattern different randomly initialized parameters behave suggests learned behavior natural result model architecture Based findings provide computational explanations criteria LLMs position flexibilities work takes pioneering step linking position generalization modern LLMs internal mechanisms
1001,IPO: Your Language Model is Secretly a Preference Classifier,"['Shivank Garg', 'Ayush Singh', 'Shweta Singh', 'Paras Chopra']","Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. While it enables LLMs to achieve human-level alignment, it often incurs significant computational and financial costs due to its reliance on training external reward models or human-labeled preferences. In this work, we propose Implicit Preference Optimization (IPO), an alternative approach that leverages generative LLMs as preference classifiers, thereby reducing the dependence on external human feedback or reward models to obtain preferences. We conduct a comprehensive evaluation on the preference classification ability of LLMs using RewardBench, assessing models across different sizes, architectures, and training levels to validate our hypothesis. Furthermore, we investigate the self-improvement capabilities of LLMs by generating multiple responses for a given instruction and employing the model itself as a preference classifier for Direct Preference Optimization (DPO)-based training. Our findings demonstrate that models trained through IPO achieve performance comparable to those utilizing state-of-the-art reward models for obtaining preferences.",IPO Language Model Secretly Preference Classifier Reinforcement learning human feedback RLHF emerged primary method aligning large language models LLMs human preferences enables LLMs achieve human level alignment incurs significant computational financial costs reliance training external reward models human labeled preferences work propose Implicit Preference Optimization IPO alternative approach leverages generative LLMs preference classifiers reducing dependence external human feedback reward models obtain preferences conduct comprehensive evaluation preference classification ability LLMs using RewardBench assessing models different sizes architectures training levels validate hypothesis Furthermore investigate self improvement capabilities LLMs generating multiple responses given instruction employing model preference classifier Direct Preference Optimization DPO based training findings demonstrate models trained IPO achieve performance comparable utilizing state art reward models obtaining preferences
1002,Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up,"['Jiahao Yuan', 'Dehui du', 'Hao Zhang', 'Zixiang Di', 'Usman Naseem']","Large language models (LLMs) have shown remarkable performance in reasoning tasks but face limitations in mathematical and complex logical reasoning. Existing methods to improve LLMs' logical capabilities either involve traceable or verifiable logical sequences that generate more reliable responses by constructing logical structures yet increase computational costs, or introduces rigid logic template rules, reducing flexibility. In this paper, we propose Reversal of Thought (RoT), a plug-and-play and cost-effective reasoning framework designed to enhance the logical reasoning abilities of LLMs during the warm-up phase prior to batch inference. RoT utilizes a Preference-Guided Reverse Reasoning warm-up strategy, which integrates logical symbols for pseudocode planning through meta-cognitive mechanisms and pairwise preference self-evaluation to generate task-specific prompts solely through demonstrations, aligning with LLMs' cognitive preferences shaped by RLHF. Through reverse reasoning, we utilize a Cognitive Preference Manager to assess knowledge boundaries and further expand LLMs' reasoning capabilities by aggregating solution logic for known tasks and stylistic templates for unknown tasks. Experiments across various tasks demonstrate that RoT surpasses existing baselines in both reasoning accuracy and efficiency.",Reversal Thought Enhancing Large Language Models Preference Guided Reverse Reasoning Warm Large language models LLMs shown remarkable performance reasoning tasks face limitations mathematical complex logical reasoning Existing methods improve LLMs logical capabilities involve traceable verifiable logical sequences generate reliable responses constructing logical structures increase computational costs introduces rigid logic template rules reducing flexibility paper propose Reversal Thought RoT plug play cost effective reasoning framework designed enhance logical reasoning abilities LLMs warm phase prior batch inference RoT utilizes Preference Guided Reverse Reasoning warm strategy integrates logical symbols pseudocode planning meta cognitive mechanisms pairwise preference self evaluation generate task specific prompts solely demonstrations aligning LLMs cognitive preferences shaped RLHF reverse reasoning utilize Cognitive Preference Manager assess knowledge boundaries expand LLMs reasoning capabilities aggregating solution logic known tasks stylistic templates unknown tasks Experiments various tasks demonstrate RoT surpasses existing baselines reasoning accuracy efficiency
1003,Déjà Vu? Decoding Repeated Reading from Eye Movements,"['Yoav Meiri', 'Omer Shubi', 'Cfir Avraham Hadar', 'Ariel Kreisberg Nitzav', 'Yevgeni Berzak']","Be it your favorite novel, a newswire article, a cooking recipe or an academic paper -- in many daily situations we read the same text more than once. In this work, we ask whether it is possible to automatically determine whether the reader has previously encountered a text based on their eye movement patterns. We introduce two variants of this task and address them with considerable success using both feature-based and neural models. We further introduce a general strategy for enhancing these models with machine generated simulations of eye movements from a cognitive model. Finally, we present an analysis of model performance which on the one hand yields insights on the information used by the models, and on the other hand leverages predictive modeling as an analytic tool for better characterization of the role of memory in repeated reading. Our work advances the understanding of the extent and manner in which eye movements in reading capture memory effects from prior text exposure, and paves the way for future applications that involve predictive modeling of repeated reading.",Déjà Vu Decoding Repeated Reading Eye Movements favorite novel newswire article cooking recipe academic paper daily situations read text work ask possible automatically determine reader previously encountered text based eye movement patterns introduce variants task address considerable success using feature based neural models introduce general strategy enhancing models machine generated simulations eye movements cognitive model Finally present analysis model performance hand yields insights information used models hand leverages predictive modeling analytic tool better characterization role memory repeated reading work advances understanding extent manner eye movements reading capture memory effects prior text exposure paves way future applications involve predictive modeling repeated reading
1004,LLMs can be easily Confused by Instructional Distractions,"['Yerin Hwang', 'Yongil Kim', 'Jahyun Koo', 'Taegwan Kang', 'Hyunkyung Bae', 'Kyomin Jung']","Despite the fact that large language models (LLMs) show exceptional skill in instruction following tasks, this strength can turn into a vulnerability when the models are required to disregard certain instructions. Instruction-following tasks typically involve a clear task description and input text containing the target data to be processed. However, when the input itself resembles an instruction, confusion may arise, even if there is explicit prompting to distinguish between the task instruction and the input. We refer to this phenomenon as instructional distraction. In this paper, we introduce a novel benchmark, named DIM-Bench, specifically designed to assess LLMs' performance under instructional distraction. The benchmark categorizes real-world instances of instructional distraction and evaluates LLMs across four instruction tasks: rewriting, proofreading, translation, and style transfer -- alongside five input tasks: reasoning, code generation, mathematical reasoning, bias detection, and question answering. Our experimental results reveal that even the most advanced LLMs are susceptible to instructional distraction, often failing to accurately follow user intent in such cases.",LLMs easily Confused Instructional Distractions Despite fact large language models LLMs exceptional skill instruction following tasks strength turn vulnerability models required disregard certain instructions Instruction following tasks typically involve clear task description input text containing target data processed input resembles instruction confusion arise explicit prompting distinguish task instruction input refer phenomenon instructional distraction paper introduce novel benchmark named DIM Bench specifically designed assess LLMs performance instructional distraction benchmark categorizes real world instances instructional distraction evaluates LLMs instruction tasks rewriting proofreading translation style transfer alongside input tasks reasoning code generation mathematical reasoning bias detection question answering experimental results reveal advanced LLMs susceptible instructional distraction failing accurately follow user intent cases
1005,PlanGenLLMs: A Modern Survey of LLM Planning Capabilities,"['Hui Wei', 'Zihao Zhang', 'Shenghua He', 'Tian Xia', 'Shijia Pan', 'Fei Liu']","LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.",PlanGenLLMs Modern Survey LLM Planning Capabilities LLMs immense potential generating plans transforming initial world state desired goal state large body research explored use LLMs various planning tasks web navigation travel planning database querying systems tailored specific problems making challenging compare determine best approach new tasks lack clear consistent evaluation criteria survey aims offer comprehensive overview current LLM planners gap builds foundational work Kartam Wilkins 1990 examines key performance criteria completeness executability optimality representation generalization efficiency provide thorough analysis representative works highlight strengths weaknesses paper identifies crucial future directions making valuable resource practitioners newcomers interested leveraging LLM planning support agentic workflows
1006,IAM: Efficient Inference through Attention Mapping between Different-scale LLMs,"['Yi Zhao', 'Zuchao Li', 'hai zhao']",,IAM Efficient Inference Attention Mapping Different scale LLMs
1007,nvAgent: Automated Data Visualization from Natural Language via Collaborative Agent Workflow,"['Geliang Ouyang', 'Jingyao Chen', 'Zhihe Nie', 'Yi Gui', 'Yao Wan', 'Hongyu Zhang', 'Dongping Chen']","Natural Language to Visualization (NL2Vis) seeks to convert natural-language descriptions into visual representations of given tables, empowering users to derive insights from large-scale data. Recent advancements in Large Language Models (LLMs) show promise in automating code generation to transform tabular data into accessible visualizations. However, they often struggle with complex queries that require reasoning across multiple tables. To address this limitation, we propose a collaborative agent workflow, termed nvAgent, for NL2Vis. Specifically, nvAgent comprises three agents: a processor agent for database processing and context filtering, a composer agent for planning visualization generation, and a validator agent for code translation and output verification. Comprehensive evaluations on the new VisEval benchmark demonstrate that nvAgent consistently surpasses state-of-the-art baselines, achieving a 7.88% improvement in single-table and a 9.23% improvement in multi-table scenarios. Qualitative analyses further highlight that nvAgent maintains nearly a 20% performance margin over previous models, underscoring its capacity to produce high-quality visual representations from complex, heterogeneous data sources.",nvAgent Automated Data Visualization Natural Language Collaborative Agent Workflow Natural Language Visualization NL2Vis seeks convert natural language descriptions visual representations given tables empowering users derive insights large scale data Recent advancements Large Language Models LLMs promise automating code generation transform tabular data accessible visualizations struggle complex queries require reasoning multiple tables address limitation propose collaborative agent workflow termed nvAgent NL2Vis Specifically nvAgent comprises agents processor agent database processing context filtering composer agent planning visualization generation validator agent code translation output verification Comprehensive evaluations new VisEval benchmark demonstrate nvAgent consistently surpasses state art baselines achieving 7 88 improvement single table 9 23 improvement multi table scenarios Qualitative analyses highlight nvAgent maintains nearly 20 performance margin previous models underscoring capacity produce high quality visual representations complex heterogeneous data sources
1008,ZIPA: A family of efficient models for multilingual phone recognition,"['Jian Zhu', 'Farhan Samir', 'Eleanor Chodroff', 'David R. Mortensen']","We present ZIPA, a family of efficient speech models that advances the state-of-the-art performance of crosslinguistic phone recognition. We first curated IPAPack++, a large-scale multilingual speech corpus with 17,132 hours of normalized phone transcriptions and a novel evaluation set capturing unseen languages and sociophonetic variation. With the large-scale training data, ZIPA, including transducer (ZIPA-T) and CTC-based (ZIPA-CR) variants, leverage the efficient Zipformer backbones and outperform existing phone recognition systems with much fewer parameters. Further scaling via noisy student training on 11,000 hours of pseudo-labeled multilingual data yields further improvement. While ZIPA achieves strong performance on benchmarks, error analysis reveals persistent limitations in modeling sociophonetic diversity, underscoring challenges for future research.",ZIPA family efficient models multilingual phone recognition present ZIPA family efficient speech models advances state art performance crosslinguistic phone recognition curated IPAPack large scale multilingual speech corpus 17 132 hours normalized phone transcriptions novel evaluation set capturing unseen languages sociophonetic variation large scale training data ZIPA including transducer ZIPA T CTC based ZIPA CR variants leverage efficient Zipformer backbones outperform existing phone recognition systems fewer parameters scaling noisy student training 11 000 hours pseudo labeled multilingual data yields improvement ZIPA achieves strong performance benchmarks error analysis reveals persistent limitations modeling sociophonetic diversity underscoring challenges future research
1009,GRACE: A Granular Benchmark for Evaluating Model Calibration against Human Calibration,"['Yoo Yeon Sung', 'Eve Fleisig', 'Yu Hou', 'Ishan Upadhyay', 'Jordan Lee Boyd-Graber']","Language models are often miscalibrated, leading to confidently incorrect answers. We introduce GRACE, a benchmark for language model calibration that incorporates comparison with human calibration. GRACE consists of question-answer pairs, in which each question contains a series of clues that gradually become easier, all leading to the same answer; models must answer correctly as early as possible as the clues are revealed. This setting permits granular measurement of model calibration based on how early, accurately, and confidently a model answers. After collecting these questions, we host live human vs. model competitions to gather 1,749 data points on human and model teams' timing, accuracy, and confidence. We propose a metric, CalScore, that uses GRACE to analyze model calibration errors and identify types of model miscalibration that differ from human behavior. We find that although humans are less accurate than models, humans are generally better calibrated. Since state-of-the-art models struggle on GRACE, it effectively evaluates progress on improving model calibration.",GRACE Granular Benchmark Evaluating Model Calibration Human Calibration Language models miscalibrated leading confidently incorrect answers introduce GRACE benchmark language model calibration incorporates comparison human calibration GRACE consists question answer pairs question contains series clues gradually easier leading answer models answer correctly early possible clues revealed setting permits granular measurement model calibration based early accurately confidently model answers collecting questions host live human vs model competitions gather 1 749 data points human model teams timing accuracy confidence propose metric CalScore uses GRACE analyze model calibration errors identify types model miscalibration differ human behavior humans accurate models humans generally better calibrated state art models struggle GRACE effectively evaluates progress improving model calibration
1010,That doesn’t sound right: Evaluating speech transcription quality in field linguistics corpora,"['Eric Le Ferrand', 'Bo Jiang', 'Emily Prud’hommeaux', 'Joshua Hartshorne']",,doesn t sound right Evaluating speech transcription quality field linguistics corpora
1011,Dynamic Evaluation with Cognitive Reasoning for Multi-turn Safety of Large Language Models,"['Lanxue Zhang', 'Yanan Cao', 'Yuqiang Xie', 'Fang Fang', 'Yangxi Li']",,Dynamic Evaluation Cognitive Reasoning Multi turn Safety Large Language Models
1012,Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering,"['William Jurayj', 'Jeffrey Cheng', 'Benjamin Van Durme']","Scaling the test-time compute of large language models has demonstrated impressive performance on reasoning benchmarks. However, existing evaluations of test-time scaling make the strong assumption that a reasoning system should always give an answer to any question provided. This overlooks concerns about whether a model is confident in its answer, and whether it is appropriate to always provide a response. To address these concerns, we extract confidence scores during reasoning for thresholding model responses. We find that increasing compute budget at inference time not only helps models answer more questions correctly, but also increases confidence in correct responses. We then extend the current paradigm of zero-risk responses during evaluation by considering settings with non-zero levels of response risk, and suggest a recipe for reporting evaluations under these settings.",Final Answer Test Time Scaling Improves Selective Question Answering Scaling test time compute large language models demonstrated impressive performance reasoning benchmarks existing evaluations test time scaling make strong assumption reasoning answer question provided overlooks concerns model confident answer appropriate provide response address concerns extract confidence scores reasoning thresholding model responses increasing compute budget inference time helps models answer questions correctly increases confidence correct responses extend current paradigm zero risk responses evaluation considering settings non zero levels response risk suggest recipe reporting evaluations settings
1013,From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions,"['Nathanaël Carraz Rakotonirina', 'Mohammed Hamdy', 'Jon Ander Campos', 'Lucas Weber', 'Alberto Testoni', 'Marzieh Fadaee', 'Sandro Pezzelle', 'Marco Del Tredici']","Large Language Models (LLMs) are increasingly used in working environments for a wide range of tasks, excelling at solving individual problems in isolation. However, are they also able to effectively collaborate over long-term interactions? To investigate this, we introduce MemoryCode, a synthetic multi-session dataset designed to test LLMs' ability to track and execute simple coding instructions amid irrelevant information, simulating a realistic setting. While all the models we tested handle isolated instructions well, even the performance of state-of-the-art models like GPT-4o deteriorates when instructions are spread across sessions. Our analysis suggests this is due to their failure to retrieve and integrate information over long instruction chains. Our results highlight a fundamental limitation of current LLMs, restricting their ability to collaborate effectively in long interactions.",Tools Teammates Evaluating LLMs Multi Session Coding Interactions Large Language Models LLMs increasingly used working environments wide range tasks excelling solving individual problems isolation able effectively collaborate long term interactions investigate introduce MemoryCode synthetic multi session dataset designed test LLMs ability track execute simple coding instructions amid irrelevant information simulating realistic setting models tested handle isolated instructions performance state art models like GPT 4o deteriorates instructions spread sessions analysis suggests failure retrieve integrate information long instruction chains results highlight fundamental limitation current LLMs restricting ability collaborate effectively long interactions
1014,Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints,"['Junxiao Yang', 'Zhexin Zhang', 'Shiyao Cui', 'Hongning Wang', 'Minlie Huang']","Jailbreaking attacks can effectively induce unsafe behaviors in Large Language Models (LLMs); however, the transferability of these attacks across different models remains limited. This study aims to understand and enhance the transferability of gradient-based jailbreaking methods, which are among the standard approaches for attacking white-box models. Through a detailed analysis of the optimization process, we introduce a novel conceptual framework to elucidate transferability and identify superfluous constraints-specifically, the response pattern constraint and the token tail constraint-as significant barriers to improved transferability. Removing these unnecessary constraints substantially enhances the transferability and controllability of gradient-based attacks. Evaluated on Llama-3-8B-Instruct as the source model, our method increases the overall Transfer Attack Success Rate (T-ASR) across a set of target models with varying safety levels from 18.4% to 50.3%, while also improving the stability and controllability of jailbreak behaviors on both source and target models.",Guiding Forcing Enhancing Transferability Jailbreaking Attacks LLMs Removing Superfluous Constraints Jailbreaking attacks effectively induce unsafe behaviors Large Language Models LLMs transferability attacks different models remains limited study aims understand enhance transferability gradient based jailbreaking methods standard approaches attacking white box models detailed analysis optimization process introduce novel conceptual framework elucidate transferability identify superfluous constraints specifically response pattern constraint token tail constraint significant barriers improved transferability Removing unnecessary constraints substantially enhances transferability controllability gradient based attacks Evaluated Llama 3 8B Instruct source model method increases overall Transfer Attack Success Rate T ASR set target models varying safety levels 18 4 50 3 improving stability controllability jailbreak behaviors source target models
1015,Multilingual Text-to-Image Generation Magnifies Gender Stereotypes,"['Felix Friedrich', 'Katharina Hämmerl', 'Patrick Schramowski', 'Manuel Brack', 'Jindřich Libovický', 'Alexander Fraser', 'Kristian Kersting']",,Multilingual Text Image Generation Magnifies Gender Stereotypes
1016,Adversarial Alignment with Anchor Dragging Drift ($A^3D^2$): Multimodal Domain Adaptation with Partially Shifted Modalities,"['Jun Sun', 'Xinxin Zhang', 'Simin Hong', 'Jian Zhu', 'Lingfang Zeng']",,Adversarial Alignment Anchor Dragging Drift 3D 2 Multimodal Domain Adaptation Partially Shifted Modalities
1017,A Reality Check on Context Utilisation for Retrieval-Augmented Generation,"['Lovisa Hagström', 'Sara Vera Marjanovic', 'Haeun Yu', 'Arnav Arora', 'Christina Lioma', 'Maria Maistro', 'Pepa Atanasova', 'Isabelle Augenstein']","Retrieval-augmented generation (RAG) helps address the limitations of parametric knowledge embedded within a language model (LM). In real world settings, retrieved information can vary in complexity, yet most investigations of LM utilisation of context has been limited to synthetic text. We introduce DRUID (Dataset of Retrieved Unreliable, Insufficient and Difficult-to-understand contexts) with real-world queries and contexts manually annotated for stance. The dataset is based on the prototypical task of automated claim verification, for which automated retrieval of real-world evidence is crucial. We compare DRUID to synthetic datasets (CounterFact, ConflictQA) and find that artificial datasets often fail to represent the complexity and diversity of realistically retrieved context. We show that synthetic datasets exaggerate context characteristics rare in real retrieved data, which leads to inflated context utilisation results, as measured by our novel ACU score. Moreover, while previous work has mainly focused on singleton context characteristics to explain context utilisation, correlations between singleton context properties and ACU on DRUID are surprisingly small compared to other properties related to context source. Overall, our work underscores the need for real-world aligned context utilisation studies to represent and improve performance in real-world RAG settings.",Reality Check Context Utilisation Retrieval Augmented Generation Retrieval augmented generation RAG helps address limitations parametric knowledge embedded language model LM real world settings retrieved information vary complexity investigations LM utilisation context limited synthetic text introduce DRUID Dataset Retrieved Unreliable Insufficient Difficult understand contexts real world queries contexts manually annotated stance dataset based prototypical task automated claim verification automated retrieval real world evidence crucial compare DRUID synthetic datasets CounterFact ConflictQA artificial datasets fail represent complexity diversity realistically retrieved context synthetic datasets exaggerate context characteristics rare real retrieved data leads inflated context utilisation results measured novel ACU score previous work mainly focused singleton context characteristics explain context utilisation correlations singleton context properties ACU DRUID surprisingly small compared properties related context source Overall work underscores need real world aligned context utilisation studies represent improve performance real world RAG settings
1018,CU-MAM: Coherence-Driven Unified Macro-Structures for Argument Mining,"['Debela Gemechu', 'Chris Reed']",,CU MAM Coherence Driven Unified Macro Structures Argument Mining
1019,Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts,"['Hongyu Chen', 'Seraphina Goldfarb-Tarrant']","Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98\%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments.",Safer Luckier LLMs Safety Evaluators Robust Artifacts Large Language Models LLMs increasingly employed automated evaluators assess safety generated content reliability role remains uncertain study evaluates diverse set 11 LLM judge models critical safety domains examining key aspects self consistency repeated judging tasks alignment human judgments susceptibility input artifacts apologetic verbose phrasing findings reveal biases LLM judges significantly distort final verdict content source safer undermining validity comparative evaluations Notably apologetic language artifacts skew evaluator preferences 98 Contrary expectations larger models consistently exhibit greater robustness smaller models higher resistance specific artifacts mitigate LLM evaluator robustness issues investigate jury based evaluations aggregating decisions multiple models approach improves robustness enhances alignment human judgements artifact sensitivity persists best jury configurations results highlight urgent need diversified artifact resistant methodologies ensure reliable safety assessments
1020,Text-to-ES Bench: A Comprehensive Benchmark for Converting Natural Language to Elasticsearch Query,"['DonggeXue', 'Zhili Pu', 'Zhentao Xia', 'Hongli Sun', 'Ruihui Hou', 'Guangya Yu', 'Yupian Lin', 'Yongqi Fan', 'Jingping Liu', 'Tong Ruan']",,Text ES Bench Comprehensive Benchmark Converting Natural Language Elasticsearch Query
1021,AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation,"['Songming Zhang', 'Xue Zhang', 'Tong Zhang', 'Bojie Hu', 'Yufeng Chen', 'Jinan Xu']","In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization.",AlignDistil Token Level Language Model Alignment Adaptive Policy Distillation modern large language models LLMs LLM alignment crucial importance typically achieved methods reinforcement learning human feedback RLHF direct preference optimization DPO existing methods LLM alignment tokens response optimized using sparse response level reward preference annotation ignorance token level rewards erroneously punish high quality tokens encourage low quality tokens resulting suboptimal performance slow convergence speed address issue propose AlignDistil RLHF equivalent distillation method token level reward optimization Specifically introduce reward learned DPO RLHF objective theoretically prove equivalence objective token level distillation process teacher distribution linearly combines logits DPO model reference model basis bridge accuracy gap reward DPO model pure reward model building contrastive DPO reward normal reverse DPO model avoid optimization different tokens design token adaptive logit extrapolation mechanism construct appropriate teacher distribution token Experimental results demonstrate superiority AlignDistil existing methods showcase fast convergence token level distributional reward optimization
1022,Acoustic Individual Identification of White-Faced Capuchin Monkeys Using Joint Multi-Species Embeddings,"['Álvaro Vega-Hidalgo', 'Artem Abzaliev', 'Thore Bergman', 'Rada Mihalcea']",,Acoustic Individual Identification White Faced Capuchin Monkeys Using Joint Multi Species Embeddings
1023,DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by Adaptive Tree Traversal,"['Vaibhav Aggarwal', 'Ojasv Kamal', 'Abhinav Japesh', 'Zhijing Jin', 'Bernhard Schölkopf']","Large Language Models (LLMs) have revolutionized various domains, including natural language processing, data analysis, and software development, by enabling automation. In software engineering, LLM-powered coding agents have garnered significant attention due to their potential to automate complex development tasks, assist in debugging, and enhance productivity. However, existing approaches often struggle with sub-optimal decision-making, requiring either extensive manual intervention or inefficient compute scaling strategies. To improve coding agent performance, we present Dynamic Action Re-Sampling (DARS), a novel inference time compute scaling approach for coding agents, that is faster and more effective at recovering from sub-optimal decisions compared to baselines. While traditional agents either follow linear trajectories or rely on random sampling for scaling compute, our approach DARS works by branching out a trajectory at certain key decision points by taking an alternative action given the history of the trajectory and execution feedback of the previous attempt from that point. We evaluate our approach on SWE-Bench Lite benchmark, demonstrating that this scaling strategy achieves a pass@k score of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of 47%, outperforming state-of-the-art (SOTA) open-source frameworks.",DARS Dynamic Action Sampling Enhance Coding Agent Performance Adaptive Tree Traversal Large Language Models LLMs revolutionized various domains including natural language processing data analysis software development enabling automation software engineering LLM powered coding agents garnered significant attention potential automate complex development tasks assist debugging enhance productivity existing approaches struggle sub optimal decision making requiring extensive manual intervention inefficient compute scaling strategies improve coding agent performance present Dynamic Action Sampling DARS novel inference time compute scaling approach coding agents faster effective recovering sub optimal decisions compared baselines traditional agents follow linear trajectories rely random sampling scaling compute approach DARS works branching trajectory certain key decision points taking alternative action given history trajectory execution feedback previous attempt point evaluate approach SWE Bench Lite benchmark demonstrating scaling strategy achieves pass k score 55 Claude 3 5 Sonnet V2 framework achieves pass 1 rate 47 outperforming state art SOTA open source frameworks
1024,Steering off Course: Reliability Challenges in Steering Language Models,"['Patrick Queiroz Da Silva', 'Hari Sethuraman', 'Dheeraj Rajagopal', 'Hannaneh Hajishirzi', 'Sachin Kumar']","Steering methods for language models (LMs) have gained traction as lightweight alternatives to fine-tuning, enabling targeted modifications to model activations. However, prior studies primarily report results on a few models, leaving critical gaps in understanding the robustness of these methods. In this work, we systematically examine three prominent steering methods -- DoLa, function vectors, and task vectors. In contrast to the original studies, which evaluated a handful of models, we test up to 36 models belonging to 14 families with sizes ranging from 1.5B to 70B parameters. Our experiments reveal substantial variability in the effectiveness of the steering approaches, with a large number of models showing no improvement and at times degradation in steering performance. Our analysis demonstrate fundamental flaws in the assumptions underlying these methods, challenging their reliability as scalable steering solutions.",Steering Course Reliability Challenges Steering Language Models Steering methods language models LMs gained traction lightweight alternatives fine tuning enabling targeted modifications model activations prior studies primarily report results models leaving critical gaps understanding robustness methods work systematically examine prominent steering methods DoLa function vectors task vectors contrast original studies evaluated handful models test 36 models belonging 14 families sizes ranging 1 5B 70B parameters experiments reveal substantial variability effectiveness steering approaches large number models showing improvement times degradation steering performance analysis demonstrate fundamental flaws assumptions underlying methods challenging reliability scalable steering solutions
1025,Impartial Multi-task Representation Learning via Variance-invariant Probabilistic Decoding,"['Dou Hu', 'Lingwei Wei', 'Wei Zhou', 'Songlin Hu']",,Impartial Multi task Representation Learning Variance invariant Probabilistic Decoding
1026,If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World,['Adrian de Wynter'],"Warning: this paper discusses content related, but not limited to, violence, sex, and suicide. Loneliness, or the lack of fulfilling relationships, significantly impacts a person's mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs in services like ChatGPT is more prevalent--and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT outside of its marketed use as a task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22x more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations to research and industry to address loneliness.",Eleanor Rigby Met ChatGPT Study Loneliness Post LLM World Warning paper discusses content related limited violence sex suicide Loneliness lack fulfilling relationships significantly impacts person s mental physical prevalent worldwide Previous research suggests large language models LLMs help mitigate loneliness argue use widespread LLMs services like ChatGPT prevalent riskier designed purpose explore analysed user interactions ChatGPT outside marketed use task oriented assistant dialogues classified lonely users frequently 37 sought advice validation received good engagement ChatGPT failed sensitive scenarios like responding appropriately suicidal ideation trauma observed 35 higher incidence toxic content women 22x likely targeted men findings underscore ethical legal questions technology note risks like radicalisation isolation conclude recommendations research industry address loneliness
1027,"Integrating Audio, Visual, and Semantic Information for Enhanced Multimodal Speaker Diarization on Multi-party Conversation","['Luyao Cheng', 'Hui Wang', 'Chong Deng', 'Siqi Zheng', 'Yafeng Chen', 'Rongjie Huang', 'Qinglin Zhang', 'Qian Chen', 'Xihao Li', 'Wen Wang']",,Integrating Audio Visual Semantic Information Enhanced Multimodal Speaker Diarization Multi party Conversation
1028,Vulnerability of LLMs to Vertically Aligned Text Manipulations,"['Zhecheng Li', 'Yiwei Wang', 'Bryan Hooi', 'Yujun Cai', 'Zhen Xiong', 'Nanyun Peng', 'Kai-Wei Chang']","Text classification involves categorizing a given text, such as determining its sentiment or identifying harmful content. With the advancement of large language models (LLMs), these models have become highly effective at performing text classification tasks. However, they still show vulnerabilities to variations in text formatting. Recent research demonstrates that modifying input formats, such as vertically aligning words for encoder-based models, can substantially lower accuracy in text classification tasks. While easily understood by humans, these inputs can significantly mislead models, posing a potential risk of bypassing detection in real-world scenarios involving harmful or sensitive information. With the expanding application of LLMs, a crucial question arises: Do decoder-based LLMs exhibit similar vulnerabilities to vertically formatted text input? In this paper, we investigate the impact of vertical text input on the performance of various LLMs across multiple text classification datasets and analyze the underlying causes. Our findings are as follows: (i) Vertical text input significantly degrades the accuracy of LLMs in text classification tasks. (ii) Chain of Thought (CoT) reasoning does not help LLMs recognize vertical input or mitigate its vulnerability, but few-shot learning with careful analysis does. (iii) We explore the underlying cause of the vulnerability by analyzing the inherent issues in tokenization and attention matrices.",Vulnerability LLMs Vertically Aligned Text Manipulations Text classification involves categorizing given text determining sentiment identifying harmful content advancement large language models LLMs models highly effective performing text classification tasks vulnerabilities variations text formatting Recent research demonstrates modifying input formats vertically aligning words encoder based models substantially lower accuracy text classification tasks easily understood humans inputs significantly mislead models posing potential risk bypassing detection real world scenarios involving harmful sensitive information expanding application LLMs crucial question arises decoder based LLMs exhibit similar vulnerabilities vertically formatted text input paper investigate impact vertical text input performance various LLMs multiple text classification datasets analyze underlying causes findings follows Vertical text input significantly degrades accuracy LLMs text classification tasks ii Chain Thought CoT reasoning does help LLMs recognize vertical input mitigate vulnerability shot learning careful analysis does iii explore underlying cause vulnerability analyzing inherent issues tokenization attention matrices
1029,AutoMixer: Checkpoint Artifacts as Automatic Data Mixers,"['Ernie Chang', 'Yang Li', 'Patrick Huber', 'Vish Vogeti', 'David Kant', 'Yangyang Shi', 'Vikas Chandra']","In language model training, it is desirable to equip models with capabilities from various tasks. However, it is not clear how to directly obtain the right data mixtures for these capabilities as the relationship between data and tasks is difficult to be modeled. In this work, we observe that checkpoint models exhibit emerging capabilities at different points in the training trajectory. Often, the training process saves checkpoints as artifacts that are under-utilized as a source of in-training data signals. We identify these artifact models based on their respective capabilities on the benchmarks and leverage them as data mixers by using their aggregated first-order influence approximation over source data. We demonstrated on eight reasoning benchmarks that the proposed framework shows significant improvements in the pretraining setting, with performance improvements of up to 1.93%. Overall, this shows the potential of checkpoint models to enhance data quality and optimize data mixtures.",AutoMixer Checkpoint Artifacts Automatic Data Mixers language model training desirable equip models capabilities various tasks clear directly obtain right data mixtures capabilities relationship data tasks difficult modeled work observe checkpoint models exhibit emerging capabilities different points training trajectory training process saves checkpoints artifacts utilized source training data signals identify artifact models based respective capabilities benchmarks leverage data mixers using aggregated order influence approximation source data demonstrated reasoning benchmarks proposed framework shows significant improvements pretraining setting performance improvements 1 93 Overall shows potential checkpoint models enhance data quality optimize data mixtures
1030,Generalized Attention Flow: Feature Attribution for Transformer Models via Maximum Flow,"['Behrooz Azarkhalili', 'Maxwell W. Libbrecht']","This paper introduces Generalized Attention Flow (GAF), a novel feature attribution method for Transformer-based models to address the limitations of current approaches. By extending Attention Flow and replacing attention weights with the generalized Information Tensor, GAF integrates attention weights, their gradients, the maximum flow problem, and the barrier method to enhance the performance of feature attributions. The proposed method exhibits key theoretical properties and mitigates the shortcomings of prior techniques that rely solely on simple aggregation of attention weights. Our comprehensive benchmarking on sequence classification tasks demonstrates that a specific variant of GAF consistently outperforms state-of-the-art feature attribution methods in most evaluation settings, providing a more reliable interpretation of Transformer model outputs.",Generalized Attention Flow Feature Attribution Transformer Models Maximum Flow paper introduces Generalized Attention Flow GAF novel feature attribution method Transformer based models address limitations current approaches extending Attention Flow replacing attention weights generalized Information Tensor GAF integrates attention weights gradients maximum flow problem barrier method enhance performance feature attributions proposed method exhibits key theoretical properties mitigates shortcomings prior techniques rely solely simple aggregation attention weights comprehensive benchmarking sequence classification tasks demonstrates specific variant GAF consistently outperforms state art feature attribution methods evaluation settings providing reliable interpretation Transformer model outputs
1031,Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering,"['Zhanghao Hu', 'Hanqi Yan', 'Qinglin Zhu', 'Zhenyi Shen', 'Yulan He', 'Lin Gui']","Large language models have recently pushed open domain question answering (ODQA) to new frontiers. However, prevailing retriever-reader pipelines often depend on multiple rounds of prompt level instructions, leading to high computational overhead, instability, and suboptimal retrieval coverage. In this paper, we propose EmbQA, an embedding-level framework that alleviates these shortcomings by enhancing both the retriever and the reader. Specifically, we refine query representations via lightweight linear layers under an unsupervised contrastive learning objective, thereby reordering retrieved passages to highlight those most likely to contain correct answers. Additionally, we introduce an exploratory embedding that broadens the model's latent semantic space to diversify candidate generation and employs an entropy-based selection mechanism to choose the most confident answer automatically. Extensive experiments across three open-source LLMs, three retrieval methods, and four ODQA benchmarks demonstrate that EmbQA substantially outperforms recent baselines in both accuracy and efficiency.",Prompting Efficient Embedding Framework Open Domain Question Answering Large language models recently pushed open domain question answering ODQA new frontiers prevailing retriever reader pipelines depend multiple rounds prompt level instructions leading high computational overhead instability suboptimal retrieval coverage paper propose EmbQA embedding level framework alleviates shortcomings enhancing retriever reader Specifically refine query representations lightweight linear layers unsupervised contrastive learning objective reordering retrieved passages highlight likely contain correct answers Additionally introduce exploratory embedding broadens model s latent semantic space diversify candidate generation employs entropy based selection mechanism choose confident answer automatically Extensive experiments open source LLMs retrieval methods ODQA benchmarks demonstrate EmbQA substantially outperforms recent baselines accuracy efficiency
1032,AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark,"['Jianlyu Chen', 'Nan Wang', 'Chaofan Li', 'Bo Wang', 'Shitao Xiao', 'Han Xiao', 'Hao Liao', 'Defu Lian', 'Zheng Liu']","Evaluation plays a crucial role in the advancement of information retrieval (IR) models. However, current benchmarks, which are based on predefined domains and human-labeled data, face limitations in addressing evaluation needs for emerging domains both cost-effectively and efficiently. To address this challenge, we propose the Automated Heterogeneous Information Retrieval Benchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1) Automated. The testing data in AIR-Bench is automatically generated by large language models (LLMs) without human intervention. 2) Heterogeneous. The testing data in AIR-Bench is generated with respect to diverse tasks, domains and languages. 3) Dynamic. The domains and languages covered by AIR-Bench are constantly augmented to provide an increasingly comprehensive evaluation benchmark for community developers. We develop a reliable and robust data generation pipeline to automatically create diverse and high-quality evaluation datasets based on real-world corpora. Our findings demonstrate that the generated testing data in AIR-Bench aligns well with human-labeled testing data, making AIR-Bench a dependable benchmark for evaluating IR models. The resources in AIR-Bench are publicly available at https://github.com/AIR-Bench/AIR-Bench.",AIR Bench Automated Heterogeneous Information Retrieval Benchmark Evaluation plays crucial role advancement information retrieval IR models current benchmarks based predefined domains human labeled data face limitations addressing evaluation needs emerging domains cost effectively efficiently address challenge propose Automated Heterogeneous Information Retrieval Benchmark AIR Bench AIR Bench distinguished key features 1 Automated testing data AIR Bench automatically generated large language models LLMs human intervention 2 Heterogeneous testing data AIR Bench generated respect diverse tasks domains languages 3 Dynamic domains languages covered AIR Bench constantly augmented provide increasingly comprehensive evaluation benchmark community developers develop reliable robust data generation pipeline automatically create diverse high quality evaluation datasets based real world corpora findings demonstrate generated testing data AIR Bench aligns human labeled testing data making AIR Bench dependable benchmark evaluating IR models resources AIR Bench publicly available https github com AIR Bench AIR Bench
1033,SELF-PERCEPT: Introspection Improves Large Language Models’ Detection of Multi-Person Mental Manipulation in Conversations,"['Danush Khanna', 'Pratinav Seth', 'Sidhaarth Sredharan Murali', 'Aditya Kumar Guru', 'Siddharth Shukla', 'Tanuj Tyagi', 'SANDEEP CHAURASIA', 'Kripabandhu Ghosh']",,SELF PERCEPT Introspection Improves Large Language Models Detection Multi Person Mental Manipulation Conversations
1034,WE-MATH: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?,"['Runqi Qiao', 'Qiuna Tan', 'Guanting Dong', 'MinhuiWu', 'Chong Sun', 'Xiaoshuai Song', 'Jiapeng Wang', 'Zhuoma GongQue', 'Shanglin Lei', 'YiFan Zhang', 'Zhe Wei', 'Miaoxuan Zhang', 'Runfeng Qiao', 'Xiao Zong', 'Yida Xu', 'Peiqing Yang', 'Zhimin Bao', 'Muxi Diao', 'Chen Li', 'Honggang Zhang']",,MATH Does Large Multimodal Model Achieve Human like Mathematical Reasoning
1035,Modeling the Evolution of English Noun Compounds with Feature-Rich Diachronic Compositionality Prediction,"['Filip Miletić', 'Sabine Schulte im Walde']",,Modeling Evolution English Noun Compounds Feature Rich Diachronic Compositionality Prediction
1036,What’s the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns,"['Michael A. Hedderich', 'Anyi Wang', 'Raoyuan Zhao', 'Florian Eichin', 'Jonas Fischer', 'Barbara Plank']",,s Difference Supporting Users Identifying Effects Prompt Model Changes Token Patterns
1037,V-Oracle: Making Progressive Reasoning in Deciphering Oracle Bones for You and Me,"['Runqi Qiao', 'Qiuna Tan', 'Guanting Dong', 'MinhuiWu', 'Jiapeng Wang', 'YiFan Zhang', 'Zhuoma GongQue', 'Chong Sun', 'Yida Xu', 'Yadong Xue', 'Ye Tian', 'Zhimin Bao', 'LAN YANG', 'Chen Li', 'Honggang Zhang']",,V Oracle Making Progressive Reasoning Deciphering Oracle Bones
1038,Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in Procedural Text Comprehension,"['Amir Hossein Yari', 'Fajri Koto']","Despite the impressive performance of multilingual large language models (mLLMs) in various natural language processing tasks, their ability to understand procedural texts, particularly those with culture-specific content, remains largely unexplored. Texts describing cultural procedures, including rituals, traditional craftsmanship, and social etiquette, require an inherent understanding of cultural context, presenting a significant challenge for mLLMs. In this work, we introduce CAPTex, a benchmark designed to evaluate mLLMs' ability to process and reason about culturally diverse procedural texts across multiple languages using various methodologies to assess their performance. Our findings indicate that (1) mLLMs face difficulties with culturally contextualized procedural texts, showing notable performance declines in low-resource languages, (2) model performance fluctuates across cultural domains, with some areas presenting greater difficulties, and (3) language models exhibit better performance on multiple-choice tasks within conversational frameworks compared to direct questioning. These results underscore the current limitations of mLLMs in handling culturally nuanced procedural texts and highlight the need for culturally aware benchmarks like CAPTex to enhance their adaptability and comprehension across diverse linguistic and cultural landscapes.",Unveiling Cultural Blind Spots Analyzing Limitations mLLMs Procedural Text Comprehension Despite impressive performance multilingual large language models mLLMs various natural language processing tasks ability understand procedural texts particularly culture specific content remains largely unexplored Texts describing cultural procedures including rituals traditional craftsmanship social etiquette require inherent understanding cultural context presenting significant challenge mLLMs work introduce CAPTex benchmark designed evaluate mLLMs ability process reason culturally diverse procedural texts multiple languages using various methodologies assess performance findings indicate 1 mLLMs face difficulties culturally contextualized procedural texts showing notable performance declines low resource languages 2 model performance fluctuates cultural domains areas presenting greater difficulties 3 language models exhibit better performance multiple choice tasks conversational frameworks compared direct questioning results underscore current limitations mLLMs handling culturally nuanced procedural texts highlight need culturally aware benchmarks like CAPTex enhance adaptability comprehension diverse linguistic cultural landscapes
1039,Improving Language and Modality Transfer in Translation by Character-level Modeling,"['Ioannis Tsiamas', 'David Dale', 'Marta R. Costa-jussà']","Current translation systems, despite being highly multilingual, cover only 5% of the world's languages. Expanding language coverage to the long-tail of low-resource languages requires data-efficient methods that rely on cross-lingual and cross-modal knowledge transfer. To this end, we propose a character-based approach to improve adaptability to new languages and modalities. Our method leverages SONAR, a multilingual fixed-size embedding space with different modules for encoding and decoding. We use a teacher-student approach with parallel translation data to obtain a character-level encoder. Then, using ASR data, we train a lightweight adapter to connect a massively multilingual CTC ASR model (MMS), to the character-level encoder, potentially enabling speech translation from 1,000+ languages. Experimental results in text translation for 75 languages on FLORES+ demonstrate that our character-based approach can achieve better language transfer than traditional subword-based models, especially outperforming them in low-resource settings, and demonstrating better zero-shot generalizability to unseen languages. Our speech adaptation, maximizing knowledge transfer from the text modality, achieves state-of-the-art results in speech-to-text translation on the FLEURS benchmark on 33 languages, surpassing previous supervised and cascade models, albeit being a zero-shot model with minimal supervision from ASR data.",Improving Language Modality Transfer Translation Character level Modeling Current translation systems despite highly multilingual cover 5 world s languages Expanding language coverage long tail low resource languages requires data efficient methods rely cross lingual cross modal knowledge transfer end propose character based approach improve adaptability new languages modalities method leverages SONAR multilingual fixed size embedding space different modules encoding decoding use teacher student approach parallel translation data obtain character level encoder using ASR data train lightweight adapter connect massively multilingual CTC ASR model MMS character level encoder potentially enabling speech translation 1 000 languages Experimental results text translation 75 languages FLORES demonstrate character based approach achieve better language transfer traditional subword based models especially outperforming low resource settings demonstrating better zero shot generalizability unseen languages speech adaptation maximizing knowledge transfer text modality achieves state art results speech text translation FLEURS benchmark 33 languages surpassing previous supervised cascade models albeit zero shot model minimal supervision ASR data
1040,DialUp! Modeling the Language Continuum by Adapting Models to Dialects and Dialects to Models,"['Niyati Bafna', 'Emily Chang', 'Nathaniel Romney Robinson', 'David R. Mortensen', 'Kenton Murray', 'David Yarowsky', 'Hale Sirin']","Most of the world's languages and dialects are low-resource, and lack support in mainstream machine translation (MT) models. However, many of them have a closely-related high-resource language (HRL) neighbor, and differ in linguistically regular ways from it. This underscores the importance of model robustness to dialectal variation and cross-lingual generalization to the HRL dialect continuum. We present DialUp, consisting of a training-time technique for adapting a pretrained model to dialectal data (M->D), and an inference-time intervention adapting dialectal data to the model expertise (D->M). M->D induces model robustness to potentially unseen and unknown dialects by exposure to synthetic data exemplifying linguistic mechanisms of dialectal variation, whereas D->M treats dialectal divergence for known target dialects. These methods show considerable performance gains for several dialects from four language families, and modest gains for two other language families. We also conduct feature and error analyses, which show that language varieties with low baseline MT performance are more likely to benefit from these approaches.",DialUp Modeling Language Continuum Adapting Models Dialects Dialects Models world s languages dialects low resource lack support mainstream machine translation MT models closely related high resource language HRL neighbor differ linguistically regular ways underscores importance model robustness dialectal variation cross lingual generalization HRL dialect continuum present DialUp consisting training time technique adapting pretrained model dialectal data M D inference time intervention adapting dialectal data model expertise D M M D induces model robustness potentially unseen unknown dialects exposure synthetic data exemplifying linguistic mechanisms dialectal variation D M treats dialectal divergence known target dialects methods considerable performance gains dialects language families modest gains language families conduct feature error analyses language varieties low baseline MT performance likely benefit approaches
1041,AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs,"['Nicholas E. Corrado', 'Julian Katz-Samuels', 'Adithya M Devraj', 'Hyokun Yun', 'Chao Zhang', 'Yi Xu', 'Yi Pan', 'Bing Yin', 'Trishul Chilimbi']","When aligning large language models (LLMs), their performance on various tasks (such as being helpful, harmless, and honest) depends heavily on the composition of their training data. However, selecting a data mixture that achieves strong performance across all tasks is challenging. Existing approaches rely on large ablation studies, heuristics, or human intuition, but these can be prohibitively expensive and suboptimal. We study this problem in the setting of preference optimization via DPO and introduce AutoMixAlign (AMA), a theoretically-grounded algorithm that adaptively mixes datasets during training to balance performance across tasks. AMA first trains \textit{specialist models} for each task to determine losses that correspond to strong task performance. Then, it trains a generalist model using a novel minimax optimization that prioritizes tasks for which generalist model losses deviate most from specialist model losses. To optimize this problem, we propose two algorithms: (1) AMA-R, which adaptively reweights the objective to prioritize tasks, and (2) AMA-S, which adaptively adjusts how much data is sampled from each task to prioritize tasks. Both algorithms achieve a convergence rate of $O(1/\sqrt{T})$ in the convex case. AMA-R's convergence result follows from Sagawa et al. (2019), and we provide a convergence proof for AMA-S using online learning techniques such as EXP3. We evaluate AMA on several multitask alignment setups and find that AMA outperforms the standard alignment approach -- which simply optimizes the total loss across all tasks -- and also outperforms model merging methods.",AutoMixAlign Adaptive Data Mixing Multi Task Preference Optimization LLMs aligning large language models LLMs performance various tasks helpful harmless honest depends heavily composition training data selecting data mixture achieves strong performance tasks challenging Existing approaches rely large ablation studies heuristics human intuition prohibitively expensive suboptimal study problem setting preference optimization DPO introduce AutoMixAlign AMA theoretically grounded algorithm adaptively mixes datasets training balance performance tasks AMA trains textit specialist models task determine losses correspond strong task performance trains generalist model using novel minimax optimization prioritizes tasks generalist model losses deviate specialist model losses optimize problem propose algorithms 1 AMA R adaptively reweights objective prioritize tasks 2 AMA S adaptively adjusts data sampled task prioritize tasks algorithms achieve convergence rate O 1 sqrt T convex case AMA R s convergence result follows Sagawa et al 2019 provide convergence proof AMA S using online learning techniques EXP3 evaluate AMA multitask alignment setups AMA outperforms standard alignment approach simply optimizes total loss tasks outperforms model merging methods
1042,A Variational Approach for Mitigating Entity Bias in Relation Extraction,"['Samuel Mensah', 'Elena Kochkina', 'Jabez Magomere', 'Joy Prakash Sain', 'Simerjot Kaur', 'Charese Smiley']","Mitigating entity bias is a critical challenge in Relation Extraction (RE), where models often rely excessively on entities, resulting in poor generalization. This paper presents a novel approach to address this issue by adapting a Variational Information Bottleneck (VIB) framework. Our method compresses entity-specific information while preserving task-relevant features. It achieves state-of-the-art performance on relation extraction datasets across general, financial, and biomedical domains, in both indomain (original test sets) and out-of-domain (modified test sets with type-constrained entity replacements) settings. Our approach offers a robust, interpretable, and theoretically grounded methodology.",Variational Approach Mitigating Entity Bias Relation Extraction Mitigating entity bias critical challenge Relation Extraction models rely excessively entities resulting poor generalization paper presents novel approach address issue adapting Variational Information Bottleneck VIB framework method compresses entity specific information preserving task relevant features achieves state art performance relation extraction datasets general financial biomedical domains indomain original test sets domain modified test sets type constrained entity replacements settings approach offers robust interpretable theoretically grounded methodology
1043,Modelling Complex Semantics Relation with Contrastively Fine-Tuned Relational Encoders,"['Naïm Es-sebbani', 'Esteban Marquer', 'Zied Bouraoui']",,Modelling Complex Semantics Relation Contrastively Fine Tuned Relational Encoders
1044,Error-driven Data-efficient Large Multimodal Model Tuning,"['Barry Menglong Yao', 'Qifan Wang', 'Lifu Huang']","Large Multimodal Models (LMMs) have demonstrated impressive performance across numerous academic benchmarks. However, fine-tuning still remains essential to achieve satisfactory performance on downstream tasks, while the task-specific tuning samples are usually not readily available or expensive and time-consuming to obtain. To address this, we propose an error-driven data-efficient tuning framework that aims to efficiently adapt generic LMMs to newly emerging tasks without requiring any task-specific training samples. In our approach, a generic LMM, acting as a student model, is first evaluated on a small validation set of the target task, and then a more powerful model, acting as a teacher model, identifies the erroneous steps within the student model's reasoning steps and analyzes its capability gaps from fully addressing the target task. Based on these gaps, targeted training samples are further retrieved from existing task-agnostic datasets to tune the student model and tailor it to the target task. We perform extensive experiments across three different training data scales and seven tasks, demonstrating that our training paradigm significantly and efficiently improves LMM's performance on downstream tasks, achieving an average performance boost of 7.01%.",Error driven Data efficient Large Multimodal Model Tuning Large Multimodal Models LMMs demonstrated impressive performance numerous academic benchmarks fine tuning remains essential achieve satisfactory performance downstream tasks task specific tuning samples usually readily available expensive time consuming obtain address propose error driven data efficient tuning framework aims efficiently adapt generic LMMs newly emerging tasks requiring task specific training samples approach generic LMM acting student model evaluated small validation set target task powerful model acting teacher model identifies erroneous steps student model s reasoning steps analyzes capability gaps fully addressing target task Based gaps targeted training samples retrieved existing task agnostic datasets tune student model tailor target task perform extensive experiments different training data scales seven tasks demonstrating training paradigm significantly efficiently improves LMM s performance downstream tasks achieving average performance boost 7 01
1045,Planning with Diffusion Models for Target-Oriented Dialogue Systems,"['Hanwen Du', 'Bo Peng', 'Xia Ning']","Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance towards diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through https://anonymous.4open.science/r/DiffTOD.",Planning Diffusion Models Target Oriented Dialogue Systems Target Oriented Dialogue TOD remains significant challenge LLM era strategic dialogue planning crucial directing conversations specific targets existing dialogue planning methods generate dialogue plans step step sequential manner suffer compounding errors myopic actions address limitations introduce novel dialogue planning framework DiffTOD leverages diffusion models enable non sequential dialogue planning DiffTOD formulates dialogue planning trajectory generation problem conditional guidance leverages diffusion language model estimate likelihood dialogue trajectory optimize dialogue action strategies DiffTOD introduces tailored guidance mechanisms different target types offering flexible guidance diverse TOD targets test time Extensive experiments diverse TOD settings DiffTOD effectively perform non myopic lookahead exploration optimize action strategies long horizon non sequential dialogue planning demonstrates strong flexibility complex diverse dialogue scenarios code data accessible https anonymous 4open science r DiffTOD
1046,Interactive and Expressive Code-Augmented Planning with Large Language Models,"['Anthony Zhe Liu', 'Xinhe Wang', 'Jacob Sansom', 'Yao Fu', 'Jongwook Choi', 'Sungryull Sohn', 'Jaekyeom Kim', 'Honglak Lee']","Large Language Models (LLMs) demonstrate strong abilities in common-sense reasoning and interactive decision-making, but often struggle with complex, long-horizon planning tasks. Recent techniques have sought to structure LLM outputs using control flow and other code-adjacent techniques to improve planning performance. These techniques include using variables (to track important information) and functions (to divide complex tasks into smaller re-usable sub-tasks). However, purely code-based approaches can be error-prone and insufficient for handling ambiguous or unstructured data. To address these challenges, we propose REPL-Plan, an LLM planning approach that is fully code-expressive (it can utilize all the benefits of code) while also being dynamic (it can flexibly adapt from errors and use the LLM for fuzzy situations). In REPL-Plan, an LLM solves tasks by interacting with a Read-Eval-Print Loop (REPL), which iteratively executes and evaluates code, similar to language shells or interactive code notebooks, allowing the model to flexibly correct errors and handle tasks dynamically. We demonstrate that REPL-Plan achieves strong results across various planning domains compared to previous methods.",Interactive Expressive Code Augmented Planning Large Language Models Large Language Models LLMs demonstrate strong abilities common sense reasoning interactive decision making struggle complex long horizon planning tasks Recent techniques sought structure LLM outputs using control flow code adjacent techniques improve planning performance techniques include using variables track important information functions divide complex tasks smaller usable sub tasks purely code based approaches error prone insufficient handling ambiguous unstructured data address challenges propose REPL Plan LLM planning approach fully code expressive utilize benefits code dynamic flexibly adapt errors use LLM fuzzy situations REPL Plan LLM solves tasks interacting Read Eval Print Loop REPL iteratively executes evaluates code similar language shells interactive code notebooks allowing model flexibly correct errors handle tasks dynamically demonstrate REPL Plan achieves strong results various planning domains compared previous methods
1047,Synergistic Weak-Strong Collaboration by Aligning Preferences,"['Yizhu Jiao', 'Xuchao Zhang', 'Zhaoyang Wang', 'Yubo Ma', 'Zhun Deng', 'Rujia Wang', 'Chetan Bansal', 'Saravan Rajmohan', 'Jiawei Han', 'Huaxiu Yao']",,Synergistic Weak Strong Collaboration Aligning Preferences
1048,Understanding Silent Data Corruption in LLM Training,"['Jeffrey Jian Ma', 'Hengzhi Pei', 'Leonard Lausen', 'George Karypis']",,Understanding Silent Data Corruption LLM Training
1049,Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback,"['Guan-Ting Lin', 'Prashanth Gurunath Shivakumar', 'Aditya Gourav', 'Yile Gu', 'Ankur Gandhe', 'Hung-yi Lee', 'Ivan Bulyko']",,Align SLM Textless Spoken Language Models Reinforcement Learning AI Feedback
1050,"Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs","['Jungsoo Park', 'Junmo Kang', 'Gabriel Stanovsky', 'Alan Ritter']",,LLMs Help Uncover Insights LLMs Large Scale Evolving Literature Analysis Frontier LLMs
1051,BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data,"['Wenkai Li', 'Jiarui Liu', 'Andy Liu', 'Xuhui Zhou', 'Mona T. Diab', 'Maarten Sap']",,BIG5 CHAT Shaping LLM Personalities Training Human Grounded Data
1052,Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times,"['Olga Loginova', 'Sofía Ortega Loguinova']",,Deep Temporal Reasoning Video Language Models Cross Linguistic Evaluation Action Duration Completion Perfect Times
1053,Amplifying Trans and Nonbinary Voices: A Community-Centred Harm Taxonomy for LLMs,"['Eddie L. Ungless', 'Sunipa Dev', 'Cynthia L. Bennett', 'Rebecca Gulotta', 'Jasmijn Bastings', 'Remi Denton']",,Amplifying Trans Nonbinary Voices Community Centred Harm Taxonomy LLMs
1054,Enhancing Human Evaluation in Machine Translation with Comparative Judgement,"['Yixiao Song', 'Parker Riley', 'Daniel Deutsch', 'Markus Freitag']",,Enhancing Human Evaluation Machine Translation Comparative Judgement
1055,Infogen: Generating Complex Statistical Infographics from Documents,"['Akash Ghosh', 'Aparna Garimella', 'Pritika Ramu', 'Sambaran Bandyopadhyay', 'Sriparna Saha']",,Infogen Generating Complex Statistical Infographics Documents
1056,Partial Colexifications Improve Concept Embeddings,"['Arne Rubehn', 'Johann-Mattis List']",,Partial Colexifications Improve Concept Embeddings
1057,Improved Unbiased Watermark for Large Language Models,"['Ruibo Chen', 'Yihan Wu', 'Junfeng Guo', 'Heng Huang']",,Improved Unbiased Watermark Large Language Models
1058,MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection,"['Yixian Shen', 'Qi Bi', 'JIA-HONG HUANG', 'Hongyi Zhu', 'Andy D. Pimentel', 'Anuj Pathania']",,MaCP Minimal Mighty Adaptation Hierarchical Cosine Projection
1059,Multi-Attribute Steering of Language Models via Targeted Intervention,"['Duy Nguyen', 'Archiki Prasad', 'Elias Stengel-Eskin', 'Mohit Bansal']",,Multi Attribute Steering Language Models Targeted Intervention
1060,AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations,"['Gaurav Verma', 'Rachneet Kaur', 'Nishan Srishankar', 'Zhen Zeng', 'Tucker Balch', 'Manuela Veloso']",,AdaptAgent Adapting Multimodal Web Agents Shot Learning Human Demonstrations
1061,Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers,"['Zhijian Xu', 'Yilun Zhao', 'Manasi Patwardhan', 'Lovekesh Vig', 'Arman Cohan']",,LLMs Identify Critical Limitations Scientific Research Systematic Evaluation AI Research Papers
1062,On the Acquisition of Shared Grammatical Representations in Bilingual Language Models,"['Catherine Arnett', 'Tyler A. Chang', 'James A. Michaelov', 'Ben Bergen']",,Acquisition Shared Grammatical Representations Bilingual Language Models
1063,GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction,"['Mohammadtaha Bagherifard', 'Sahar Rajabi', 'Ali Edalat', 'Yadollah Yaghoobzadeh']",,GenKnowSub Improving Modularity Reusability LLMs General Knowledge Subtraction
1064,Using Shapley interactions to understand how models use structure,"['Diganta Misra', 'Divyansh Singhvi', 'Andrej Erkelens', 'Raghav Jain', 'Isabel Papadimitriou', 'Naomi Saphra']",,Using Shapley interactions understand models use structure
1065,Adversarial Tokenization,"['Renato Geh', 'Zilei Shao', 'Guy Van den Broeck']",,Adversarial Tokenization
1066,Classifying Unreliable Narrators with Large Language Models,"['Anneliese Brei', 'Katharine Henry', 'Abhisheik Sharma', 'Shashank Srivastava', 'Snigdha Chaturvedi']",,Classifying Unreliable Narrators Large Language Models
1067,ConceptCarve: Dynamic Realization of Evidence,"['Eylon Caplan', 'Dan Goldwasser']",,ConceptCarve Dynamic Realization Evidence
1068,QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering,"['An Quang Tang', 'Xiuzhen Zhang', 'Minh Ngoc Dinh', 'Zhuang Li']",,QQSUM Novel Task Model Quantitative Query Focused Summarization Review based Product Question Answering
1069,Navigating Rifts in Human-LLM Grounding: Study and Benchmark,"['Omar Shaikh', 'Hussein Mozannar', 'Gagan Bansal', 'Adam Fourney', 'Eric Horvitz']",,Navigating Rifts Human LLM Grounding Study Benchmark
1070,Substance over Style: Evaluating Proactive Conversational Coaching Agents,"['Vidya Srinivas', 'Xuhai Xu', 'Xin Liu', 'Kumar Ayush', 'Isaac Galatzer-Levy', 'Shwetak Patel', 'Daniel McDuff', 'Tim Althoff']",,Substance Style Evaluating Proactive Conversational Coaching Agents
1071,Open-World Planning via Lifted Regression with LLM-Inferred Affordances for Embodied Agents,"['Xiaotian Liu', 'Ali Pesaranghader', 'HANZE LI', 'Punyaphat Sukcharoenchaikul', 'Jaehong Kim', 'Tanmana Sadhu', 'Hyejeong Jeon', 'Scott Sanner']",,Open World Planning Lifted Regression LLM Inferred Affordances Embodied Agents
1072,(RSA)2: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding,"['Cesare Spinoso-Di Piano', 'David Eric Austin', 'Pablo Piantanida', 'Jackie CK Cheung']",,RSA 2 Rhetorical Strategy Aware Rational Speech Act Framework Figurative Language Understanding
1073,The Role of Abstract Representations and Observed Preferences in the Ordering of Binomials in Large Language Models,"['Zachary Nicholas Houghton', 'Kenji Sagae', 'Emily Morgan']",,Role Abstract Representations Observed Preferences Ordering Binomials Large Language Models
1074,SYNTHIA: Novel Concept Design with Affordance Composition,"['Hyeonjeong Ha', 'Xiaomeng Jin', 'Jeonghwan Kim', 'Jiateng Liu', 'Zhenhailong Wang', 'Khanh Duy Nguyen', 'Ansel Blume', 'Nanyun Peng', 'Kai-Wei Chang', 'Heng Ji']",,SYNTHIA Novel Concept Design Affordance Composition
1075,Consistent Client Simulation for Motivational Interviewing-based Counseling,"['Yizhe Yang', 'Palakorn Achananuparp', 'Heyan Huang', 'Jing Jiang', 'Nicholas Gabriel Lim', 'Cameron Tan Shi Ern', 'Phey Ling KIT', 'Jenny Giam Xiuhui', 'John Pinto', 'Ee-Peng Lim']",,Consistent Client Simulation Motivational Interviewing based Counseling
1076,AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context,"['Naba Rizvi', 'Harper Strickland', 'Daniel Gitelman', 'Tristan Cooper', 'Alexis Morales Flores', 'Aekta Kallepalli', 'Akshat Alurkar', 'Haaset Owens', 'Saleha Ahmedi', 'Isha Khirwadkar', 'Imani N. S. Munyaka', 'Nedjma Ousidhoum']",,AUTALIC Dataset Anti AUTistic Ableist Language Context
1077,Structural Reasoning Improves Molecular Understanding of LLM,"['Yunhui Jang', 'Jaehyung Kim', 'Sungsoo Ahn']",,Structural Reasoning Improves Molecular Understanding LLM
1078,CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration,"['Yizhe Yang', 'Palakorn Achananuparp', 'Heyan Huang', 'Jing Jiang', 'Phey Ling KIT', 'Nicholas Gabriel Lim', 'Cameron Tan Shi Ern', 'Ee-Peng Lim']",,CAMI Counselor Agent Supporting Motivational Interviewing State Inference Topic Exploration
1079,Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles,"['Kuang Wang', 'Xianfei Li', 'Shenghao Yang', 'Li Zhou', 'Feng Jiang', 'Haizhou Li']",,Know Better Modeling Human Like User Simulators Implicit Profiles
1080,Targeted Syntactic Evaluation for Grammatical Error Correction,"['Aomi Koyama', 'Masato Mita', 'Su-Youn Yoon', 'Yasufumi Takama', 'Mamoru Komachi']",,Targeted Syntactic Evaluation Grammatical Error Correction
1081,VQ-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos,"['Tingyu Song', 'Guo Gan', 'Tongyan Hu', 'Yilun Zhao']",,VQ Eval Evaluating Multimodal LLMs Generating Feedback AIGC Videos
1082,Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions,"['Joseph Suh', 'Erfan Jahanparast', 'Suhong Moon', 'Minwoo Kang', 'Serina Chang']",,Language Model Fine Tuning Scaled Survey Data Predicting Distributions Public Opinions
1083,SAD-LM: A Large-Scale Generalist Diffusion Language Model,"['Jaesung Tae', 'Hamish Ivison', 'Sachin Kumar', 'Arman Cohan']",,SAD LM Large Scale Generalist Diffusion Language Model
1084,Detecting LLM-Generated Korean Text through Linguistic Feature Analysis,"['Shinwoo Park', 'Shubin Kim', 'Do-Kyung Kim', 'Yo-Sub Han']",,Detecting LLM Generated Korean Text Linguistic Feature Analysis
1085,Uncovering the Impact of Chain-of-Thought Reasoning for Direct Preference Optimization: Lessons from Text-to-SQL,"['Hanbing Liu', 'Haoyang Li', 'Xiaokang Zhang', 'Ruotong Chen', 'Haiyong Xu', 'Tian Tian', 'Qi Qi', 'Jing Zhang']",,Uncovering Impact Chain Thought Reasoning Direct Preference Optimization Lessons Text SQL
1086,On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures,"['Minh Duc Bui', 'Kyung eun Park', 'Goran Glavaš', 'Fabian David Schmidt', 'Katharina von der Wense']",,Generalization Measurement Systems LLMs Entail Test Time Compute Underrepresented Cultures
1087,CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?,"['Aashish Anantha Ramakrishnan', 'Aadarsh Anantha Ramakrishnan', 'Dongwon Lee']",,CORDIAL Multimodal Large Language Models Effectively Understand Coherence Relationships
1088,Veracity Bias and Beyond: Uncovering LLMs’ Hidden Beliefs in Problem-Solving Reasoning,"['Yue Zhou', 'Barbara Di Eugenio']",,Veracity Bias Uncovering LLMs Hidden Beliefs Problem Solving Reasoning
1089,Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization,"['Meng Li', 'Guangda Huzhang', 'Haibo Zhang', 'Xiting Wang', 'Anxiang Zeng']",,Optimal Transport Based Token Weighting scheme Enhanced Preference Optimization
1090,LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study,"['Dongil Yang', 'Minjin Kim', 'Sunghwan Kim', 'Beong-woo Kwak', 'Minjun Park', 'Jinseok Hong', 'Woontack Woo', 'Jinyoung Yeo']",,LLM Meets Scene Graph Large Language Models Understand Generate Scene Graphs Benchmark Empirical Study
1091,Beyond Frameworks: Unpacking Collaboration Strategies in Multi-Agent Systems,"['Haochun Wang', 'Sendong Zhao', 'Jingbo Wang', 'Zewen Qiang', 'Bing Qin', 'Ting Liu']",,Frameworks Unpacking Collaboration Strategies Multi Agent Systems
1092,The Invisible Hand: Unveiling Provider Bias in Large Language Models for Code Generation,"['Xiaoyu Zhang', 'Juan Zhai', 'Shiqing Ma', 'Qingshuang Bao', 'Weipeng Jiang', 'Qian Wang', 'Chao Shen', 'Yang Liu']",,Invisible Hand Unveiling Provider Bias Large Language Models Code Generation
1093,K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly Offensive Language in Korean,"['Minkyeong Jeon', 'Hyemin Jeong', 'Yerang Kim', 'Jiyoung Kim', 'Jae Hyeon Cho', 'Byung-Jun Lee']",,K DA Automated Data Generation Pipeline Detoxifying Implicitly Offensive Language Korean
1094,THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation,"['Yunlong Liang', 'Fandong Meng', 'Jie Zhou']",,THOR MoE Hierarchical Task Guided Context Responsive Routing Neural Machine Translation
1095,Neuron Empirical Gradient: Discovering and Quantifying Neurons’ Global Linear Controllability,"['Xin Zhao', 'Zehui Jiang', 'Naoki Yoshinaga']",,Neuron Empirical Gradient Discovering Quantifying Neurons Global Linear Controllability
1096,Can third-parties read our emotions?,"['Jiayi Li', 'Yingfan Zhou', 'Pranav Narayanan Venkit', 'Halima Binte Islam', 'Sneha Arya', 'Shomir Wilson', 'Sarah Rajtmajer']",,parties read emotions
1097,OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching,"['Nghia Huynh Nguyen Hieu', 'Ngoc Son Nguyen', 'Huynh Nguyen Dang', 'Thieu Vo', 'Truong-Son Hy', 'Van Nguyen']",,OZSpeech step Zero shot Speech Synthesis Learned Prior Conditioned Flow Matching
1098,World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning,"['Siyin Wang', 'Zhaoye Fei', 'Qinyuan Cheng', 'Shiduo Zhang', 'Panpan Cai', 'Jinlan Fu', 'Xipeng Qiu']",,World Modeling Makes Better Planner Dual Preference Optimization Embodied Task Planning
1099,JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs,"['Junjie Chu', 'Yugeng Liu', 'Ziqing Yang', 'Xinyue Shen', 'Michael Backes', 'Yang Zhang']",,JailbreakRadar Comprehensive Assessment Jailbreak Attacks LLMs
1100,CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models,"['Xiaqiang Tang', 'Jian Li', 'Keyu Hu', 'nan du', 'Xiaolong Li', 'Xi Zhang', 'Weigao Sun', 'Sihong Xie']",,CogniBench Legal inspired Framework Dataset Assessing Cognitive Faithfulness Large Language Models
1101,Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models,"['Yuqiao Tan', 'Shizhu He', 'Kang Liu', 'Jun Zhao']",,Neural Incompatibility Unbridgeable Gap Cross Scale Parametric Knowledge Transfer Large Language Models
1102,Enhancing Mathematical Reasoning in LLMs by Stepwise Correction,"['Zhenyu Wu', 'Qingkai Zeng', 'Zhihan Zhang', 'Zhaoxuan Tan', 'Chao Shen', 'Meng Jiang']",,Enhancing Mathematical Reasoning LLMs Stepwise Correction
1103,PsyDial: A Large-scale Long-term Conversational Dataset for Mental Health Support,"['Huachuan Qiu', 'Zhenzhong Lan']",,PsyDial Large scale Long term Conversational Dataset Mental Health Support
1104,Enhancing Goal-oriented Proactive Dialogue Systems via Consistency Reflection and Correction,"['Didi Zhang', 'Yaxin Fan', 'Peifeng Li', 'Qiaoming Zhu']",,Enhancing Goal oriented Proactive Dialogue Systems Consistency Reflection Correction
1105,Exclusion of Thought: Mitigating Cognitive Load in Large Language Models for Enhanced Reasoning in Multiple-Choice Tasks,"['Qihang Fu', 'Yongbin Qin', 'Ruizhang Huang', 'Yanping Chen', 'Yulin Zhou', 'Lintao Long']",,Exclusion Thought Mitigating Cognitive Load Large Language Models Enhanced Reasoning Multiple Choice Tasks
1106,Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation,"['Zhi Qu', 'Yiran Wang', 'Jiannan Mao', 'Chenchen Ding', 'Hideki Tanaka', 'Masao Utiyama', 'Taro Watanabe']",,Registering Source Tokens Target Language Spaces Multilingual Neural Machine Translation
1107,VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search,"['Yikun Wang', 'Siyin Wang', 'Qinyuan Cheng', 'Zhaoye Fei', 'Liang Ding', 'Qipeng Guo', 'Dacheng Tao', 'Xipeng Qiu']",,VisuoThink Empowering LVLM Reasoning Multimodal Tree Search
1108,Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models,"['JianXing Liao', 'Junyan Xu', 'Yatao Sun', 'Maowen Tang', 'Sicheng He', 'Jingxian Liao', 'Shui Yu', 'Yun Li', 'Xiaohong Guan']",,Automated CAD Modeling Sequence Generation Text Descriptions Transformer Based Large Language Models
1109,LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint,"['Qianli Ma', 'Dongrui Liu', 'Qian Chen', 'Linfeng Zhang', 'Jing Shao']",,LED Merging Mitigating Safety Utility Conflicts Model Merging Location Election Disjoint
1110,"Dolphin: Moving Towards Closed-loop Auto-research through Thinking, Practice, and Feedback","['Jiakang Yuan', 'Xiangchao Yan', 'Bo Zhang', 'Tao Chen', 'Botian Shi', 'Wanli Ouyang', 'Yu Qiao', 'LEI BAI', 'Bowen Zhou']",,Dolphin Moving Closed loop Auto research Thinking Practice Feedback
1111,PerSphere: A Comprehensive Framework for Multi-Faceted Perspective Retrieval and Summarization,"['Yun Luo', 'Yingjie Li', 'Xiangkun Hu', 'Qinglin Qi', 'Fang Guo', 'Qipeng Guo', 'Zheng Zhang', 'Yue Zhang']",,PerSphere Comprehensive Framework Multi Faceted Perspective Retrieval Summarization
1112,Prompt-Guided Internal States for Hallucination Detection of Large Language Models,"['Fujie Zhang', 'Peiqi Yu', 'Biao Yi', 'Baolei Zhang', 'Tong Li', 'Zheli Liu']",,Prompt Guided Internal States Hallucination Detection Large Language Models
1113,Typology-Guided Adaptation for African NLP,['Ndapa Nakashole'],,Typology Guided Adaptation African NLP
1114,"Don’t Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections","['Orfeas Menis Mastromichalakis', 'Jason Liartis', 'Kristina Rose', 'Antoine Isaac', 'Giorgos Stamou']",,Don t Erase Inform Detecting Contextualizing Harmful Language Cultural Heritage Collections
1115,ECLM: Entity Level Language Model for Spoken Language Understanding with Chain of Intent,"['Shangjian Yin', 'Peijie Huang', 'JiaTian Chen', 'Haojing Huang', 'Yuhong Xu']",,ECLM Entity Level Language Model Spoken Language Understanding Chain Intent
1116,FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation,"['Qinggang Zhang', 'Zhishang Xiang', 'Yilin Xiao', 'Le Wang', 'Junhui Li', 'Jinsong Su', 'Xinrun Wang']",,FaithfulRAG Fact Level Conflict Modeling Context Faithful Retrieval Augmented Generation
1117,Knowledge Image Matters: Improving Knowledge-Based Visual Reasoning with Multi-Image Large Language Models,"['Guanghui Ye', 'Huan Zhao', 'Zhixue Zhao', 'Xupeng Zha', 'Yang Liu', 'Zhihua Jiang']",,Knowledge Image Matters Improving Knowledge Based Visual Reasoning Multi Image Large Language Models
1118,Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity,"['Yupu Hao', 'Pengfei Cao', 'Zhuoran Jin', 'Huanxuan Liao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao']",,Evaluating Personalized Tool Augmented LLMs Perspectives Personalization Proactivity
1119,GUICourse: From General Vision Language Model to Versatile GUI Agent,"['Wentong Chen', 'Junbo Cui', 'Jinyi Hu', 'Yujia Qin', 'Junjie Fang', 'Yue Zhao', 'Chongyi Wang', 'Jun Liu', 'Guirong Chen', 'Yupeng Huo', 'Yuan Yao', 'Yankai Lin', 'Zhiyuan Liu', 'Maosong Sun']",,GUICourse General Vision Language Model Versatile GUI Agent
1120,Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration,"['ChaeHun Park', 'Yujin Baek', 'Jaeseok Kim', 'Yu-Jung Heo', 'Du-Seong Chang', 'Jaegul Choo']",,Evaluating Visual Cultural Interpretation K Viscuit Benchmark Human VLM Collaboration
1121,Maximizing the Effectiveness of Larger BERT Models for Compression,"['Wen-Shu Fan', 'Su Lu', 'Shangyu Xing', 'Xin-Chun Li', 'De-Chuan Zhan']",,Maximizing Effectiveness Larger BERT Models Compression
1122,Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of LLMs on Formal Specification Inference,"['Thanh Le-Cong', 'Bach Le', 'Toby Murray']",,LLMs Reason Program Semantics Comprehensive Evaluation LLMs Formal Specification Inference
1123,HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring,"['Zhixiong Su', 'Yichen Wang', 'Herun Wan', 'Zhaohan Zhang', 'Minnan Luo']",,HACo Det Study Fine Grained Machine Generated Text Detection Human AI Coauthoring
1124,IndicSynth: A Large-Scale Multilingual Synthetic Speech Dataset for Low-Resource Indian Languages,"['Divya V Sharma', 'Vijval Ekbote', 'Anubha Gupta']",,IndicSynth Large Scale Multilingual Synthetic Speech Dataset Low Resource Indian Languages
1125,Reinforced IR: A Dual Reinforcement Framework For Domain-Adapted Information Retrieval,"['Chaofan Li', 'Jianlyu Chen', 'Yingxia Shao', 'Chaozhuo Li', 'Quanqing Xu', 'Defu Lian', 'Zheng Liu']",,Reinforced IR Dual Reinforcement Framework Domain Adapted Information Retrieval
1126,CoIR: A Comprehensive Benchmark for Code Information Retrieval Models,"['Xiangyang Li', 'Kuicai Dong', 'Yi Quan Lee', 'Wei Xia', 'Hao Zhang', 'Xinyi Dai', 'Yasheng Wang', 'Ruiming Tang']",,CoIR Comprehensive Benchmark Code Information Retrieval Models
1127,Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment,"['Delong Zeng', 'Yuexiang Xie', 'Yaliang Li', 'Ying Shen']",,Enhancing Multimodal Retrieval Complementary Information Extraction Alignment
1128,JoPA: Explaining Large Language Model’s Generation via Joint Prompt Attribution,"['Yurui Chang', 'Bochuan Cao', 'Yujia Wang', 'Jinghui Chen', 'Lu Lin']",,JoPA Explaining Large Language Model s Generation Joint Prompt Attribution
1129,Proxy-Driven Robust Multimodal Sentiment Analysis with Incomplete Data,"['Aoqiang Zhu', 'Min Hu', 'Xiaohua Wang', 'Jiaoyun Yang', 'Yiming Tang', 'Ning An']",,Proxy Driven Robust Multimodal Sentiment Analysis Incomplete Data
1130,Not All Terms Matter: Recall-Oriented Adaptive Learning for PLM-aided Query Expansion in Open-Domain Question Answering,"['Xinran Chen', 'Ben He', 'Xuanang Chen', 'Le Sun']",,Terms Matter Recall Oriented Adaptive Learning PLM aided Query Expansion Open Domain Question Answering
1131,A Mutual Information Perspective on Knowledge Graph Embedding,"['Jiang Li', 'Xiangdong Su', 'Zehua Duo', 'Tian Lan', 'Xiaotao Guo', 'Guanglai Gao']",,Mutual Information Perspective Knowledge Graph Embedding
1132,Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race,"['Lihao Sun', 'Chengzhi Mao', 'Valentin Hofmann', 'Xuechunzi Bai']",,Aligned Blind Alignment Increases Implicit Bias Reducing Awareness Race
1133,IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization,"['Xinghua Zhang', 'Haiyang Yu', 'ChengFu', 'Fei Huang', 'Yongbin Li']",,IOPO Empowering LLMs Complex Instruction Following Input Output Preference Optimization
1134,ProMALex: Progressive Modular Adapters for Multi-Jurisdictional Legal Language Modeling,"['Santosh T.Y.S.S', 'Mohamed Hesham Elganayni']",,ProMALex Progressive Modular Adapters Multi Jurisdictional Legal Language Modeling
1135,Flipping Knowledge Distillation: Leveraging Small Models’ Expertise to Enhance LLMs in Text Matching,"['Mingzhe Li', 'Jing Xiang', 'Qishen Zhang', 'Kaiyang Wan', 'Xiuying Chen']",,Flipping Knowledge Distillation Leveraging Small Models Expertise Enhance LLMs Text Matching
1136,Disentangling Language Medium and Culture Context for Evaluating Multilingual Large Language Models,"['Jiahao Ying', 'Wei Tang', 'Yiran Zhao', 'Yixin Cao', 'Yu Rong', 'Wenxuan Zhang']",,Disentangling Language Medium Culture Context Evaluating Multilingual Large Language Models
1137,Detecting Sockpuppetry on Wikipedia Using Meta-Learning,"['Christine de Kock', 'Luc Raszewski']",,Detecting Sockpuppetry Wikipedia Using Meta Learning
1138,Diversity-oriented Data Augmentation with Large Language Models,"['Zaitian Wang', 'Jinghan Zhang', 'Xinhao Zhang', 'Kunpeng Liu', 'pengfei wang', 'Yuanchun Zhou']",,Diversity oriented Data Augmentation Large Language Models
1139,Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs,"['Payal Mohapatra', 'Akash Pandey', 'Xiaoyuan Zhang', 'Qi Zhu']",,LLMs Understand Unvoiced Speech Exploring EMG Text Conversion LLMs
1140,CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation,"['Jingqian Zhao', 'Bingbing Wang', 'Geng Tu', 'Yice Zhang', 'Qianlong Wang', 'Bin Liang', 'Jing Li', 'Ruifeng Xu']",,CoreEval Automatically Building Contamination Resilient Datasets Real World Knowledge Reliable LLM Evaluation
1141,RiOT: Efficient Prompt Refinement with Residual Optimization Tree,"['Chenyi Zhou', 'Zhengyan Shi', 'Yuan Yao', 'Lei Liang', 'Huajun Chen', 'Qiang Zhang']",,RiOT Efficient Prompt Refinement Residual Optimization Tree
1142,Caution for the Environment: LLM Agents are Susceptible to Environmental Distractions,"['Xinbei Ma', 'Yiting Wang', 'Yao Yao', 'Tongxin Yuan', 'Aston Zhang', 'Zhuosheng Zhang', 'hai zhao']",,Caution Environment LLM Agents Susceptible Environmental Distractions
1143,Decoder-Only LLMs can be Masked Auto-Encoders,"['Dan Qiao', 'Yuan Gao', 'Zheming Yang', 'Di Yang', 'Ziheng Wu', 'Pengcheng Lu', 'Minghui Qiu', 'Juntao Li', 'Min Zhang']",,Decoder LLMs Masked Auto Encoders
1144,"Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark","['Rong-Cheng Tu', 'Zi-Ao Ma', 'Tian Lan', 'Yuehao Zhao', 'Heyan Huang', 'Xian-Ling Mao']",,Automatic Evaluation Text image Generation Task decomposed Framework Distilled Training Meta evaluation Benchmark
1145,Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering,"['Rongzhi Zhu', 'Xiangyu Liu', 'Zequn Sun', 'Yiwei Wang', 'Wei Hu']",,Mitigating Lost Retrieval Problems Retrieval Augmented Multi Hop Question Answering
1146,TableLoRA: Low-rank Adaptation on Table Structure Understanding for Large Language Models,"['Xinyi He', 'Yihao Liu', 'Mengyu Zhou', 'Yeye He', 'Haoyu Dong', 'Shi Han', 'Zejian Yuan', 'Dongmei Zhang']",,TableLoRA Low rank Adaptation Table Structure Understanding Large Language Models
1147,Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement,"['Maosongcao', 'Taolin Zhang', 'Mo Li', 'Chuyu Zhang', 'Yunxin Liu', 'Conghui He', 'Haodong Duan', 'Songyang Zhang', 'Kai Chen']",,Condor Enhance LLM Alignment Knowledge Driven Data Synthesis Refinement
1148,CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis,"['Ruixiang Feng', 'Shen Gao', 'Xiuying Chen', 'Lisi Chen', 'Shuo Shang']",,CulFiT Fine grained Cultural aware LLM Training Paradigm Multilingual Critique Data Synthesis
1149,Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis,"['Junzhuo Li', 'Bo Wang', 'Xiuze Zhou', 'Peijie Jiang', 'Jia Liu', 'Xuming Hu']",,Decoding Knowledge Attribution Mixture Experts Framework Basic Refinement Collaboration Efficiency Analysis
1150,ChartLens: Fine-grained Visual Attribution in Charts,"['Manan Suri', 'Puneet Mathur', 'Nedim Lipka', 'Franck Dernoncourt', 'Ryan A. Rossi', 'Dinesh Manocha']",,ChartLens Fine grained Visual Attribution Charts
1151,LESA: Learnable LLM Layer Scaling-Up,"['Yifei Yang', 'zouying cao', 'Xinbei Ma', 'Yao Yao', 'Zhi Chen', 'Libo Qin', 'hai zhao']",,LESA Learnable LLM Layer Scaling
1152,MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation,"['Haochen Xue', 'Feilong Tang', 'Ming Hu', 'Yexin Liu', 'Qidong Huang', 'Yulong Li', 'Chengzhi Liu', 'Zhongxing Xu', 'Chong Zhang', 'Chun-Mei Feng', 'Yutong Xie', 'Imran Razzak', 'Zongyuan Ge', 'Jionglong Su', 'Junjun He', 'Yu Qiao']",,MMRC Large Scale Benchmark Understanding Multimodal Large Language Model Real World Conversation
1153,Towards the Law of Capacity Gap in Distilling Language Models,"['Chen Zhang', 'Qiuchi Li', 'Dawei Song', 'Zheyu Ye', 'Yan Gao', 'Yao Hu']",,Law Capacity Gap Distilling Language Models
1154,WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning,"['Rajath Rao', 'Adithya V Ganesan', 'Oscar Kjell', 'Jonah Luby', 'Akshay Raghavan', 'Scott M. Feltman', 'Whitney Ringwald', 'Ryan L. Boyd', 'Benjamin J. Luft', 'Camilo J. Ruggero', 'Neville Ryant', 'ROMAN KOTOV', 'H. Schwartz']",,WhiSPA Semantically Psychologically Aligned Whisper Self Supervised Contrastive Student Teacher Learning
1155,Keys to Robust Edits: From Theoretical Insights to Practical Advances,"['Jianhao Yan', 'Futing Wang', 'Yun Luo', 'Yafu Li', 'Yue Zhang']",,Keys Robust Edits Theoretical Insights Practical Advances
1156,Boosting LLM’s Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning,"['Xiang Zhuang', 'Bin Wu', 'Jiyu Cui', 'Kehua Feng', 'Xiaotong Li', 'Huabin Xing', 'Keyan Ding', 'Qiang Zhang', 'Huajun Chen']",,Boosting LLM s Molecular Structure Elucidation Knowledge Enhanced Tree Search Reasoning
1157,MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation,"['María Andrea Cruz Blandón', 'Jayasimha Talur', 'Bruno Charron', 'Dong Liu', 'Saab Mansour', 'Marcello Federico']",,MEMERAG Multilingual End End Meta Evaluation Benchmark Retrieval Augmented Generation
1158,The Role of Visual Modality in Multimodal Mathematical Reasoning: Challenges and Insights,"['Yufang Liu', 'Yao Du', 'Tao Ji', 'Jianing Wang', 'Yang Liu', 'Yuanbin Wu', 'Aimin Zhou', 'Mengdi Zhang', 'Xunliang Cai']",,Role Visual Modality Multimodal Mathematical Reasoning Challenges Insights
1159,The Essence of Contextual Understanding in Theory of Mind: A Study on Question Answering with Story Characters,"['Chulun Zhou', 'Qiujing Wang', 'Mo Yu', 'Xiaoqian Yue', 'Rui Lu', 'Jiangnan Li', 'Yifan Zhou', 'Shunchi Zhang', 'Jie Zhou', 'Wai Lam']",,Essence Contextual Understanding Theory Mind Study Question Answering Story Characters
1160,S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning,"['Ruotian Ma', 'Peisong Wang', 'Cheng Liu', 'Xingyan Liu', 'Jiaqi Chen', 'Bang Zhang', 'Xin Zhou', 'nan du', 'Jia Li']",,S 2 R Teaching LLMs Self verify Self correct Reinforcement Learning
1161,Advancing Collaborative Debates with Role Differentiation through Multi-Agent Reinforcement Learning,"['Haoran Li', 'Ziyi Su', 'Yun Xue', 'Zhiliang Tian', 'YIPING SONG', 'Minlie Huang']",,Advancing Collaborative Debates Role Differentiation Multi Agent Reinforcement Learning
1162,Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation,"['Deokhyung Kang', 'Jeonghun Cho', 'Yejin Jeon', 'Sunbin Jang', 'Minsub Lee', 'JAWOON CHO', 'Gary Lee']",,Retrieval Augmented Fine Tuning Preference Optimization Visual Program Generation
1163,STRICTA: Structured Reasoning in Critical Text Assessment for Peer Review and Beyond,"['Nils Dycke', 'Matej Zečević', 'Ilia Kuznetsov', 'Beatrix Suess', 'Kristian Kersting', 'Iryna Gurevych']",,STRICTA Structured Reasoning Critical Text Assessment Peer Review
1164,XDAC: XAI-Driven Detection and Attribution of LLM-Generated News Comments in Korean,"['Wooyoung Go', 'Hyoungshick Kim', 'Alice Oh', 'Yongdae Kim']",,XDAC XAI Driven Detection Attribution LLM Generated News Comments Korean
1165,"CENTAUR: Bridging the Impossible Trinity of Privacy, Efficiency, and Performance in Privacy-Preserving Transformer Inference","['Jinglong Luo', 'Guanzhong Chen', 'Yehong Zhang', 'SHIYU LIU', 'Hui Wang', 'Yue Yu', 'Xun Zhou', 'Yuan Qi', 'Zenglin Xu']",,CENTAUR Bridging Impossible Trinity Privacy Efficiency Performance Privacy Preserving Transformer Inference
1166,"Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch","['Prarabdh Shukla', 'Wei Yin Chong', 'Yash Patel', 'Brennan Schaffner', 'Danish Pruthi', 'Arjun Bhagoji']",,Silencing Empowerment Allowing Bigotry Auditing Moderation Hate Speech Twitch
1167,EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models,"['Che Hyun Lee', 'Heeseung Kim', 'Jiheum Yeom', 'Sungroh Yoon']",,EdiText Controllable Coarse Fine Text Editing Diffusion Language Models
1168,TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages,"['Jafar Isbarov', 'Arofat Akhundjanova', 'Mammad Hajili', 'Kavsar Huseynova', 'Dmitry Gaynullin', 'Anar Rzayev', 'Osman Tursun', 'Aizirek Turdubaeva', 'Ilshat Saetov', 'Rinat Kharisov', 'Saule Belginova', 'Ariana Kenbayeva', 'Amina Alisheva', 'Abdullatif Köksal', 'SAMIR RUSTAMOV', 'Duygu Ataman']",,TUMLU Unified Native Language Understanding Benchmark Turkic Languages
1169,Look Both Ways and No Sink: Converting LLMs into Text Encoders without Training,"['Ziyong Lin', 'Haoyi Wu', 'Shu Wang', 'Kewei Tu', 'Zilong Zheng', 'Zixia Jia']",,Look Ways Sink Converting LLMs Text Encoders Training
1170,Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding,"['Zikai Xiao', 'Ziyang Wang', 'Wen MA', 'Yan Zhang', 'Wei Shen', 'WangYan', 'Luqi Gong', 'Zuozhu Liu']",,Mitigating Posterior Salience Attenuation Long Context LLMs Positional Contrastive Decoding
1171,A Statistical and Multi-Perspective Revisiting of the Membership Inference Attack in Large Language Models,"['Bowen Chen', 'Namgi Han', 'Yusuke Miyao']",,Statistical Multi Perspective Revisiting Membership Inference Attack Large Language Models
1172,Around the World in 24 Hours: Probing LLM Knowledge of Time and Place,"['Carolin Holtermann', 'Paul Röttger', 'Anne Lauscher']",,World 24 Hours Probing LLM Knowledge Time Place
1173,Mining the uncertainty patterns of humans and models in the annotation of moral foundations and human values,"['Neele Falk', 'Gabriella Lapesa']",,Mining uncertainty patterns humans models annotation moral foundations human values
1174,“What do you call a dog that is incontrovertibly true? Dogma’’: Testing LLM Generalization through Humor,"['Alessio Cocchieri', 'Luca Ragazzi', 'Paolo Italiani', 'Giuseppe Tagliavini', 'Gianluca Moro']",,dog incontrovertibly true Dogma Testing LLM Generalization Humor
1175,Towards Harmonized Uncertainty Estimation for Large Language Models,"['Rui Li', 'Jing Long', 'Muge Qi', 'Heming Xia', 'Lei Sha', 'Peiyi Wang', 'Zhifang Sui']",,Harmonized Uncertainty Estimation Large Language Models
1176,VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare,"['Anudeex Shetty', 'Amin Beheshti', 'Mark Dras', 'Usman Naseem']",,VITAL New Dataset Benchmarking Pluralistic Alignment Healthcare
1177,Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media,"['Zhen Sun', 'Zongmin Zhang', 'Xinyue Shen', 'Ziyi Zhang', 'Yule Liu', 'Michael Backes', 'Yang Zhang', 'Xinlei He']",,AI Generated Text World Quantifying Monitoring AIGT Social Media
1178,From English to Second Language Mastery: Enhancing LLMs with Cross-Lingual Continued Instruction Tuning,"['Linjuan Wu', 'Hao-Ran Wei', 'Baosong Yang', 'Weiming Lu']",,English Second Language Mastery Enhancing LLMs Cross Lingual Continued Instruction Tuning
1179,WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks,"['Anudeex Shetty', 'Qiongkai Xu', 'Jey Han Lau']",,WET Overcoming Paraphrasing Vulnerabilities Embeddings Service Linear Transformation Watermarks
1180,HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation,"['Yuhan Chen', 'Ang Lv', 'Jian Luan', 'Bin Wang', 'Wei Liu']",,HoPE Novel Positional Encoding Long Term Decay Enhanced Context Awareness Extrapolation
1181,One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments,"['Ke Yi', 'Yuhui Xu', 'Heng Chang', 'Yuan Meng', 'Tong Zhang', 'Jia Li']",,QuantLLM Fine tuning Quantized LLMs Efficient Deployments
1182,Beyond Logits: Aligning Feature Dynamics for Effective Knowledge Distillation,"['Guoqiang Gong', 'Jiaxing Wang', 'Jin Xu', 'Deping Xiang', 'Zicheng Zhang', 'Leqi Shen', 'Yifeng Zhang', 'JunhuaShu', 'ZhaolongXing', 'Zhen Chen', 'Pengzhang Liu', 'Ke Zhang']",,Logits Aligning Feature Dynamics Effective Knowledge Distillation
1183,Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention,"['Jingyang Yuan', 'Huazuo Gao', 'Damai Dai', 'Junyu Luo', 'Liang Zhao', 'Zhengyan Zhang', 'Zhenda Xie', 'Yuxing Wei', 'Lean Wang', 'Zhiping Xiao', 'Yuqing Wang', 'Chong Ruan', 'Ming Zhang', 'Wenfeng Liang', 'Wangding Zeng']",,Native Sparse Attention Hardware Aligned Natively Trainable Sparse Attention
1184,DRAE: Dynamic Retrieval-Augmented Expert Networks for Lifelong Learning and Task Adaptation in Robotics,"['Yayu Long', 'Kewei Chen', 'Long Jin', 'Mingsheng Shang']",,DRAE Dynamic Retrieval Augmented Expert Networks Lifelong Learning Task Adaptation Robotics
1185,MT-RAIG: Novel Benchmark and Evaluation Framework for Retrieval-Augmented Insight Generation over Multiple Tables,"['Kwangwook Seo', 'Donguk Kwon', 'Dongha Lee']",,MT RAIG Novel Benchmark Evaluation Framework Retrieval Augmented Insight Generation Multiple Tables
1186,Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning,"['Chenxi Huang', 'Shaotian Yan', 'Liang Xie', 'Binbin Lin', 'Sinan Fan', 'Yue Xin', 'Deng Cai', 'Chen Shen', 'Jieping Ye']",,Enhancing Chain Thought Reasoning Critical Representation Fine tuning
1187,Does the Emotional Understanding of LVLMs Vary Under High-Stress Environments and Across Different Demographic Attributes?,"['Jaewook Lee', 'Yeajin Jang', 'Oh-Woog KWON', 'Harksoo Kim']",,Does Emotional Understanding LVLMs Vary High Stress Environments Different Demographic Attributes
1188,S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling,"['Suman Adhya', 'Debarshi Kumar Sanyal']",,S2WTM Spherical Sliced Wasserstein Autoencoder Topic Modeling
1189,Learning to Look at the Other Side: A Probing Study of Word Semantics in LLMs with Enabled Bidirectional Attention,"['Zhaoxin Feng', 'MA Jianfei', 'Xiaoyi Bao', 'Xiaojing Zhao', 'Emmanuele Chersoni']",,Learning Look Probing Study Word Semantics LLMs Enabled Bidirectional Attention
1190,Tracing and Dissecting How LLMs Recall Factual Knowledge for Real World Questions,"['Yiqun Wang', 'Chaoqun Wan', 'Sile Hu', 'Yonggang Zhang', 'Xiang Tian', 'Yaowu Chen', 'Xu Shen', 'Jieping Ye']",,Tracing Dissecting LLMs Recall Factual Knowledge Real World Questions
1191,Employing Discourse Coherence Enhancement to Improve Cross-Document Event and Entity Coreference Resolution,"['Xinyu Chen', 'Peifeng Li', 'Qiaoming Zhu']",,Employing Discourse Coherence Enhancement Improve Cross Document Event Entity Coreference Resolution
1192,Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning,"['Shaobo Wang', 'Xiangqi Jin', 'Ziming Wang', 'Jize Wang', 'Jiajun Zhang', 'Kaixin Li', 'Zichen Wen', 'Zhong Li', 'Conghui He', 'Xuming Hu', 'Linfeng Zhang']",,Data Whisperer Efficient Data Selection Task Specific LLM Fine Tuning Shot Context Learning
1193,Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation,"['Shuo Tang', 'Xianghe Pang', 'Zexi Liu', 'Bohan Tang', 'Rui Ye', 'Tian Jin', 'Xiaowen Dong', 'Yanfeng Wang', 'Siheng Chen']",,Synthesizing Post Training Data LLMs Multi Agent Simulation
1194,SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs,"['Yige Xu', 'Xu Guo', 'Zhiwei Zeng', 'Chunyan Miao']",,SoftCoT Soft Chain Thought Efficient Reasoning LLMs
1195,FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning,"['Seunghee Kim', 'Changhyeon Kim', 'Taeuk Kim']",,FCMR Robust Evaluation Financial Cross Modal Multi Hop Reasoning
1196,Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms,"['Mengru Wang', 'Ziwen Xu', 'Shengyu Mao', 'Shumin Deng', 'Zhaopeng Tu', 'Huajun Chen', 'Ningyu Zhang']",,Prompt Engineering Robust Behavior Control LLMs Steering Target Atoms
1197,MobiLoRA: Accelerating LoRA-based LLM Inference on Mobile Devices via Context-aware KV Cache Optimization,"['Borui Li', 'Yitao Wang', 'Haoran Ma', 'Ligeng Chen', 'Jun Xiao', 'Shuai Wang']",,MobiLoRA Accelerating LoRA based LLM Inference Mobile Devices Context aware KV Cache Optimization
1198,Language Models Resist Alignment: Evidence From Data Compression,"['Jiaming Ji', 'Kaile Wang', 'Tianyi Qiu', 'Boyuan Chen', 'Jiayi Zhou', 'Changye Li', 'Hantao Lou', 'Josef Dai', 'Yunhuai Liu', 'Yaodong Yang']",,Language Models Resist Alignment Evidence Data Compression
1199,Beyond the Answer: Advancing Multi-Hop QA with Fine-Grained Graph Reasoning and Evaluation,"['Qichuan Liu', 'Chentao Zhang', 'Chenfeng Zheng', 'Guosheng Hu', 'Xiaodong Li', 'Zhihong Zhang']",,Answer Advancing Multi Hop QA Fine Grained Graph Reasoning Evaluation
1200,Mamba Knockout for Unraveling Factual Information Flow,"['Nir Endy', 'Idan Daniel Grosbard', 'Yuval Ran-Milo', 'Yonatan Slutzky', 'Itay Tshuva', 'Raja Giryes']",,Mamba Knockout Unraveling Factual Information Flow
1201,"Small Changes, Big Impact: How Manipulating a Few Neurons Can Drastically Alter LLM Aggression","['Jaewook Lee', 'Junseo Jang', 'Oh-Woog KWON', 'Harksoo Kim']",,Small Changes Big Impact Manipulating Neurons Drastically Alter LLM Aggression
1202,Towards Widening The Distillation Bottleneck for Reasoning Models,"['Huifeng Yin', 'Yu Zhao', 'Minghao Wu', 'Xuanfan Ni', 'Bo Zeng', 'huaiyu.wh', 'Tianqi Shi', 'Liangying Shao', 'Chenyang Lyu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang']",,Widening Distillation Bottleneck Reasoning Models
1203,Curiosity-Driven Reinforcement Learning from Human Feedback,"['Haoran Sun', 'Yekun Chai', 'Shuohuan Wang', 'Yu Sun', 'Hua Wu', 'Haifeng Wang']",,Curiosity Driven Reinforcement Learning Human Feedback
1204,T2A-Feedback: Improving Basic Capabilities of Text-to-Audio Generation via Fine-grained AI Feedback,"['Zehan Wang', 'Ke Lei', 'Chen Zhu', 'Jiawei Huang', 'Sashuai zhou', 'Luping Liu', 'Xize Cheng', 'Shengpeng Ji', 'Zhenhui Ye', 'Tao Jin', 'Zhou Zhao']",,T2A Feedback Improving Basic Capabilities Text Audio Generation Fine grained AI Feedback
1205,CoE: A Clue of Emotion Framework for Emotion Recognition in Conversations,"['Zhiyu Shen', 'Yunhe Pang', 'Yanghui Rao', 'Jianxing Yu']",,CoE Clue Emotion Framework Emotion Recognition Conversations
1206,MPO: Multilingual Safety Alignment via Reward Gap Optimization,"['Weixiang Zhao', 'Yulin Hu', 'Yang Deng', 'Tongtong Wu', 'Wenxuan Zhang', 'Jiahe Guo', 'An Zhang', 'Yanyan Zhao', 'Bing Qin', 'Tat-Seng Chua', 'Ting Liu']",,MPO Multilingual Safety Alignment Reward Gap Optimization
1207,QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions,"['Siyin Wang', 'Wenyi Yu', 'Xianzhao Chen', 'Xiaohai Tian', 'Jun Zhang', 'Lu Lu', 'Yu Tsao', 'Junichi Yamagishi', 'Yuxuan Wang', 'Chao Zhang']",,QualiSpeech Speech Quality Assessment Dataset Natural Language Reasoning Descriptions
1208,"On the Relation Between Fine-Tuning, Topological Properties, and Task Performance in Sense-Enhanced Embeddings","['Deniz Ekin Yavas', 'Timothée Bernard', 'Benoit Crabbé', 'Laura Kallmeyer']",,Relation Fine Tuning Topological Properties Task Performance Sense Enhanced Embeddings
1209,Finding Needles in Images: Can Multi-modal LLMs Locate Fine Details?,"['Parth Thakkar', 'Ankush Agarwal', 'Prasad Kasu', 'Pulkit Bansal', 'Chaitanya Devaguptapu']",,Finding Needles Images Multi modal LLMs Locate Fine Details
1210,Don’t Half-listen: Capturing Key-part Information in Continual Instruction Tuning,"['Yongquan He', 'Wenyuan Zhang', 'Xuancheng Huang', 'peng zhang', 'Lingxun Meng', 'Xiang Zhou', 'Ke Zeng', 'Xunliang Cai']",,Don t Half listen Capturing Key Information Continual Instruction Tuning
1211,Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction,"['Yooseop Lee', 'Suin Kim', 'Yohan Jo']",,Generating Plausible Distractors Multiple Choice Questions Student Choice Prediction
1212,Exploring Explanations Improves the Robustness of In-Context Learning,"['Ukyo Honda', 'Tatsushi Oka']",,Exploring Explanations Improves Robustness Context Learning
1213,Prediction Hubs are Context-Informed Frequent Tokens in LLMs,"['Beatrix Miranda Ginn Nielsen', 'Iuri Macocco', 'Marco Baroni']",,Prediction Hubs Context Informed Frequent Tokens LLMs
1214,Capability Salience Vector: Fine-grained Alignment of Loss and Capabilities for Downstream Task Scaling Law,"['Qiming Ge', 'Shuhao Xing', 'Songyang Gao', 'Yunhua Zhou', 'Yicheng Zou', 'Songyang Zhang', 'Zhi Chen', 'Hang Yan', 'Qi Zhang', 'Qipeng Guo', 'Kai Chen']",,Capability Salience Vector Fine grained Alignment Loss Capabilities Downstream Task Scaling Law
1215,"CRUXEVAL-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution","['Ruiyang Xu', 'Jialun Cao', 'Yaojie Lu', 'Ming Wen', 'Hongyu Lin', 'Xianpei Han', 'Ben He', 'Shing-Chi Cheung', 'Le Sun']",,CRUXEVAL X Benchmark Multilingual Code Reasoning Understanding Execution
1216,Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs,"['Haozhen Zhang', 'Tao Feng', 'Jiaxuan You']",,Graph Records Boosting Retrieval Augmented Generation Long context Summarization Graphs
1217,Rubrik’s Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset,"['Diana Galvan-Sosa', 'Gabrielle Gaudeau', 'Pride Kavumba', 'Yunmeng Li', 'Hongyi gu', 'Zheng Yuan', 'Keisuke Sakaguchi', 'Paula Buttery']",,Rubrik s Cube Testing New Rubric Evaluating Explanations CUBE dataset
1218,A Dual-Mind Framework for Strategic and Expressive Negotiation Agent,"['Yutong Liu', 'Lida Shi', 'Rui Song', 'Hao Xu']",,Dual Mind Framework Strategic Expressive Negotiation Agent
1219,Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models,"['Junjie Wu', 'Gefei Gu', 'Yanan Zheng', 'Dit-Yan Yeung', 'Arman Cohan']",,Ref Long Benchmarking Long context Referencing Capability Long context Language Models
1220,Revisiting Scaling Laws for Language Models: The Role of Data Quality and Training Strategies,"['Zhengyu Chen', 'Siqi Wang', 'Teng Xiao', 'Yudong Wang', 'Shiqi Chen', 'Xunliang Cai', 'Junxian He', 'Jingang Wang']",,Revisiting Scaling Laws Language Models Role Data Quality Training Strategies
1221,"Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments","['Marc Feger', 'Katarina Boland', 'Stefan Dietze']",,Limited Generalizability Argument Mining State Art Models Learn Datasets Arguments
1222,Enhancing Machine Translation with Self-Supervised Preference Data,"['Haoxiang Sun', 'Ruize Gao', 'Pei Zhang', 'Baosong Yang', 'Rui Wang']",,Enhancing Machine Translation Self Supervised Preference Data
1223,Unveil: Unified Visual-Textual Integration and Distillation for Multi-modal Document Retrieval,"['Hao Sun', 'Yingyan Hou', 'Jiayan Guo', 'Bo Wang', 'Chunyu Yang', 'Jinsong Ni', 'Yan Zhang']",,Unveil Unified Visual Textual Integration Distillation Multi modal Document Retrieval
1224,Don’t Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls,"['Ante Wang', 'Linfeng Song', 'Ye Tian', 'Dian Yu', 'Haitao Mi', 'Xiangyu Duan', 'Zhaopeng Tu', 'Jinsong Su', 'Dong Yu']",,Don t Lost Trees Streamlining LLM Reasoning Overcoming Tree Search Exploration Pitfalls
1225,MEXMA: Token-level objectives improve sentence representations,"['João Maria Janeiro', 'Benjamin Piwowarski', 'Patrick Gallinari', 'Loic Barrault']",,MEXMA Token level objectives improve sentence representations
1226,Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs,"['Xuan Zhang', 'Cunxiao Du', 'Sicheng Yu', 'Jiawei Wu', 'Fengzhuo Zhang', 'Wei Gao', 'Qian Liu']",,Sparse Dense Free Lunch Lossless Acceleration Video Understanding LLMs
1227,Uncertainty-Aware Iterative Preference Optimization for Enhanced LLM Reasoning,"['Lei Li', 'Hehuan Liu', 'Yaxin Zhou', 'ZhaoYang Gui', 'Xudong Weng', 'Yi YUAN', 'Zheng Wei', 'Zang Li']",,Uncertainty Aware Iterative Preference Optimization Enhanced LLM Reasoning
1228,AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration,"['Zhexuan Wang', 'Yutong Wang', 'Xuebo Liu', 'Liang Ding', 'Miao Zhang', 'Jie Liu', 'Min Zhang']",,AgentDropout Dynamic Agent Elimination Token Efficient High Performance LLM Based Multi Agent Collaboration
1229,Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States,"['Yang Xiao', 'Jiashuo WANG', 'Qiancheng Xu', 'Changhe Song', 'Chunpu Xu', 'Yi Cheng', 'Wenjie Li', 'Pengfei Liu']",,Dynamic Theory Mind Evaluating LLM Adaptation Temporal Evolution Human States
1230,M-IFEval: On Multilingual Instruction-Following Capability of Large Language Models,"['Bo Zeng', 'Chenyang Lyu', 'Sinuo Liu', 'Mingyan Zeng', 'Minghao Wu', 'Xuanfan Ni', 'Tianqi Shi', 'Yu Zhao', 'Yefeng Liu', 'Chenyu Zhu', 'Ruizhe Li', 'Jiahui Geng', 'Qing Li', 'Yu Tong', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang']",,M IFEval Multilingual Instruction Following Capability Large Language Models
1231,Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results,"['Andrea Santilli', 'Adam Golinski', 'Michael Kirchhof', 'Federico Danieli', 'Arno Blaas', 'Miao Xiong', 'Luca Zappella', 'Sinead Williamson']",,Revisiting Uncertainty Quantification Evaluation Language Models Spurious Interactions Response Length Bias Results
1232,Representation Bending for Large Language Model Safety,"['Ashkan Yousefpour', 'Taeheon Kim', 'Ryan Sungmo Kwon', 'Seungbeen Lee', 'Wonje Jeung', 'Seungju Han', 'Alvin Wan', 'Harrison Ngan', 'Youngjae Yu', 'Jonghyun Choi']",,Representation Bending Large Language Model Safety
1233,Analyzing LLMs’ Cognition of Knowledge Boundary Across Languages Through the Lens of Internal Representation,"['Chenghao Xiao', 'Hou Pong Chan', 'Hao Zhang', 'Mahani Aljunied', 'Lidong Bing', 'Noura Al Moubayed', 'Yu Rong']",,Analyzing LLMs Cognition Knowledge Boundary Languages Lens Internal Representation
1234,Enhancing Retrieval-Augmented Generation via Evidence Tree Search,"['Hao Sun', 'Hengyi Cai', 'Yuchen Li', 'Xuanbo Fan', 'Xiaochi Wei', 'Shuaiqiang Wang', 'Yan Zhang', 'Dawei Yin']",,Enhancing Retrieval Augmented Generation Evidence Tree Search
1235,HalluLens: LLM Hallucination Benchmark,"['Yejin Bang', 'Ziwei Ji', 'Alan Schelten', 'Anthony Hartshorn', 'Tara Fowler', 'Cheng Zhang', 'Nicola Cancedda', 'Pascale Fung']",,HalluLens LLM Hallucination Benchmark
1236,DEEPER Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling,"['Aili Chen', 'Chengyu Du', 'Jiangjie Chen', 'Jinghan Xu', 'Yikai Zhang', 'Siyu Yuan', 'Zulong Chen', 'Liangyue Li', 'Yanghua Xiao']",,DEEPER Insight User Directed Persona Refinement Dynamic Persona Modeling
1237,Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models,"['Jie Liu', 'Wenxuan Wang', 'SU Yihang', 'Jingyuan Huang', 'Yudi Zhang', 'Cheng-Yi Li', 'Wenting Chen', 'Xiaohan Xing', 'Kao-Jung Chang', 'Linlin Shen', 'Michael R. Lyu']",,Asclepius Spectrum Evaluation Benchmark Medical Multi Modal Large Language Models
1238,InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning,"['Zifu Wan', 'Yaqi Xie', 'Ce Zhang', 'Zhiqiu Lin', 'Zihan Wang', 'Simon Stepputtis', 'Deva Ramanan', 'Katia P. Sycara']",,InstructPart Task Oriented Segmentation Instruction Reasoning
1239,GRaMPa: Subword Regularisation by Skewing Uniform Segmentation Distributions with an Efficient Path-counting Markov Model,"['Thomas Bauwens', 'David Kaczér', 'Miryam de Lhoneux']",,GRaMPa Subword Regularisation Skewing Uniform Segmentation Distributions Efficient Path counting Markov Model
1240,Evaluating the Evaluation of Diversity in Commonsense Generation,"['Tianhui Zhang', 'Bei Peng', 'Danushka Bollegala']",,Evaluating Evaluation Diversity Commonsense Generation
1241,"Generate First, Then Sample: Enhancing Fake News Detection with LLM-Augmented Reinforced Sampling","['Zhao Tong', 'Yimeng Gu', 'Huidong Liu', 'Qiang Liu', 'Shu Wu', 'Haichao Shi', 'Xiao-Yu Zhang']",,Generate Sample Enhancing Fake News Detection LLM Augmented Reinforced Sampling
1242,ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data,"['Yu Zhang', 'Ruijie Yu', 'Jidong Tian', 'Feng Zhu', 'Jiapeng Liu', 'Xiaokang Yang', 'Yaohui Jin', 'Yanyan Xu']",,ChemActor Enhancing Automated Extraction Chemical Synthesis Actions LLM Generated Data
1243,Towards Fully Exploiting LLM Internal States to Enhance Knowledge Boundary Perception,"['Shiyu Ni', 'Keping Bi', 'Jiafeng Guo', 'Lulu Yu', 'Baolong Bi', 'Xueqi Cheng']",,Fully Exploiting LLM Internal States Enhance Knowledge Boundary Perception
1244,ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment and Generation,"['Yiyi Chen', 'Qiongkai Xu', 'Johannes Bjerva']",,ALGEN shot Inversion Attacks Textual Embeddings using Alignment Generation
1245,Decoding on Graphs: Faithful and Sound Reasoning on Knowledge Graphs through Generation of Well-Formed Chains,"['Kun LI', 'Tianhua Zhang', 'Xixin Wu', 'Hongyin Luo', 'James R. Glass', 'Helen M. Meng']",,Decoding Graphs Faithful Sound Reasoning Knowledge Graphs Generation Formed Chains
1246,STaR-SQL: Self-Taught Reasoner for Text-to-SQL,"['Mingqian He', 'Yongliang Shen', 'Wenqi Zhang', 'Qiuying Peng', 'Jun Wang', 'Weiming Lu']",,STaR SQL Self Taught Reasoner Text SQL
1247,Fairness Beyond Performance: Investigating Reliability Disparities Across Groups in Legal NLP,"['Santosh T.Y.S.S', 'Irtiza Chowdhury']",,Fairness Performance Investigating Reliability Disparities Groups Legal NLP
1248,Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning Data Selection,"['Yang Zhao', 'Li Du', 'Xiao Ding', 'Yangou Ouyang', 'Hepeng Wang', 'Kai Xiong', 'Jinglong Gao', 'Zhouhao Sun', 'Dongliang Xu', 'Qing Yang', 'Dongchen Li', 'Bing Qin', 'Ting Liu']",,Similarity Gradient based Graph Method Instruction Tuning Data Selection
1249,FastMCTS: A Simple Sampling Strategy for Data Synthesis,"['Peiji Li', 'Kai Lv', 'Yunfan Shao', 'Yichuan Ma', 'Linyang Li', 'Xiaoqing Zheng', 'Xipeng Qiu', 'Qipeng Guo']",,FastMCTS Simple Sampling Strategy Data Synthesis
1250,Dialogue-RAG: Enhancing Retrieval for LLMs via Node-Linking Utterance Rewriting,"['Qiwei Li', 'Teng Xiao', 'Zuchao Li', 'Ping Wang', 'Mengjia Shen', 'hai zhao']",,Dialogue RAG Enhancing Retrieval LLMs Node Linking Utterance Rewriting
1251,"Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent","['Ethan Wilcox', 'Cui Ding', 'Giovanni Acampa', 'Tiago Pimentel', 'Alex Warstadt', 'Tamar I Regev']",,Using Information Theory Characterize Prosodic Typology Case Tone Pitch Accent Stress Accent
1252,Evaluating LLMs for Portuguese Sentence Simplification with Linguistic Insights,"['ARTHUR MARIANO ROCHA DE AZEVEDO SCALERCIO', 'Elvis A. de Souza', 'Maria José Bocorny Finatto', 'Aline Paes']",,Evaluating LLMs Portuguese Sentence Simplification Linguistic Insights
1253,LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models,"['Hugo Pitorro', 'Marcos Vinicius Treviso']",,LaTIM Measuring Latent Token Token Interactions Mamba Models
1254,Memorization Inheritance in Sequence-Level Knowledge Distillation for Neural Machine Translation,"['Verna Dankers', 'Vikas Raunak']",,Memorization Inheritance Sequence Level Knowledge Distillation Neural Machine Translation
1255,Improving Low-Resource Morphological Inflection via Self-Supervised Objectives,"['Adam Wiemerslage', 'Katharina von der Wense']",,Improving Low Resource Morphological Inflection Self Supervised Objectives
1256,Don’t Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation,"['Yingchaojie Feng', 'Yiqun Sun', 'Yandong Sun', 'Minfeng Zhu', 'Qiang Huang', 'Anthony Kum Hoe Tung', 'Wei Chen']",,Don t Reinvent Wheel Efficient Instruction Following Text Embedding based Guided Space Transformation
1257,BOOKCOREF: Coreference Resolution at Book Scale,"['Giuliano Martinelli', 'Tommaso Bonomo', 'Pere-Lluís Huguet Cabot', 'Roberto Navigli']",,BOOKCOREF Coreference Resolution Book Scale
1258,OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval,"['Wei Yang', 'Jingjing Fu', 'Rui Wang', 'Jinyu Wang', 'Lei Song', 'Jiang Bian']",,OMGM Orchestrate Multiple Granularities Modalities Efficient Multimodal Retrieval
1259,Alleviating Hallucinations from Knowledge Misalignment in Large Language Models via Selective Abstention Learning,"['Lei Huang', 'Xiaocheng Feng', 'Weitao Ma', 'Yuchun Fan', 'Xiachong Feng', 'Yuxuan Gu', 'Yangfan Ye', 'Liang Zhao', 'Weihong Zhong', 'Baoxin Wang', 'Dayong Wu', 'Guoping Hu', 'Lingpeng Kong', 'Tong Xiao', 'Ting Liu', 'Bing Qin']",,Alleviating Hallucinations Knowledge Misalignment Large Language Models Selective Abstention Learning
1260,Retrospective Learning from Interactions,"['Zizhao Chen', 'Mustafa Omer Gul', 'Yiwei Chen', 'Gloria Geng', 'Anne Wu', 'Yoav Artzi']",,Retrospective Learning Interactions
1261,Personalized Generation In Large Model Era: A Survey,"['Yiyan Xu', 'Jinghao Zhang', 'Alireza Salemi', 'Xinting Hu', 'Wenjie Wang', 'Fuli Feng', 'Hamed Zamani', 'Xiangnan He', 'Tat-Seng Chua']",,Personalized Generation Large Model Era Survey
1262,Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning,"['Junqi Gao', 'Xiang Zou', 'Ying Ai', 'Dong Li', 'Yichen Niu', 'Biqing Qi', 'Jianxing Liu']",,Graph Counselor Adaptive Graph Exploration Multi Agent Synergy Enhance LLM Reasoning
1263,SOTOPIA-Ω: Dynamic Strategy Injection Learning and Social Instruction Following Evaluation for Social Agents,"['Wenyuan Zhang', 'Tianyun Liu', 'Mengxiao Song', 'Xiaodong Li', 'Tingwen Liu']",,SOTOPIA Ω Dynamic Strategy Injection Learning Social Instruction Following Evaluation Social Agents
1264,Can Language Models Replace Programmers? REPOCOD Says ‘Not Yet’,"['Shanchao Liang', 'Nan Jiang', 'Yiran Hu', 'Lin Tan']",,Language Models Replace Programmers REPOCOD Says
1265,Leveraging In-Context Learning for Political Bias Testing of LLMs,"['Patrick Haller', 'Jannis Vamvas', 'Rico Sennrich', 'Lena Ann Jäger']",,Leveraging Context Learning Political Bias Testing LLMs
1266,CoRet: Improved Retriever for Code Editing,"['Fabio James Fehr', 'Prabhu Teja S', 'Luca Franceschi', 'Giovanni Zappella']",,CoRet Improved Retriever Code Editing
1267,ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting,"['Steven H Wang', 'Maksim Zubkov', 'Kexin Fan', 'Sarah Harrell', 'Yuyang Sun', 'Wei Chen', 'Andreas Plesner', 'Roger Wattenhofer']",,ACORD Expert Annotated Retrieval Dataset Legal Contract Drafting
1268,LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts,"['Qibing Ren', 'Hao Li', 'Dongrui Liu', 'Zhanxu Xie', 'Xiaoya Lu', 'Yu Qiao', 'Lei Sha', 'Junchi Yan', 'Lizhuang Ma', 'Jing Shao']",,LLMs know vulnerabilities Uncover Safety Gaps Natural Distribution Shifts
1269,WAFFLE: Fine-tuning Multi-Modal Model for Automated Front-End Development,"['Shanchao Liang', 'Nan Jiang', 'Shangshu Qian', 'Lin Tan']",,WAFFLE Fine tuning Multi Modal Model Automated End Development
1270,Math Neurosurgery: Isolating Language Models’ Math Reasoning Abilities Using Only Forward Passes,"['Bryan R Christ', 'Zachary Gottesman', 'Jonathan Kropko', 'Thomas Hartvigsen']",,Math Neurosurgery Isolating Language Models Math Reasoning Abilities Using Forward Passes
1271,Multiple LLM Agents Debate for Equitable Cultural Alignment,"['Dayeon Ki', 'Rachel Rudinger', 'Tianyi Zhou', 'Marine Carpuat']",,Multiple LLM Agents Debate Equitable Cultural Alignment
1272,RefreshKV: Updating Small KV Cache During Long-form Generation,"['Fangyuan Xu', 'Tanya Goyal', 'Eunsol Choi']",,RefreshKV Updating Small KV Cache Long form Generation
1273,SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings,"['Weikai Lu', 'Hao Peng', 'Huiping Zhuang', 'Cen Chen', 'Ziqian Zeng']",,SEA Low Resource Safety Alignment Multimodal Large Language Models Synthetic Embeddings
1274,Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress,"['Lorenzo Proietti', 'Stefano Perrella', 'Roberto Navigli']",,Machine Translation Evaluation Achieved Human Parity Human Reference Limits Progress
1275,Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective,"['Yiyao Yu', 'Yuxiang Zhang', 'Dongdong Zhang', 'Xiao Liang', 'Hengyuan Zhang', 'Xingxing Zhang', 'MAHMOUD KHADEMI', 'Hany Hassan Awadalla', 'Junjie Wang', 'Yujiu Yang', 'Furu Wei']",,Chain Reasoning Unified Mathematical Reasoning Large Language Models Multi Paradigm Perspective
1276,Language Models Grow Less Humanlike beyond Phase Transition,"['Tatsuya Aoyama', 'Ethan Wilcox']",,Language Models Grow Humanlike Phase Transition
1277,PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation,"['Arkadiusz Modzelewski', 'Witold Sosnowski', 'Tiziano Labruna', 'Adam Wierzbicki', 'Giovanni Da San Martino']",,PCoT Persuasion Augmented Chain Thought Detecting Fake News Social Media Disinformation
1278,Coordinating Chaos: A Structured Review of Linguistic Coordination Methodologies,"['Benjamin Roger Litterer', 'David Jurgens', 'Dallas Card']",,Coordinating Chaos Structured Review Linguistic Coordination Methodologies
1279,iNews: A Multimodal Dataset for Modeling Personalized Affective Responses to News,"['Tiancheng Hu', 'Nigel Collier']",,iNews Multimodal Dataset Modeling Personalized Affective Responses News
1280,Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal Gestures,"['Akhila Yerukola', 'Saadia Gabriel', 'Nanyun Peng', 'Maarten Sap']",,Mind Gesture Evaluating AI Sensitivity Culturally Offensive Non Verbal Gestures
1281,500xCompressor: Generalized Prompt Compression for Large Language Models,"['Zongqian Li', 'Yixuan Su', 'Nigel Collier']",,500xCompressor Generalized Prompt Compression Large Language Models
1282,Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models,"['James Flemings', 'Bo Jiang', 'Wanrong Zhang', 'Zafar Takhirov', 'Murali Annavaram']",,Estimating Privacy Leakage Augmented Contextual Knowledge Language Models
1283,Document-Level Event-Argument Data Augmentation for Challenging Role Types,"['Joseph Gatto', 'Omar Sharif', 'Parker Seegmiller', 'Sarah Masud Preum']",,Document Level Event Argument Data Augmentation Challenging Role Types
1284,Mapping the Podcast Ecosystem with the Structured Podcast Research Corpus,"['Benjamin Roger Litterer', 'David Jurgens', 'Dallas Card']",,Mapping Podcast Ecosystem Structured Podcast Research Corpus
1285,Unravelling the Logic: Investigating the Generalisation of Transformers in Numerical Satisfiability Problems,"['Tharindu Madusanka', 'Marco Valentino', 'Iqra Zahid', 'Ian Pratt-Hartmann', 'Riza Batista-Navarro']",,Unravelling Logic Investigating Generalisation Transformers Numerical Satisfiability Problems
1286,The Nature of NLP: Analyzing Contributions in NLP Papers,"['Aniket Pramanick', 'Yufang Hou', 'Saif M. Mohammad', 'Iryna Gurevych']",,Nature NLP Analyzing Contributions NLP Papers
1287,$\mathtt{GeLLM^3O}$: Generalizing Large Language Models for Multi-property Molecule Optimization,"['Vishal Dey', 'Xiao Hu', 'Xia Ning']",,mathtt GeLLM 3O Generalizing Large Language Models Multi property Molecule Optimization
1288,Diffusion Directed Acyclic Transformer for Non-Autoregressive Machine Translation,"['Quan Nguyen-Tri', 'Cong Dao Tran', 'Hoang Thanh-Tung']",,Diffusion Directed Acyclic Transformer Non Autoregressive Machine Translation
1289,Follow-up Question Generation For Enhanced Patient-Provider Conversations,"['Joseph Gatto', 'Parker Seegmiller', 'Timothy E. Burdick', 'Inas S. Khayal', 'Sarah DeLozier', 'Sarah Masud Preum']",,Follow Question Generation Enhanced Patient Provider Conversations
1290,Unveiling Privacy Risks in LLM Agent Memory,"['Bo Wang', 'Weiyi He', 'Shenglai Zeng', 'Zhen Xiang', 'Yue Xing', 'Jiliang Tang', 'Pengfei He']",,Unveiling Privacy Risks LLM Agent Memory
1291,Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation,"['Emmanouil Zaranis', 'Giuseppe Attanasio', 'Sweta Agrawal', 'Andre Martins']",,Watching Watchers Exposing Gender Disparities Machine Translation Quality Estimation
1292,Language Constrained Multimodal Hyper Adapter For Many-to-Many Multimodal Summarization,"['Nayu Liu', 'Fanglong Yao', 'Haoran Luo', 'Yong Yang', 'Chen Tang', 'Bo Lv']",,Language Constrained Multimodal Hyper Adapter Multimodal Summarization
1293,PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models,"['Mingyang Song', 'Zhaochen Su', 'Xiaoye Qu', 'Jiawei Zhou', 'Yu Cheng']",,PRMBench Fine grained Challenging Benchmark Process Level Reward Models
1294,Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets,"['Dongyue Li', 'Ziniu Zhang', 'Lu Wang', 'Hongyang R. Zhang']",,Efficient Ensemble Fine tuning Language Models Multiple Datasets
1295,Library-Like Behavior In Language Models is Enhanced by Self-Referencing Causal Cycles,"['Munachiso S Nwadike', 'Zangir Iklassov', 'Toluwani Aremu', 'Tatsuya Hiraoka', 'Benjamin Heinzerling', 'Velibor Bojkovic', 'Hilal AlQuabeh', 'Martin Takáč', 'Kentaro Inui']",,Library Like Behavior Language Models Enhanced Self Referencing Causal Cycles
1296,Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models,"['Lang Gao', 'Jiahui Geng', 'Xiangliang Zhang', 'Preslav Nakov', 'Xiuying Chen']",,Shaping Safety Boundaries Understanding Defending Jailbreaks Large Language Models
1297,ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution,"['Alexandru Coca', 'Mark Gaynor', 'Zhenxing Zhang', 'Jianpeng Cheng', 'Bo-Hsiang Tseng', 'Peter Boothroyd', 'Hector Martinez Alonso', 'Diarmuid O Seaghdha', 'Anders Johannsen']",,ASPERA Simulated Environment Evaluate Planning Complex Action Execution
1298,ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework,"['Jiahao Yuan', 'Zixiang Di', 'Zhiqing Cui', 'Guisong Yang', 'Usman Naseem']",,ReflectDiffu Reflect Emotion intent Contagion Mimicry Empathetic Response Generation RL Diffusion Framework
1299,SARA: Salience-Aware Reinforced Adaptive Decoding for Large Language Models in Abstractive Summarization,"['Nayu Liu', 'Junnan Zhu', 'Yiming Ma', 'Zhicong Lu', 'Wenlei Xu', 'Yong Yang', 'Jiang Zhong', 'kaiwen wei']",,SARA Salience Aware Reinforced Adaptive Decoding Large Language Models Abstractive Summarization
1300,Embedding-Converter: A Unified Framework for Cross-Model Embedding Transformation,"['Jinsung Yoon', 'Sercan O Arik']",,Embedding Converter Unified Framework Cross Model Embedding Transformation
1301,Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge,"['Md Tahmid Rahman Laskar', 'Israt Jahan', 'Elham Dolatabadi', 'Chun Peng', 'Enamul Hoque', 'Jimmy Huang']",,Improving Automatic Evaluation Large Language Models LLMs Biomedical Relation Extraction LLMs Judge
1302,Answering Complex Geographic Questions by Adaptive Reasoning with Visual Context and External Commonsense Knowledge,"['Fan Li', 'Jianxing Yu', 'Jielong Tang', 'Wenqing Chen', 'Hanjiang Lai', 'Yanghui Rao', 'Jian Yin']",,Answering Complex Geographic Questions Adaptive Reasoning Visual Context External Commonsense Knowledge
1303,Efficient Knowledge Editing via Minimal Precomputation,"['Akshat Gupta', 'Maochuan Lu', 'Thomas Hartvigsen', 'Gopala Anumanchipalli']",,Efficient Knowledge Editing Minimal Precomputation
1304,Safety Alignment via Constrained Knowledge Unlearning,"['Zesheng Shi', 'Yucheng Zhou', 'Jing Li', 'Yuxin Jin', 'YU LI', 'Daojing He', 'Fangming Liu', 'Saleh Alharbi', 'Jun Yu', 'Min Zhang']",,Safety Alignment Constrained Knowledge Unlearning
1305,Response Wide Shut:Surprising Observations in Basic Vision Language Model Capabilities,"['Shivam Chandhok', 'Wan-Cyuan Fan', 'Vered Shwartz', 'Vineeth N. Balasubramanian', 'Leonid Sigal']",,Response Wide Shut Surprising Observations Basic Vision Language Model Capabilities
1306,EffiVLM-Bench: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Visual-Languge Models,"['Zekun Wang', 'MingHua Ma', 'Zexin Wang', 'Rongchuan Mu', 'hongtao liu', 'liping shan', 'Ming Liu', 'Bing Qin']",,EffiVLM Bench Comprehensive Benchmark Evaluating Training Free Acceleration Large Visual Languge Models
1307,Pre-Training Curriculum for Multi-Token Prediction in Language Models,"['Ansar Aynetdinov', 'Alan Akbik']",,Pre Training Curriculum Multi Token Prediction Language Models
1308,Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks,"['Xingxuan Li', 'Weiwen Xu', 'Ruochen Zhao', 'Fangkai Jiao', 'Shafiq Joty', 'Lidong Bing']",,Elicit Reasoning LLMs Critic Guided Planning Retrieval Augmentation Solving Challenging Tasks
1309,On Many-Shot In-Context Learning for Long-Context Evaluation,"['Kaijian Zou', 'Muhammad Khalifa', 'Lu Wang']",,Shot Context Learning Long Context Evaluation
1310,Meaning Variation and Data Quality in the Corpus of Founding Era American English,['Dallas Card'],,Meaning Variation Data Quality Corpus Founding Era American English
1311,Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks,"['Zhilin Wang', 'Jiaqi Zeng', 'Olivier Delalleau', 'Daniel Egert', 'Ellie Evans', 'Hoo-Chang Shin', 'Felipe Soares', 'Yi Dong', 'Oleksii Kuchaiev']",,Dedicated Feedback Edit Models Empower Inference Time Scaling Open Ended General Domain Tasks
1312,"CulturalBench: A Robust, Diverse and Challenging Benchmark for Measuring LMs’ Cultural Knowledge Through Human-AI Red-Teaming","['Yu Ying Chiu', 'Liwei Jiang', 'Bill Yuchen Lin', 'Chan Young Park', 'Shuyue Stella Li', 'Sahithya Ravi', 'Mehar Bhatia', 'Maria Antoniak', 'Yulia Tsvetkov', 'Vered Shwartz', 'Yejin Choi']",,CulturalBench Robust Diverse Challenging Benchmark Measuring LMs Cultural Knowledge Human AI Red Teaming
1313,Balancing the Budget: Understanding Trade-offs Between Supervised and Preference-Based Finetuning,"['Mohit Raghavendra', 'Junmo Kang', 'Alan Ritter']",,Balancing Budget Understanding Trade offs Supervised Preference Based Finetuning
1314,All That Glitters is Not Novel: Plagiarism in AI Generated Research,"['Tarun Gupta', 'Danish Pruthi']",,Glitters Novel Plagiarism AI Generated Research
1315,Writing Like the Best: Exemplar-Based Expository Text Generation,"['Yuxiang Liu', 'Kevin Chen-Chuan Chang']",,Writing Like Best Exemplar Based Expository Text Generation
1316,Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach,"['Rochana Chaturvedi', 'Peyman Baghershahi', 'Sourav Medya', 'Barbara Di Eugenio']",,Temporal Relation Extraction Clinical Texts Span based Graph Transformer Approach
1317,Finding A Voice: Exploring the Potential of African American Dialect and Voice Generation for Chatbots,"['Sarah E. Finch', 'Ellie S. Paek', 'Ikseon Choi', 'Jinho D. Choi']",,Finding Voice Exploring Potential African American Dialect Voice Generation Chatbots
1318,Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer’s Disease Detection,"['Chuyuan Li', 'Raymond Li', 'Thalia S. Field', 'Giuseppe Carenini']",,Delta KNN Improving Demonstration Selection Context Learning Alzheimer s Disease Detection
1319,Help Me Write a Story: Evaluating LLMs’ Ability to Generate Writing Feedback,"['Hannah Rashkin', 'Elizabeth Clark', 'Fantine Huot', 'Mirella Lapata']",,Help Write Story Evaluating LLMs Ability Generate Writing Feedback
1320,Language Fusion for Parameter-Efficient Cross-lingual Transfer,"['Philipp Borchert', 'Ivan Vulić', 'Marie-Francine Moens', 'Jochen De Weerdt']",,Language Fusion Parameter Efficient Cross lingual Transfer
1321,Culture is Not Trivia: Sociocultural Theory for Cultural NLP,"['Naitian Zhou', 'David Bamman', 'Isaac L. Bleaman']",,Culture Trivia Sociocultural Theory Cultural NLP
1322,AAD-LLM: Neural Attention-Driven Auditory Scene Understanding,"['Xilin Jiang', 'Sukru Samet Dindar', 'Vishal Choudhari', 'Stephan Bickel', 'Ashesh Mehta', 'Guy M McKhann', 'Daniel Friedman', 'Adeen Flinker', 'Nima Mesgarani']",,AAD LLM Neural Attention Driven Auditory Scene Understanding
1323,MindRef: Mimicking Human Memory for Hierarchical Reference Retrieval with Fine-Grained Location Awareness,"['Ye Wang', 'Xinrun Xu', 'Zhiming Ding']",,MindRef Mimicking Human Memory Hierarchical Reference Retrieval Fine Grained Location Awareness
1324,Do Language Models Have Semantics? On the Five Standard Positions,['Anders Søgaard'],,Language Models Semantics Standard Positions
1325,Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems,"['Myra Cheng', 'Su Lin Blodgett', 'Alicia DeVrio', 'Lisa Egede', 'Alexandra Olteanu']",,Dehumanizing Machines Mitigating Anthropomorphic Behaviors Text Generation Systems
1326,Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users,"['Antonia Karamolegkou', 'Malvina Nikandrou', 'Georgios Pantazopoulos', 'Danae Sanchez Villegas', 'Phillip Rust', 'Ruchira Dhar', 'Daniel Hershcovich', 'Anders Søgaard']",,Evaluating Multimodal Language Models Visual Assistants Visually Impaired Users
1327,HumT DumT: Measuring and controlling human-like language in LLMs,"['Myra Cheng', 'Sunny Yu', 'Dan Jurafsky']",,HumT DumT Measuring controlling human like language LLMs
1328,ChatBench: From Static Benchmarks to Human-AI Evaluation,"['Serina Chang', 'Ashton Anderson', 'Jake M. Hofman']",,ChatBench Static Benchmarks Human AI Evaluation
1329,LLMs syntactically adapt their language use to their conversational partner,"['Florian Kandra', 'Vera Demberg', 'Alexander Koller']",,LLMs syntactically adapt language use conversational partner
1330,Teaching an Old LLM Secure Coding: Localized Preference Optimization on Distilled Preferences,"['Mohammad Saqib Hasan', 'Saikat Chakraborty', 'Santu Karmaker', 'Niranjan Balasubramanian']",,Teaching Old LLM Secure Coding Localized Preference Optimization Distilled Preferences
1331,Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs,"['Xiulin Yang', 'Tatsuya Aoyama', 'Yuekun Yao', 'Ethan Wilcox']",,Goes Crosslinguistic Study Im possible Language Learning LMs
1332,Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat,"['Roland Daynauth', 'Christopher Clarke', 'Krisztian Flautner', 'Lingjia Tang', 'Jason Mars']",,Ranking Unraveled Recipes LLM Rankings Head Head AI Combat
1333,LLM Agents Making Agent Tools,"['Georg Wölflein', 'Dyke Ferber', 'Daniel Truhn', 'Ognjen Arandjelovic', 'Jakob Nikolas Kather']",,LLM Agents Making Agent Tools
1334,CrafText Benchmark: Advancing Language Grounding in Complex Multimodal Open-Ended World,"['Zoya Volovikova', 'Gregory Gorbov', 'Petr Kuderov', 'Aleksandr Panov', 'Alexey Skrynnik']",,CrafText Benchmark Advancing Language Grounding Complex Multimodal Open Ended World
1335,QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation,"['Bang Nguyen', 'Tingting Du', 'Mengxia Yu', 'Lawrence Angrave', 'Meng Jiang']",,QG SMS Enhancing Test Item Analysis Student Modeling Simulation
1336,Causal Graph based Event Reasoning using Semantic Relation Experts,"['Mahnaz Koupaee', 'Xueying Bai', 'Mudan Chen', 'Greg Durrett', 'Nathanael Chambers', 'Niranjan Balasubramanian']",,Causal Graph based Event Reasoning using Semantic Relation Experts
1337,LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning,"['Jin Jiang', 'Yuchen Yan', 'Yang Liu', 'Jianing Wang', 'Shuai Peng', 'Xunliang Cai', 'Yixin Cao', 'Mengdi Zhang', 'Liangcai Gao']",,LogicPro Improving Complex Logical Reasoning Program Guided Learning
1338,Do LLMs Understand Dialogues? A Case Study on Dialogue Acts,"['Ayesha Qamar', 'Jonathan Tong', 'Ruihong Huang']",,LLMs Understand Dialogues Case Study Dialogue Acts
1339,Research Borderlands: Analysing Scientific Writing Across Research Cultures,"['Shaily Bhatt', 'Tal August', 'Maria Antoniak']",,Research Borderlands Analysing Scientific Writing Research Cultures
1340,CEAES: Bidirectional Reinforcement Learning Optimization for Consistent and Explainable Essay Assessment,"['Xia Li', 'Wenjing Pan']",,CEAES Bidirectional Reinforcement Learning Optimization Consistent Explainable Essay Assessment
1341,DeAL: Decoding-time Alignment Framework for Large Language Models,"['James Y. Huang', 'Sailik Sengupta', 'Daniele Bonadiman', 'Yi-An Lai', 'Arshit Gupta', 'Nikolaos Pappas', 'Saab Mansour', 'Katrin Kirchhoff', 'Dan Roth']",,DeAL Decoding time Alignment Framework Large Language Models
1342,Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors,"['Senqi Yang', 'Dongyu Zhang', 'Jing Ren', 'Ziqi Xu', 'Xiuzhen Zhang', 'Yiliao Song', 'Hongfei Lin', 'Feng Xia']",,Cultural Bias Matters Cross Cultural Benchmark Dataset Sentiment Enriched Model Understanding Multimodal Metaphors
1343,OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction,"['Haonan Zhang', 'Run Luo', 'Xiong Liu', 'Yuchuan Wu', 'Ting-En Lin', 'Pengpeng Zeng', 'QIANG QU', 'Feiteng Fang', 'Min Yang', 'Lianli Gao', 'Jingkuan Song', 'Fei Huang', 'Yongbin Li']",,OmniCharacter Immersive Role Playing Agents Seamless Speech Language Personality Interaction
1344,Mixtures of In-Context Learners,"['Giwon Hong', 'Emile van Krieken', 'Edoardo Ponti', 'Nikolay Malkin', 'Pasquale Minervini']",,Mixtures Context Learners
1345,Balancing Diversity and Risk in LLM Sampling: How to Select Your Method and Parameter for Open-Ended Text Generation,"['Yuxuan Zhou', 'Margret Keuper', 'Mario Fritz']",,Balancing Diversity Risk LLM Sampling Select Method Parameter Open Ended Text Generation
1346,RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection,"['Wenjun Hou', 'Yi Cheng', 'Kaishuai Xu', 'Heng Li', 'Yan Hu', 'Wenjie Li', 'Jiang Liu']",,RADAR Enhancing Radiology Report Generation Supplementary Knowledge Injection
1347,Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates,"['Jaewoo Ahn', 'Heeseung Yun', 'Dayoon Ko', 'Gunhee Kim']",,LLMs Deceive CLIP Benchmarking Adversarial Compositionality Pre trained Multimodal Representation Text Updates
1348,Attention Speaks Volumes: Localizing and Mitigating Bias in Language Models,"['Rishabh Adiga', 'Besmira Nushi', 'Varun Chandrasekaran']",,Attention Speaks Volumes Localizing Mitigating Bias Language Models
1349,MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming,"['Weiyang Guo', 'Jing Li', 'Wenya Wang', 'YU LI', 'Daojing He', 'Jun Yu', 'Min Zhang']",,MTSA Multi turn Safety Alignment LLMs Multi round Red teaming
1350,The Efficiency vs. Accuracy Trade-off: Optimizing RAG-Enhanced LLM Recommender Systems Using Multi-Head Early Exit,"['Huixue Zhou', 'Hengrui Gu', 'Zaifu Zhan', 'Xi Liu', 'Kaixiong Zhou', 'Yongkang Xiao', 'Mingfu Liang', 'Srinivas Prasad Govindan', 'Piyush Chawla', 'Jiyan Yang', 'Xiangfei Meng', 'Huayu Li', 'Buyun Zhang', 'Liang Luo', 'Wen-Yen Chen', 'Yiping Han', 'Bo Long', 'Rui Zhang', 'Tianlong Chen']",,Efficiency vs Accuracy Trade Optimizing RAG Enhanced LLM Recommender Systems Using Multi Head Early Exit
1351,Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging,"['Haobo Zhang', 'Jiayu Zhou']",,Unraveling LoRA Interference Orthogonal Subspaces Robust Model Merging
1352,BIG-Bench Extra Hard,"['Mehran Kazemi', 'Bahare Fatemi', 'Hritik Bansal', 'John Palowitch', 'Chrysovalantis Anastasiou', 'Sanket Vaibhav Mehta', 'Lalit K Jain', 'Virginia Aglietti', 'Disha Jindal', 'Peter Chen', 'Nishanth Dikkala', 'Gladys Tyen', 'Xin Liu', 'Uri Shalit', 'Silvia Chiappa', 'Kate Olszewska', 'Yi Tay', 'Vinh Q. Tran', 'Quoc V Le', 'Orhan Firat']",,BIG Bench Extra Hard
1353,CSTree-SRI: Introspection-Driven Cognitive Semantic Tree for Multi-Turn Question Answering over Extra-Long Contexts,"['Zhaowen Wang', 'Xiang Wei', 'Kangshao Du', 'Yiting Zhang', 'Libo Qin', 'Yingjie Xia', 'Li Kuang']",,CSTree SRI Introspection Driven Cognitive Semantic Tree Multi Turn Question Answering Extra Long Contexts
1354,TigerLLM - A Family of Bangla Large Language Models,"['Nishat Raihan', 'Marcos Zampieri']",,TigerLLM Family Bangla Large Language Models
1355,InductionBench: LLMs Fail in the Simplest Complexity Class,"['Wenyue Hua', 'Tyler Wong', 'Fei Sun', 'Liangming Pan', 'Adam Jardine', 'William Yang Wang']",,InductionBench LLMs Fail Simplest Complexity Class
1356,RATIONALYST: Pre-training Process-Supervision for Improving Reasoning,"['Dongwei Jiang', 'Guoxuan Wang', 'Yining Lu', 'Andrew Wang', 'Jingyu Zhang', 'Chuyu Liu', 'Benjamin Van Durme', 'Daniel Khashabi']",,RATIONALYST Pre training Process Supervision Improving Reasoning
1357,Make Imagination Clearer! Stable Diffusion-based Visual Imagination for Multimodal Machine Translation,"['Andong Chen', 'Yuchen Song', 'Kehai Chen', 'Xuefeng Bai', 'Muyun Yang', 'Liqiang Nie', 'Jie Liu', 'Tiejun Zhao', 'Min zhang']",,Make Imagination Clearer Stable Diffusion based Visual Imagination Multimodal Machine Translation
1358,Advancing SMoE for Continuous Domain Adaptation of MLLMs: Adaptive Router and Domain-Specific Loss,"['Liang Zhang', 'Ziyao Lu', 'Fandong Meng', 'Hui Li', 'Jie Zhou', 'Jinsong Su']",,Advancing SMoE Continuous Domain Adaptation MLLMs Adaptive Router Domain Specific Loss
1359,Mitigating Media Bias through Multi-document Events Reasoning in LLMs,"['Yuanyuan Lei', 'Ruihong Huang']",,Mitigating Media Bias Multi document Events Reasoning LLMs
1360,Who Writes What: Unveiling the Impact of Author Roles on AI-generated Text Detection,"['Jiatao Li', 'Xiaojun Wan']",,Writes Unveiling Impact Author Roles AI generated Text Detection
1361,RoCoFT: Efficient Finetuning of Large Language Models with Row-Column Updates,"['Md Kowsher', 'Tara Esmaeilbeig', 'Chun-Nam Yu', 'Chen Chen', 'Mojtaba Soltanalian', 'Niloofar Yousefi']",,RoCoFT Efficient Finetuning Large Language Models Row Column Updates
1362,Scaling Laws and Efficient Inference for Ternary Language Models,"['Tejas Vaidhya', 'Ayush Kaushal', 'Vineet Jain', 'Francis Couture-Harpin', 'Prashant Shishodia', 'Majid Behbahani', 'Irina Rish', 'Yuriy Nevmyvaka']",,Scaling Laws Efficient Inference Ternary Language Models
1363,Exploring the Impact of Instruction-Tuning on LLM’s Susceptibility to Misinformation,"['Kyubeen Han', 'Junseo Jang', 'Hongjin Kim', 'Geunyeong Jeong', 'Harksoo Kim']",,Exploring Impact Instruction Tuning LLM s Susceptibility Misinformation
1364,Do Language Models Understand Honorific Systems in Javanese?,"['Mohammad Rifqi Farhansyah', 'Iwan Darmawan', 'Adryan Kusumawardhana', 'Genta Indra Winata', 'Alham Fikri Aji', 'Derry Tanti Wijaya']",,Language Models Understand Honorific Systems Javanese
1365,Generative Reward Modeling via Synthetic Criteria Preference Learning,"['xiaobo liang', 'Haoke Zhang', 'Juntao Li', 'Kehai Chen', 'Qiaoming Zhu', 'Min Zhang']",,Generative Reward Modeling Synthetic Criteria Preference Learning
1366,Relation Extraction of Hierarchical Tables Using Multimodal Large Language Models,"['Xinyu Zhang', 'Aibo Song', 'Jingyi Qiu', 'Jiahui Jin', 'Tianbo zhang', 'Xiaolin Fang']",,Relation Extraction Hierarchical Tables Using Multimodal Large Language Models
1367,A Self-Denoising Model for Robust Few-Shot Relation Extraction,"['Liang Zhang', 'yang zhang', 'Ziyao Lu', 'Fandong Meng', 'Jie Zhou', 'Jinsong Su']",,Self Denoising Model Robust Shot Relation Extraction
1368,QuASAR: A Question-Driven Structure-Aware Approach for Table-to-Text Generation,"['WeiJie Liu', 'Yibin Zheng', 'Fang Kong']",,QuASAR Question Driven Structure Aware Approach Table Text Generation
1369,Automated Structured Radiology Report Generation,"['Jean-Benoit Delbrouck', 'Justin Xu', 'Johannes Moll', 'Alois Thomas', 'Zhihong Chen', 'Maya Varma', 'Asfandyar Azhar', 'Sophie Ostmeier', 'Andrew Johnston', 'Eduardo Pontes Reis', 'Christian Bluethgen', 'Mohamed S Muneer', 'Kelvin Zhenghao Li', 'Curtis Langlotz']",,Automated Structured Radiology Report Generation
1370,LPOI: Listwise Preference Optimization for Vision Language Models,"['Fatemeh Pesaran zadeh', 'Yoojin Oh', 'Gunhee Kim']",,LPOI Listwise Preference Optimization Vision Language Models
1371,Predicting Through Generation: Why Generation Is Better for Prediction,"['Md Kowsher', 'Nusrat Jahan Prottasha', 'Prakash Bhat', 'Chun-Nam Yu', 'Mojtaba Soltanalian', 'Ivan Garibay', 'Ozlem Garibay', 'Chen Chen', 'Niloofar Yousefi']",,Predicting Generation Generation Better Prediction
1372,“Give Me BF16 or Give Me Death”? Accuracy-Performance Trade-Offs in LLM Quantization,"['Eldar Kurtic', 'Alexandre Noll Marques', 'Shubhra Pandit', 'Mark Kurtz', 'Dan Alistarh']",,BF16 Death Accuracy Performance Trade Offs LLM Quantization
1373,"StitchLLM: Serving LLMs, One Block at a Time","['Bodun Hu', 'Shuozhe Li', 'Saurabh Agarwal', 'Myungjin Lee', 'Akshay Jajoo', 'Jiamin Li', 'Le Xu', 'Geon-Woo Kim', 'Donghyun Kim', 'Hong Xu', 'Amy Zhang', 'Aditya Akella']",,StitchLLM Serving LLMs Block Time
1374,Walk in Others’ Shoes with a Single Glance: Human-Centric Visual Grounding with Top-View Perspective Transformation,"['Yuqi Bu', 'Xin Wu', 'Zirui Zhao', 'Yi Cai', 'David Hsu', 'Qiong Liu']",,Walk Shoes Single Glance Human Centric Visual Grounding View Perspective Transformation
1375,From Citations to Criticality: Predicting Legal Decision Influence in the Multilingual Swiss Jurisprudence,"['Ronja Stern', 'Ken Kawamura', 'Matthias Stürmer', 'Ilias Chalkidis', 'Joel Niklaus']",,Citations Criticality Predicting Legal Decision Influence Multilingual Swiss Jurisprudence
1376,Is linguistically-motivated data augmentation worth it?,"['Ray Groshan', 'Michael Ginn', 'Alexis Palmer']",,linguistically motivated data augmentation worth
1377,From Lists to Emojis: How Format Bias Affects Model Alignment,"['Xuanchang Zhang', 'Wei Xiong', 'Lichang Chen', 'Tianyi Zhou', 'Heng Huang', 'Tong Zhang']",,Lists Emojis Format Bias Affects Model Alignment
1378,Colloquial Singaporean English Style Transfer with Fine-Grained Explainable Control,"['Jinggui Liang', 'Dung Vo', 'Yap Hong Xian', 'Hai Leong Chieu', 'Kian Ming A. Chai', 'Jing Jiang', 'Lizi Liao']",,Colloquial Singaporean English Style Transfer Fine Grained Explainable Control
1379,From Informal to Formal – Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs,"['Jialun Cao', 'Yaojie Lu', 'Meiziniu Li', 'Haoyang Ma', 'Haokun Li', 'Mengda He', 'Cheng Wen', 'Le Sun', 'Hongyu Zhang', 'Shengchao Qin', 'Shing-Chi Cheung', 'Cong Tian']",,Informal Formal Incorporating Evaluating LLMs Natural Language Requirements Verifiable Formal Proofs
1380,CoAM: Corpus of All-Type Multiword Expressions,"['Yusuke Ide', 'Joshua Tanner', 'Adam Nohejl', 'Jacob Hoffman', 'Justin Vasselli', 'Hidetaka Kamigaito', 'Taro Watanabe']",,CoAM Corpus Type Multiword Expressions
1381,SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation,"['Zijun Yao', 'Weijian Qi', 'Liangming Pan', 'Shulin Cao', 'Linmei Hu', 'Liu Weichuan', 'Lei Hou', 'Juanzi Li']",,SeaKR Self aware Knowledge Retrieval Adaptive Retrieval Augmented Generation
1382,Exposing the Achilles’ Heel: Evaluating LLMs Ability to Handle Mistakes in Mathematical Reasoning,"['Joykirat Singh', 'Akshay Nambi', 'Vibhav Vineet']",,Exposing Achilles Heel Evaluating LLMs Ability Handle Mistakes Mathematical Reasoning
1383,Revisiting LLMs as Zero-Shot Time Series Forecasters: Small Noise Can Break Large Models,"['Junwoo Park', 'Hyuck Lee', 'Dohyun Lee', 'Daehoon Gwak', 'Jaegul Choo']",,Revisiting LLMs Zero Shot Time Series Forecasters Small Noise Break Large Models
1384,Transferring Textual Preferences to Vision-Language Understanding through Model Merging,"['Chen-An Li', 'Tzu-Han Lin', 'Yun-Nung Chen', 'Hung-yi Lee']",,Transferring Textual Preferences Vision Language Understanding Model Merging
1385,Understanding the Dark Side of LLMs’ Intrinsic Self-Correction,"['Qingjie Zhang', 'Di Wang', 'Haoting Qian', 'Yiming Li', 'Tianwei Zhang', 'Minlie Huang', 'Han Qiu']",,Understanding Dark LLMs Intrinsic Self Correction
1386,"VideoVista2: 360° Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension","['Xinyu Chen', 'yunxin li', 'Haoyuan Shi', 'Baotian Hu', 'Wenhan Luo', 'Yaowei Wang', 'Min Zhang']",,VideoVista2 360 Horizons Bridging Cultures Languages Domains Video Comprehension
1387,What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices,"['Zhi Chen', 'Qiguang Chen', 'Libo Qin', 'Qipeng Guo', 'haijun Lv', 'Yicheng Zou', 'Hang Yan', 'Kai Chen', 'Dahua Lin']",,Essential Factors Crafting Effective Long Context Multi Hop Instruction Datasets Insights Best Practices
1388,Knowledge Graph Retrieval-Augmented Generation for LLM-based Recommendation,"['Shijie Wang', 'Wenqi Fan', 'Yue Feng', 'LIN SHANRU', 'Xinyu Ma', 'Shuaiqiang Wang', 'Dawei Yin']",,Knowledge Graph Retrieval Augmented Generation LLM based Recommendation
1389,SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment,"['Qin Liu', 'Fei Wang', 'Chaowei Xiao', 'Muhao Chen']",,SudoLM Learning Access Control Parametric Knowledge Authorization Alignment
1390,ProgCo: Program Helps Self-Correction of Large Language Models,"['Xiaoshuai Song', 'Yanan Wu', 'Weixun Wang', 'Jiaheng Liu', 'Wenbo Su', 'Bo Zheng']",,ProgCo Program Helps Self Correction Large Language Models
1391,I0T: Embedding Standardization Method Towards Zero Modality Gap,"['Na Min An', 'Eunki Kim', 'James Thorne', 'Hyunjung Shim']",,I0T Embedding Standardization Method Zero Modality Gap
1392,Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs,"['Ananth Muppidi', 'Abhilash Nandy', 'Sambaran Bandyopadhyay']",,Leveraging Self Attention Input Dependent Soft Prompting LLMs
1393,Odysseus Navigates the Sirens’ Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation,"['Wen Luo', 'Feifan Song', 'Wei Li', 'Guangyue Peng', 'Shaohang Wei', 'Houfeng Wang']",,Odysseus Navigates Sirens Song Dynamic Focus Decoding Factual Diverse Open Ended Text Generation
1394,Better Embeddings with Coupled Adam,"['Felix Stollenwerk', 'Tobias Stollenwerk']",,Better Embeddings Coupled Adam
1395,Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation,"['Guofu Xie', 'Xiao Zhang', 'Ting Yao', 'Yunsheng Shi']",,Bone Soups Seek Soup Model Merging Approach Controllable Multi Objective Generation
1396,Controllable and Reliable Knowledge-Intensive Task Agents with Declarative GenieWorksheets,"['Harshit Joshi', 'Shicheng Liu', 'James Chen', 'Larsen Weigle', 'Monica Lam']",,Controllable Reliable Knowledge Intensive Task Agents Declarative GenieWorksheets
1397,Benchmarking Long-Context Language Models on Long Code Understanding,"['Jia Li', 'Xuyuan Guo', 'Lei Li', 'Kechi Zhang', 'Ge Li', 'Jia Li', 'Zhengwei Tao', 'Fang Liu', 'Chongyang Tao', 'Yuqi Zhu', 'Zhi Jin']",,Benchmarking Long Context Language Models Long Code Understanding
1398,MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities,"['Savya Khosla', 'Aditi Tiwari', 'Kushal Kafle', 'Simon Jenni', 'Handong Zhao', 'John Collomosse', 'Jing Shi']",,MAGNET Augmenting Generative Decoders Representation Learning Infilling Capabilities
1399,Internal Value Alignment in Large Language Models through Controlled Value Vector Activation,"['Haoran Jin', 'Meng Li', 'Xiting Wang', 'Zhihao Xu', 'Minlie Huang', 'Yantao Jia', 'Defu Lian']",,Internal Value Alignment Large Language Models Controlled Value Vector Activation
1400,A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability,"['Xinyu Hu', 'Mingqi Gao', 'Li Lin', 'Zhenghan Yu', 'Xiaojun Wan']",,Dual Perspective NLG Meta Evaluation Framework Automatic Benchmark Better Interpretability
1401,Recurrent Knowledge Localization and Fusion for Language Model Continual Learning,"['Yujie Feng', 'Xujia Wang', 'ZEXIN LU', 'FuShenghong', 'Guangyuan SHI', 'Yongxin Xu', 'Yasha Wang', 'Philip S. Yu', 'Xu Chu', 'Xiao-Ming Wu']",,Recurrent Knowledge Localization Fusion Language Model Continual Learning
1402,Data-Constrained Synthesis of Training Data for De-Identification,"['Thomas Vakili', 'Aron Henriksson', 'Hercules Dalianis']",,Data Constrained Synthesis Training Data Identification
1403,Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection through Intent Differentiation and Emoji Interpretation,"['Soumitra Ghosh', 'gopendra Vikram singh', 'Shambhavi', 'Sabarna Choudhury', 'Asif Ekbal']",,Just Scratch Enhancing LLM Capabilities Self harm Detection Intent Differentiation Emoji Interpretation
1404,Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing,"['Peiming Guo', 'Meishan Zhang', 'jianling li', 'Min Zhang', 'Yue Zhang']",,Contrastive Learning LLM Generation Treebank Cross domain Constituency Parsing
1405,MMDEND: Dendrite-Inspired Multi-Branch Multi-Compartment Parallel Spiking Neuron for Sequence Modeling,"['Kexin Wang', 'Yuhong Chou', 'Di Shang', 'Shijie Mei', 'Jiahong Zhang', 'Yanbin Huang', 'Man Yao', 'Bo XU', 'Guoqi Li']",,MMDEND Dendrite Inspired Multi Branch Multi Compartment Parallel Spiking Neuron Sequence Modeling
1406,Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar,"['Andrew Gambardella', 'Takeshi Kojima', 'Yusuke Iwasawa', 'Yutaka Matsuo']",,Inconsistent Tokenizations Cause Language Models Perplexed Japanese Grammar
1407,Understanding Impact of Human Feedback via Influence Functions,"['Taywon Min', 'Haeone Lee', 'Yongchan Kwon', 'Kimin Lee']",,Understanding Impact Human Feedback Influence Functions
1408,T2I-FactualBench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts,"['Ziwei Huang', 'Wanggui He', 'Quanyu Long', 'Yandi Wang', 'Haoyuan Li', 'Zhelun Yu', 'Fangxun Shu', 'Weilong Dai', 'Hao Jiang', 'Leilei Gan', 'Fei Wu']",,T2I FactualBench Benchmarking Factuality Text Image Models Knowledge Intensive Concepts
1409,InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating,"['Fuyu Wang', 'Jiangtong Li', 'Kun Zhu', 'Changjun Jiang']",,InspireDebate Multi Dimensional Subjective Objective Evaluation Guided Reasoning Optimization Debating
1410,"WAVE: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization","['Hongliang He', 'Wenlin Yao', 'Kaixin Ma', 'Wenhao Yu', 'Hongming Zhang', 'Tianqing Fang', 'Zhenzhong Lan', 'Dong Yu']",,WAVE Building Multimodal Web Agents Iterative Real World Exploration Feedback Optimization
1411,FOCUS: Evaluating Pre-trained Vision-Language Models on Underspecification Reasoning,"['Kankan Zhou', 'Eason Lai', 'Kyriakos Mouratidis', 'Jing Jiang']",,FOCUS Evaluating Pre trained Vision Language Models Underspecification Reasoning
1412,Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions,"['Wan Ju Kang', 'Eunki Kim', 'Na Min An', 'Sangryul Kim', 'Haemin Choi', 'Ki Hoon Kwak', 'James Thorne']",,Sightation Counts Leveraging Sighted User Feedback Building BLV aligned Dataset Diagram Descriptions
1413,Personal Travel Solver: A Preference-Driven LLM-Solver System for Travel Planning,"['Zijian Shao', 'Jiancan Wu', 'Weijian Chen', 'Xiang Wang']",,Personal Travel Solver Preference Driven LLM Solver Travel Planning
1414,Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning,"['Aswini Kumar Padhi', 'Anil Bandhakavi', 'Tanmoy Chakraborty']",,Counterspeech ultimate shield Multi Conditioned Counterspeech Generation Attributed Prefix Learning
1415,Unique Hard Attention: A Tale of Two Sides,"['Selim Jerad', 'Anej Svete', 'Jiaoda Li', 'Ryan Cotterell']",,Unique Hard Attention Tale Sides
1416,LLM$\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models,"['Zihan Zhou', 'Chong Li', '陈昕怡', 'Shuo Wang', 'Yu Chao', 'Zhili Li', 'Haoyu Wang', 'Qi Shi', 'Zhixing Tan', 'Xu Han', 'Xiaodong Shi', 'Zhiyuan Liu', 'Maosong Sun']",,LLM times MapReduce Simplified Long Sequence Processing using Large Language Models
1417,CheXalign: Preference fine-tuning in chest X-ray interpretation models without human feedback,"['Dennis Hein', 'Zhihong Chen', 'Sophie Ostmeier', 'Justin Xu', 'Maya Varma', 'Eduardo Pontes Reis', 'Arne Edward Michalson MD', 'Christian Bluethgen', 'Hyun Joo Shin', 'Curtis Langlotz', 'Akshay S Chaudhari']",,CheXalign Preference fine tuning chest X ray interpretation models human feedback
1418,Knowledge Tracing in Programming Education Integrating Students’ Questions,"['Doyoun Kim', 'Suin Kim', 'Yohan Jo']",,Knowledge Tracing Programming Education Integrating Students Questions
1419,PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder,"['Yiqun Sun', 'Qiang Huang', 'Anthony Kum Hoe Tung', 'Jun Yu']",,PRISM Framework Producing Interpretable Political Bias Embeddings Political Aware Cross Encoder
1420,"Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes","['Meng Li', 'Michael Vrazitulis', 'David Schlangen']",,Representations Fact Fiction Forecast Large Language Models Epistemics Attitudes
1421,Lexical Diversity-aware Relevance Assessment for Retrieval-Augmented Generation,"['Zhange Zhang', 'Yuqing Ma', 'Yulong Wang', 'Shan He', 'Tianbo Wang', 'Siqi He', 'Jiakai Wang', 'Xianglong Liu']",,Lexical Diversity aware Relevance Assessment Retrieval Augmented Generation
1422,Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains,"['Juntian Zhang', 'Chuanqi Cheng', 'Yuhan Liu', 'Wei Liu', 'Jian Luan', 'Rui Yan']",,Weaving Context Images Improving Vision Language Models Focus Centric Visual Chains
1423,Online Iterative Self-Alignment for Radiology Report Generation,"['Ting Xiao', 'Lei Shi', 'Yang Zhang', 'HaoFeng Yang', 'Zhe Wang', 'Chenjia Bai']",,Online Iterative Self Alignment Radiology Report Generation
1424,Chinese Inertial GAN for Handwriting Signal Generation and Recognition,"['Yifeng Wang', 'Yi Zhao']",,Chinese Inertial GAN Handwriting Signal Generation Recognition
1425,LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges,"['Haoyang Li', 'Huan Gao', 'Zhiyuan Zhao', 'Zhiyu Lin', 'Junyu Gao', 'Xuelong Li']",,LLMs Caught Crossfire Malware Requests Jailbreak Challenges
1426,Evaluating Sequence Labeling on the basis of Information Theory,"['Enrique Amigo', 'Elena Álvarez-Mellado', 'Julio Gonzalo', 'Jorge Carrillo-de-Albornoz']",,Evaluating Sequence Labeling basis Information Theory
1427,GRAT: Guiding Retrieval-Augmented Reasoning through Process Rewards Tree Search,"['Xianshu Peng', 'Wei Wei']",,GRAT Guiding Retrieval Augmented Reasoning Process Rewards Tree Search
1428,T-REG: Preference Optimization with Token-Level Reward Regularization,"['Wenxuan Zhou', 'Shujian Zhang', 'Lingxiao Zhao', 'Tao Meng']",,T REG Preference Optimization Token Level Reward Regularization
1429,Enhancing Input-Label Mapping in In-Context Learning with Contrastive Decoding,"['Keqin Peng', 'Liang Ding', 'Yuanxin Ouyang', 'Meng Fang', 'Yancheng Yuan', 'Dacheng Tao']",,Enhancing Input Label Mapping Context Learning Contrastive Decoding
1430,Gödel Agent: A Self-Referential Agent Framework for Recursively Self-Improvement,"['Xunjian Yin', 'Xinyi Wang', 'Liangming Pan', 'Li Lin', 'Xiaojun Wan', 'William Yang Wang']",,Gödel Agent Self Referential Agent Framework Recursively Self Improvement
1431,AgentGym: Evaluating and Training Large Language Model-based Agents across Diverse Environments,"['Zhiheng Xi', 'Yiwen Ding', 'Wenxiang Chen', 'Boyang Hong', 'Honglin Guo', 'Junzhe Wang', 'Xin Guo', 'Dingwen Yang', 'Chenyang Liao', 'Wei He', 'Songyang Gao', 'Lu Chen', 'Rui Zheng', 'Yicheng Zou', 'Tao Gui', 'Qi Zhang', 'Xipeng Qiu', 'Xuanjing Huang', 'Zuxuan Wu', 'Yu-Gang Jiang']",,AgentGym Evaluating Training Large Language Model based Agents Diverse Environments
1432,Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory,"['Yexiang Liu', 'Zekun Li', 'Zhi Fang', 'Nan Xu', 'Ran He', 'Tieniu Tan']",,Rethinking Role Prompting Strategies LLM Test Time Scaling Perspective Probability Theory
1433,Learnability on the Information-Theoretic Continuum: Inductive Bias for Information Locality in Neural Language Models,"['Taiga Someya', 'Anej Svete', 'Brian DuSell', 'Timothy J. O’Donnell', 'Mario Giulianelli', 'Ryan Cotterell']",,Learnability Information Theoretic Continuum Inductive Bias Information Locality Neural Language Models
1434,Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models,"['Adrián Bazaga', 'Rexhina Blloshmi', 'Bill Byrne', 'Adrià de Gispert']",,Learning Reason Time Timeline Self Reflection Improved Temporal Reasoning Language Models
1435,Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies,"['Massimiliano Pronesti', 'Joao H Bettencourt-Silva', 'Paul Flanagan', 'Alessandra Pascale', 'Oisín Redmond', 'Anya Belz', 'Yufang Hou']",,Query driven Document level Scientific Evidence Extraction Biomedical Studies
1436,"Towards Robust Universal Information Extraction: Dataset, Evaluation, and Solution","['Jizhao Zhu', 'Akang Shi', 'Zixuan Li', 'Long Bai', 'Xiaolong Jin', 'Jiafeng Guo', 'Xueqi Cheng']",,Robust Universal Information Extraction Dataset Evaluation Solution
1437,Multi-perspective Alignment for Increasing Naturalness in Neural Machine Translation,"['Huiyuan Lai', 'Esther Ploeger', 'Rik van Noord', 'Antonio Toral']",,Multi perspective Alignment Increasing Naturalness Neural Machine Translation
1438,Temporal reasoning for timeline summarisation in social media,"['Jiayu Song', 'Mahmud Elahi Akhter', 'Dana Atzil-Slonim', 'Maria Liakata']",,Temporal reasoning timeline summarisation social media
1439,Beyond Negative Stereotypes – Non-Negative Abusive Utterances about Identity Groups and Their Semantic Variants,"['Tina Lommel', 'Elisabeth Eder', 'Josef Ruppenhofer', 'Michael Wiegand']",,Negative Stereotypes Non Negative Abusive Utterances Identity Groups Semantic Variants
1440,Persistent Homology of Topic Networks for the Prediction of Reader Curiosity,"['Manuel D.S. Hopp', 'Vincent Labatut', 'Arthur Amalvy', 'Richard Dufour', 'Hannah Stone', 'Hayley K Jach', 'Kou Murayama']",,Persistent Homology Topic Networks Prediction Reader Curiosity
1441,Tokenisation is NP-Complete,"['Philip Whittington', 'Gregor Bachmann', 'Tiago Pimentel']",,Tokenisation NP Complete
1442,Understanding Language Model Scaling Laws in Terms of Training Dynamics via Loss Deceleration and Zero-Sum Learning,"['Andrei Mircea', 'Ekaterina Lobacheva', 'Supriyo Chakraborty', 'Nima Chitsazan', 'Irina Rish']",,Understanding Language Model Scaling Laws Terms Training Dynamics Loss Deceleration Zero Sum Learning
1443,Parameter-Aware Contrastive Knowledge Editing: Tracing and Rectifying based on Critical Transmission Paths,"['Songlin Zhai', 'Yuan Meng', 'Yuxin Zhang', 'Guilin Qi']",,Parameter Aware Contrastive Knowledge Editing Tracing Rectifying based Critical Transmission Paths
1444,Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System,"['Haoyang Su', 'Renqi Chen', 'SHIXIANG TANG', 'Zhenfei Yin', 'Xinzhe Zheng', 'Jinzhe Li', 'Biqing Qi', 'Qi Wu', 'Hui Li', 'Wanli Ouyang', 'Philip Torr', 'Bowen Zhou', 'Nanqing Dong']",,Heads Better Improved Scientific Idea Generation LLM Based Multi Agent
1445,Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking,"['Yilong Chen', 'Junyuan Shang', 'Zhenyu Zhang', 'Yanxi Xie', 'Jiawei Sheng', 'Tingwen Liu', 'Shuohuan Wang', 'Yu Sun', 'Hua Wu', 'Haifeng Wang']",,Inner Thinking Transformer Leveraging Dynamic Depth Scaling Foster Adaptive Internal Thinking
1446,Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport,['Yuu Jinnai'],,Document Level Text Generation Minimum Bayes Risk Decoding using Optimal Transport
1447,Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport,"['Minseok Choi', 'Daniel Rim', 'Dohyun Lee', 'Jaegul Choo']",,Opt Investigating Entity Level Unlearning Large Language Models Optimal Transport
1448,Mixture of Small and Large Models for Chinese Spelling Check,"['Ziheng Qiao', 'Houquan Zhou', 'Zhenghua Li']",,Mixture Small Large Models Chinese Spelling Check
1449,DISC: Plug-and-Play Decoding Intervention with Similarity of Characters for Chinese Spelling Check,"['Ziheng Qiao', 'Houquan Zhou', 'Yumeng Liu', 'Zhenghua Li', 'Min Zhang', 'Bo Zhang', 'Chen Li', 'Ji Zhang', 'Fei Huang']","One key characteristic of the Chinese spelling check (CSC) task is that incorrect characters are usually similar to the correct ones in either phonetics or glyph. To accommodate this, previous works usually leverage confusion sets, which suffer from two problems, i.e., difficulty in determining which character pairs to include and lack of probabilities to distinguish items in the set. In this paper, we propose a light-weight plug-and-play DISC (i.e., decoding intervention with similarity of characters) module for CSC models.DISC measures phonetic and glyph similarities between characters and incorporates this similarity information only during the inference phase. This method can be easily integrated into various existing CSC models, such as ReaLiSe, SCOPE, and ReLM, without additional training costs. Experiments on three CSC benchmarks demonstrate that our proposed method significantly improves model performance, approaching and even surpassing the current state-of-the-art models.",DISC Plug Play Decoding Intervention Similarity Characters Chinese Spelling Check key characteristic Chinese spelling check CSC task incorrect characters usually similar correct ones phonetics glyph accommodate previous works usually leverage confusion sets suffer problems e difficulty determining character pairs include lack probabilities distinguish items set paper propose light weight plug play DISC e decoding intervention similarity characters module CSC models DISC measures phonetic glyph similarities characters incorporates similarity information inference phase method easily integrated various existing CSC models ReaLiSe SCOPE ReLM additional training costs Experiments CSC benchmarks demonstrate proposed method significantly improves model performance approaching surpassing current state art models
1450,The Causal Effect of Merge Operations in Bottom-up Tokenisers,"['Pietro Lesci', 'Clara Meister', 'Thomas Hofmann', 'Andreas Vlachos', 'Tiago Pimentel']",,Causal Effect Merge Operations Tokenisers
1451,Value Residual Learning,"['Zhanchao Zhou', 'Tianyi Wu', 'Zhiyun Jiang', 'Fares Obeid', 'Zhenzhong Lan']",,Value Residual Learning
1452,SGIC: A Self-Guided Iterative Calibration Framework for RAG,"['Guanhua Chen', 'Yutong Yao', 'Lidia S. Chao', 'Xuebo Liu', 'Derek F. Wong']",,SGIC Self Guided Iterative Calibration Framework RAG
1453,NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts,"['Muhammad Farid Adilazuarda', 'Musa Izzanardi Wijanarko', 'Lucky Susanto', 'Khumaisa Nur’aini', 'Derry Tanti Wijaya', 'Alham Fikri Aji']",,NusaAksara Multimodal Multilingual Benchmark Preserving Indonesian Indigenous Scripts
1454,LLM-based Rumor Detection via Influence Guided Sample Selection and Game-based Perspective Analysis,"['Zhiliang Tian', 'jingyuan huang', 'Zejiang He', 'Zhen Huang', 'Menglong Lu', 'Linbo Qiao', 'Songzhu Mei', 'Yijie Wang', 'Dongsheng Li']",,LLM based Rumor Detection Influence Guided Sample Selection Game based Perspective Analysis
1455,Hierarchical-Task-Aware Multi-modal Mixture of Incremental LoRA Experts for Embodied Continual Learning,"['Ziqi Jia', 'Anmin Wang', 'Xiaoyang Qu', 'Xiaowen Yang', 'Jianzong Wang']",,Hierarchical Task Aware Multi modal Mixture Incremental LoRA Experts Embodied Continual Learning
1456,SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers,"['Zicong Tang', 'Shi Luohe', 'Zuchao Li', 'Baoyuan Qi', 'Liu Guoming', 'Lefei Zhang', 'Ping Wang']",,SpindleKV Novel KV Cache Reduction Method Balancing Shallow Deep Layers
1457,Medical Graph RAG: Evidence-based Medical Large Language Model via Graph Retrieval-Augmented Generation,"['Junde Wu', 'Jiayuan Zhu', 'Yunli Qi', 'Jingkun Chen', 'Min Xu', 'Filippo Menolascina', 'Yueming Jin', 'Vicente Grau']",,Medical Graph RAG Evidence based Medical Large Language Model Graph Retrieval Augmented Generation
1458,Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models,"['Seungcheol Park', 'Jeongin Bae', 'Beomseok Kwon', 'Minjun Kim', 'Byeongwook Kim', 'Se Jung Kwon', 'U Kang', 'Dongsoo Lee']",,Unifying Uniform Binary coding Quantization Accurate Compression Large Language Models
1459,Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools,"['Junde Wu', 'Jiayuan Zhu', 'Yuyuan Liu', 'Min Xu', 'Yueming Jin']",,Agentic Reasoning Streamlined Framework Enhancing LLM Reasoning Agentic Tools
1460,Probing Relative Interaction and Dynamic Calibration in Multi-modal Entity Alignment,"['Chenxiao Li', 'Jingwei Cheng', 'Qiang Tong', 'Fu Zhang', 'Cairui Wang']",,Probing Relative Interaction Dynamic Calibration Multi modal Entity Alignment
1461,Learn to Memorize: Scalable Continual Learning in Semiparametric Language Models with Mixture-of-Neighbors Induction Memory,"['Guangyue Peng', 'Tao Ge', 'Wen Luo', 'Wei Li', 'Houfeng Wang']",,Learn Memorize Scalable Continual Learning Semiparametric Language Models Mixture Neighbors Induction Memory
1462,"Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings","['Imane Guellil', 'Salomé Andres', 'Atul Anand', 'Bruce Guthrie', 'Huayu Zhang', 'Abul Hasan', 'Honghan Wu', 'Beatrice Alex']",,Adverse Event Extraction Discharge Summaries New Dataset Annotation Scheme Initial Findings
1463,Speed Up Your Code: Progressive Code Acceleration Through Bidirectional Tree Editing,"['Longhui Zhang', 'Jiahao Wang', 'Meishan Zhang', 'GaoXiong Cao', 'Ensheng Shi', 'mayuchi', 'Jun Yu', 'Honghai LIU', 'Jing Li', 'Min Zhang']",,Speed Code Progressive Code Acceleration Bidirectional Tree Editing
1464,Multi-Facet Blending for Faceted Query-by-Example Retrieval,"['Heejin Do', 'Sangwon Ryu', 'Jonghwi Kim', 'Gary Lee']",,Multi Facet Blending Faceted Query Example Retrieval
1465,PIPER: Benchmarking and Prompting Event Reasoning Boundary of LLMs via Debiasing-Distillation Enhanced Tuning,"['Zhicong Lu', 'Changyuan Tian', 'PeiguangLi', 'Li Jin', 'Sirui Wang', 'Wei Jia', 'Ying Shen', 'Guangluan Xu']",,PIPER Benchmarking Prompting Event Reasoning Boundary LLMs Debiasing Distillation Enhanced Tuning
1466,MIR: Methodology Inspiration Retrieval for Scientific Research Problems,"['Aniketh Garikaparthi', 'Manasi Patwardhan', 'Aditya Sanjiv Kanade', 'Aman Hassan', 'Lovekesh Vig', 'Arman Cohan']",,MIR Methodology Inspiration Retrieval Scientific Research Problems
1467,Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models,"['Kexin Chen', 'Dongxia Wang', 'Yi Liu', 'Haonan Zhang', 'Wenhai Wang']",,Sticking Mean Detecting Sticky Tokens Text Embedding Models
1468,Different Speech Translation Models Encode and Translate Speaker Gender Differently,"['Dennis Fucci', 'Marco Gaido', 'Matteo Negri', 'Luisa Bentivogli', 'Andre Martins', 'Giuseppe Attanasio']",,Different Speech Translation Models Encode Translate Speaker Gender Differently
1469,Memorizing is Not Enough: Deep Knowledge Injection Through Reasoning,"['Ruoxi Xu', 'Yunjie Ji', 'Boxi Cao', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Ben He', 'Yingfei Sun', 'Xiangang Li', 'Le Sun']",,Memorizing Deep Knowledge Injection Reasoning
1470,Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples,"['Haesung Pyun', 'Yoonah Park', 'Yohan Jo']",,Improving Dialogue State Tracking Combinatorial Search Context Examples
1471,Pretraining Context Compressor for Large Language Models with Embedding-Based Memory,"['Yuhong Dai', 'Jianxun Lian', 'Yitian Huang', 'Wei Zhang', 'Mingyang Zhou', 'Mingqi Wu', 'Xing Xie', 'Hao Liao']",,Pretraining Context Compressor Large Language Models Embedding Based Memory
1472,Dialogue Systems for Emotional Support via Value Reinforcement,"['Juhee Kim', 'Chunghu Mok', 'Jisun LEE', 'Hyang Sook Kim', 'Yohan Jo']",,Dialogue Systems Emotional Support Value Reinforcement
1473,Length-Induced Embedding Collapse in PLM-based Models,"['Yuqi Zhou', 'Sunhao Dai', 'Zhanshuo Cao', 'Xiao Zhang', 'Jun Xu']",,Length Induced Embedding Collapse PLM based Models
1474,SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction,"['Shester Gueuwou', 'Xiaodan Du', 'Greg Shakhnarovich', 'Karen Livescu', 'Alexander H. Liu']",,SHuBERT Self Supervised Sign Language Representation Learning Multi Stream Cluster Prediction
1475,ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation,"['Lam Thanh Do', 'Aaditya Bodke', 'Pritom Saha Akash', 'Kevin Chen-Chuan Chang']",,ERU KG Efficient Reference aligned Unsupervised Keyphrase Generation
1476,Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling,"['Suvodip Dey', 'Yi-Jyun Sun', 'Gokhan Tur', 'Dilek Hakkani-Tür']",,Know Mistakes Preventing Overreliance Task Oriented Conversational AI Accountability Modeling
1477,"LLMs Trust Humans More, That’s a Problem! Unveiling and Mitigating the Authority Bias in Retrieval-Augmented Generation","['Yuxuan LI', 'Xinwei Guo', 'Jiashi Gao', 'Guanhua Chen', 'Xiangyu Zhao', 'Jiaxin Zhang', 'Quanying Liu', 'Haiyan Wu', 'Xin Yao', 'Xuetao Wei']",,LLMs Trust Humans s Problem Unveiling Mitigating Authority Bias Retrieval Augmented Generation
1478,Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation,"['Dongsheng Zhu', 'Weixian Shi', 'Zhengliang Shi', 'Zhaochun Ren', 'Shuaiqiang Wang', 'Lingyong Yan', 'Dawei Yin']",,Divide Aggregate Efficient Tool Learning Method Parallel Tool Invocation
1479,Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration,"['Yuyi Zhang', 'Peirong Zhang', 'Zhenhua Yang', 'Pengyu Yan', 'Yongxin Shi', 'Pengwei Liu', 'Fengjun Guo', 'Lianwen Jin']",,Reviving Cultural Heritage Novel Approach Comprehensive Historical Document Restoration
1480,PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment,"['Zekun Moore Wang', 'Shenzhi Wang', 'King Zhu', 'Jiaheng Liu', 'Ke Xu', 'Jie Fu', 'Wangchunshu Zhou', 'Wenhao Huang']",,PopAlign Diversifying Contrasting Patterns Comprehensive Alignment
1481,Robust Utility-Preserving Text Anonymization Based on Large Language Models,"['Tianyu Yang', 'Xiaodan Zhu', 'Iryna Gurevych']",,Robust Utility Preserving Text Anonymization Based Large Language Models
1482,SEAL: Scaling to Emphasize Attention for Long-Context Retrieval,"['Changhun Lee', 'Minsang Seok', 'Jun-gyu Jin', 'YoungHyun Cho', 'Eunhyeok Park']",,SEAL Scaling Emphasize Attention Long Context Retrieval
1483,From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment,"['Chongxuan Huang', 'Yongshi Ye', 'Biao Fu', 'Qifeng Su', 'Xiaodong Shi']",,Neurons Semantics Evaluating Cross Linguistic Alignment Capabilities Large Language Models Neurons Alignment
1484,$\mathcal{A}^3$: Automatic Alignment Framework for Attributed Text Generation,"['Yue Wang', 'Haoke Zhang', 'Juntao Li', 'Jinxiong Chang', 'Min Zhang']",,mathcal 3 Automatic Alignment Framework Attributed Text Generation
1485,Towards Better Value Principles for Large Language Model Alignment: A Systematic Evaluation and Enhancement,"['Bingbing Xu', 'Jing Yao', 'Xiaoyuan Yi', 'Aishan Maoliniyazi', 'Xing Xie', 'Xiaofeng Meng']",,Better Value Principles Large Language Model Alignment Systematic Evaluation Enhancement
1486,Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints,"['Kaikai An', 'Shuzheng Si', 'Helan Hu', 'Haozhe Zhao', 'Yuchi Wang', 'Qingyan Guo', 'Baobao Chang']",,Rethinking Semantic Parsing Large Language Models Enhancing LLM Performance Semantic Hints
1487,"Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More",['Arvid Frydenlund'],,Language Models Graph Searching Supervision Adulteration Supervision Make
1488,Comprehensive Analysis of Minimum Bayes Risk Decoding through Bias and Diversity Decomposition,"['Hidetaka Kamigaito', 'Hiroyuki Deguchi', 'Yusuke Sakai', 'Katsuhiko Hayashi', 'Taro Watanabe']",,Comprehensive Analysis Minimum Bayes Risk Decoding Bias Diversity Decomposition
1489,Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models,"['Ido Cohen', 'Daniela Gottesman', 'Mor Geva', 'Raja Giryes']",,Performance Gap Entity Knowledge Extraction Modalities Vision Language Models
1490,SDD: Self-Degraded Defense against Malicious Fine-tuning,"['ZiXuan Chen', 'Weikai Lu', 'Xin Lin', 'Ziqian Zeng']",,SDD Self Degraded Defense Malicious Fine tuning
1491,CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model,"['Wei-Hsin Yeh', 'Yu-An Su', 'Chih-Ning Chen', 'Yi-Hsueh Lin', 'Calvin Ku', 'WENHSIN CHIU', 'Min-Chun Hu', 'Lun-Wei Ku']",,CoachMe Decoding Sport Elements Reference Based Coaching Instruction Generation Model
1492,DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization,"['Hexuan Deng', 'Wenxiang Jiao', 'Xuebo Liu', 'Jing Li', 'Min Zhang', 'Zhaopeng Tu']",,DRPruning Efficient Large Language Model Pruning Distributionally Robust Optimization
1493,How LLMs Comprehend Temporal Structure in Narratives: A Case Study in Cognitive Evaluation of LLMs,"['Karin De Langis', 'Jong Inn Park', 'Andreas Schramm', 'Bin Hu', 'Khanh Chi Le', 'Dongyeop Kang']",,LLMs Comprehend Temporal Structure Narratives Case Study Cognitive Evaluation LLMs
1494,Data Caricatures: On the Representation of African American Language in Pretraining Corpora,"['Nicholas Deas', 'Blake Vente', 'Amith Ananthram', 'Jessica A Grieser', 'Desmond U. Patton', 'Shana Kleiner', 'James R. Shepard III', 'Kathleen McKeown']",,Data Caricatures Representation African American Language Pretraining Corpora
1495,Language Model Probabilities are $Not$ Calibrated in Numeric Contexts,"['Charles Lovering', 'Michael Krumdick', 'Viet Dac Lai', 'Varshini Reddy', 'Seth Ebner', 'Nilesh Kumar', 'Rik Koncel-Kedziorski', 'Chris Tanner']",,Language Model Probabilities Calibrated Numeric Contexts
1496,MDCure: A Scalable Pipeline for Multi-Document Instruction-Following,"['Gabrielle Kaili-May Liu', 'Bowen Shi', 'Avi Caciularu', 'Idan Szpektor', 'Arman Cohan']",,MDCure Scalable Pipeline Multi Document Instruction Following
1497,Misattribution Matters: Quantifying Unfairness in Authorship Attribution,"['Pegah Alipoormolabashi', 'Ajay Patel', 'Niranjan Balasubramanian']",,Misattribution Matters Quantifying Unfairness Authorship Attribution
1498,Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs,"['Sumanth Doddapaneni', 'Mohammed Safi Ur Rahman Khan', 'Dilip Venkatesh', 'Raj Dabre', 'Anoop Kunchukuttan', 'Mitesh M Khapra']",,Cross Lingual Auto Evaluation Assessing Multilingual LLMs
1499,DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process,"['Minjun Zhu', 'Yixuan Weng', 'Linyi Yang', 'Yue Zhang']",,DeepReview Improving LLM based Paper Review Human like Deep Thinking Process
1500,Bypass Back-propagation: Optimization-based Structural Pruning for Large Language Models via Policy Gradient,"['Yuan Gao', 'Zujing Liu', 'WEIZHONG ZHANG', 'Bo Du', 'Gui-Song Xia']",,Bypass propagation Optimization based Structural Pruning Large Language Models Policy Gradient
1501,Zero-Shot Text-to-Speech for Vietnamese,"['Thi Vu', 'Linh The Nguyen', 'Dat Quoc Nguyen']",,Zero Shot Text Speech Vietnamese
1502,Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis,"['Priyanka Kargupta', 'Ishika Agarwal', 'Tal August', 'Jiawei Han']",,Tree Debate Multi Persona Debate Trees Elicit Critical Thinking Scientific Comparative Analysis
1503,Hierarchical Memory Organization for Wikipedia Generation,"['Eugene J. Yu', 'Dawei Zhu', 'Yifan Song', 'Xiangyu Wong', 'Jiebin Zhang', 'Wenxuan Shi', 'Xiaoguang Li', 'Qun Liu', 'Sujian Li']",,Hierarchical Memory Organization Wikipedia Generation
1504,Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks,"['Chenlu Wang', 'Weimin Lyu', 'Ritwik Banerjee']",,Class Distillation Mahalanobis Contrast Efficient Training Paradigm Pragmatic Language Understanding Tasks
1505,Structure-aware Domain Knowledge Injection for Large Language Models,"['Kai Liu', 'Ze Chen', 'Zhihang Fu', 'Wei Zhang', 'Rongxin Jiang', 'Fan Zhou', 'Yaowu Chen', 'Yue Wu', 'Jieping Ye']",,Structure aware Domain Knowledge Injection Large Language Models
1506,FinMME: A Financial Multi-Modal Evaluation Dataset,"['Junyu Luo', 'Zhizhuo KOU', 'Liming Yang', 'Xiao Luo', 'Jinsheng Huang', 'Zhiping Xiao', 'Jingshu Peng', 'Chengzhong LIU', 'Jiaming Ji', 'Xuanzhe Liu', 'Sirui Han', 'Ming Zhang', 'Yike Guo']",,FinMME Financial Multi Modal Evaluation Dataset
1507,Dialectal Coverage And Generalization in Arabic Speech Recognition,"['Amirbek Djanibekov', 'Hawau Olamide Toyin', 'Raghad Alshalan', 'Abdullah Alatir', 'Hanan Aldarmaki']",,Dialectal Coverage Generalization Arabic Speech Recognition
1508,Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure,"['Zheyuan Yang', 'Zexi Kuang', 'Yilun Zhao']",,LLMs Generate High Quality Test Cases Algorithm Problems TestCase Eval Systematic Evaluation Fault Coverage Exposure
1509,EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits,"['Ron Yosef', 'Yonatan Bitton', 'Dani Lischinski', 'Moran Yanuka']",,EditInspector Benchmark Evaluation Text Guided Image Edits
1510,Reconsidering LLM Uncertainty Estimation Methods in the Wild,"['Duygu Nur Yaldiz', 'Yavuz Faruk Bakman', 'Sungmin Kang', 'Tuo Zhang', 'Baturalp Buyukates', 'Sai Praneeth Karimireddy', 'Salman Avestimehr']",,Reconsidering LLM Uncertainty Estimation Methods Wild
1511,Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching,"['Juan Wisznia', 'Cecilia Bolaños', 'Juan Tollo', 'Giovanni Franco Gabriel Marraffini', 'Agustín Andrés Gianolini', 'Noe Fabian Hsueh', 'Luciano Del Corro']",,Optimal Algorithms Optimal Rethinking Sorting LLM Based Pairwise Ranking Batching Caching
1512,Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms,"['Caio Corro', 'Mathieu Lacroix', 'Joseph Le Roux']",,Bregman Conditional Random Fields Sequence Labeling Parallelizable Inference Algorithms
1513,SEE: Strategic Exploration and Exploitation for Cohesive In-Context Prompt Optimization,"['Wendi Cui', 'Jiaxin Zhang', 'Zhuohang Li', 'Hao Sun', 'Damien Lopez', 'Kamalika Das', 'Bradley A. Malin', 'Sricharan Kumar']",,Strategic Exploration Exploitation Cohesive Context Prompt Optimization
1514,Programming by Example meets Historical Linguistics: A Large Language Model Based Approach to Sound Law Induction,"['Atharva Naik', 'Darsh Agrawal', 'Hong Sng', 'Clayton Marr', 'Kexun Zhang', 'Nathaniel Romney Robinson', 'Kalvin Chang', 'Rebecca Byrnes', 'Aravind Mysore', 'Carolyn Rose', 'David R Mortensen']",,Programming Example meets Historical Linguistics Large Language Model Based Approach Sound Law Induction
1515,Synergizing Unsupervised Episode Detection with LLMs for Large-Scale News Events,"['Priyanka Kargupta', 'Yunyi Zhang', 'Yizhu Jiao', 'Siru Ouyang', 'Jiawei Han']",,Synergizing Unsupervised Episode Detection LLMs Large Scale News Events
1516,Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims,"['Priyanka Kargupta', 'Runchu Tian', 'Jiawei Han']",,True False Retrieval Augmented Hierarchical Analysis Nuanced Claims
1517,The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents,"['Feiran Jia', 'Tong Wu', 'Xin Qin', 'Anna Squicciarini']",,Task Shield Enforcing Task Alignment Defend Indirect Prompt Injection LLM Agents
1518,Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking,"['Fabrice Y Harel-Canada', 'Boran Erol', 'Connor Choi', 'Jason Liu', 'Gary Jiarui Song', 'Nanyun Peng', 'Amit Sahai']",,Sandcastles Storm Revisiting Im possibility Strong Watermarking
1519,Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement,"['Yaxuan Kong', 'Yiyuan Yang', 'Yoontae Hwang', 'Wenjie Du', 'Stefan Zohren', 'Zhangyang Wang', 'Ming Jin', 'Qingsong Wen']",,Time MQA Time Series Multi Task Question Answering Context Enhancement
1520,From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs,"['Ruxiao Chen', 'Chenguang Wang', 'Yuran Sun', 'Xilei Zhao', 'Susu Xu']",,Perceptions Decisions Wildfire Evacuation Decision Prediction Behavioral Theory informed LLMs
1521,GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning,"['Shikhhar Siingh', 'Abhinav Rawat', 'Chitta Baral', 'Vivek Gupta']",,GETReason Enhancing Image Context Extraction Hierarchical Multi Agent Reasoning
1522,Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations,"['Vivian Nguyen', 'Lillian Lee', 'Cristian Danescu-Niculescu-Mizil']",,Hanging Balance Pivotal Moments Crisis Counseling Conversations
1523,"Unveiling the Potential of BERT-family: A New Recipe for Building Scalable, General and Competitive Large Language Models","['Yisheng Xiao', 'Juntao Li', 'Wenpeng Hu', 'Min Zhang']",,Unveiling Potential BERT family New Recipe Building Scalable General Competitive Large Language Models
1524,TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora,"['Priyanka Kargupta', 'Nan Zhang', 'Yunyi Zhang', 'Rui Zhang', 'Prasenjit Mitra', 'Jiawei Han']",,TaxoAdapt Aligning LLM Based Multidimensional Taxonomy Construction Evolving Research Corpora
1525,An Empirical Study of Iterative Refinements for Non-autoregressive Translation,"['Yisheng Xiao', 'Pei Guo', 'Zechen Sun', 'Juntao Li', 'Kai Song', 'Min Zhang']",,Empirical Study Iterative Refinements Non autoregressive Translation
1526,Retrofitting Large Language Models with Dynamic Tokenization,"['Darius Feher', 'Ivan Vulić', 'Benjamin Minixhofer']",,Retrofitting Large Language Models Dynamic Tokenization
1527,Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries,"['Vishakh Padmakumar', 'Zichao Wang', 'David Arbour', 'Jennifer Healey']",,Principled Content Selection Generate Diverse Personalized Multi Document Summaries
1528,Bilingual Zero-Shot Stance Detection,"['Chenye Zhao', 'Cornelia Caragea']",,Bilingual Zero Shot Stance Detection
1529,GrammaMT: Improving Machine Translation with Grammar-Informed In-Context Learning,"['Rita Ramos', 'Everlyn Asiko Chimoto', 'Maartje Ter Hoeve', 'Natalie Schluter']",,GrammaMT Improving Machine Translation Grammar Informed Context Learning
1530,Theorem Prover as a Judge for Synthetic Data Generation,"['Joshua Ong Jun Leang', 'Giwon Hong', 'Wenda Li', 'Shay B Cohen']",,Theorem Prover Judge Synthetic Data Generation
1531,Measuring the Effect of Transcription Noise on Downstream Language Understanding Tasks,"['Ori Shapira', 'Shlomo Chazan', 'Amir David Nissan Cohen']",,Measuring Effect Transcription Noise Downstream Language Understanding Tasks
1532,Can You Trust LLMs’ Judgements of the Validity of Simple Inferences With Partisan Conclusions? – No!,"['Reto Gubelmann', 'Ghassen Karray']",,Trust LLMs Judgements Validity Simple Inferences Partisan Conclusions
1533,PARME: Parallel Corpora for Low-Resourced Middle Eastern Languages,"['Sina Ahmadi', 'Rico Sennrich', 'Erfan Karami', 'Ako Marani', 'Parviz Fekrazad', 'Gholamreza Akbarzadeh Baghban', 'Hanah Hadi', 'Semko Heidari', 'Mahîr Dogan', 'Pedram Asadi', 'Dashne Bashir', 'Mohammad Amin Ghodrati', 'Kourosh Amini', 'Zeynab Ashourinezhad', 'Mana Baladi', 'Farshid Ezzati', 'Alireza Ghasemifar', 'Daryoush Hosseinpour', 'Behrooz Abbaszadeh', 'Amin Hassanpour', 'Bahaddin jalal hamaamin', 'Saya Kamal Hama', 'Ardeshir Mousavi', 'Sarko Nazir Hussein', 'Isar Nejadgholi', 'Mehmet Ölmez', 'Horam Osmanpour', 'Rashid Roshan Ramezani', 'Aryan Sediq Aziz', 'Ali Salehi', 'Mohammadreza Yadegari', 'Kewyar Yadegari', 'Sedighe Zamani Roodsari']",,PARME Parallel Corpora Low Resourced Middle Eastern Languages
1534,METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling,"['Bingxuan Li', 'Yiwei Wang', 'Jiuxiang Gu', 'Kai-Wei Chang', 'Nanyun Peng']",,METAL Multi Agent Framework Chart Generation Test Time Scaling
1535,ConLoan: A Contrastive Multilingual Dataset for Evaluating Loanwords,"['Sina Ahmadi', 'Micha David Hess', 'Elena Álvarez-Mellado', 'Alessia Battisti', 'Cui Ding', 'Anne Göhring', 'Yingqiang Gao', 'Zifan Jiang', 'Andrianos Michail', 'Peshmerge Morad', 'Joel Niklaus', 'Maria Christina Panagiotopoulou', 'Stefano Perrella', 'Juri Opitz', 'Anastassia Shaitarova', 'Rico Sennrich']",,ConLoan Contrastive Multilingual Dataset Evaluating Loanwords
1536,A Theory of LLM Sampling: Part Descriptive and Part Prescriptive,"['Sarath Sivaprasad', 'Pramod Kaushik', 'Sahar Abdelnabi', 'Mario Fritz']",,Theory LLM Sampling Descriptive Prescriptive
1537,MEraser: An Effective Fingerprint Erasure Approach for Large Language Models,"['Jingxuan Zhang', 'Zhenhua Xu', 'Rui Hu', 'Wenpeng Xing', 'Xuhong Zhang', 'Meng Han']",,MEraser Effective Fingerprint Erasure Approach Large Language Models
1538,VISA: Retrieval Augmented Generation with Visual Source Attribution,"['Xueguang Ma', 'Shengyao Zhuang', 'Bevan Koopman', 'Guido Zuccon', 'Wenhu Chen', 'Jimmy Lin']",,VISA Retrieval Augmented Generation Visual Source Attribution
1539,DRAMA: Diverse Augmentation from Large Language Models Towards Smaller Generalizable Dense Retrievers,"['Xueguang Ma', 'Xi Victoria Lin', 'Barlas Oguz', 'Jimmy Lin', 'Wen-tau Yih', 'Xilun Chen']",,DRAMA Diverse Augmentation Large Language Models Smaller Generalizable Dense Retrievers
1540,Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs,"['Ziling Cheng', 'Meng Cao', 'Marc-Antoine Rondeau', 'Jackie CK Cheung']",,Stochastic Chameleons Irrelevant Context Hallucinations Reveal Class Based Mis Generalization LLMs
1541,TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation,['Jialin Ouyang'],,TreeCut Synthetic Unanswerable Math Word Problem Dataset LLM Hallucination Evaluation
1542,MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning,"['Chanwoo Park', 'Seungju Han', 'Xingzhi Guo', 'Asuman E. Ozdaglar', 'Kaiqing Zhang', 'Joo-Kyung Kim']",,MAPoRL Multi Agent Post Training Collaborative Large Language Models Reinforcement Learning
1543,Map&Make: Schema Guided Text to Table Generation,"['Naman Ahuja', 'Fenil Bardoliya', 'Chitta Baral', 'Vivek Gupta']",,Map Make Schema Guided Text Table Generation
1544,WinSpot: GUI Grounding Benchmark with Multimodal Large Language Models,"['Zheng Hui', 'Yinheng Li', 'Dan Zhao', 'Colby Banbury', 'Tianyi Chen', 'Kazuhito Koishida']",,WinSpot GUI Grounding Benchmark Multimodal Large Language Models
1545,IRIS: Interpretable Retrieval-Augmented Classification for Long Interspersed Document Sequences,"['Fengnan Li', 'Elliot D. Hill', 'JIANG SHU', 'Jiaxin Gao', 'Matthew M. Engelhard']",,IRIS Interpretable Retrieval Augmented Classification Long Interspersed Document Sequences
1546,Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models,"['Fardin Ahsan Sakib', 'Ziwei Zhu', 'Karen Trister Grace', 'Meliha Yetisgen', 'Ozlem Uzuner']",,Spurious Correlations Understanding Mitigating Shortcut Learning SDOH Extraction Large Language Models
1547,Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images,"['Shengguang Wu', 'Fan-Yun Sun', 'Kaiyue Wen', 'Nick Haber']",,Symmetrical Visual Contrastive Optimization Aligning Vision Language Models Minimal Contrastive Images
1548,Enhancing NER by Harnessing Multiple Datasets with Conditional Variational Autoencoders,"['Taku Oi', 'Makoto Miwa']",,Enhancing NER Harnessing Multiple Datasets Conditional Variational Autoencoders
1549,CHEER-Ekman: Fine-grained Embodied Emotion Classification,"['Phan Anh Duong', 'Cat Luong', 'Divyesh Bommana', 'Tianyu Jiang']",,CHEER Ekman Fine grained Embodied Emotion Classification
1550,Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented LLM-based Retrieval Method,"['Peter Baile Chen', 'Yi Zhang', 'Mike Cafarella', 'Dan Roth']",,Retrieve ARM Alignment Oriented LLM based Retrieval Method
1551,"R2D2: Remembering, Replaying and Dynamic Decision Making with a Reflective Agentic Memory","['Tenghao Huang', 'Kinjal Basu', 'Ibrahim Abdelaziz', 'Pavan Kapanipathi', 'Jonathan May', 'Muhao Chen']",,R2D2 Remembering Replaying Dynamic Decision Making Reflective Agentic Memory
1552,ScanEZ: Integrating Cognitive Models with Self-Supervised Learning for Spatiotemporal Scanpath Prediction,"['Ekta Sood', 'Prajit Dhar', 'Enrica Troiano', 'Rosy Southwell', 'Sidney K. DMello']",,ScanEZ Integrating Cognitive Models Self Supervised Learning Spatiotemporal Scanpath Prediction
1553,FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes,"['Janki Atul Nawale', 'Mohammed Safi Ur Rahman Khan', 'Janani D', 'Danish Pruthi', 'Mitesh M Khapra']",,FairI Tales Evaluation Fairness Indian Contexts Focus Bias Stereotypes
1554,SpeechIQ: Speech-Agentic Intelligence Quotient Across Cognitive Levels in Voice Understanding by Large Language Models,"['Zhen Wan', 'Chao-Han Huck Yang', 'Yahan Yu', 'Jinchuan Tian', 'Sheng Li', 'Ke Hu', 'Zhehuai Chen', 'Shinji Watanabe', 'Fei Cheng', 'Chenhui Chu', 'Yu-Chiang Frank Wang', 'Sadao Kurohashi']",,SpeechIQ Speech Agentic Intelligence Quotient Cognitive Levels Voice Understanding Large Language Models
1555,Predicting Implicit Arguments in Procedural Video Instructions,"['Anil Batra', 'Laura Sevilla-Lara', 'Marcus Rohrbach', 'Frank Keller']",,Predicting Implicit Arguments Procedural Video Instructions
1556,InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models,"['Hao Li', 'Xiaogeng Liu', 'Ning Zhang', 'Chaowei Xiao']",,InjecGuard Benchmarking Mitigating defense Prompt Injection Guardrail Models
1557,CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP,"['Tianyu Yang', 'Lisen Dai', 'Xiangqi Wang', 'Minhao Cheng', 'Yapeng Tian', 'Xiangliang Zhang']",,CLIPErase Efficient Unlearning Visual Textual Associations CLIP
1558,ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding,"['Austin Wang', 'ZeMing Gong', 'Angel X Chang']",,ViGiL3D Linguistically Diverse Dataset 3D Visual Grounding
1559,The time scale of redundancy between prosody and linguistic context,"['Tamar I Regev', 'Chiebuka Ohams', 'Shaylee Xie', 'Lukas Wolf', 'Evelina Fedorenko', 'Alex Warstadt', 'Ethan Wilcox', 'Tiago Pimentel']",,time scale redundancy prosody linguistic context
1560,Improving Fairness of Large Language Models in Multi-document Summarization,"['Haoyuan Li', 'Rui Zhang', 'Snigdha Chaturvedi']",,Improving Fairness Large Language Models Multi document Summarization
1561,Basic Reading Distillation,"['Zhi Zhou', 'Sirui Miao', 'Xiangyu Duan', 'Hao Yang', 'Min Zhang']",,Basic Reading Distillation
1562,Quantized Can Still Be Calibrated: A Unified Framework to Calibration in Quantized Large Language Models,"['Mingyu Zhong', 'Guanchu Wang', 'Yu-Neng Chuang', 'Na Zou']",,Quantized Calibrated Unified Framework Calibration Quantized Large Language Models
1563,Fine-Grained Spatio-Temporal Modeling of Reading Behavior,"['Francesco Ignazio Re', 'Andreas Opedal', 'Glib Manaiev', 'Mario Giulianelli', 'Ryan Cotterell']",,Fine Grained Spatio Temporal Modeling Reading Behavior
1564,More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives,"['Xiaoqing Zhang', 'Ang Lv', 'Yuhan Liu', 'Flood Sung', 'Wei Liu', 'Jian Luan', 'Shuo Shang', 'Xiuying Chen', 'Rui Yan']",,better Enhancing Shot Context Learning Differentiated Reweighting Objectives
1565,Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models,"['Fei Wang', 'Xingchen Wan', 'Ruoxi Sun', 'Jiefeng Chen', 'Sercan O Arik']",,Astute RAG Overcoming Imperfect Retrieval Augmentation Knowledge Conflicts Large Language Models
1566,SubLIME: Subset Selection via Rank Correlation Prediction for Data-Efficient LLM Evaluation,"['Gayathri Saranathan', 'Cong Xu', 'Mahammad Parwez Alam', 'Tarun Kumar', 'Martin Foltin', 'Soon Yee Wong', 'Suparna Bhattacharya']",,SubLIME Subset Selection Rank Correlation Prediction Data Efficient LLM Evaluation
1567,$\text{M}^3\text{GQA}$: A Multi-Entity Multi-Hop Multi-Setting Graph Question Answering Benchmark,"['Boci Peng', 'Yongchao Liu', 'Xiaohe Bo', 'Jiaxin Guo', 'Yun Zhu', 'Xuanbo Fan', 'Chuntao Hong', 'Yan Zhang']",,text M 3 text GQA Multi Entity Multi Hop Multi Setting Graph Question Answering Benchmark
1568,LSSF: Safety Alignment for Large Language Models through Low-Rank Safety Subspace Fusion,"['Guanghao Zhou', 'Panjia Qiu', 'Cen Chen', 'Hongyu Li', 'Jason Chu', 'Xin Zhang', 'JUN ZHOU']",,LSSF Safety Alignment Large Language Models Low Rank Safety Subspace Fusion
1569,Should I Believe in What Medical AI Says? A Chinese Benchmark for Medication Based on Knowledge and Reasoning,"['Yue Wu', 'Yangmin Huang', 'Qianyun Du', 'Lixian Lai', 'Zhiyang He', 'Jiaxue Hu', 'Xiaodong Tao']",,Believe Medical AI Says Chinese Benchmark Medication Based Knowledge Reasoning
1570,ETF: An Entity Tracing Framework for Hallucination Detection in Code Summaries,"['Kishan Maharaj', 'Vitobha Munigala', 'Srikanth G. Tamilselvam', 'Prince Kumar', 'Sayandeep Sen', 'Palani Kodeswaran', 'Abhijit Mishra', 'Pushpak Bhattacharyya']",,ETF Entity Tracing Framework Hallucination Detection Code Summaries
1571,Meta-Tool: Unleash Open-World Function Calling Capabilities of General-Purpose Large Language Models,"['Shengqian Qin', 'Yakun Zhu', 'Linjie Mu', 'Shaoting Zhang', 'Xiaofan Zhang']",,Meta Tool Unleash Open World Function Calling Capabilities General Purpose Large Language Models
1572,Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning,"['Yingjie Zhu', 'Xuefeng Bai', 'Kehai Chen', 'Yang Xiang', 'Jun Yu', 'Min Zhang']",,Benchmarking Improving Large Vision Language Models Fundamental Visual Graph Understanding Reasoning
1573,ISR: Self-Refining Referring Expressions for Entity Grounding,"['Zhuocheng Yu', 'Bingchan Zhao', 'Yifan Song', 'Sujian Li', 'ZHONGHUI HE']",,ISR Self Refining Referring Expressions Entity Grounding
1574,Activating Distributed Visual Region within LLMs for Efficient and Effective Vision-Language Training and Inference,"['Siyuan Wang', 'Dianyi Wang', 'Chengxing Zhou', 'Zejun Li', 'Zhihao Fan', 'Xuanjing Huang', 'zhongyu wei']",,Activating Distributed Visual Region LLMs Efficient Effective Vision Language Training Inference
1575,CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models,"['Yongheng Zhang', 'Xu Liu', 'Ruoxi Zhou', 'Qiguang Chen', 'Hao Fei', 'Wenpeng Lu', 'Libo Qin']",,CCHall Novel Benchmark Joint Cross Lingual Cross Modal Hallucinations Detection Large Language Models
1576,TestNUC: Enhancing Test-Time Computing Approaches through Neighboring Unlabeled Data Consistency,"['Henry Peng Zou', 'Zhengyao Gu', 'Yue Zhou', 'Yankai Chen', 'Weizhi Zhang', 'Liancheng Fang', 'Yibo Wang', 'Yangning Li', 'Kay Liu', 'Philip S. Yu']",,TestNUC Enhancing Test Time Computing Approaches Neighboring Unlabeled Data Consistency
1577,The Esethu Framework: Reimagining Sustainable Dataset Governance and Curation for Low-Resource Languages,"['Jenalea Rajab', 'Anuoluwapo Aremu', 'Everlyn Asiko Chimoto', 'Graham Morrissey', 'Fadel Thior', 'Jessica Ojo', 'Atnafu Lambebo Tonja', 'Wilhelmina Nekoto', 'Pelonomi Moiloa', 'Jade Abbott', 'Vukosi Marivate', 'Benjamin Rosman']",,Esethu Framework Reimagining Sustainable Dataset Governance Curation Low Resource Languages
1578,Theoretical Analysis of Hierarchical Language Recognition and Generation by Transformers without Positional Encoding,"['Daichi Hayakawa', 'Issei Sato']",,Theoretical Analysis Hierarchical Language Recognition Generation Transformers Positional Encoding
1579,Less is More: Explainable and Efficient ICD Code Prediction with Clinical Entities,"['James Douglas', 'Yidong Gan', 'Ben Hachey', 'Jonathan K. Kummerfeld']",,Explainable Efficient ICD Code Prediction Clinical Entities
1580,Benchmarking LLMs and LLM-based Agents in Practical Vulnerability Detection for Code Repositories,"['Alperen Yildiz', 'Sin G Teo', 'Yiling Lou', 'Yebo Feng', 'Chong Wang', 'Dinil Mon Divakaran']",,Benchmarking LLMs LLM based Agents Practical Vulnerability Detection Code Repositories
1581,Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling,"['Junlin Li', 'Guodong DU', 'Jing Li', 'Sim Kuan Goh', 'Wenya Wang', 'Yequan Wang', 'Fangming Liu', 'Ho-Kin Tang', 'Saleh Alharbi', 'Daojing He', 'Min Zhang']",,Multi Modality Expansion Retention LLMs Parameter Merging Decoupling
1582,Serial Lifelong Editing via Mixture of Knowledge Experts,"['YuJu Cheng', 'Yu-Chu Yu', 'Kai-Po Chang', 'Yu-Chiang Frank Wang']",,Serial Lifelong Editing Mixture Knowledge Experts
1583,Towards Efficient LLM Post Training: A Data-centric Perspective,"['Junyu Luo', 'Bohan Wu', 'Xiao Luo', 'Zhiping Xiao', 'Yiqiao Jin', 'Rong-Cheng Tu', 'Nan Yin', 'Yifan Wang', 'Jingyang Yuan', 'Wei Ju', 'Ming Zhang']",,Efficient LLM Post Training Data centric Perspective
1584,IMOL: Incomplete-Modality-Tolerant Learning for Multi-Domain Fake News Video Detection,"['Zhi Zeng', 'Jiaying Wu', 'Minnan Luo', 'Herun Wan', 'Xiangzheng Kong', 'Zihan Ma', 'Guang Dai', 'Qinghua Zheng']",,IMOL Incomplete Modality Tolerant Learning Multi Domain Fake News Video Detection
1585,DDxTutor: Clinical Reasoning Tutoring System with Differential Diagnosis-Based Structured Reasoning,"['Qian Wu', 'Zheyao Gao', 'Longfei Gou', 'Qi Dou']",,DDxTutor Clinical Reasoning Tutoring Differential Diagnosis Based Structured Reasoning
1586,SocialEval: Evaluating Social Intelligence of Large Language Models,"['Jinfeng Zhou', 'Yuxuan Chen', 'Yihan Shi', 'Xuanming Zhang', 'Leqi Lei', 'Yi Feng', 'Zexuan Xiong', 'Miao Yan', 'Xunzhi Wang', 'Yaru Cao', 'Jianing Yin', 'Shuai Wang', 'Quanyu Dai', 'Zhenhua Dong', 'Hongning Wang', 'Minlie Huang']",,SocialEval Evaluating Social Intelligence Large Language Models
1587,Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings,"['Md Messal Monem Miah', 'Adrita Anika', 'Xi Shi', 'Ruihong Huang']",,Hidden Plain Sight Evaluation Deception Detection Capabilities LLMs Multimodal Settings
1588,Analyzing and Mitigating Inconsistency in Discrete Speech Tokens for Neural Codec Language Models,"['Wenrui Liu', 'Zhifang Guo', 'Jin Xu', 'Yuanjun Lv', 'Yunfei Chu', 'Zemin Liu', 'Junyang Lin']",,Analyzing Mitigating Inconsistency Discrete Speech Tokens Neural Codec Language Models
1589,PlanningArena: A Modular Benchmark for Multidimensional Evaluation of Planning and Tool Learning,"['Zihan Zheng', 'Tianle Cui', 'Chuwen Xie', 'Jiahui Pan', 'Qianglong Chen', 'Lewei He']",,PlanningArena Modular Benchmark Multidimensional Evaluation Planning Tool Learning
1590,FocusLLM: Precise Understanding of Long Context by Dynamic Condensing,"['Zhenyu Li', 'Yike Zhang', 'Tengyu Pan', 'Yutao Sun', 'Zhichao Duan', 'Junjie Fang', 'Rong Han', 'Zixuan Wang', 'Jianyong Wang']",,FocusLLM Precise Understanding Long Context Dynamic Condensing
1591,Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings,"['Tengyu Pan', 'Zhichao Duan', 'Zhenyu Li', 'Bowen Dong', 'Ning Liu', 'Xiuxing Li', 'Jianyong Wang']",,Negative Matters Multi Granularity Hard Negative Synthesis Anchor Token Aware Pooling Enhanced Text Embeddings
1592,GPT-4 as a Homework Tutor Can Improve Student Engagement and Learning Outcomes,"['Alessandro Vanzo', 'Sankalan Pal Chowdhury', 'Mrinmaya Sachan']",,GPT 4 Homework Tutor Improve Student Engagement Learning Outcomes
1593,Diffusion Models Through a Global Lens: Are They Culturally Inclusive?,"['Zahra Bayramli', 'Ayhan Suleymanzade', 'Na Min An', 'Huzama Ahmad', 'Eunsu Kim', 'Junyeong Park', 'James Thorne', 'Alice Oh']",,Diffusion Models Global Lens Culturally Inclusive
1594,Representation-based Reward Modeling for Efficient Safety Alignment of Large Language Model,"['Deng Qiyuan', 'Xuefeng Bai', 'Kehai Chen', 'Yaowei Wang', 'Liqiang Nie', 'Min Zhang']",,Representation based Reward Modeling Efficient Safety Alignment Large Language Model
1595,English-based acoustic models perform well in the forced-alignment of two English-Based Pacific Creoles,"['Sam Passmore', 'Lila San Roque', 'Saurabh Nath', 'Keira Mullan', 'Kira Davey', 'Rosey Billington', 'Nick Thieberger', 'Danielle Barth']",,English based acoustic models perform forced alignment English Based Pacific Creoles
1596,Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing,"['Kaishuai Xu', 'Tiezheng YU', 'Wenjun Hou', 'Yi Cheng', 'Chak Tou Leong', 'Liangyou Li', 'Xin Jiang', 'Lifeng Shang', 'Qun Liu', 'Wenjie Li']",,Subtle Errors Reasoning Preference Learning Error injected Self editing
1597,Truth Knows No Language: Evaluating Truthfulness Beyond English,"['Blanca Calvo Figueras', 'Eneko Sagarzazu', 'Julen Etxaniz', 'Jeremy Barnes', 'Pablo Gamallo', 'Iria de-Dios-Flores', 'Rodrigo Agerri']",,Truth Knows Language Evaluating Truthfulness English
1598,Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability,"['Yusuke Sakai', 'Hidetaka Kamigaito', 'Taro Watanabe']",,Revisiting Compositional Generalization Capability Large Language Models Considering Instruction Following Ability
1599,Batayan: A Filipino NLP benchmark for evaluating Large Language Models,"['Jann Railey Montalan', 'Jimson Paulo Layacan', 'David Demitri Africa', 'Richell Isaiah S. Flores', 'Michael T. Lopez II', 'Theresa Denise Magsajo', 'Anjanette Cayabyab', 'William Chandra Tjhi']",,Batayan Filipino NLP benchmark evaluating Large Language Models
1600,HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real and Synthetic Claims,"['Michiel van der Meer', 'Pavel Korshunov', 'Sébastien Marcel', 'Lonneke van der Plas']",,HintsOfTruth Multimodal Checkworthiness Detection Dataset Real Synthetic Claims
1601,CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory,"['Weichen Zhang', 'Chen Gao', 'Shiquan Yu', 'Ruiying Peng', 'Baining Zhao', 'Qian Zhang', 'Jinqiang Cui', 'Xinlei Chen', 'Yong Li']",,CityNavAgent Aerial Vision Language Navigation Hierarchical Semantic Planning Global Memory
1602,It’s Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems,"['Iuliia Zaitova', 'Badr M. Abdullah', 'Wei Xue', 'Dietrich Klakow', 'Bernd Möbius', 'Tania Avgustinova']",,s Walk Park Challenges Idiom Translation Speech text Systems
1603,"PolyNarrative: A Multilingual, Multilabel, Multi-domain Dataset for Narrative Extraction from News Articles","['Nikolaos Nikolaidis', 'Nicolas Stefanovitch', 'Purificação Silvano', 'Dimitar Iliyanov Dimitrov', 'Roman Yangarber', 'Nuno Guimarães', 'Elisa Sartori', 'Ion Androutsopoulos', 'Preslav Nakov', 'Giovanni Da San Martino', 'Jakub Piskorski']",,PolyNarrative Multilingual Multilabel Multi domain Dataset Narrative Extraction News Articles
1604,Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human?,"['Takumi Goto', 'Yusuke Sakai', 'Taro Watanabe']",,Rethinking Evaluation Metrics Grammatical Error Correction Use Different Evaluation Process Human
1605,A Parameter-Efficient and Fine-Grained Prompt Learning for Vision-Language Models,"['Yongbin Guo', 'Shuzhen Li', 'zhulin liu', 'Tong Zhang', 'C.L.Philip Chen']",,Parameter Efficient Fine Grained Prompt Learning Vision Language Models
1606,Persona Dynamics: Unveiling the Impact of Persona Traits on Agents in Text-Based Games,"['Seungwon Lim', 'Seungbeen Lee', 'Dongjun Min', 'Youngjae Yu']",,Persona Dynamics Unveiling Impact Persona Traits Agents Text Based Games
1607,SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science,"['Jie Ying', 'Zihong Chen', 'Zhefan Wang', 'Wanli Jiang', 'Chenyang Wang', 'Zhonghang Yuan', 'Haoyang Su', 'Huanjun Kong', 'Fan Yang', 'Nanqing Dong']",,SeedBench Multi task Benchmark Evaluating Large Language Models Seed Science
1608,𝛿-Stance: A Large-Scale Real World Dataset of Stances in Legal Argumentation,"['Ankita Gupta', 'Douglas Rice', 'Brendan O’Connor']",,𝛿 Stance Large Scale Real World Dataset Stances Legal Argumentation
1609,Re$^{3}$Syn: A Dependency-Based Data Synthesis Framework for Long-Context Post-training,"['Zhiyang Zhang', 'Ziqiang Liu', 'Huiming Wang', 'Renke Shan', 'Li Kuang', 'Lu Wang']",,3 Syn Dependency Based Data Synthesis Framework Long Context Post training
1610,Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions,"['Jihyoung Jang', 'Minwook Bae', 'Minji Kim', 'Dilek Hakkani-Tür', 'Hyounghun Kim']",,Enabling Chatbots Eyes Ears Immersive Multimodal Conversation Dynamic Interactions
1611,Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation,"['Chengwei Qin', 'Wenxuan Zhou', 'Karthik Abinav Sankararaman', 'Nanshu Wang', 'Tengyu Xu', 'Alexander Radovic', 'Eryk Helenowski', 'Arya Talebzadeh', 'Aditya Tayade', 'Sinong Wang', 'Shafiq Joty', 'Han Fang', 'Hao Ma']",,Learning Auxiliary Tasks Improves Reference Free Hallucination Detection Open Domain Long Form Generation
1612,Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach,"['Xingyu Li', 'Chen Gong', 'Guohong Fu']",,Multimodal Coreference Resolution Chinese Social Media Dialogues Dataset Benchmark Approach
1613,TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification,"['Yindu Su', 'Huike Zou', 'Lin Sun', 'Ting Zhang', 'Haiyang Yang', 'Chen Li Yu', 'David Lo', 'qingheng zhang', 'Shuguang Han', 'jufeng chen']",,TACLR Scalable Efficient Retrieval based Method Industrial Product Attribute Value Identification
1614,A Review of Theory of Mind Capabilities in Large Language Models,"['Ruirui Chen', 'Weifeng Jiang', 'Chengwei Qin', 'Cheston Tan']",,Review Theory Mind Capabilities Large Language Models
1615,Completing A Systematic Review in Hours instead of Months with Interactive AI Agents,"['Rui Qiu', 'Shijie Chen', 'Yu Su', 'Po-Yin Yen', 'Han Wei Shen']",,Completing Systematic Review Hours instead Months Interactive AI Agents
1616,CMHKF: Cross-Modality Heterogeneous Knowledge Fusion for Weakly Supervised Video Anomaly Detection,"['Shengping Song', 'Yongsen Zheng', 'Wuchun He', 'Guohua Wang']",,CMHKF Cross Modality Heterogeneous Knowledge Fusion Weakly Supervised Video Anomaly Detection
1617,CLaSp: In-Context Layer Skip for Self-Speculative Decoding,"['Longze Chen', 'Renke Shan', 'Huiming Wang', 'Lu Wang', 'Ziqiang Liu', 'Run Luo', 'Jiawei Wang', 'Hamid Alinejad-Rokny', 'Min Yang']",,CLaSp Context Layer Skip Self Speculative Decoding
1618,Teaching Text Agents to Learn Sequential Decision Making from Failure,"['Canasai Kruengkrai', 'Koichiro Yoshino']",,Teaching Text Agents Learn Sequential Decision Making Failure
1619,The Harmonic Structure of Information Contours,"['Eleftheria Tsipidi', 'Samuel Kiegeland', 'Franz Nowak', 'Tianyang Xu', 'Ethan Wilcox', 'Alex Warstadt', 'Ryan Cotterell', 'Mario Giulianelli']",,Harmonic Structure Information Contours
1620,REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark,"['navve wasserman', 'roi pony', 'Oshri Naparstek', 'Adi Raz Goldfarb', 'Eli Schwartz', 'Udi Barzelay', 'Leonid Karlinsky']",,REAL MM RAG Real World Multi Modal Retrieval Benchmark
1621,Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models,"['Mats Faulborn', 'Indira Sen', 'Max Pellert', 'Andreas Spitz', 'David Garcia']",,Little Left Theory grounded Measure Political Bias Large Language Models
1622,LongSafety: Evaluating Long-Context Safety of Large Language Models,"['Yida Lu', 'Jiale Cheng', 'Zhexin Zhang', 'Shiyao Cui', 'Cunxiang Wang', 'Xiaotao Gu', 'Yuxiao Dong', 'Jie Tang', 'Hongning Wang', 'Minlie Huang']",,LongSafety Evaluating Long Context Safety Large Language Models
1623,Exploiting Contextual Knowledge in LLMs through $\mathcal{V}$-usable Information based Layer Enhancement,"['Xiaowei Yuan', 'Zhao Yang', 'Ziyang Huang', 'Yequan Wang', 'Siqi Fan', 'Yiming Ju', 'Jun Zhao', 'Kang Liu']",,Exploiting Contextual Knowledge LLMs mathcal V usable Information based Layer Enhancement
1624,Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights,"['Sooyung Choi', 'Jaehyeok Lee', 'Xiaoyuan Yi', 'Jing Yao', 'Xing Xie', 'JinYeong Bak']",,Unintended Harms Value Aligned LLMs Psychological Empirical Insights
1625,Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval,"['Hani Alomari', 'Anushka Sivakumar', 'Andrew Zhang', 'Chris Thomas']",,Maximal Matching Matters Preventing Representation Collapse Robust Cross Modal Retrieval
1626,The Noisy Path from Source to Citation: Measuring How Scholars Engage with Past Research,"['Hong Chen', 'Misha Teplitskiy', 'David Jurgens']",,Noisy Path Source Citation Measuring Scholars Engage Past Research
1627,MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in Explainable Recommendation,"['Ching-Wen Yang', 'Zhi-Quan Feng', 'Ying-Jia Lin', 'Che Wei Chen', 'Kun-da Wu', 'Hao Xu', 'Yao Jui-Feng', 'Hung-Yu Kao']",,MAPLE Enhancing Review Generation Multi Aspect Prompt LEarning Explainable Recommendation
1628,Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers,"['Clément Dumas', 'Chris Wendler', 'Veniamin Veselovsky', 'Giovanni Monea', 'Robert West']",,Separating Tongue Thought Activation Patching Reveals Language Agnostic Concept Representations Transformers
1629,Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey,"['Ivan Vegner', 'Sydelle de Souza', 'Valentin Forch', 'Martha Lewis', 'Leonidas A. A. Doumas']",,Behavioural vs Representational Systematicity End End Models Opinionated Survey
1630,Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models,"['Boheng Sheng', 'Jiacheng Yao', 'Meicong Zhang', 'Guoxiu He']",,Dynamic Chunking Selection Reading Comprehension Ultra Long Context Large Language Models
1631,DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering,"['Rong Cheng', 'Jinyi Liu', 'YAN ZHENG', 'Fei Ni', 'Jiazhen Du', 'Hangyu Mao', 'Fuzheng Zhang', 'Bo Wang', 'Jianye HAO']",,DualRAG Dual Process Approach Integrate Reasoning Retrieval Multi Hop Question Answering
1632,Deliberate Reasoning for Language Models as Structure-aware Planning with Accurate World Model,"['Siheng Xiong', 'Ali Payani', 'Yuan Yang', 'Faramarz Fekri']",,Deliberate Reasoning Language Models Structure aware Planning Accurate World Model
1633,Refining Salience-Aware Sparse Fine-Tuning Strategies for Language Models,"['Xinxin Liu', 'Aaron Thomas', 'Cheng Zhang', 'Jianyi Cheng', 'Yiren Zhao', 'Xitong Gao']",,Refining Salience Aware Sparse Fine Tuning Strategies Language Models
1634,Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse Attention,"['Emily Xiao', 'Chin-Jou Li', 'Yilin Zhang', 'Graham Neubig', 'Amanda Bertsch']",,Efficient Shot Context Learning Dynamic Block Sparse Attention
1635,ScaleBiO: Scalable Bilevel Optimization for LLM Data Reweighting,"['Rui Pan', 'Dylan Zhang', 'Hanning Zhang', 'Xingyuan Pan', 'Minrui Xu', 'Jipeng Zhang', 'Renjie Pi', 'Xiaoyu Wang', 'Tong Zhang']",,ScaleBiO Scalable Bilevel Optimization LLM Data Reweighting
1636,BeaverTails v2: Towards Multi-Level Safety Alignment for LLMs with Human Preference,"['Jiaming Ji', 'Donghai Hong', 'Borong Zhang', 'Boyuan Chen', 'Josef Dai', 'Boren Zheng', 'Tianyi Qiu', 'Jiayi Zhou', 'Kaile Wang', 'Boxun Li', 'Sirui Han', 'Yike Guo', 'Yaodong Yang']",,BeaverTails v2 Multi Level Safety Alignment LLMs Human Preference
1637,What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective,"['Ming Li', 'Yanhong Li', 'Tianyi Zhou']",,Happened LLMs Layers Trained Fast vs Slow Thinking Gradient Perspective
1638,Beyond Text Compression: Evaluating Tokenizers Across Scales,"['Jonas F. Lotz', 'António V. Lopes', 'Stephan Peitz', 'Hendra Setiawan', 'Leonardo Emili']",,Text Compression Evaluating Tokenizers Scales
1639,WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging,"['Ahmed Elhady', 'Eneko Agirre', 'Mikel Artetxe']",,WiCkeD Simple Method Make Multiple Choice Benchmarks Challenging
1640,Emergent Abilities of Large Language Models under Continued Pre-training for Language Adaptation,"['Ahmed Elhady', 'Eneko Agirre', 'Mikel Artetxe']",,Emergent Abilities Large Language Models Continued Pre training Language Adaptation
1641,R-Fairness: Assessing Fairness of Ranking in Subjective Data,"['Lorenzo Balzotti', 'Donatella Firmani', 'Jerin George Mathew', 'Riccardo Torlone', 'Sihem Amer-Yahia']",,R Fairness Assessing Fairness Ranking Subjective Data
1642,RePanda: Pandas-powered Tabular Verification and Reasoning,"['Atoosa Chegini', 'Keivan Rezaei', 'Hamid Eghbalzadeh', 'Soheil Feizi']",,RePanda Pandas powered Tabular Verification Reasoning
1643,Towards Style Alignment in Cross-Cultural Translation,"['Shreya Havaldar', 'Adam Stein', 'Eric Wong', 'Lyle Ungar']",,Style Alignment Cross Cultural Translation
1644,TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining,"['Jeffrey Li', 'Mohammadreza Armandpour', 'Seyed Iman Mirzadeh', 'Sachin Mehta', 'Vaishaal Shankar', 'Raviteja Vemulapalli', 'Samy Bengio', 'Oncel Tuzel', 'Mehrdad Farajtabar', 'Hadi Pouransari', 'Fartash Faghri']",,TiC LM Web Scale Benchmark Time Continual LLM Pretraining
1645,Entailed Between the Lines: Incorporating Implication into NLI,"['Shreya Havaldar', 'Hamidreza Alvari', 'John Palowitch', 'Mohammad Javad Hosseini', 'Senaka Buthpitiya', 'Alex Fabrikant']",,Entailed Lines Incorporating Implication NLI
1646,Multi-Level Explanations for Generative Language Models,"['Lucas Monteiro Paes', 'Dennis Wei', 'Hyo Jin Do', 'Hendrik Strobelt', 'Ronny Luss', 'Amit Dhurandhar', 'Manish Nagireddy', 'Karthikeyan Natesan Ramamurthy', 'Prasanna Sattigeri', 'Werner Geyer', 'Soumya Ghosh']",,Multi Level Explanations Generative Language Models
1647,A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems,"['Đorđe Klisura', 'Astrid R Bernaga Torres', 'Anna Karen Gárate-Escamilla', 'Rajesh Roshan Biswal', 'Ke Yang', 'Hilal Pataci', 'Anthony Rios']",,Multi Agent Framework Mitigating Dialect Biases Privacy Policy Question Answering Systems
1648,Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens,"['Xu Ouyang', 'Tao Ge', 'Thomas Hartvigsen', 'Zhisong Zhang', 'Haitao Mi', 'Dong Yu']",,Low Bit Quantization Favors Undertrained LLMs Scaling Laws Quantized LLMs 100T Training Tokens
1649,Enhancing User-Controlled Text-to-Image Generation with Layout-Aware Personalization,"['Hongliang Luo', 'Wei Xi']",,Enhancing User Controlled Text Image Generation Layout Aware Personalization
1650,LETS-C: Leveraging Text Embedding for Time Series Classification,"['Rachneet Kaur', 'Zhen Zeng', 'Tucker Balch', 'Manuela Veloso']",,LETS C Leveraging Text Embedding Time Series Classification
1651,Benchmarking Video-Language Models for Embodied Motion Cognition in Urban Open-Ended Spaces,"['Baining Zhao', 'Jianjie Fang', 'Zichao Dai', 'Ziyou Wang', 'Jirong Zha', 'Weichen Zhang', 'Chen Gao', 'Yue Wang', 'Jinqiang Cui', 'Xinlei Chen', 'Yong Li']",,Benchmarking Video Language Models Embodied Motion Cognition Urban Open Ended Spaces
1652,"HELIOS: Harmonizing Early Fusion, Late Fusion, and LLM Reasoning for Multi-Granular Table-Text Retrieval","['Sungho Park', 'Joohyung Yun', 'Jongwuk Lee', 'Wook-Shin Han']",,HELIOS Harmonizing Early Fusion Late Fusion LLM Reasoning Multi Granular Table Text Retrieval
1653,ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities,"['Adhiraj Ghosh', 'Sebastian Dziadzio', 'Ameya Prabhu', 'Vishaal Udandarao', 'Samuel Albanie', 'Matthias Bethge']",,ONEBench Test Sample Level Benchmarking Open Ended Capabilities
1654,La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America,"['María Grandury', 'Javier Aula-Blasco', 'Júlia Falcão', 'Clémentine Fourrier', 'Miguel González Saiz', 'Gonzalo Martínez', 'Gonzalo Santamaria Gomez', 'Rodrigo Agerri', 'Nuria Aldama García', 'Luis Chiruzzo', 'Javier Conde', 'Helena Gomez Adorno', 'Marta Guerrero Nieto', 'Guido Ivetta', 'Natàlia López Fuertes', 'Flor Miriam Plaza-del-Arco', 'María-Teresa Martín-Valdivia', 'Helena Montoro Zamorano', 'Carmen Muñoz Sanz', 'Pedro Reviriego', 'Leire Rosado Plaza', 'Alejandro Vaca Serrano', 'Estrella Vallecillo-Rodríguez', 'Jorge Vallego', 'Irune Zubiaga']",,La Leaderboard Large Language Model Leaderboard Spanish Varieties Languages Spain Latin America
1655,Navigating the Prompt Space: Supervision Matters in CoT When Reasoning Misleads,"['Xiang Zhang', 'Juntai Cao', 'Chenyu You', 'Dujian Ding']",,Navigating Prompt Space Supervision Matters CoT Reasoning Misleads
1656,Energy Considerations of Large Language Model Inference and Efficiency Optimizations,"['Jared Fernandez', 'Clara Na', 'Vashisth Tiwari', 'Yonatan Bisk', 'Sasha Luccioni', 'Emma Strubell']",,Energy Considerations Large Language Model Inference Efficiency Optimizations
1657,Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models,"['Lior Belenki', 'Alekh Agarwal', 'Tianze Shi', 'Kristina Toutanova']",,Optimizing Pre Training Data Mixtures Mixtures Data Expert Models
1658,BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving,"['Ran Xin', 'Chenguang Xi', 'Jie Yang', 'Feng Chen', 'Hang Wu', 'Xia Xiao', 'Yifan Sun', 'Shen Zheng', 'Ming Ding']",,BFS Prover Scalable Best Tree Search LLM based Automatic Theorem Proving
1659,Magnet: Multi-turn Tool-use Data Synthesis and Distillation via Graph Translation,"['Fan Yin', 'Zifeng Wang', 'I-Hung Hsu', 'Jun Yan', 'Ke Jiang', 'Yanfei Chen', 'Jindong Gu', 'Long Le', 'Kai-Wei Chang', 'Chen-Yu Lee', 'Hamid Palangi', 'Tomas Pfister']",,Magnet Multi turn Tool use Data Synthesis Distillation Graph Translation
1660,Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning,"['Nathaniel Krasner', 'Nicholas Lanuzo', 'Antonios Anastasopoulos']",,Cross Lingual Representation Alignment Contrastive Image Caption Tuning
1661,Logic-Regularized Verifier Elicits Reasoning from LLMs,"['Xinyu Wang', 'Changzhi Sun', 'Lian Cheng', 'Yuanbin Wu', 'Dell Zhang', 'Xuelong Li', 'Xiaoling Wang']",,Logic Regularized Verifier Elicits Reasoning LLMs
1662,Squeezed Attention: Fast Fixed Context Processing for Long Context Length LLM Applications,"['Coleman Richard Charles Hooper', 'Sehoon Kim', 'Hiva Mohammadzadeh', 'Monishwaran Maheswaran', 'Sebastian Zhao', 'June Paik', 'Michael W. Mahoney', 'Kurt Keutzer', 'Amir Gholami']",,Squeezed Attention Fast Fixed Context Processing Long Context Length LLM Applications
1663,LangMark: A Multilingual Dataset for Automatic Post-Editing,"['Diego Velazquez', 'Mikaela Grace', 'Konstantinos Karageorgos', 'Lawrence Carin', 'Aaron Schliem', 'Dimitrios Zaikis', 'Roger Wechsler']",,LangMark Multilingual Dataset Automatic Post Editing
1664,Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer,"['Guodong DU', 'Jing Li', 'Zitao Fang', 'Junlin Li', 'Runhua Jiang', 'Shuyang Yu', 'Yifei Guo', 'Yangneng Chen', 'Sim Kuan Goh', 'Ho-Kin Tang', 'Daojing He', 'Honghai LIU', 'Min Zhang']",,Neural Parameter Search Slimmer Fine Tuned Models Better Transfer
1665,Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models,"['Zenghui Yuan', 'Yangming Xu', 'Jiawen Shi', 'Pan Zhou', 'Lichao Sun']",,Merge Hijacking Backdoor Attacks Model Merging Large Language Models
1666,LAMB: A Training-Free Method to Enhance the Long-Context Understanding of SSMs via Attention-Guided Token Filtering,"['Zhifan Ye', 'Zheng Wang', 'Kejing Xia', 'Jihoon Hong', 'Leshu Li', 'Lexington Whalen', 'Cheng Wan', 'Yonggan Fu', 'Yingyan Celine Lin', 'Souvik Kundu']",,LAMB Training Free Method Enhance Long Context Understanding SSMs Attention Guided Token Filtering
1667,Where Are We? Evaluating LLM Performance on African Languages,"['Ife Adebara', 'Hawau Olamide Toyin', 'Nahom Tesfu Ghebremichael', 'AbdelRahim A. Elmadany', 'Muhammad Abdul-Mageed']",,Evaluating LLM Performance African Languages
1668,Beyond Output Matching: Bidirectional Alignment for Enhanced In-Context Learning,"['Chengwei Qin', 'Wenhan Xia', 'Fangkai Jiao', 'Chen Chen', 'Yuchen Hu', 'Bosheng Ding', 'Ruirui Chen', 'Shafiq Joty']",,Output Matching Bidirectional Alignment Enhanced Context Learning
1669,CiteEval: Principle-Driven Citation Evaluation for Source Attribution,"['Yumo Xu', 'Peng Qi', 'Jifan Chen', 'Kunlun Liu', 'Rujun Han', 'Lan Liu', 'Bonan Min', 'Vittorio Castelli', 'Arshit Gupta', 'Zhiguo Wang']",,CiteEval Principle Driven Citation Evaluation Source Attribution
1670,HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agentic Tasks with Large Language Models,"['Mengkang Hu', 'Tianxing Chen', 'Qiguang Chen', 'Yao Mu', 'Wenqi Shao', 'Ping Luo']",,HiAgent Hierarchical Working Memory Management Solving Long Horizon Agentic Tasks Large Language Models
1671,Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models,"['Jongho Kim', 'seung-won hwang']",,Counterfactual Consistency Prompting Relative Temporal Understanding Large Language Models
1672,EducationQ: Evaluating LLMs’ Teaching Capabilities Through Multi-Agent Dialogue Framework,"['Yao Shi', 'Rongkeng Liang', 'Yong Xu']",,EducationQ Evaluating LLMs Teaching Capabilities Multi Agent Dialogue Framework
1673,KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning,"['Peiqi Sui', 'Juan Diego Rodriguez', 'Philippe Laban', 'J. Dean Murphy', 'Joseph P. Dexter', 'Richard Jean So', 'Samuel Baker', 'Pramit Chaudhuri']",,KRISTEVA Close Reading Novel Task Benchmarking Interpretive Reasoning
1674,Efficient Domain Continual pretraining by Mitigating the Stability Gap,"['Yiduo Guo', 'Jie Fu', 'Huishuai Zhang', 'Dongyan Zhao']",,Efficient Domain Continual pretraining Mitigating Stability Gap
1675,Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic LLMs,"['Fakhraddin Alwajih', 'Abdellah EL MEKKI', 'Samar Mohamed Magdy', 'AbdelRahim A. Elmadany', 'OMER NACAR', 'El Moatez Billah Nagoudi', 'Reem Abdel-Salam', 'Hanin atwany', 'Youssef Nafea', 'Abdulfattah Mohammed Yahya', 'Rahaf Alhamouri', 'Hamzah A. Alsayadi', 'Hiba Zayed', 'Sara Shatnawi', 'Serry Sibaee', 'Yasir ECH-CHAMMAKHY', 'Walid Al-Dhabyani', 'Marwa Mohamed Ali', 'Imen JARRAYA', 'Ahmed Oumar El-Shangiti', 'Aisha Alraeesi', 'Mohammed Anwar AL-Ghrawi', 'Abdulrahman S. Al-Batati', 'Elgizouli Mohamed', 'Noha Taha Elgindi', 'Muhammed Saeed', 'Houdaifa Atou', 'Issam AIT YAHIA', 'Abdelhak Bouayad', 'Mohammed Machrouh', 'AMAL MAKOUAR', 'Dania Alkawi', 'Mukhtar Mohamed', 'Safaa Taher Abdelfadil', 'Amine Ziad Ounnoughene', 'Anfel ROUABHIA', 'Rwaa Assi', 'Ahmed Sorkatti', 'Mohamedou cheikh tourad', 'Anis Koubaa', 'Ismail Berrada', 'Mustafa Jarrar', 'Shady Shehata', 'Muhammad Abdul-Mageed']",,Palm Culturally Inclusive Linguistically Diverse Dataset Arabic LLMs
1676,NewsInterview: a Dataset and a Playground to Evaluate LLMs’ Grounding Gap via Informational Interviews,"['Alexander Spangher', 'Michael Lu', 'Sriya Kalyan', 'Hyundong Justin Cho', 'Tenghao Huang', 'Weiyan Shi', 'Jonathan May']",,NewsInterview Dataset Playground Evaluate LLMs Grounding Gap Informational Interviews
1677,CFBench: A Comprehensive Constraints-Following Benchmark for LLMs,"['Tao Zhang', 'ChengLIn Zhu', 'Yanjun Shen', 'Wenjing Luo', 'Yan Zhang', 'Hao Liang', 'Tao Zhang', 'Fan Yang', 'Mingan Lin', 'Yujing Qiao', 'Weipeng Chen', 'Bin CUI', 'Wentao Zhang', 'Zenan Zhou']",,CFBench Comprehensive Constraints Following Benchmark LLMs
1678,Towards Building Large Scale Datasets and State-of-the-Art Automatic Speech Translation Systems for 13 Indian Languages,"['Ashwin Sankar', 'Sparsh Jain', 'Nikhil Narasimhan', 'Devilal Choudhary', 'Dhairya Suman', 'Mohammed Safi Ur Rahman Khan', 'Anoop Kunchukuttan', 'Mitesh M Khapra', 'Raj Dabre']",,Building Large Scale Datasets State Art Automatic Speech Translation Systems 13 Indian Languages
1679,CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG,"['yang tian', 'Fan Liu', 'Jingyuan Zhang', 'V. W.', 'Yupeng Hu', 'Liqiang Nie']",,CoRe MMRAG Cross Source Knowledge Reconciliation Multimodal RAG
1680,"Mapping 1,000+ Language Models via the Log-Likelihood Vector","['Momose Oyama', 'Hiroaki Yamagiwa', 'Yusuke Takase', 'Hidetoshi Shimodaira']",,Mapping 1 000 Language Models Log Likelihood Vector
1681,ConsistencyChecker: Tree-based Evaluation of LLM Generalization Capabilities,"['Zhaochen Hong', 'Haofei Yu', 'Jiaxuan You']",,ConsistencyChecker Tree based Evaluation LLM Generalization Capabilities
1682,Robust Estimation of Population-Level Effects in Repeated-Measures NLP Experimental Designs,"['Alejandro Benito-Santos', 'Adrian Ghajari', 'Víctor Fresno']",,Robust Estimation Population Level Effects Repeated Measures NLP Experimental Designs
1683,FactBench: A Dynamic Benchmark for In-the-Wild Language Model Factuality Evaluation,"['Farima Fatahi Bayat', 'Lechen Zhang', 'Sheza Munir', 'Lu Wang']",,FactBench Dynamic Benchmark Wild Language Model Factuality Evaluation
1684,Training-free LLM Merging for Multi-task Learning,"['Zichuan Fu', 'Xian Wu', 'Yejing Wang', 'Wanyu Wang', 'Shanshan Ye', 'Hongzhi Yin', 'Yi Chang', 'Yefeng Zheng', 'Xiangyu Zhao']",,Training free LLM Merging Multi task Learning
1685,Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection,"['Mingyu Derek Ma', 'Yanna Ding', 'Zijie Huang', 'Jianxi Gao', 'Yizhou Sun', 'Wei Wang']",,Inferring Logits Exploring Best Practices Decoding Free Generative Candidate Selection
1686,Comparison-based Active Preference Learning for Multi-dimensional Personalization,"['Minhyeon Oh', 'Seungjoon Lee', 'Jungseul Ok']",,Comparison based Active Preference Learning Multi dimensional Personalization
1687,OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models,"['Siming Huang', 'Tianhao Cheng', 'Jason Klein Liu', 'Weidi Xu', 'JIARAN HAO', 'Liuyihan Song', 'Yang Xu', 'Jian Yang', 'Jiaheng Liu', 'Chenchen Zhang', 'Linzheng Chai', 'Ruifeng Yuan', 'Xianzhen Luo', 'Qiufeng Wang', 'YuanTao Fan', 'Qingfu Zhu', 'Zhaoxiang Zhang', 'Yang Gao', 'Jie Fu', 'Qian Liu', 'Houyi Li', 'Ge Zhang', 'Yuan Qi', 'Xu Yinghui', 'Wei Chu', 'Zili Wang']",,OpenCoder Open Cookbook Tier Code Large Language Models
1688,LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs,"['Chansung Park', 'Juyong Jiang', 'Fan Wang', 'Sayak Paul', 'Jing Tang']",,LlamaDuo LLMOps Pipeline Seamless Migration Service LLMs Small Scale Local LLMs
1689,AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment,"['Anastasia Ivanova', 'Zoya Volovikova', 'Bakaeva Eva', 'Alexey Kovalev', 'Aleksandr Panov']",,AmbiK Dataset Ambiguous Tasks Kitchen Environment
1690,SocialDuolingo: Interactive Evaluation for Cultural Competence in Language Agents,"['Jincenzi Wu', 'Jianxun Lian', 'Dingdong WANG', 'Helen M. Meng']",,SocialDuolingo Interactive Evaluation Cultural Competence Language Agents
1691,Scalable Vision Language Model Training via High Quality Data Curation,"['Hongyuan Dong', 'Zijian Kang', 'Weijie Yin', 'LiangXiao', 'ChaoFeng', 'Ran Jiao']",,Scalable Vision Language Model Training High Quality Data Curation
1692,GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion,"['Sunkyung Lee', 'Minjin Choi', 'Eunseong Choi', 'Hye-young Kim', 'Jongwuk Lee']",,GRAM Generative Recommendation Semantic aware Multi granular Late Fusion
1693,Towards Economical Inference: Enabling DeepSeek’s Multi-Head Latent Attention in Any Transformer-based LLMs,"['Tao Ji', 'Bin Guo', 'Yuanbin Wu', 'Qipeng Guo', 'shenlixing', 'chenzhan', 'Xipeng Qiu', 'Qi Zhang', 'Tao Gui']",,Economical Inference Enabling DeepSeek s Multi Head Latent Attention Transformer based LLMs
1694,TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding,"['Zhaoxuan Wu', 'Zijian Zhou', 'Arun Verma', 'Alok Prakash', 'Daniela Rus', 'Bryan Kian Hsiang Low']",,TETRIS Optimal Draft Token Selection Batch Speculative Decoding
1695,Introducing Verification Task of Set Consistency with Set-Consistency Energy Networks,"['Mooho Song', 'Hye Ryung Son', 'Jay-Yoon Lee']",,Introducing Verification Task Set Consistency Set Consistency Energy Networks
1696,A subtle deception beyond lying: LLMs for strategic phrasing in legislation,"['Atharvan Dogra', 'Krishna Pillutla', 'Ameet Deshpande', 'Ananya B. Sai', 'John J Nay', 'Tanmay Rajpurohit', 'Ashwin Kalyan', 'Balaraman Ravindran']",,subtle deception lying LLMs strategic phrasing legislation
1697,"AfroCS-xs: Creating a Compact, High-Quality, Human-Validated Code-Switched Dataset for African Languages","['Kayode Olaleye', 'Arturo Oncevay', 'Mathieu Sibue', 'Nombuyiselo Zondi', 'Michelle Terblanche', 'Sibongile Mapikitla', 'Richard Lastrucci', 'Charese Smiley', 'Vukosi Marivate']",,AfroCS xs Creating Compact High Quality Human Validated Code Switched Dataset African Languages
1698,Just Go Parallel: Improving the Multilingual Capabilities of Large Language Models,"['Muhammad Reza Qorib', 'Junyi Li', 'Hwee Tou Ng']",,Just Parallel Improving Multilingual Capabilities Large Language Models
1699,Design Choices for Extending the Context Length of Visual Language Models,"['Mukai Li', 'Lei Li', 'Shansan Gong', 'Qi Liu']",,Design Choices Extending Context Length Visual Language Models
